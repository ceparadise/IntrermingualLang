issue_id,issue_content
1021,1.0.24 canal hv模式下解析mysql timestamp问题 单机情况下解析mysql timestamp 数据返回是正确 实例： mysql 时间 2018-10-19 19:09:09 单机解析过来，canal获取到是 2018-10-19 19:09:09            hv模式下 解析获取到为2018-10-19 07:02:09 相差为12小时 单机和hv为同一个服务器 hv三台服务器时间均为CTC时区 时间也是正确的，有没有大佬知道是什么问题导致            
1019,canal 1.1.0 的Entry解析性能问题 这里entry.toByteString()是基于什么考虑？我们从canal 1.0.24升级到1.1.0，发现性能差了很多。 我看到这个地方，已经拿到Entry对象了，应该可以缓存起来，这样业务上就不需要再次做一次parse了。 ```     public Event(LogIdentity logIdentity  CanalEntry.Entry entry){         this.logIdentity = logIdentity;         this.entryType = entry.getEntryType();         this.executeTime = entry.getHeader().getExecuteTime();         this.journalName = entry.getHeader().getLogfileName();         this.position = entry.getHeader().getLogfileOffset();         this.serverId = entry.getHeader().getServerId();         this.gtid = entry.getHeader().getGtid();         this.eventType = entry.getHeader().getEventType();         // build raw         this.rawEntry = entry.toByteString();         this.rawLength = rawEntry.size();         if (entryType == EntryType.ROWDATA) {             List<CanalEntry.Pair> props = entry.getHeader().getPropsList();             if (props != null) {                 for (CanalEntry.Pair p : props) {                     if ("rowsCount".equals(p.getKey())) {                         rowsCount = Integer.parseInt(p.getValue());                         break;                     }                 }             }         }     } ``` 类似的还有这里，其实已经有RowChange了，但是都被序列化成bytes了。 ```             RowChange rowChange = rowChangeBuider.build();             if (tableError) {                 Entry entry = createEntry(header  EntryType.ROWDATA  ByteString.EMPTY);                 logger.warn("table parser error : {}storeValue: {}"  entry.toString()  rowChange.toString());                 return null;             } else {                 Entry entry = createEntry(header  EntryType.ROWDATA  rowChange.toByteString());                 return entry;             } ``` 我们用的是 CanalServerWithEmbedded
1018,有没有计划支持mysql 8 有没有计划对8版本的mysql数据库进行支持？
1017,1.1.0版本的canal的RDS配置属性   支持使用manager的方法么？ canalInstanceWithManager 这种形式的配置 是不是还不支持RDS新加的那3个配置
1016,每天遇到一次 should execute connector.connect() first 异常，是什么原因。 otter每天遇到一次下面的异常，看堆栈是连接中断，需要重联。 但是遇到这种错误，有时同步迅速就恢复了，有时同步会中断10分钟左右。是什么原因，如何解决？ pid:4 nid:3 exception:canal:hd_canal_236:com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first     at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4832)     at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:160)     at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:759)     at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:428)     at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:114)     at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:66)     at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:337)     at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:184)     at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:145)     at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220)     at java.lang.Thread.run(Thread.java:724) Caused by: java.io.IOException: should execute connector.connect() first     at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.(MysqlQueryExecutor.java:30)     at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:87)     at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMetaByDB(TableMetaCache.java:80)     at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:30)     at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:55)     at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:50)     at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527)     at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319)     at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282)     at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197)     at com.google.common.cache.LocalCache.get(LocalCache.java:3937)     at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941)     at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824)     at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830) 对应的otter版本是啥？
1015,求指导：canal获取的事务号(tranactionID)和mysql中的事务号(transactionID)不一致，如何解决？  1.canal的transactionID获取方法： TransactionEnd txEnd = TransactionEnd.parseFrom(entry.getStoreValue()); String txID = txEnd.getTransactionId(); 结果：46 2. mysql中查询结果： Trx id counter 88363                         
1004,支持Mysql8么 有测过支持Mysql8么 目前不支持 mysql8 那支持的mysql最高版本是多少？
997,不知道为啥报这个做，是不是因为编码？ 2018-10-11 16:23:00.584 [[scheduler-table-meta-snapshot]] ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - pls submit github issue  show create table ddl:CREATE TABLE `template_config` (   `id` bigint(50) NOT NULL   `module` varchar(100) DEFAULT '模板类型'   `title` varchar(100) DEFAULT '字段名称'   `field` varchar(100) DEFAULT '字段标记'   `type` varchar(50) DEFAULT '字段类型'   `isShow` tinyint(1) unsigned DEFAULT '0' COMMENT '业务可输入'   `isNull` tinyint(1) DEFAULT '0' COMMENT ' 是否为空'   `defaultValue` varchar(200) DEFAULT '默认值'   PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8   compare failed .   db : TableMeta [schema=ec_gome_bfs_uat  table=gome_cms_template_config  fileds=         FieldMeta [columnName=id  columnType=bigint(50)  nullable=false  key=true  defaultValue=null  extra=null  unique=false]         FieldMeta [columnName=module  columnType=varchar(100)  nullable=true  key=false  defaultValue=模板类型  extra=null  unique=false]         FieldMeta [columnName=title  columnType=varchar(100)  nullable=true  key=false  defaultValue=字段名称  extra=null  unique=false]         FieldMeta [columnName=field  columnType=varchar(100)  nullable=true  key=false  defaultValue=字段标记  extra=null  unique=false]         FieldMeta [columnName=type  columnType=varchar(50)  nullable=true  key=false  defaultValue=字段类型  extra=null  unique=false]         FieldMeta [columnName=isShow  columnType=tinyint(1) unsigned  nullable=true  key=false  defaultValue=0  extra=null  unique=false]         FieldMeta [columnName=isNull  columnType=tinyint(1)  nullable=true  key=false  defaultValue=0  extra=null  unique=false]         FieldMeta [columnName=defaultValue  columnType=varchar(200)  nullable=true  key=false  defaultValue=默认值  extra=null  unique=false] ]   mem : TableMeta [schema=ec_gome_bfs_uat  table=gome_cms_template_config  fileds=         FieldMeta [columnName=id  columnType=bigint(50)  nullable=false  key=true  defaultValue=null  extra=null  unique=false]         FieldMeta [columnName=module  columnType=varchar(100)  nullable=true  key=false  defaultValue=????  extra=null  unique=false]         FieldMeta [columnName=title  columnType=varchar(100)  nullable=true  key=false  defaultValue=????  extra=null  unique=false]         FieldMeta [columnName=field  columnType=varchar(100)  nullable=true  key=false  defaultValue=????  extra=null  unique=false]         FieldMeta [columnName=type  columnType=varchar(50)  nullable=true  key=false  defaultValue=????  extra=null  unique=false]         FieldMeta [columnName=isShow  columnType=tinyint(1) unsigned  nullable=true  key=false  defaultValue=0  extra=null  unique=false]         FieldMeta [columnName=isNull  columnType=tinyint(1)  nullable=true  key=false  defaultValue=0  extra=null  unique=false]         FieldMeta [columnName=defaultValue  columnType=varchar(200)  nullable=true  key=false  defaultValue=???  extra=null  unique=false] ] 我开启了TSDB，这个应该是数据库和内存里的数据进行比对，怀疑是编码问题。跟踪canal代码跟不到指定位置。  数据库是utf8，表是utf8.    instance.properties文件里的采集是 canal.instance.connectionCharset=UTF-8 canal.instance.tsdb.url=jdbc:mysql://xx.xx.xx.xx:40001/canal_tsdb 报错的信息里都有中文与问号 你订阅canal binlog指定的是啥编码？ 可以debug QueryLogEvent的解析过程，看看clientCharset是否有正确解析到
987,解析create schema日志后，调用不断抛异常 > 疑问1 场景：启动canal，执行create schema语句，通过 Message getWithoutAck(int batchSize)获得日志数据。当下一次再调用该方法时，抛出以下异常。 如果canal解析create schema日志前，解析了其他语句（crud）的日志，则不会出现这样的错误。 ### 异常信息栈： 2018-10-09 10:19:20.755 [main] ERROR syncLogger - !!!!增量同步异常|com.alibaba.otter.canal.protocol.exception.CanalClientException: something goes wrong with reason: something goes wrong with channel:[id: 0xcf561b9e  /192.168.1.179:50803 => /192.168.1.179:2017]  exception=java.lang.NullPointerException 	at com.alibaba.otter.canal.store.helper.CanalEventUtils.createPosition(CanalEventUtils.java:69) 	at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:375) 	at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:36) 	at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:307) 	at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:273) 	at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:124) 	at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88) 	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) 	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791) 	at org.jboss.netty.channel.SimpleChannelHandler.messageReceived(SimpleChannelHandler.java:142) 	at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88) 	at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:36) 	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) 	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791) 	at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:294) 	at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70) 	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) 	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296) 	at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:536) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:435) 	at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70) 	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) 	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255) 	at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88) 	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:109) 	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312) 	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90) 	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178) 	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) 	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) 	at java.lang.Thread.run(Thread.java:748) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.receiveMessages(SimpleCanalConnector.java:344) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:315) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:287) > 疑问2 另外，create schema语句对应的event type是Query，isDdl=false，感觉不太对。 ![image](https://user-images.githubusercontent.com/22972651/46643311-2167d100-cbae-11e8-8695-a9a2559f92b3.png) 1.  升级一下1.1.11版本 2.  create schema 语法没有识别，ddl语法主要识别了create/alter/drop table等操作
985,server配置了多个实例，某个实例报错后如何恢复 server监听了多个库，有个库由于网络问题中断了一会，后面一直连不上，报timeout。 要重新接的话是不是只能重启server，还是说可以只重启出错的实例？ ![default](https://user-images.githubusercontent.com/22339074/46599888-0e57f100-cb1b-11e8-81e4-38203ac9e231.PNG) 建议直接测试一下最新的1.1.1版本
977,同步位点问题 canal 版本：1.0.24 源实例：RDS 问题：canal server在启动之后，配置了多个intancaes，如果在订阅的过程发生了rds主备切换或者实例迁移，怎么修复？(经过试过，修改配置文件只是将intances进行reload，修改信息不会同步至zk中，那么这个时候应该怎么做)
975,数据一直在同步但数据量不对  ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x127a1b89  /5.39.221.52:60106 => 我的canal ip 每隔几秒就报一次这个错，大佬，是有人偷连我的canal吗 ，这个ip好像是荷兰的 5.39.221.52:60106 你自己不要暴露到公网上啊 -_-#
974,canal解析出的binlog文件名有乱码，导致服务重启后就再也无法定位日志位置 ``` cat  conf/example/meta.dat { 	"clientDatas": [{ 		"clientIdentity": { 			"clientId": 1001 			"destination": "example" 		} 		"cursor": { 			"identity": { 				"slaveId": -1 				"sourceAddress": { 					"address": "xxxxxxxxxxxx" 					"port": 3306 				} 			} 			"postion": { 				"gtid": "" 				"included": false 				"journalName": "mysql-bin.000227\u000F»8\u0089" 				"position": 805106264 				"serverId": 1352 				"timestamp": 1538124951000 			} 		} 	}] 	"destination": "example" } ``` 服务端和 客户端都为：1.1.0 正式版本 数据库版本：Mariadb 10.1.19 版本 可以参考下 重写CanalEntry的getLogfileName()对binlog名称进行过滤 编译install 参考地址: https://blog.csdn.net/qq_20641565/article/details/78957181 @liuhuanjs 你们这样的改法算是比较hack.  @wkjun 考虑把你当前binlog的文件导出发给我，我看一下mariadb是否在binlog格式上有一些变化 @agapple  原文件和解析文件已经分别发到你gmail 请帮忙看下。 补充一点：服务器上binlog checksum 为未启用状态。 @wkjun  DirectLogFetcherTest基于这个测试类在你的mysql上跑一下吧，单纯的binlog文件解析没问题，感觉是mariadb是否在处理RotateLogEvent存在不兼容的情况，可以debug看看，给我一些你环境里看到的数据，这样可以有针对性的修复 这是从客户端抓出来的日志 ``` ----------------> binlog[mysql-bin.000227;Õ:143563627]   name[xxx ebk_service_records]   eventType : INSERT    executeTime : 1538015914000(2018-09-27 10:38:34)    gtid : ()    delay : 3717 ms 2018-09-27 10:38:37.809  ``` 我的意思是需要debug canalserver的代码，看下RotateLogEvent的解析过程 ![tim 20181018173553](https://user-images.githubusercontent.com/8199403/47145431-5259ab80-d2fc-11e8-8842-f6219a388359.png) canal运行一段时间后乱码报错 ![tim 20181018174334](https://user-images.githubusercontent.com/8199403/47145864-5a661b00-d2fd-11e8-8c3b-8c3d01f829b6.png) [mysql-bin.zip](https://github.com/alibaba/canal/files/2491259/mysql-bin.zip) 
965,tsdb持久化存储的方案使用mysql时可以正常监听到数据，改成h2之后就监听不到数据了 版本是1.1.0 canal.properties中的h2配置 #table meta tsdb info canal.instance.tsdb.enable=true canal.instance.tsdb.dir=${canal.file.data.dir:../conf}/${canal.instance.destination:} canal.instance.tsdb.url=jdbc:h2:${canal.instance.tsdb.dir}/h2;CACHE_SIZE=1000;MODE=MYSQL; canal.instance.tsdb.dbUsername=canal canal.instance.tsdb.dbPassword=canal instance.properties 中的h2配置 #table meta tsdb info  #记录按时间序列记录表结构  canal.instance.tsdb.enable=true #将表结构数据保存在h2 嵌入式数据库中 canal.instance.tsdb.url=jdbc:h2:${canal.instance.tsdb.dir}/h2;CACHE_SIZE=1000;MODE=MYSQL; canal.instance.tsdb.dbUsername=canal canal.instance.tsdb.dbPassword=canal 打开h2 的数据库文件发现meta_history 和 meta_snapshot 中都没有数据。 ![h2](https://user-images.githubusercontent.com/10652165/45876725-0cc5c380-bdce-11e8-8d64-dd5814762051.png) 问题太笼统，关注server异常日志
956,有没有canal未来一年的roadmap 
952,canal 1.1.0 不打印数据rowdata信息  ![default](https://user-images.githubusercontent.com/16894071/45675324-b3595c80-bb61-11e8-9d31-103f0386ba22.png) 类信息参考 https://github.com/alibaba/canal/blob/master/example/src/main/java/com/alibaba/otter/canal/example/AbstractCanalClientTest.java 写的  ` protected void printEntry(List<Entry> entrys) {         for (Entry entry : entrys) {             long executeTime = entry.getHeader().getExecuteTime();             long delayTime = System.currentTimeMillis() - executeTime;             Date date = new Date(entry.getHeader().getExecuteTime());             SimpleDateFormat simpleDateFormat = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");             if (entry.getEntryType() == EntryType.TRANSACTIONBEGIN || entry.getEntryType() == EntryType.TRANSACTIONEND) {                 if (entry.getEntryType() == EntryType.TRANSACTIONBEGIN) {                     TransactionBegin begin = null;                     try {                         begin = TransactionBegin.parseFrom(entry.getStoreValue());                     } catch (InvalidProtocolBufferException e) {                         throw new RuntimeException("parse event has an error   data:" + entry.toString()  e);                     }                     // 打印事务头信息，执行的线程id，事务耗时                     logger.info(transaction_format                             new Object[] { entry.getHeader().getLogfileName()                                     String.valueOf(entry.getHeader().getLogfileOffset())                                     String.valueOf(entry.getHeader().getExecuteTime())  simpleDateFormat.format(date)                                     entry.getHeader().getGtid()  String.valueOf(delayTime) });                     logger.info(" BEGIN ----> Thread id: {}"  begin.getThreadId());                     printXAInfo(begin.getPropsList());                 } else if (entry.getEntryType() == EntryType.TRANSACTIONEND) {                     TransactionEnd end = null;                     try {                         end = TransactionEnd.parseFrom(entry.getStoreValue());                     } catch (InvalidProtocolBufferException e) {                         throw new RuntimeException("parse event has an error   data:" + entry.toString()  e);                     }                     // 打印事务提交信息，事务id                     logger.info("----------------\n");                     logger.info(" END ----> transaction id: {}"  end.getTransactionId());                     printXAInfo(end.getPropsList());                     logger.info(transaction_format                             new Object[] { entry.getHeader().getLogfileName()                                     String.valueOf(entry.getHeader().getLogfileOffset())                                     String.valueOf(entry.getHeader().getExecuteTime())  simpleDateFormat.format(date)                                     entry.getHeader().getGtid()  String.valueOf(delayTime) });                 }                 continue;             }             if (entry.getEntryType() == EntryType.ROWDATA) {                 RowChange rowChage = null;                 try {                     rowChage = RowChange.parseFrom(entry.getStoreValue());                 } catch (Exception e) {                     throw new RuntimeException("parse event has an error   data:" + entry.toString()  e);                 }                 EventType eventType = rowChage.getEventType();                 String tableName = entry.getHeader().getTableName();                 for (CanalEntry.RowData rowData : rowChage.getRowDatasList()) {                     Map map = new HashMap<>();                     map.put("tableName" tableName);                     map.put("eventType" eventType);                     map.put("eventDate"  org.apache.http.client.utils.DateUtils.formatDate(new Date() "yyyy-MM-dd"));                     if (eventType == CanalEntry.EventType.DELETE) {                         printColumn(map rowData.getBeforeColumnsList());                         //发送删除命令                     } else if (eventType == CanalEntry.EventType.INSERT) {                         //发送插入命令                         printColumn(map rowData.getAfterColumnsList());                     } else {                         //发送修改命令                         printColumn(map rowData.getAfterColumnsList());                     }                 }                 if (eventType == EventType.QUERY || rowChage.getIsDdl()) {                     logger.info(" sql ----> " + rowChage.getSql() + SEP);                     continue;                 } //                printXAInfo(rowChage.getPropsList());             }         }     } ` 检查过滤条件
946,解决在数据库重装或者在数据库中执行了reset master导致binlog后缀变成000001后 自动切换的问题. 投产中发现，当canal连接的数据库被重新安装或者执行reset master 后，canal会一致通过zk过去位置，然后去mysql中寻找这个位置的点，但是mysql已经不存在这个点了，于是一致循环报错，除非人为干预。 对于该问题，添加了一个位置解析错误的异常，在发生异常时，做了一个次数的判断，默认是5次，如果连续5次都是位置解析错误，那么重新设置新位置：mysql-bin.000001的154offset处，开始同步数据。可以自动切换。 默认改功能不开启。 [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=946) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=946) before we can accept your contribution.<br/><hr/>**lulin** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=946) it.</sub> 直接回退到000001文件，有点不太合理吧，找不到位点很多的业务情况都是binlog purge而已 数据库都重装或者reset master mysql binllog 会从000001开始的，这样做好处是保证canal的连续性。减少人为的参与。 @mycat-lulin 其实问题在于程序无法确定是什么原因导致的位点找不到的异常，所有导致位点找不到的异常全部都回退到初始，这个不合理.同时，数据库重装这种操作，重置一下位点这种操作还是需要的吧. @wingerx 找不到点位的原因就是寻找的binlog的文件不存在和位置错轮，位置错乱的可能性不大，mysql会保证这块东西。
934,如何设置一张表到一个topic的指定partition 这个把这个表名作为message的key就行了吧。
926,在版本1.1.0中，默认开启了tsdb，报错信息如下 2018-09-11 11:10:08.724 [destination = example   address = /127.0.0.1:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: apply failed Caused by: org.springframework.jdbc.BadSqlGrammarException: SqlMapClient operation; bad SQL grammar []; nested exception is com.ibatis.common.jdbc.exception.NestedSQLException: --- The error occurred in spring/tsdb/sql-map/sqlmap_history.xml. --- The error occurred while executing query. --- Check the          select                     a.id as id a.gmt_create as gmtCreate a.gmt_modified as gmtModified    a.destination as destination a.binlog_file as binlogFile a.binlog_offest as binlogOffest a.binlog_master_id as binlogMasterId a.binlog_timestamp as binlogTimestamp    a.use_schema as useSchema a.sql_schema as sqlSchema a.sql_table as sqlTable a.sql_text as sqlText a.sql_type as sqlType a.extra as extra                       from meta_history a         where destination = ? and binlog_timestamp >= ? and binlog_timestamp <= ?         order by binlog_timestamp asc id asc              . --- Check the SQL Statement (preparation failed). --- Cause: org.h2.jdbc.JdbcSQLException: Table "META_HISTORY" not found; SQL statement:          select                     a.id as id a.gmt_create as gmtCreate a.gmt_modified as gmtModified    a.destination as destination a.binlog_file as binlogFile a.binlog_offest as binlogOffest a.binlog_master_id as binlogMasterId a.binlog_timestamp as binlogTimestamp    a.use_schema as useSchema a.sql_schema as sqlSchema a.sql_table as sqlTable a.sql_text as sqlText a.sql_type as sqlType a.extra as extra                       from meta_history a                  where destination = ? and binlog_timestamp >= ? and binlog_timestamp <= ?         order by binlog_timestamp asc id asc               [42102-196]         at org.springframework.jdbc.support.SQLErrorCodeSQLExceptionTranslator.doTranslate(SQLErrorCodeSQLExceptionTranslator.java:237)         at org.springframework.jdbc.support.AbstractFallbackSQLExceptionTranslator.translate(AbstractFallbackSQLExceptionTranslator.java:72)         at org.springframework.orm.ibatis.SqlMapClientTemplate.execute(SqlMapClientTemplate.java:206)         at org.springframework.orm.ibatis.SqlMapClientTemplate.queryForList(SqlMapClientTemplate.java:296)         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.dao.MetaHistoryDAO.findByTimestamp(MetaHistoryDAO.java:28)         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.applyHistoryOnMemory(DatabaseTableMeta.java:367)         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:123)         at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:91)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:188)         at java.lang.Thread.run(Thread.java:748) Caused by: com.ibatis.common.jdbc.exception.NestedSQLException: --- The error occurred in spring/tsdb/sql-map/sqlmap_history.xml. --- The error occurred while executing query. --- Check the          select                     a.id as id a.gmt_create as gmtCreate a.gmt_modified as gmtModified    a.destination as destination a.binlog_file as binlogFile a.binlog_offest as binlogOffest a.binlog_master_id as binlogMasterId a.binlog_timestamp as binlogTimestamp    a.use_schema as useSchema a.sql_schema as sqlSchema a.sql_table as sqlTable a.sql_text as sqlText a.sql_type as sqlType a.extra as extra                       from meta_history a                  where destination = ? and binlog_timestamp >= ? and binlog_timestamp <= ?         order by binlog_timestamp asc id asc              . --- Check the SQL Statement (preparation failed). --- Cause: org.h2.jdbc.JdbcSQLException: Table "META_HISTORY" not found; SQL statement:          select                     a.id as id a.gmt_create as gmtCreate a.gmt_modified as gmtModified    a.destination as destination a.binlog_file as binlogFile a.binlog_offest as binlogOffest a.binlog_master_id as binlogMasterId a.binlog_timestamp as binlogTimestamp    a.use_schema as useSchema a.sql_schema as sqlSchema a.sql_table as sqlTable a.sql_text as sqlText a.sql_type as sqlType a.extra as extra                       from meta_history a                  where destination = ? and binlog_timestamp >= ? and binlog_timestamp <= ?         order by binlog_timestamp asc id asc               [42102-196]         at com.ibatis.sqlmap.engine.mapping.statement.MappedStatement.executeQueryWithCallback(MappedStatement.java:201)         at com.ibatis.sqlmap.engine.mapping.statement.MappedStatement.executeQueryForList(MappedStatement.java:139)         at com.ibatis.sqlmap.engine.impl.SqlMapExecutorDelegate.queryForList(SqlMapExecutorDelegate.java:567)         at com.ibatis.sqlmap.engine.impl.SqlMapExecutorDelegate.queryForList(SqlMapExecutorDelegate.java:541)         at com.ibatis.sqlmap.engine.impl.SqlMapSessionImpl.queryForList(SqlMapSessionImpl.java:118)         at org.springframework.orm.ibatis.SqlMapClientTemplate$3.doInSqlMapClient(SqlMapClientTemplate.java:298)         at org.springframework.orm.ibatis.SqlMapClientTemplate$3.doInSqlMapClient(SqlMapClientTemplate.java:296)         at org.springframework.orm.ibatis.SqlMapClientTemplate.execute(SqlMapClientTemplate.java:203)         ... 7 more Caused by: org.h2.jdbc.JdbcSQLException: Table "META_HISTORY" not found; SQL statement:          select                     a.id as id a.gmt_create as gmtCreate a.gmt_modified as gmtModified    a.destination as destination a.binlog_file as binlogFile a.binlog_offest as binlogOffest a.binlog_master_id as binlogMasterId a.binlog_timestamp as binlogTimestamp    a.use_schema as useSchema a.sql_schema as sqlSchema a.sql_table as sqlTable a.sql_text as sqlText a.sql_type as sqlType a.extra as extra                       from meta_history a                  where destination = ? and binlog_timestamp >= ? and binlog_timestamp <= ?         order by binlog_timestamp asc id asc               [42102-196]         at org.h2.message.DbException.getJdbcSQLException(DbException.java:345)         at org.h2.message.DbException.get(DbException.java:179)         at org.h2.message.DbException.get(DbException.java:155)         at org.h2.command.Parser.readTableOrView(Parser.java:5552)         at org.h2.command.Parser.readTableFilter(Parser.java:1266)         at org.h2.command.Parser.parseSelectSimpleFromPart(Parser.java:1946)         at org.h2.command.Parser.parseSelectSimple(Parser.java:2095)         at org.h2.command.Parser.parseSelectSub(Parser.java:1940)         at org.h2.command.Parser.parseSelectUnion(Parser.java:1755)         at org.h2.command.Parser.parseSelect(Parser.java:1743)         at org.h2.command.Parser.parsePrepared(Parser.java:449)         at org.h2.command.Parser.parse(Parser.java:321)         at org.h2.command.Parser.parse(Parser.java:293)         at org.h2.command.Parser.prepareCommand(Parser.java:258)         at org.h2.engine.Session.prepareLocal(Session.java:578)         at org.h2.engine.Session.prepareCommand(Session.java:519)         at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1204)         at org.h2.jdbc.JdbcPreparedStatement.<init>(JdbcPreparedStatement.java:73)         at org.h2.jdbc.JdbcConnection.prepareStatement(JdbcConnection.java:288)         at com.alibaba.druid.pool.DruidPooledConnection.prepareStatement(DruidPooledConnection.java:349)         at com.ibatis.sqlmap.engine.execution.SqlExecutor.prepareStatement(SqlExecutor.java:497)         at com.ibatis.sqlmap.engine.execution.SqlExecutor.executeQuery(SqlExecutor.java:175)         at com.ibatis.sqlmap.engine.mapping.statement.MappedStatement.sqlExecuteQuery(MappedStatement.java:221)         at com.ibatis.sqlmap.engine.mapping.statement.MappedStatement.executeQueryWithCallback(MappedStatement.java:189)  Table "META_HISTORY" not found   conf下有一些初始化的ddl文件，检查一下 @agapple 在canal-1.1.0/conf/spring/tsdb/sql/create_table.sql 有这个文件 尝试debug看一下吧  为啥initTable的操作没有生效  MetaHistoryDAO里的initTable 好的，谢谢 @agapple 
923,怎么解决宿主机访问docker 绑定的ip啊 使用docker 模式时，canal向zookeeper注册的ip是docker容器的网络的ip：172.17.0.3 比如docker里的canal-server绑定的地址是/172.17.0.3:11111 那么我的客户端访问不了的啊 com.alibaba.otter.canal.protocol.exception.CanalClientException: java.net.ConnectException: Connection timed out: connect 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.doConnect(SimpleCanalConnector.java:171) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.access$000(SimpleCanalConnector.java:48) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector$1.processActiveEnter(SimpleCanalConnector.java:396) 	at com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor.processActiveEnter(ClientRunningMonitor.java:207) 	at com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor.initRunning(ClientRunningMonitor.java:118) 	at com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor.start(ClientRunningMonitor.java:92) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.connect(SimpleCanalConnector.java:93) 	at com.alibaba.otter.canal.client.impl.ClusterCanalConnector.connect(ClusterCanalConnector.java:63) 	at com.fcbox.canal.scheduling.SchedulerTask.run(SchedulerTask.java:39) Caused by: java.net.ConnectException: Connection timed out: connect 	at sun.nio.ch.Net.connect0(Native Method) 	at sun.nio.ch.Net.connect(Net.java:454) 	at sun.nio.ch.Net.connect(Net.java:446) 	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.doConnect(SimpleCanalConnector.java:132) 	... 8 common frames omitted 2018-09-10 11:03:05 454 WARN (ClusterCanalConnector.java:66)- failed to connect to:**### /172.17.0.3:11111** after retry 0 times 而且canal.ip也不能直接写成宿主机的ip，会报错的。 可以考虑docker的host模式
911,最新canal.kafka-1.1.0.tar部署无法启动，调试源码也是同样的异常，求关注 2018-09-04 16:10:09.223 [main] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2018-09-04 16:10:09.226 [main] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties] 2018-09-04 16:10:09.400 [main] WARN  o.s.beans.GenericTypeAwarePropertyDescriptor - Invalid JavaBean property 'connectionCharset' being accessed! Ambiguous write methods found next to actually use d [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.lang.String)]: [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlE ventParser.setConnectionCharset(java.nio.charset.Charset)] 2018-09-04 16:10:09.435 [main] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2018-09-04 16:10:09.435 [main] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties] 2018-09-04 16:10:09.667 [main] ERROR com.alibaba.druid.pool.DruidDataSource - testWhileIdle is true  validationQuery not set 2018-09-04 16:10:09.891 [main] INFO  c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-example 2018-09-04 16:10:09.905 [main] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - start successful.... 2018-09-04 16:10:09.948 [destination = example   address = /192.168.200.30:3306   EventParser] WARN  c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - prepare to find start position just las t position  {"identity":{"slaveId":-1 "sourceAddress":{"address":"mySlave" "port":3306}} "postion":{"gtid":"" "included":false "journalName":"mysql-bin.000029" "position":132895982 "serverId":2 "timestamp":1 536044055000}} 2018-09-04 16:10:09.993 [Thread-7] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - stop CannalInstance for null-example 2018-09-04 16:10:09.996 [destination = example   address = /192.168.200.30:3306   EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - parse events has an error com.alibaba.otter.canal.parse.exception.CanalParseException: dump address /192.168.200.30:3306 has an error  retrying. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: should execute connector.connect() first Caused by: java.io.IOException: should execute connector.connect() first         at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.<init>(MysqlQueryExecutor.java:30) ~[canal.parse.driver-1.1.0.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.queryMulti(MysqlConnection.java:106) ~[canal.parse-1.1.0.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:175) ~[canal.parse-1.1.0.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:129) ~[canal.parse-1.1.0.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:91) ~[canal.parse-1.1.0.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:188) ~[canal.parse-1.1.0.jar:na]         at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_171] 2018-09-04 16:10:09.997 [Thread-7] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - stop successful.... 在我们环境里无法重试，你尝试debug看一下MysqlConnection什么条件下被disconnect
909,[v1.1.0]canal如何在Zookeeper中处理云服务器IP问题（大小网IP问题）？ canal在Zookeeper中注册云服务器的内网IP，如192.168.4.x，而对应大网服务器地址是其他IP，如10.100.x.x 这样导致canal以及Zookeeper、client必须部署到同一云服务器小网段中，无法使用大网段进行访问 @agapple  这种特殊网络没在设计考虑的范畴 @agapple 能否计划支持，Zuul也有类似情况进行网络段位过滤机制 每个canal server自己人肉设置对应的ip? 你可以考虑提交一个pr给我
907,example里面的报错 2018-09-04 11:39:04.244 [WrapperSimpleAppMain] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2018-09-04 11:39:04.244 [WrapperSimpleAppMain] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties] 2018-09-04 11:39:04.400 [WrapperSimpleAppMain] WARN  o.s.beans.GenericTypeAwarePropertyDescriptor - Invalid JavaBean property 'connectionCharset' being accessed! Ambiguous write methods found next to actually used [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.nio.charset.Charset)]: [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.lang.String)] 2018-09-04 11:39:04.400 [WrapperSimpleAppMain] INFO  c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-example  2018-09-04 11:39:04.416 [WrapperSimpleAppMain] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - start successful.... 2018-09-04 11:39:04.416 [canal-instance-scan-0] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - stop CannalInstance for null-example  2018-09-04 11:39:04.432 [destination = example   address = /127.0.0.1:3308   EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - parse events has an error com.alibaba.otter.canal.parse.exception.CanalParseException: dump address /127.0.0.1:3308 has an error  retrying.  Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: java.net.SocketException: Socket Closed Caused by: java.net.SocketException: Socket Closed 	at java.net.SocketInputStream.socketRead0(Native Method) ~[na:1.8.0_77] 	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116) ~[na:1.8.0_77] 	at java.net.SocketInputStream.read(SocketInputStream.java:170) ~[na:1.8.0_77] 	at java.net.SocketInputStream.read(SocketInputStream.java:141) ~[na:1.8.0_77] 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246) ~[na:1.8.0_77] 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286) ~[na:1.8.0_77] 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345) ~[na:1.8.0_77] 	at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.read(BioSocketChannel.java:52) ~[canal.parse.driver-1.1.0.jar:na] 	at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.readHeader(PacketManager.java:12) ~[canal.parse.driver-1.1.0.jar:na] 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.readNextPacket(MysqlQueryExecutor.java:159) ~[canal.parse.driver-1.1.0.jar:na] 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.query(MysqlQueryExecutor.java:56) ~[canal.parse.driver-1.1.0.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:102) ~[canal.parse-1.1.0.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.loadBinlogFormat(MysqlConnection.java:433) ~[canal.parse-1.1.0.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.getBinlogFormat(MysqlConnection.java:582) ~[canal.parse-1.1.0.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:95) ~[canal.parse-1.1.0.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) ~[canal.parse-1.1.0.jar:na] 	at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_77] 2018-09-04 11:39:04.432 [canal-instance-scan-0] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - stop successful.... Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: java.net.SocketException: Socket Closed Caused by: java.net.SocketException: Socket Closed 感觉是主动关闭
903,Could not find first log file name in binary log index file 错误信息： 2018-09-03 17:06:05.707 [main] INFO  c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-example 2018-09-03 17:06:05.709 [main] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - start successful.... 2018-09-03 17:06:05.781 [destination = example   address = xxxxx:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position  {"identity":{"slaveId":-1 "sourceAddress":{"address":"xxxx" "port":3306}} "postion":{"included":false "journalName":"mysql-bin.002736" "position":30948432 "serverId":2118896143 "timestamp":1535632015000}} 2018-09-03 17:06:05.886 [destination = example   address = xxxx:3306   EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Could not find first log file name in binary log index file         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) ~[canal.parse-1.0.24.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122) [canal.parse-1.0.24.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) [canal.parse-1.0.24.jar:na]         at java.lang.Thread.run(Thread.java:748) [na:1.7.0_141] 2018-09-03 17:06:05.887 [destination = example   address = xxxx:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address xxxxx:3306 has an error  retrying. caused by java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Could not find first log file name in binary log index file         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) ~[canal.parse-1.0.24.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122) ~[canal.parse-1.0.24.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) ~[canal.parse-1.0.24.jar:na]         at java.lang.Thread.run(Thread.java:748) [na:1.7.0_141] 2018-09-03 17:06:05.893 [destination = example   address = xxxx:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Could not find first log file name in binary log index file         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209)         at java.lang.Thread.run(Thread.java:748) ] 问题应该是meta.dat中的binlog文件跟不上show master STATUS中的日志文件。 meta.dat中的文件是mysql-bin.002736: {"clientDatas":[{"clientIdentity":{"clientId":1001 "destination":"example" "filter":""} "cursor":{"identity":{"slaveId":-1 "sourceAddress":{"address":"":"xxxx" "p" "port":3306}} "postion":{"included":false "journalName":"mysql-bin.002736" "position":30948432 "serverId":2118896143 "timestamp":1535632015000}}}] "destination":"example"} show master STATUS最新的文件; mysql-bin.002752	24891270	 之前的做法是直接删除meta.dat文件，重新开始同步，但是会丢失数据。运行一段时间后又会出现该问题。请问大佬们怎么解决。 mysql master会基于一定的策略去sweep binlog(根据size或时间)，如果消费的慢，meta.dat里的log就被清理了。 这种情况考虑下提升整体吞吐量吧。 2018-09-04 09:35:33.645 [destination = example   address = xxx:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: column size is not match for table:`skstandard`.`tbl_20180824090548lyqbbcqqjdt68u5m` 103 vs 15 应该是由于表结构改变导致服务一直卡在这里，请问有什么好的方法可以跳过去吗，针对这种表结构改变的情况怎么处理的呢？我现在是手动修改meta.dat中的position跳过的。 升级新版本吧 https://github.com/alibaba/canal/wiki/TableMetaTSDB 大佬，你这个column size is not match for table问题是怎么解决的 望赐教！ 本地使用的是1.1.0版本，但是还会出现column size is not match for table，望大佬赐教解决办法
899,数据消费一段后，canal-client无法拉取新的数据，确定db是在不断更新的。 ![image](https://user-images.githubusercontent.com/5965173/44947782-0e424280-ae45-11e8-973a-b2c102d7d704.png) 使用最新版本。1.10 检查一下server端是否有报错 ![image](https://user-images.githubusercontent.com/5965173/45035342-57d09e80-b08c-11e8-9bf8-bba18de5ae43.png) server端，出现一个异常后，看到client自动重新subscribe的信息。然后client 就读不到数据了。 又一次，出现异常后，消费停止了。 ![image](https://user-images.githubusercontent.com/5965173/45036036-fad5e800-b08d-11e8-8b33-a956e293438d.png) mysql版本 5.6.28 getWithoutAck 使用无需确认的模式，出现上面问题。 使用get Ack的模式就没有问题了。 先考虑升级一下canal版本
897,canal-1.1.0源码中canal-kafka入口启动一直报异常。 2018-08-31 09:05:51.141 [destination = example   address = /192.168.200.42:3306   EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address /192.168.200.42:3306 has an error  retrying. caused by  com.alibaba.otter.canal.parse.exception.CanalParseException: java.net.SocketException: Socket closed Caused by: java.net.SocketException: Socket closed 	at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:116) ~[na:1.8.0_20] 	at java.net.SocketOutputStream.write(SocketOutputStream.java:141) ~[na:1.8.0_20] 	at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.write(BioSocketChannel.java:36) ~[classes/:na] 	at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.writeBody0(PacketManager.java:42) ~[classes/:na] 	at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.writeBody(PacketManager.java:35) ~[classes/:na] 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.query(MysqlQueryExecutor.java:55) ~[classes/:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:102) ~[classes/:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.loadBinlogFormat(MysqlConnection.java:433) ~[classes/:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.getBinlogFormat(MysqlConnection.java:582) ~[classes/:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:95) ~[classes/:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) ~[classes/:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_20] 2018-08-31 09:05:51.146 [destination = example   address = /192.168.200.42:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: java.net.SocketException: Socket closed Caused by: java.net.SocketException: Socket closed 	at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:116) 	at java.net.SocketOutputStream.write(SocketOutputStream.java:141) 	at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.write(BioSocketChannel.java:36) 	at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.writeBody0(PacketManager.java:42) 	at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.writeBody(PacketManager.java:35) 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.query(MysqlQueryExecutor.java:55) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:102) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.loadBinlogFormat(MysqlConnection.java:433) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.getBinlogFormat(MysqlConnection.java:582) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:95) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) 	at java.lang.Thread.run(Thread.java:745) ] mysql连接没有问题，同样的mysql配置用canal入口启动没有问题
895,canal 到kafka 数据一致性的问题 如果我设置多个partition，canal如何保证数据的一致性，有序性 无论kafka还是rocketmq都只能是单分区全局有序，或单分区有序。 如果表之间要没有强关联，是可以单个表hash到同一个分区保证单表有序的 好像听说canal的项目作者，可能正在实现多个分区，一个表，并保证顺序。不知道是否是真的（理论上，我觉得，如果有张索引表--按时间排序，先读取索引表，再读真正的数据，就可以实现读取放在多个分区上的binglog数据了）
891,canal.deployer-1.1.0 cursor持久化频率问题 测试1：client重连时，meta.dat的修改时间发生更新，但数据还是没变。 测试2：由于测试1的问题，导致position没有更新。导致如果此时server重启，读取的还是“旧”的持久化cursor，client在“旧”的position消费。 issue canal版本：1.0.26 alpha5，1.1.0 1.0.26 alpha2不存在这个问题：每次client消费都更新curosr
887,canal 运行发生ack错误 canal运行发生ack错误： ack error   clientId:1001 batchId:261365 is not exist   please check。 这个应该是服务器端instance重启导致。现在服务器端的配置为： canal.instance.get.ddl.isolation = false ################################################# #########               destinations            ############# ################################################# canal.destinations= db9002 # conf root dir canal.conf.dir = ../conf # auto scan instance dir add/remove and start/stop instance canal.auto.scan = false canal.auto.scan.interval = 5 canal.instance.global.mode = spring canal.instance.global.lazy = false 请问instance为什么会发生重启？ 怎样能控制服务端的instance不发生重启？ 
885,关于kafka topic设置 请问目前是否支持对应每张mysql表设置一个对应的kafka topic？ 初步看代码，目前似乎只支持对应一个destination设置多个topic，数据会发往设置的所有topic。
884,通过canal.instance.filter.regex= 过滤 和通过java程序connector.subscribe(" ")有什么不同的吗？ 实际使用中，只让canal监听一个库中的某几张表，怎么配性能会更好一点，望大佬指教
880,aliyun  rds 日志解析失败 Could not find first log file name in binary log index file 不是说全面支持rds了吗，为啥还有这个错，canal.instance.rds.accesskey 那几个参数也配了，但是貌似没起作用啊 这是binlog被删除，清理掉offest，使用binlog时间戳来定位
879,为什么我指定里的一张表，但是从kafka里面就看不到schema名和表名 我设置了这个 canal.instance.filter.regex=db_shopdog_test.deli_order 但是kafka里面解析不到schema名和表名
875,canal.log启动canal报错 2018-08-24 06:25:04.409 [New I/O server worker #1-1] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x599715d9  /192.168.254.1:57576 => /192.168.254.128:11111]  exception=java.io.IOException: Connection reset by peer         at sun.nio.ch.FileDispatcherImpl.read0(Native Method)         at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)         at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)         at sun.nio.ch.IOUtil.read(IOUtil.java:192)         at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)         at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:322)         at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281)         at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201)         at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46)         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)         at java.lang.Thread.run(Thread.java:745) 望大佬们指点。 升级到最新的1.1.0
869,canal-kafka-1.0.26-preview-4: CanalKafkaStarter  CPU占用率高 CanalKafkaStarter.worker 线程中调用getWithoutAck未使用timeout参数。 ``` "pool-8-thread-1" #21 prio=5 os_prio=0 tid=0x00007facb47b1000 nid=0x5fe runnable [0x00007fac8d16b000]    java.lang.Thread.State: RUNNABLE         at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.isStart(CanalServerWithEmbedded.java:126)         at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.checkStart(CanalServerWithEmbedded.java:484)         at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:279)         at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:259)         at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.worker(CanalKafkaStarter.java:118)         at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.access$000(CanalKafkaStarter.java:26)         at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter$1.run(CanalKafkaStarter.java:67)         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)         at java.lang.Thread.run(Thread.java:748) ```
868,canal-kafka-1.1.0: 无限循环CanalKafkaStarter - process error! kafka: n1:9092 n2:9092 zk: n3:2181 n4:2181 n5:2181 canal: n1 错误日志： ``` 2018-08-23 16:25:08.274 [destination = example   address = n1/192.168.4.11:3306   EventParser] WARN  c.a.o.c.p.inboun d.mysql.rds.RdsBinlogEventParserProxy - find start position : EntryPosition[included=false journalName=mysql-bin.0000 01 position=3024 serverId=<null> gtid=<null> timestamp=<null>] 2018-08-23 16:25:08.299 [main] INFO  com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - ## start the kafka wo rkers. 2018-08-23 16:25:08.300 [main] INFO  com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - ## the kafka workers  is running now ...... 2018-08-23 16:25:08.301 [pool-4-thread-1] INFO  com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - ## start t he canal consumer: example. 2018-08-23 16:25:08.305 [pool-4-thread-1] INFO  com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - ## the can al consumer example is running now ...... 2018-08-23 16:25:15.105 [pool-4-thread-1] ERROR com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - process er ror! java.lang.NullPointerException: null         at com.alibaba.otter.canal.store.helper.CanalEventUtils.createPosition(CanalEventUtils.java:69) ~[canal.store -1.1.0.jar:na]         at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffe r.java:375) ~[canal.store-1.1.0.jar:na]         at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffe r.java:36) ~[canal.store-1.1.0.jar:na]         at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java :307) ~[canal.server-1.1.0.jar:na]         at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java :273) ~[canal.server-1.1.0.jar:na]         at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.worker(CanalKafkaStarter.java:121) [canal.kafka-1 .1.0.jar:na]         at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.access$000(CanalKafkaStarter.java:26) [canal.kafk a-1.1.0.jar:na]         at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter$1.run(CanalKafkaStarter.java:70) [canal.kafka-1.1 .0.jar:na]         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_161]         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_161]         at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-08-23 16:25:15.106 [pool-4-thread-1] ERROR com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - process er ror! java.lang.NullPointerException: null         at com.alibaba.otter.canal.store.helper.CanalEventUtils.createPosition(CanalEventUtils.java:69) ~[canal.store -1.1.0.jar:na]         at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffe : ``` 配置： canal.properties ```################################################# ######### 		common argument		#############  ################################################# canal.id= 1 canal.ip= canal.port=11111 canal.metrics.pull.port=11112 canal.zkServers=n3:2181 n4:2181 n5:2181 # flush data to zk canal.zookeeper.flush.period = 1000 canal.withoutNetty = true # flush meta cursor/parse position to file canal.file.data.dir = ${canal.conf.dir} canal.file.flush.period = 1000 ## memory store RingBuffer size  should be Math.pow(2 n) canal.instance.memory.buffer.size = 16384 ## memory store RingBuffer used memory unit size   default 1kb canal.instance.memory.buffer.memunit = 1024  ## meory store gets mode used MEMSIZE or ITEMSIZE canal.instance.memory.batch.mode = MEMSIZE ## detecing config canal.instance.detecting.enable = false #canal.instance.detecting.sql = insert into retl.xdual values(1 now()) on duplicate key update x=now() canal.instance.detecting.sql = select 1 canal.instance.detecting.interval.time = 3 canal.instance.detecting.retry.threshold = 3 canal.instance.detecting.heartbeatHaEnable = false # support maximum transaction size  more than the size of the transaction will be cut into multiple transactions delivery canal.instance.transaction.size =  1024 # mysql fallback connected to new master should fallback times canal.instance.fallbackIntervalInSeconds = 60 # network config canal.instance.network.receiveBufferSize = 16384 canal.instance.network.sendBufferSize = 16384 canal.instance.network.soTimeout = 30 # binlog filter config canal.instance.filter.druid.ddl = true canal.instance.filter.query.dcl = true canal.instance.filter.query.dml = false canal.instance.filter.query.ddl = true canal.instance.filter.table.error = false canal.instance.filter.rows = false canal.instance.filter.transaction.entry = true # binlog format/image check canal.instance.binlog.format = ROW STATEMENT MIXED  canal.instance.binlog.image = FULL MINIMAL NOBLOB # binlog ddl isolation canal.instance.get.ddl.isolation = false # parallel parser config canal.instance.parser.parallel = true ## concurrent thread number  default 60% available processors  suggest not to exceed Runtime.getRuntime().availableProcessors() #canal.instance.parser.parallelThreadSize = 16 ## disruptor ringbuffer size  must be power of 2 canal.instance.parser.parallelBufferSize = 256 # table meta tsdb info canal.instance.tsdb.enable=true canal.instance.tsdb.dir=${canal.file.data.dir:../conf}/${canal.instance.destination:} canal.instance.tsdb.url=jdbc:h2:${canal.instance.tsdb.dir}/h2;CACHE_SIZE=1000;MODE=MYSQL; canal.instance.tsdb.dbUsername=canal canal.instance.tsdb.dbPassword=canal # rds oss binlog account canal.instance.rds.accesskey = canal.instance.rds.secretkey = ################################################# ######### 		destinations		#############  ################################################# canal.destinations= example # conf root dir canal.conf.dir = ../conf # auto scan instance dir add/remove and start/stop instance canal.auto.scan = true canal.auto.scan.interval = 5 canal.instance.tsdb.spring.xml=classpath:spring/tsdb/h2-tsdb.xml #canal.instance.tsdb.spring.xml=classpath:spring/tsdb/mysql-tsdb.xml canal.instance.global.mode = spring  canal.instance.global.lazy = false #canal.instance.global.manager.address = 127.0.0.1:1099 #canal.instance.global.spring.xml = classpath:spring/memory-instance.xml #canal.instance.global.spring.xml = classpath:spring/file-instance.xml canal.instance.global.spring.xml = classpath:spring/default-instance.xml ``` kafka.yml ```servers: n1:9092 n2:9092 retries: 0 batchSize: 16384 lingerMs: 1 bufferMemory: 33554432 # canal的批次大小，单位 k，量大建议改为1M canalBatchSize: 50 filterTransactionEntry: true canalDestinations:   - canalDestination: example     topic: example     partition: ``` example/instance.properties ```################################################# ## mysql serverId   v1.0.26+ will autoGen  # canal.instance.mysql.slaveId=0 # enable gtid use true/false canal.instance.gtidon=false # position info canal.instance.master.address=n1:3306 canal.instance.master.journal.name= canal.instance.master.position= canal.instance.master.timestamp= canal.instance.master.gtid= # rds oss binlog canal.instance.rds.accesskey= canal.instance.rds.secretkey= canal.instance.rds.instanceId= # table meta tsdb info canal.instance.tsdb.enable=false #canal.instance.tsdb.url=jdbc:mysql://127.0.0.1:3306/canal_tsdb #canal.instance.tsdb.dbUsername=canal #canal.instance.tsdb.dbPassword=canal #canal.instance.standby.address = #canal.instance.standby.journal.name = #canal.instance.standby.position =  #canal.instance.standby.timestamp = #canal.instance.standby.gtid= # username/password canal.instance.dbUsername=root canal.instance.dbPassword=root canal.instance.connectionCharset=UTF-8 # table regex canal.instance.filter.regex=test\\..* # table black regex canal.instance.filter.black.regex= ################################################# ``` 第一次insert into table 成功解析binlog，然后就出现这个问题 使用官方canal的example里的SimpleCanalClientTest读取。第一次插入的数据能解析，之后的就报错，日志如下： 一直在换端口？ ``` **************************************************** * Batch Id: [1]  count : [1]   memsize : [55]   Time : 2018-08-25 15:56:55 * Start : [mysql-bin.000005:642:1535212613000(2018-08-25 23:56:53)]  * End : [mysql-bin.000005:642:1535212613000(2018-08-25 23:56:53)]  **************************************************** ----------------> binlog[mysql-bin.000005:642]   name[test ar_tmp]   eventType : INSERT   executeTime : 1535212613000(2018-08-25 23:56:53)   gtid : ()   delay : -28797618 ms id : 1    type=int(11)    update=true name : a    type=varchar(32)    update=true price : 1.1    type=double    update=true time : 2018-08-25 23:56:53    type=datetime    update=true process error!com.alibaba.otter.canal.protocol.exception.CanalClientException: something goes wrong with reason: something goes wrong with channel:[id: 0x12e25627  /192.168.4.1:9230 => /192.168.4.21:11111]  exception=java.lang.NullPointerException 	at com.alibaba.otter.canal.store.helper.CanalEventUtils.createPosition(CanalEventUtils.java:69) 	at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:375) 	at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:36) 	at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:307) 	at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:273) 	at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:124) 	at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:100) 	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) 	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754) 	at org.jboss.netty.channel.SimpleChannelHandler.messageReceived(SimpleChannelHandler.java:154) 	at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:100) 	at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) 	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) 	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754) 	at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) 	at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80) 	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) 	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:444) 	at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80) 	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) 	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:540) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261) 	at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:350) 	at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) 	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) 	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) 	at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) 	at java.lang.Thread.run(Thread.java:748) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.receiveMessages(SimpleCanalConnector.java:344) ~[classes/:na] 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:315) ~[classes/:na] 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:287) ~[classes/:na] 	at com.alibaba.otter.canal.example.AbstractCanalClientTest.process(AbstractCanalClientTest.java:110) ~[classes/:na] 	at com.alibaba.otter.canal.example.AbstractCanalClientTest$2.run(AbstractCanalClientTest.java:76) [classes/:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171] process error!com.alibaba.otter.canal.protocol.exception.CanalClientException: failed to subscribe with reason: something goes wrong with channel:[id: 0x1fda2da3  /192.168.4.1:9232 => /192.168.4.21:11111]  exception=java.lang.NullPointerException 	at com.alibaba.otter.canal.store.helper.CanalEventUtils.createPosition(CanalEventUtils.java:69) 	at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:375) 	at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:36) 	at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.subscribe(CanalServerWithEmbedded.java:157) 	at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:77) 	at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:100) 	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) 	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754) 	at org.jboss.netty.channel.SimpleChannelHandler.messageReceived(SimpleChannelHandler.java:154) 	at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:100) 	at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) 	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) 	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754) 	at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) 	at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80) 	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) 	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:444) 	at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80) 	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) 	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:540) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261) 	at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:350) 	at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) 	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) 	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) 	at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) 	at java.lang.Thread.run(Thread.java:748) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.subscribe(SimpleCanalConnector.java:241) ~[classes/:na] 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.subscribe(SimpleCanalConnector.java:218) ~[classes/:na] 	at com.alibaba.otter.canal.example.AbstractCanalClientTest.process(AbstractCanalClientTest.java:108) ~[classes/:na] 	at com.alibaba.otter.canal.example.AbstractCanalClientTest$2.run(AbstractCanalClientTest.java:76) [classes/:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171] process error!com.alibaba.otter.canal.protocol.exception.CanalClientException: failed to subscribe with reason: something goes wrong with channel:[id: 0x1c0b056f  /192.168.4.1:9233 => /192.168.4.21:11111]  exception=java.lang.NullPointerException 	at com.alibaba.otter.canal.store.helper.CanalEventUtils.createPosition(CanalEventUtils.java:69) 	at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:375) 	at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:36) 	at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.subscribe(CanalServerWithEmbedded.java:157) 	at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:77) 	at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:100) 	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) 	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754) 	at org.jboss.netty.channel.SimpleChannelHandler.messageReceived(SimpleChannelHandler.java:154) 	at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:100) 	at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) 	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) 	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754) 	at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) 	at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80) 	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) 	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:444) 	at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80) 	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) 	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:540) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261) 	at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:350) 	at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) 	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) 	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) 	at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) 	at java.lang.Thread.run(Thread.java:748) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.subscribe(SimpleCanalConnector.java:241) ~[classes/:na] 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.subscribe(SimpleCanalConnector.java:218) ~[classes/:na] 	at com.alibaba.otter.canal.example.AbstractCanalClientTest.process(AbstractCanalClientTest.java:108) ~[classes/:na] 	at com.alibaba.otter.canal.example.AbstractCanalClientTest$2.run(AbstractCanalClientTest.java:76) [classes/:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171] process error!com.alibaba.otter.canal.protocol.exception.CanalClientException: failed to subscribe with reason: something goes wrong with channel:[id: 0x6db99e95  /192.168.4.1:9234 => /192.168.4.21:11111]  exception=java.lang.NullPointerException 	at com.alibaba.otter.canal.store.helper.CanalEventUtils.createPosition(CanalEventUtils.java:69) 	at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:375) 	at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:36) 	at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.subscribe(CanalServerWithEmbedded.java:157) 	at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:77) ``` 我使用的是spring/file-instance.xml。初步估计，应该是${destination_dir}/meta.dat的问题 alpha5版本写入的meta.dat ``` {"clientDatas":[{"clientIdentity":{"clientId":1001 "destination":"local_mysql" "filter":""} "cursor":{"identity":{"slaveId":-1 "sourceAddress":{"address":"192.168.4.101" "port":3306}} "postion":{"gtid":"" "included":false "journalName":"mysql-bin.000005" "position":14016 "serverId":1 "timestamp":1535189721000}}}] "destination":"local_mysql"} ``` 而1.1.0写入的meta.dat ``` {"clientDatas":[{"clientIdentity":{"clientId":1001 "destination":"local_mysql" "filter":""}}] "destination":"local_mysql"} ``` 如果我把alpha5的meta.dat覆盖1.1.0的，那1.1.0的就没有楼上的问题了 NPE的问题  我们关注一下 2018-09-03 17:21:00.046 [main] INFO  com.alibaba.otter.canal.kafka.CanalServerStarter - ## the canal server is running now ...... 2018-09-03 17:21:00.048 [main] INFO  com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - ## load kafka configurations 2018-09-03 17:21:00.169 [main] INFO  com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - ## start the kafka workers. 2018-09-03 17:21:00.169 [main] INFO  com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - ## the kafka workers is running now ...... 2018-09-03 17:21:00.169 [pool-5-thread-1] INFO  com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - ## start the canal consumer: ordertest3317. 2018-09-03 17:21:53.135 [canal-instance-scan-0] WARN  o.s.beans.GenericTypeAwarePropertyDescriptor - Invalid JavaBean property 'connectionCharset' being accessed! Ambiguous write methods found next to actually used [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.lang.String)]: [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.nio.charset.Charset)] 2018-09-03 17:21:53.157 [canal-instance-scan-0] INFO  c.a.o.canal.deployer.monitor.SpringInstanceConfigMonitor - auto notify reload ordertest3317 successful. 2018-09-03 17:21:54.161 [destination = ordertest3317   address = yunjitest.mysql.rds.aliyuncs.com/47.98.70.247:3306   EventParser] WARN  c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - prepare to find start position just show master status 2018-09-03 17:21:54.204 [destination = ordertest3317   address = yunjitest.mysql.rds.aliyuncs.com/47.98.70.247:3306   EventParser] WARN  c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - find start position : EntryPosition[included=false journalName=mysql-bin.001277 position=264457500 serverId=<null> gtid=<null> timestamp=<null>] 2018-09-03 17:21:54.230 [pool-5-thread-1] INFO  com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - ## the canal consumer ordertest3317 is running now ...... **2018-09-03 17:21:55.892 [pool-5-thread-1] ERROR com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - process error! java.lang.NullPointerException: null 	at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.calculateSize(MemoryEventStoreWithBuffer.java:555) ~[canal.store-1.1.0.jar:na] 	at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.doGet(MemoryEventStoreWithBuffer.java:322) ~[canal.store-1.1.0.jar:na] 	at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.tryGet(MemoryEventStoreWithBuffer.java:261) ~[canal.store-1.1.0.jar:na] 	at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getEvents(CanalServerWithEmbedded.java:478) ~[canal.server-1.1.0.jar:na] 	at** com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:310) ~[canal.server-1.1.0.jar:na] 	at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:273) ~[canal.server-1.1.0.jar:na] 	at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.worker(CanalKafkaStarter.java:121) [canal.kafka-1.1.0.jar:na] 	at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.access$000(CanalKafkaStarter.java:26) [canal.kafka-1.1.0.jar:na] 	at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter$1.run(CanalKafkaStarter.java:70) [canal.kafka-1.1.0.jar:na] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_181] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_181] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_181] 2018-09-03 17:21:55.896 [pool-5-thread-1] INFO  com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - ## the canal consumer ordertest3317 is running now ...... 2018-09-03 17:21:55.896 [pool-5-thread-1] ERROR com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - process error! java.lang.NullPointerException: null 	at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.calculateSize(MemoryEventStoreWithBuffer.java:555) ~[canal.store-1.1.0.jar:na] 	at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.doGet(MemoryEventStoreWithBuffer.java:322) ~[canal.store-1.1.0.jar:na] 	at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.tryGet(MemoryEventStoreWithBuffer.java:261) ~[canal.store-1.1.0.jar:na] 	at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getEvents(CanalServerWithEmbedded.java:478) ~[canal.server-1.1.0.jar:na] 	at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:310) ~[canal.server-1.1.0.jar:na] 	at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:273) ~[canal.server-1.1.0.jar:na] 	at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.worker(CanalKafkaStarter.java:121) [canal.kafka-1.1.0.jar:na] 	at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.access$000(CanalKafkaStarter.java:26) [canal.kafka-1.1.0.jar:na] 	at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter$1.run(CanalKafkaStarter.java:70) [canal.kafka-1.1.0.jar:na] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_181] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_181] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_181] 版本：1.1.0 spring.xml /default-instance.xml 我这里也遇到无限NPE错误的问题了，错误的关键日志见上面加粗部分，canal server 上次调试好之后，有1周多没有使用了，这次启动就报错误了，我把zk清空后重启问题依旧 at com.alibaba.otter.canal.store.helper.CanalEventUtils.createPosition(CanalEventUtils.java:69) 这个错误我也看到过，不过找不到对应的日志了 我把数据库实例给换了一个其他的测试库，清空zk，启动的时候就没有出现这个错误了，然后我重新把数据库实例换到原来出问题的这个 把timestamp设置为当前时间(我怀疑是binlog有问题引起，设置当前时间是为了跳过有问题的binlog)，启动后果然没有报错，我想让错误复现，把timestamp改回到出问题之前的时间点清空zk，但是错误也没有再次出现了 我是想定位出导致这个NPE错误的原因到底是什么，以后在生产中才能够避免或者解决掉 请大佬赐教，这个到底是什么问题啊，我很担心上线后出现问题hold不住就完蛋了，求大佬指点！ 2018-10-19 03:08:14.957 [pool-4-thread-1] INFO  com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - ## the canal consumer example is running now ...... 2018-10-19 03:08:14.957 [pool-4-thread-1] ERROR com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - process error! java.lang.NullPointerException: null   ![image](https://user-images.githubusercontent.com/14846522/47178034-9a082380-d34c-11e8-9b2a-d9079e6c5485.png) 我的版本是 canal.kafka-1.1.0 这是上面的报错 你试着 cat canal.properties 不要参考wiki 上的，使用默认的就好，我的是这样解决的 #canal.instance.global.spring.xml = classpath:spring/memory-instance.xml canal.instance.global.spring.xml = classpath:spring/file-instance.xml #canal.instance.global.spring.xml = classpath:spring/default-instance.xml ![image](https://user-images.githubusercontent.com/14846522/47182391-e311a500-d357-11e8-8d21-929ddd83e007.png) 
867,一遇到mysql 进行大批量操作的时候，客户端就会报错 com.alibaba.otter.canal.protocol.exception.CanalClientException: java.io.IOException: Broken pipe 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.ack(SimpleCanalConnector.java:339) 	at com.adups.canal.CanalHandler.handler(CanalHandler.java:72) 	at com.adups.canal.CanalClient$2.run(CanalClient.java:52) 	at java.lang.Thread.run(Thread.java:748) Caused by: java.io.IOException: Broken pipe 	at sun.nio.ch.FileDispatcherImpl.write0(Native Method) 	at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47) 	at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93) 	at sun.nio.ch.IOUtil.write(IOUtil.java:65) 	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:471) 	at java.nio.channels.Channels.writeFullyImpl(Channels.java:78) 	at java.nio.channels.Channels.writeFully(Channels.java:98) 	at java.nio.channels.Channels.access$000(Channels.java:61) 	at java.nio.channels.Channels$1.write(Channels.java:174) 	at java.nio.channels.Channels$WritableByteChannelImpl.write(Channels.java:458) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.writeWithHeader(SimpleCanalConnector.java:382) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.writeWithHeader(SimpleCanalConnector.java:369) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.ack(SimpleCanalConnector.java:333) 	... 3 common frames omitted 	 客户端处理速度慢了，超过超时时间了？升级1.1.0试试 是不是客户端ack太慢了，导致超时报错？
866,canal.deployer-1.1.0版本，当监听到数据库变动时，server端报异常，望大佬指教 2018-08-23 22:52:32.366 [destination = example   address = /106.12.14.74:3306   EventParser] WARN  c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - find start position : EntryPosition[included=false journalName=mysql-bin.000004 position=5377 serverId=1 gtid=<null> timestamp=1535008711000] 2018-08-23 22:52:32.582 [destination = example   address = /106.12.14.74:3306   EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address /106.12.14.74:3306 has an error  retrying. caused by  java.lang.IllegalArgumentException: null         at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1314) ~[na:1.8.0_181]         at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1237) ~[na:1.8.0_181]         at java.util.concurrent.Executors.newFixedThreadPool(Executors.java:151) ~[na:1.8.0_181]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor.start(MysqlMultiStageCoprocessor.java:84) ~[canal.parse-1.1.0.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:238) ~[canal.parse-1.1.0.jar:na]         at java.lang.Thread.run(Thread.java:748) [na:1.8.0_181] 2018-08-23 22:52:32.582 [destination = example   address = /106.12.14.74:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.lang.IllegalArgumentException         at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1314)         at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1237)         at java.util.concurrent.Executors.newFixedThreadPool(Executors.java:151)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor.start(MysqlMultiStageCoprocessor.java:84)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:238)         at java.lang.Thread.run(Thread.java:748) ] 遇到同样的问题了，不知道楼主有没有解决 我刚才还在给开发大佬发邮件 他说看看是否canal.properties里修改过参数配置。我并没修改 还在等待他回复。 --- from Dcein. 在2018年8月23日 17:23，platypus0127<notifications@github.com> 写道： 遇到同样的问题了，不知道楼主有没有解决 — You are receiving this because you authored the thread. Reply to this email directly  view it on GitHub  or mute the thread. 问个问题，是不是在单核环境下跑的，可能有个bug，稍后就改。canal.instance.parser.parallel设成false可以先绕过去 @lcybo 感谢大佬，成功了。如果多核环境下这个是不是就不用设置了？ 多核不用 感谢大佬！膜拜~！如果canal监视多个mysql服务器变化，是不是需要配置多个canal.instance.standby.address = 被监视mysql地址，还是配置多个canal.instance.master.address呢？ 在2018年8月23日 18:20，lcybo<notifications@github.com> 写道： 多核不用 — You are receiving this because you authored the thread. Reply to this email directly  view it on GitHub  or mute the thread. 大佬：      你好，真是打扰你了。🙏有时间麻烦帮我解决个问题。我在我们linux中mysql配置文件已开启row模式，而且让运维师也开启的canal相关操作权限，但是启动后查新example日志，仍然报错，提示没权限，麻烦大佬给予指点。     2018-08-27 18:27:16.849 [destination = example   address = /192.168.100.249:3306   EventParser] WARN  c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - prepare to find start position just show master status 2018-08-27 18:27:16.851 [destination = example   address = /192.168.100.249:3306   EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address /192.168.100.249:3306 has an error  retrying. caused by  com.alibaba.otter.canal.parse.exception.CanalParseException: command : 'show master status' has an error! pls check. you need (at least one of) the SUPER REPLICATION CLIENT privilege(s) for this operation 2018-08-27 18:27:16.851 [destination = example   address = /192.168.100.249:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: command : 'show master status' has an error! pls check. you need (at least one of) the SUPER REPLICATION CLIENT privilege(s) for this operation ]     多谢大佬赐教！ | | Dcein520 | | 邮箱：Dcein520@163.com | 签名由 网易邮箱大师 定制 在2018年08月23日 18:26，Dcein520 写道： 感谢大佬！膜拜~！如果canal监视多个mysql服务器变化，是不是需要配置多个canal.instance.standby.address = 被监视mysql地址，还是配置多个canal.instance.master.address呢？ 在2018年8月23日 18:20，lcybo<notifications@github.com> 写道： 多核不用 — You are receiving this because you authored the thread. Reply to this email directly  view it on GitHub  or mute the thread. you need (at least one of) the SUPER REPLICATION CLIENT privilege(s) for this 检查mysql grant  > 大佬： 你好，真是打扰你了。有时间麻烦帮我解决个问题。我在我们linux中mysql配置文件已开启row模式，而且让运维师也开启的canal相关操作权限，但是启动后查新example日志，仍然报错，提示没权限，麻烦大佬给予指点。 2018-08-27 18:27:16.849 [destination = example   address = /192.168.100.249:3306   EventParser] WARN c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - prepare to find start position just show master status 2018-08-27 18:27:16.851 [destination = example   address = /192.168.100.249:3306   EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address /192.168.100.249:3306 has an error  retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: command : 'show master status' has an error! pls check. you need (at least one of) the SUPER REPLICATION CLIENT privilege(s) for this operation 2018-08-27 18:27:16.851 [destination = example   address = /192.168.100.249:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: command : 'show master status' has an error! pls check. you need (at least one of) the SUPER REPLICATION CLIENT privilege(s) for this operation ] 多谢大佬赐教！ | | Dcein520 | | 邮箱：Dcein520@163.com | 签名由 网易邮箱大师 定制 在2018年08月23日 18:26，Dcein520 写道： 感谢大佬！膜拜~！如果canal监视多个mysql服务器变化，是不是需要配置多个canal.instance.standby.address = 被监视mysql地址，还是配置多个canal.instance.master.address呢？ 在2018年8月23日 18:20，lcybo<notifications@github.com> 写道： 多核不用 — You are receiving this because you authored the thread. Reply to this email directly  view it on GitHub  or mute the thread. 建议在example/instance.properties 里面修改 canal.instance.filter.regex=.*\\..*   只监控自己这个instance要的数据库。 我发现用rds的时候即使超级用户也提示权限不足 估计内建库权限不足 rds的超级账号还是有对mysql库的部分表无权限，需要过滤掉mysql库
862,kafka 版本wiki配置说明不详 canal_kafka的deploy版本，按照wiki配置方法，配置kafka.xml，其他的参考canal独立配置，kafka console消费没有输出，canal这边也没有日志反馈，有需要特别的配置吗 canal_server不是直接从deploy模块下的target/canal/bin里启动的，而是从kafka模块下的target/canal/bin下启动，启动server后先观察有没有往kafka发送数据 release里面的，刚看了，文件目录都是一样的。canal启动的时候有kafka workers is running now  就是不会输出到kafka client看不到数据 同问，怎么解决，也不报错，也没有日志输出，kafka就是消费不到数据
858,在1.1.0中，LogEventConvert.java中的tableMetaCache属性是null。 tableMetaCache是什么时候，初始化的呢？ 具体复现的办法?  还有你是自己改造过代码么?  是改到过代码。 直接继承 ```java extends MysqlEventParser ``` 多谢，这样tableMetaCache就初始化了
856,ErrotCode:400 canal 1.0.26-SNAPSHOT-2 2018-08-20 09:46:27.050 [New I/O server worker #1-1] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400   Caused by :  something goes wrong with channel:[id: 0x2ae8ccbc  /10.31.152.38:34114 => /10.31.152.38:11111]  exception=com.alibaba.otter.canal.server.exception.CanalServerException: ack error   clientId:1001 batchId:36171 is not exist   please check 2018-08-20 09:48:12.626 [New I/O server worker #1-3] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400   Caused by :  something goes wrong with channel:[id: 0x568191e2  /10.31.152.38:34184 => /10.31.152.38:11111]  exception=com.alibaba.otter.canal.server.exception.CanalServerException: ack error   clientId:1001 batchId:36172 is not exist   please check 2018-08-20 09:50:13.754 [New I/O server worker #1-1] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400   Caused by :  something goes wrong with channel:[id: 0x4176eecb  /10.31.152.38:34258 => /10.31.152.38:11111]  exception=com.alibaba.otter.canal.server.exception.CanalServerException: ack error   clientId:1001 batchId:36173 is not exist   please check 2018-08-20 09:52:11.962 [New I/O server worker #1-3] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400   Caused by :  something goes wrong with channel:[id: 0x49e5e3d0  /10.31.152.38:34332 => /10.31.152.38:11111]  exception=com.alibaba.otter.canal.server.exception.CanalServerException: ack error   clientId:1001 batchId:36174 is not exist   please check 2018-08-20 09:54:03.646 [New I/O server worker #1-1] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400   Caused by :  something goes wrong with channel:[id: 0x74b1d81d  /10.31.152.38:34400 => /10.31.152.38:11111]  exception=com.alibaba.otter.canal.server.exception.CanalServerException: ack error   clientId:1001 batchId:36175 is not exist   please check 2018-08-20 09:55:57.774 [New I/O server worker #1-3] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400   Caused by :  something goes wrong with channel:[id: 0x2b72e577  /10.31.152.38:34460 => /10.31.152.38:11111]  exception=com.alibaba.otter.canal.server.exception.CanalServerException: ack error   clientId:1001 batchId:36176 is not exist   please check 2018-08-20 09:57:52.922 [New I/O server worker #1-1] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400   Caused by :  something goes wrong with channel:[id: 0x61134480  /10.31.152.38:34530 => /10.31.152.38:11111]  exception=com.alibaba.otter.canal.server.exception.CanalServerException: ack error   clientId:1001 batchId:36177 is not exist   please check
852,canal解析到的字段和值不匹配，大佬给看看呗 ![image](https://user-images.githubusercontent.com/18712087/44260217-51e25d00-a246-11e8-9622-32655791da82.png) 这一行取到的rowchange 里边可以看到解析到的字段 和值 与原库的不符，甚至类型都不对，原库int  解析后是date了 是不是发生ddl变更了? @wingerx   比如说 原表 有一条insert，但是canal没有消费，如果原表又做了ddl，那么canal再消费之前的数据就会发生这种情况么 是的，这种情况可以开启tsdb来避免后面类似的问题，当前只能跳过了 . tsdb是指什么呢。。不了解 @wingerx  之前我记得也有这种情况啊。改了表结构的。。也没出现过这个问题
851,canal sever-client heartbeat Server端的Idle检测，只是检测Socket读写通道的阻塞时间 Client与Server订阅关系建立后，CanalConnector没有提供纯粹的心跳检测方法，只能向Server端只能发送get请求才能证明自己活着，如果消费速度过慢，超时就会被Server端关闭Socket连接。 可否增加一个单纯的心跳检测实现？ 短期内先调大超时时间吧
850,alter语句无法解析 正常的mysql alter语句，server端解析出错。 无法跳过，目前想到的暂时解决办法是手动在目标库执行，然后把offset往后移。 2018-08-14 16:53:22.500 [destination =xxxx   address = /xxxx   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : ALTER TABLE `loan_withdraw_record` ADD COLUMN `remark` varchar(255) DEFAULT NULL COMMENT '备注信息' AFTER `is_remind_limit` ALGORITHM=inplace LOCK=NONE com.alibaba.fastsql.sql.parser.ParserException: syntax error  expect TABLES or TABLE  actual EQ  pos 143  line 1  column 143  token =         at com.alibaba.fastsql.sql.dialect.mysql.parser.MySqlStatementParser.parseStatementListDialect(MySqlStatementParser.java:863) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371]         at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:483) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371]         at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371]         at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.apply(DatabaseTableMeta.java:104) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.apply(TableMetaCache.java:228) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseQueryEvent(LogEventConvert.java:265) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:126) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:68) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:345) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:187) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:154) [canal.parse-1.0.26-SNAPSHOT.jar:na] SHTERM: session timeoutotter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at java.lang.Thread.run(Thread.java:745) [na:1.8.0_51]
848,canal支持一下同步到elasticserach 目前mysql + es的组合会是一个比较共性的需求，期望能提供一个基于canal增量同步到elasticserach的能力 canal数据是单表的  es数据一般是关联之后的宽表. 设计上可以考虑扩展点，实现宽表的join逻辑，其余的数据读取和写入可以共用代码 可以实现一层的左外关联的宽表的数据定位同步和整行同步，同时支持返回字段加函数和复合字段
846,[v1.0.26-alpha5]客户端偶尔频发报错：no alive canal server com.alibaba.otter.canal.protocol.exception.CanalClientException:no alive canal server  at com.alibaba.otter.canal.client.impl.ClusterNodeAccessStrategy.nextNode(ClusterNodeAccessStrategy.java:76)~ 客户端偶尔频发报以上错误 在Zookeeper中检查 get /otter/canal/destinations/example/running {"active":true "address":"xxx.xxx.xxx.xxx" "cid":1} 服务端没有报错也是正常的，但客户端会频发报错 如果client自我恢复的话  检查一下当时canal server是否发生过切换或者退出?  @agapple 怎么检查canal server呢？ @agapple   在Zookeeper中检查 get /otter/canal/destinations/example/running 结果： Node does not exist : /otter/canal/destinations/example/running get /otter/canal/destinations/sample/running 结果： Node does not exist : /otter/canal/destinations/sample/running 这种情况频率有些多，虽然可以重试成功，但出现这种问题是不是canal server有问题啊 目前场景： 两个实例example、sample，两个实例中配置一模一样 canal.deployer中log日志中并没有报错，只是Zookeeper中检查不到节点信息 
832,大佬，这个以后会支持条件筛选吗 比如说同步一张表的数据，但是我不想全部同步过去想做一些筛选，类似于在where 后面定义一些条件，符合条件的才同步过去 client里面写代码做数据过滤
829,请问下，example下instance.properties和rds_properties里面的数据库配置有啥区别啊 我记得1.0.24是配置在instance.properties里面的，rds_properties是干嘛用的啊，还有这个canal.instance.tsdb.dbUsername 是啥啊，帮帮忙啊老哥 这块新版本会换成新方案，可以看一下 : https://github.com/alibaba/canal/issues/727
824,client如何控制同一个事物内的insert  update消费顺序？ client如何控制同一个事物内的insert  update消费顺序？有时候client往往先拿到了update的事件，后拿到insert事件。 client拿到的顺序就是数据库里binlog记录的顺序
820,canal是支持到MySQL5.7.18吗？ 5.7.18以上的版本是完全不支持，还是说某些功能的支持有问题？有没有在更高版本上使用过的童鞋哇？ 我们现在在 5.7.20 上使用，没发现问题.
815,canal HA 模式，查看position canal server 启了两台，开启了HA高可用。配置如下： 但是zk不小心被删除了，现在active canal 还是正常接收binlog解析，但是已经没法更新zk中的position信息了。请问一下，zk不小心误删了，除了zk还能在哪查到当前instance的position数据？ ` ################################################# ######### 		common argument		############# ################################################# canal.id= 1 canal.ip= canal.port= 11111 canal.zkServers=10.100.1.10:2181 10.100.1.11:2181 10.100.1.12:2181 canal.zookeeper.flush.period = 1000 canal.file.data.dir = ${canal.conf.dir} canal.file.flush.period = 1000 canal.instance.memory.buffer.size = 16384 canal.instance.memory.buffer.memunit = 1024 ` logs下的meta.log会有记录每次的消费 HA模式下，logs下的meta.log 没有记录，只有非HA的时候才会记录meta.log。 是我HA下，配置有问题吗？
812,链接失效 ### wiki的home + **相关资料**：与阿里巴巴的rocketMQ配合使用 连接失效 ### wiki的Introduction + **知识科普**：mysql的Binlay Log介绍的第二个链接失效 + **EventParser设计**：binlog event structure，详细信息请参考的连接 Page Not Found taobaodba的blog 已经不存在了.其他连接已经修复，tks. @wingerx 可以说十分的效率了，赞一个。 赞一个 原谅我的强迫症 README.md 里面关于RocketMQ的连接未更改。 https://github.com/apache/rocketmq
809,canal ack error 2018-08-03 15:17:45.414 [New I/O server worker #1-4] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x7cec5107  /10.111.61.32:47512 :> /10.111.61.32:11111]  exception=java.nio.channels.ClosedChannelException         at org.jboss.netty.channel.socket.nio.NioWorker.cleanUpWriteBuffer(NioWorker.java:649)         at org.jboss.netty.channel.socket.nio.NioWorker.writeFromUserCode(NioWorker.java:370)         at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:137)         at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76)         at org.jboss.netty.channel.Channels.write(Channels.java:611)         at org.jboss.netty.channel.Channels.write(Channels.java:578)         at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:46)         at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:174)         at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48)         at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276)         at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302)         at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526)         at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507)         at org.jboss.netty.handler.codec.replay.ReplayingDecoder.cleanup(ReplayingDecoder.java:542)         at org.jboss.netty.handler.codec.replay.ReplayingDecoder.channelDisconnected(ReplayingDecoder.java:450)         at org.jboss.netty.channel.Channels.fireChannelDisconnected(Channels.java:360)         at org.jboss.netty.channel.socket.nio.NioWorker.close(NioWorker.java:599)         at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:119)         at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76)         at org.jboss.netty.channel.Channels.close(Channels.java:720)         at org.jboss.netty.channel.AbstractChannel.close(AbstractChannel.java:208)         at org.jboss.netty.channel.ChannelFutureListener$1.operationComplete(ChannelFutureListener.java:46)         at org.jboss.netty.channel.DefaultChannelFuture.notifyListener(DefaultChannelFuture.java:381)         at org.jboss.netty.channel.DefaultChannelFuture.addListener(DefaultChannelFuture.java:148)         at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:48)         at com.alibaba.otter.canal.server.netty.NettyUtils.error(NettyUtils.java:69)         at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:253)         at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48)         at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276)         at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302)         at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526)         at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507)         at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:444)         at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274)         at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261)         at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:350)         at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281)         at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201)         at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46)         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)         at java.lang.Thread.run(Thread.java:748) 
805,关于position设置 请问我设置日志读取位置，是不是只需要设置instance.proerties中的jouralName和position就行，那meda.dat里的呢，为什么它与instance.proerties中的不一样呢，到底哪个生效？ 我是用default-instance.xml zk集群 1. 使用default-instance.xml后，默认是通过zk管理位点信息的.所以 meda.dat在这里无效的 2. meda.dat针对的模式是file-instance.xml 3. instance.properties 中的位点与zk 位点 关系(以 default-instance.xml为例)  * 当zk中位点中不存在时，canal启动的位点以 instance.properties中为准，若没有，则通过 `show master status` 取最新的位点信息  *  当 zk 中存在位点信息时，以zk中的记录为准.  所以在正常数据同步的情况下，zk的位点和instance.properties中配置的位点是不同的。canal 也是通过其来实现断点续传的能力 @wingerx 你好，我尝试删除了zk上的位点信息，让其重新生成，按照你的说法应该同步Instance.properties中的位点信息（我设置过了），但是却没有，这个原因可能是出在哪里呢 1. Instance.properties中配置的只和启动时找位点有关 2. zk中记录的是client端消费成功后ack 时的位点 并不是说再instance.properties中配置了某位点信息，zk就会同步更新成这个位点. 
804,canal如何处理mysql中的blob数据类型？ 求大神请教canal中是如何处理mysql中的blob数据类型？ 在网上查找到的资料如下（不知道是否正确）： Canal 将 binlog 中的值序列化成了 String 格式给下游程序，因此在 Blob 格式的数据序列化成 String 时为了节省空间，强制使用了 IOS_8859_0 作为编码。因此，在如下情况下会造成中文乱码： 同步服务 JVM 使用了 UTF-8 编码 BLOB 字段中存储有中文字符 作者：haitaoyao 链接：https://www.jianshu.com/p/be3f62d4dce0 來源：简书 简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。 求大神指导！ 按照iso8859-1进行反解为bytes[]即可 谢谢指导！但是我将sqltype为longblob类型的字段使用iso8859-1反解为bytes[]再utf-8编码为string时，仍然是乱码： 这是longblob字段的内容： <img width="1128" alt="canal-blob-getvalue" src="https://user-images.githubusercontent.com/28953872/43885058-677e5658-9bea-11e8-956c-edeb50a0da5b.png"> 这是使用iso8859-1进行 反解： <img width="1123" alt="iso" src="https://user-images.githubusercontent.com/28953872/43885109-8a92545a-9bea-11e8-9102-2b5a0c10ec0d.png"> 我的代码如下： if(column.getMysqlType().toUpperCase().equals("LONGBLOB")){                     logger.info(">>>>>> is blob\n");                     logger.info("\n>>>>>> column.getValue() is: " + column.getValue());                     logger.info("\n\n\n\n\n\n>>>>>> new String(column.getValue().getBytes(\"iso8859-1\") \"UTF-8\") is: " + new String(column.getValue().getBytes("iso8859-1"))); } 请教大神如何fix？ 用 getValueBytes方法拿原始bytes 还是getBytesValue？用的爪机没办法确认。 能讲的详细点吗？大神 column没有getBytesValue方法，应该是getBytesValue ``` try {                 if (StringUtils.containsIgnoreCase(column.getMysqlType()  "BLOB")                     || StringUtils.containsIgnoreCase(column.getMysqlType()  "BINARY")) {                     // get value bytes                     builder.append(column.getName() + " : "                                    + new String(column.getValue().getBytes("ISO-8859-1")  "UTF-8"));                 } else {                     builder.append(column.getName() + " : " + column.getValue());                 }             } catch (UnsupportedEncodingException e) {             } ```             public java.lang.String getValue() {                 java.lang.Object ref = value_;                 if (!(ref instanceof java.lang.String)) {                     com.google.protobuf.ByteString bs = (com.google.protobuf.ByteString) ref;                     //用getValue()这里就已经用utf8编码了                     java.lang.String s = bs.toStringUtf8();                     if (bs.isValidUtf8()) {                         value_ = s;                     }                     return s;                 } else {                     return (java.lang.String) ref;                 }             }             public com.google.protobuf.ByteString getValueBytes() {                 java.lang.Object ref = value_;                 if (ref instanceof String) {                     com.google.protobuf.ByteString b = com.google.protobuf.ByteString.copyFromUtf8((java.lang.String) ref);                     value_ = b;                     return b;                 } else {                     //拿到原始的ByteString                     return (com.google.protobuf.ByteString) ref;                 }             } @alexlgj 先试试 @agapple 的吧。    印象在1.0.22版本里getValue （varbinary）会有信息丢失；那时候直接进去改了编码。 @agapple @lcybo 好的，谢谢两位大神的指导，我这边网速不怎么样。回复慢了，不好意思 做了个实验：         byte[] bs = {-1  -2};   //byte是负数的情况，这个就看你数据库的编码了         ByteString bstring = ByteString.copyFrom(bs);         String utf = bstring.toStringUtf8();         System.out.println(utf);    //丢数据了，变成了两个65533         String iso = new String(utf.getBytes("ISO-8859-1")  "UTF-8");         System.out.println(iso);    //没有办法还原，变成两个63 @alexlgj 编码问题蛮有意思的   按照@agapple的方法可以成功解码（blob字段内容为中文“北京”） column.getvalue得到； "image\\\":\\\"å\\\\u008C\\\\u0097äº¬\\\" \\\ new String(column.getValue().getBytes("ISO-8859-1")  "UTF-8"))得到： "image\\\":\\\"北京\ 我用的canal是1.0.24版本，您例子中的是没有访问数据库，为什么说“//byte是负数的情况，这个就看你数据库的编码了”？ latin1 128-255用一个byte是负数表示，有负数在utf-8里面代表多字节，是有一定规范的。128-255的sequence很有可能违反这些规范，就变成�了 如果有硬编码的东西（无序的128-255，非utf-8的编码），还是拿原始的byteString吧。 如果canal放进去的string，就是用的iso-8859-1编码过的，那忽略我说的吧，逃了。 @lcybo 明白了，非常感谢，大大的赞 @lcybo 之前server端传blob类型时强转了string类型，比较合理的做法是用ByteString，但现在不能改，已改老用户的blob解析处理都会乱码了 理解 两位大神，我还有些疑问： 1. 是不是canal server端会对mysql binlog的所有字段做iso-8859-1编码成string？ 2. 对编码完全不懂， 能帮忙解释下为什么这样更改编码就能正确解析出存入blob类型的字符串（"北京“）：new String(column.getValue().getBytes("ISO-8859-1")  "UTF-8")) ？该代码中的不同编码的执行顺序是怎样的？ （我的理解是canal server对blob字段的byte数组做iso-8859-1编码，但是iso-8859-1编码不能表示中文“北京”（乱码）。这时执行column.getValue()方法会进入到：if (!(ref instanceof java.lang.String)) {   对ref进行utf8-string） 大神，另外我想问下，column.getValueBytes().toByteArray()返回的是blob字段original 字节数组吗？还是总是utf-8编码或者其他编码的字节数组？为什么canal中(column.getValueBytes().toByteArray()一个汉字对应6个字节呢？ 这是我的测试结果： 插入的blob字段值： ”0123abc" java程序中打印插入的string的二进制数组内容： Arrays.toString(”0123abc".getBytes()) is： [48  49  50  51  97  98  99] canal中获得的二进制数组内容： Arrays.toString(column.getValueBytes().toByteArray() is： [48  49  50  51  97  98  99] blob字段值：”上”: Arrays.toString(”上".getBytes()) is： [-28  -72  -118]  Arrays.toString(column.getValueBytes().toByteArray() is: [-61  -92  -62  -72  -62  -118] blob字段值：”上海”: Arrays.toString(”上海".getBytes()) is： [-28  -72  -118  -26  -75  -73] Arrays.toString(column.getValueBytes().toByteArray() is： [-61  -92  -62  -72  -62  -118  -61  -90  -62  -75  -62  -73] 1.从agapple大神的描述，应该是只有blob和binary这么做了 另一个问题，如果用iso编码成string。那么string内一个char实际上放的就是单个byte，打印出来是乱码，但实际数据格式没有丢失。 您的意思是说虽然打印出来是乱码，但是二进制数据本身还是保持不变的吗？那为什么：new String(column.getValue().getBytes("UTF-8")  "UTF-8"))打印的结果是乱码呢？ getBytes("UTF-8")等同于用utf-8去编码那一堆乱码，打印出来的当然是乱码 我刚请教了下同学一些编码的知识，现在基本能理解了，谢谢大神的耐心指导，棒棒哒
803,canalConnector.subscribe()会把线程阻塞，造成stop无法停止 当以zookeeper HA模式来启动多个Canal Client时，如果某个Canal Client已经成功subsribe，则当前启动的Canal Client会阻塞在canalConnector.subscribe()方法上。 此时再调用如下的stop方法，会阻塞在join处，造成无法正常的stop     public void stop() {         if (!running) {             return;         }         //canalConnector.stopRunning();该版本的canal-client不支持         running = false;         if (thread != null) {             try {                 **thread.join();**             } catch (InterruptedException e) {                 e.printStackTrace();             }         }         KafkaProducerUtil.stop();         MDC.remove("destination");     } 我看新版的example的stop方法中增加了如下的代码，不知道是不是为了解决上面的问题的？ connector.stopRunning(); 如果是，那么之前版本该怎么解决阻塞的问题呢？ canal是通过这个机制来实现client端的HA的. 堵塞是为了避免同时有两个相同的client进行消费
800,canal原生支持一下RocketMQ对接 RocketMQ： https://rocketmq.apache.org/ canal原生支持一下数据写出到RocketMQ
799,关于canal接收到数据的排序问题? 当同时同一个cust_id 依次进行新增 修改 删除 新增 canal接收到两条数据的数据: 如: cust_id 类型 sql执行时间 001 add 1533002422 001 update 1533002422 001 delete 1533002423 001 add  1533002423 这样的话出现一个问题 我做数据仓库的拉链表的时候 无法知道 001 用户在1533002423毫秒情况下add和delete的顺序 我的问题是 这种情况下 还有其他的字段能提供对于同一个sql执行时间 对数据进行排序区分前后顺序 binlog里就是顺序关系 cust_id 是主键吗？如果是主键，为什么会有add两次的情况呢？？ mysql binlog机制比较复杂，内部很多优化。transaction内部只保证slave执行后的最终结果一致。
797,canal如何区分一次transaction中的多条记录rowdata mysql中一次transaction可能包含多条记录（rowdata），那么canal是如何区分这些记录的呢？有没有类似oracle中的scn(system change number)来唯一标识对某个记录所做的更改？ #751 看一下这个pr是不是满足你说的场景 pr? 不好意思，方便解释的详细点吗？ 不好意思，刚没看到链接，谢谢
795,与guava 20有兼容问题，guava 18 MapMaker.makeComputingMap方法已过时  Error starting ApplicationContext. To display the conditions report re-run your application with 'debug' enabled. 2018-07-31 18:35:10 269 ERROR org.springframework.boot.SpringApplication [842] [main] - Application run failed java.lang.NoSuchMethodError: com.google.common.collect.MapMaker.makeComputingMap(Lcom/google/common/base/Function;)Ljava/util/concurrent/ConcurrentMap; 	at com.google.common.collect.MigrateMap.makeComputingMap(MigrateMap.java:17) ~[canal.common-1.0.24.jar:?] 	at com.alibaba.otter.canal.common.zookeeper.ZkClientx.<clinit>(ZkClientx.java:26) ~[canal.common-1.0.24.jar:?] 	at com.alibaba.otter.canal.client.CanalConnectors.newClusterConnector(CanalConnectors.java:66) ~[canal.client-1.0.24.jar:?] 	at com.star.profile.canal.service.impl.CanalServiceImpl.canalServiceRunner(CanalServiceImpl.java:32) ~[bin/:?] 	at com.star.profile.canal.CanalClientAppliationRunner.run(CanalClientAppliationRunner.java:23) ~[bin/:?] 	at org.springframework.boot.SpringApplication.callRunner(SpringApplication.java:788) [spring-boot-2.0.3.RELEASE.jar:2.0.3.RELEASE] 	at org.springframework.boot.SpringApplication.callRunners(SpringApplication.java:778) [spring-boot-2.0.3.RELEASE.jar:2.0.3.RELEASE] 	at org.springframework.boot.SpringApplication.run(SpringApplication.java:335) [spring-boot-2.0.3.RELEASE.jar:2.0.3.RELEASE] 	at com.star.profile.ProfileApplication.main(ProfileApplication.java:18) [bin/:?] 之前有同学说解决办法是重写CanalNodeAccessStrategy，请问大家都是怎么解决的，能给个解决思路吗 墙裂建议官方升级一下guava版本，项目中若要使用guava20+版本的新特性，有引用canal的话，就会报错 NoSuchMethodError: com.google.common.collect.MapMaker.makeComputingMap，很不方便 最新的master已经支持all in打包的模式，把guava的依赖jar打包进canal client 这个仅仅是canal client的升级吗？我们这边在canal server也有自定义开发，使用嵌入式方式将server当成一个agent用的，canal目前有没有整体升级一些第三方依赖的计划呢？ 可以考虑提交PR给我
794,kafka集成，canal event始终接收不到数据，不知道什么原因 see PR #790 
787,用show processlist 命令看不到dump进程 [v1.0.26.alpha4]  canal启动起来后，开始dump 为什么在mysql端，用show processlist 命令看不到dump进程 确认一下是不是开始消费数据了： instance日志里有没有find start position : EntryPosition[included=false journalName=xxxx position=xxxx serverId=<null> gtid=<null> timestamp=<null>]. 然后meta.log有没有持续刷新
784,canalserver支持一个instance实例抽取多库的么？ canalserver使用单库instance配置，不分组，然后在一个实例中抽取多个库的binlog日志。这种方式配置方法支持么？ 已找到配置方法canal.instance.defaultDatabaseName=db1 db2。这个配置适用于同一个ip、port实例下不同库。库名之间使用逗号隔开。这样配置即可。 我觉得你理解得有一点小问题，你那样是配置默认监听的多个库，但最好是在下面这个配置项中进行正则匹配就好了 所有库 canal.instance.filter.regex = .*\\..* db1 db2 canal.instance.filter.regex = db1\\..* db2\\..*
778,parse row data failed canal 1.0.26-SNAPSHOT 5.5.52-MariaDB GRANT SELECT  REPLICATION SLAVE  REPLICATION CLIENT ON *.* TO 'czbrepclient'@'%' IDENTIFIED BY PASSWORD '*A6A643DF20257C290F26693E1A59F69448428157' Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: java.lang.IllegalArgumentException: limit excceed: 148         at com.taobao.tddl.dbsync.binlog.LogBuffer.getFullString(LogBuffer.java:1123) 提供binlog文件或者测试数据复现常静  发你gmail里了，请查收
774,[v1.0.26.alpha4]connect timed out错误 018-07-25 10:50:58.209 [destination = example   address = /127.0.0.1:3306   EventParser] WARN  c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - Received EOF packet from server  apparent master disconnected. It's may be duplicate slaveId   check instance config 2018-07-25 10:51:17.034 [destination = example   address = /127.0.0.1:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position  {"identity":{"slaveId":-1 "sourceAddress":{"address":"127.0.0.1" "port":3306}} "postion":{"gtid":"" "included":false "journalName":"mysql-bin.000027" "position":299492902 "serverId":2 "timestamp":1532487056000}} 2018-07-25 10:51:17.086 [destination = example   address = /127.0.0.1:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000027 position=299492902 serverId=2 gtid= timestamp=1532487056000] 2018-07-25 10:51:18.081 [destination = example   address = /127.0.0.1:3306   EventParser] WARN  c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - Received EOF packet from server  apparent master disconnected. It's may be duplicate slaveId   check instance config 2018-07-25 10:51:33.753 [destination = example   address = /127.0.0.1:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position  {"identity":{"slaveId":-1 "sourceAddress":{"address":"127.0.0.1" "port":3306}} "postion":{"gtid":"" "included":false "journalName":"mysql-bin.000027" "position":299516130 "serverId":2 "timestamp":1532487071000}} 2018-07-25 10:51:33.760 [destination = example   address = /127.0.0.1:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000027 position=299516130 serverId=2 gtid= timestamp=1532487071000] 2018-07-25 10:51:43.767 [destination = example   address = /127.0.0.1:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address 127.0.0.1/127.0.0.1:3306 has an error  retrying. caused by  java.io.IOException: connect /127.0.0.1:3306 failure 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:77) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.reconnect(MysqlConnector.java:86) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.reconnect(MysqlConnection.java:85) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:186) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Unknown Source) [na:1.8.0_152] Caused by: java.net.SocketTimeoutException: connect timed out 	at java.net.TwoStacksPlainSocketImpl.socketConnect(Native Method) ~[na:1.8.0_152] 	at java.net.AbstractPlainSocketImpl.doConnect(Unknown Source) ~[na:1.8.0_152] 	at java.net.AbstractPlainSocketImpl.connectToAddress(Unknown Source) ~[na:1.8.0_152] 	at java.net.AbstractPlainSocketImpl.connect(Unknown Source) ~[na:1.8.0_152] 	at java.net.PlainSocketImpl.connect(Unknown Source) ~[na:1.8.0_152] 	at java.net.SocksSocketImpl.connect(Unknown Source) ~[na:1.8.0_152] 	at java.net.Socket.connect(Unknown Source) ~[na:1.8.0_152] 	at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannelPool.open(BioSocketChannelPool.java:18) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannelPool.open(SocketChannelPool.java:18) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:72) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	... 4 common frames omitted 2018-07-25 10:51:43.774 [destination = example   address = /127.0.0.1:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: connect /127.0.0.1:3306 failure 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:77) 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.reconnect(MysqlConnector.java:86) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.reconnect(MysqlConnection.java:85) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:186) 	at java.lang.Thread.run(Unknown Source) Caused by: java.net.SocketTimeoutException: connect timed out 	at java.net.TwoStacksPlainSocketImpl.socketConnect(Native Method) 	at java.net.AbstractPlainSocketImpl.doConnect(Unknown Source) 	at java.net.AbstractPlainSocketImpl.connectToAddress(Unknown Source) 	at java.net.AbstractPlainSocketImpl.connect(Unknown Source) 	at java.net.PlainSocketImpl.connect(Unknown Source) 	at java.net.SocksSocketImpl.connect(Unknown Source) 	at java.net.Socket.connect(Unknown Source) 	at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannelPool.open(BioSocketChannelPool.java:18) 	at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannelPool.open(SocketChannelPool.java:18) 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:72) 	... 4 more ] 2018-07-25 10:52:13.657 [destination = example   address = /127.0.0.1:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address 127.0.0.1/127.0.0.1:3306 has an error  retrying. caused by  java.io.IOException: connect /127.0.0.1:3306 failure 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:77) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:81) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:172) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Unknown Source) [na:1.8.0_152] Caused by: java.net.SocketTimeoutException: connect timed out 	at java.net.TwoStacksPlainSocketImpl.socketConnect(Native Method) ~[na:1.8.0_152] 	at java.net.AbstractPlainSocketImpl.doConnect(Unknown Source) ~[na:1.8.0_152] 	at java.net.AbstractPlainSocketImpl.connectToAddress(Unknown Source) ~[na:1.8.0_152] 	at java.net.AbstractPlainSocketImpl.connect(Unknown Source) ~[na:1.8.0_152] 	at java.net.PlainSocketImpl.connect(Unknown Source) ~[na:1.8.0_152] 	at java.net.SocksSocketImpl.connect(Unknown Source) ~[na:1.8.0_152] 	at java.net.Socket.connect(Unknown Source) ~[na:1.8.0_152] 	at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannelPool.open(BioSocketChannelPool.java:18) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannelPool.open(SocketChannelPool.java:18) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:72) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	... 3 common frames omitted 2018-07-25 10:52:13.658 [destination = example   address = /127.0.0.1:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: connect /127.0.0.1:3306 failure 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:77) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:81) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:172) 	at java.lang.Thread.run(Unknown Source) Caused by: java.net.SocketTimeoutException: connect timed out 	at java.net.TwoStacksPlainSocketImpl.socketConnect(Native Method) 	at java.net.AbstractPlainSocketImpl.doConnect(Unknown Source) 	at java.net.AbstractPlainSocketImpl.connectToAddress(Unknown Source) 	at java.net.AbstractPlainSocketImpl.connect(Unknown Source) 	at java.net.PlainSocketImpl.connect(Unknown Source) 	at java.net.SocksSocketImpl.connect(Unknown Source) 	at java.net.Socket.connect(Unknown Source) 	at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannelPool.open(BioSocketChannelPool.java:18) 	at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannelPool.open(SocketChannelPool.java:18) 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:72) 	... 3 more ] @agapple  本机的MySQL通过 127.0.0.1:3306 连不上吧 @wingerx 这个127.0.0.1是其他具体IP，只是贴日志时替换了
773, [v1.0.26.alpha4]parse events has an error错误 ![image](https://user-images.githubusercontent.com/9798724/43177235-d82ca3c0-8ff9-11e8-87f1-e43826acb891.png) 2018-07-25 10:52:13.662 [destination = example   address = /127.0.0.1:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - parse events has an error java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.FutureTask@869cc59 rejected from java.util.concurrent.ThreadPoolExecutor@36f77020[Terminated  pool size = 0  active threads = 0  queued tasks = 0  completed tasks = 0] 	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(Unknown Source) ~[na:1.8.0_152] 	at java.util.concurrent.ThreadPoolExecutor.reject(Unknown Source) ~[na:1.8.0_152] 	at java.util.concurrent.ThreadPoolExecutor.execute(Unknown Source) ~[na:1.8.0_152] 	at java.util.concurrent.AbstractExecutorService.submit(Unknown Source) ~[na:1.8.0_152] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor.start(MysqlMultiStageCoprocessor.java:120) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor.reset(MysqlMultiStageCoprocessor.java:187) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:306) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Unknown Source) ~[na:1.8.0_152] 相同问题啊，解决没 @agapple 这不是root cause，找最前面的日志 使用v1.0.25没有问题
764,canal 1.0.26 偶尔断开连接然后从新连接后 部分同步停止 使用场景: 1 单台虚拟机连接20个数据库的同步任务 某一时刻突然出现大批量如下日志 但是从新连接后 部分数据库同步正常 部分数据库同步停止 zk的cursor不变化 停留在断开连接的地方.日志如下 MysqlConnector - |disConnect Mysql Connection to 10.x.x.x/10.x.x.x:3306..... MysqlConnector - |connect Mysql Connection to 10.x.x.x/10.x.x.x:3306..... MysqlConnector - |handshake initialization packet received  prepare the client authentication packet to send MysqlConnector - |client authentication packet is sent out. 目前出现多次这种情况 原因未知. 信息太少，无法定位，可以看看当时的jstack +1 不断有kill dump以及disConnect、connect 、handshake，内网测试是这样的 @kervin521 你也遇到这个问题了?还是让我提供更多的信息?现在在按照大神的意思 分析jstack.
760,canal+OTTER client端batchid=-1 canal版本：1.0.22 配置启动后，client端使用canalConnector.getWithoutAck(batchSize)getId();得到的值一直为-1。且在zk上1001node下cursor节点一直没有生成，canal端log正常。 client端log： ![image](https://user-images.githubusercontent.com/41414514/42923813-0c834b50-8b5a-11e8-83f4-bf00bdb56d34.png) 在canal端设置binlog名字以及position依旧创建不了cursor。且文件存在。 有人遇到过这样的问题吗？求教 看看canal server上的日志是否有异常
746,canal server 连接数据库超时 server连接数据库超时，然后就不再重新连接了，过了大概两个小时后 ，又自动重新连接了，请问是哪个参数可以设置 超时之后过多长时间重新连接呢 你换成v1.0.25版本吧，v1.0.26有问题 v1.0.25 有这个参数设置么 我的没有问题？看一下日志？有报错吗？ 25版本已经标记不建议使用了，nio的问题 @WithLin 26版本这不停的断开重连也太大啦（下午7点之后出现此情况比较多），换成25版本就没问题，看是否是定时器哪里有问题 @WithLin  报错就是简单的连接超时 connect timeout 感觉可能是网络波动引起的，但是 它隔了2个小时才去重新获取连接，是不是太太久了。超时之后 多久去重新获取连接，没有可以自己设置的参数么 网络的问题重试就好了啊？ while(true)  catch 的时候休息多久，做重试，就好了
731,canal显示连接成功，但是数据库改变的时候收不到订阅消息 canal启动并没有报错，但是无论怎么操作数据库，都没有数据库改变的日志打印出来。求大神指教。 ![image](https://user-images.githubusercontent.com/8399936/42198827-071f8a10-7ebd-11e8-95ee-d0438707b334.png) 我也遇到类似的问题，用的.26版本客户端，.24版本server，zk集群连接，启动应用的时候可以消费，但是过了一段时间后，通过堆栈发现client程序被阻塞在信号量获取的地方，是不是哪里没有处理好死锁了，2个client实例消费！求解 解决了吗？
707,canal启动正常 canal客户端订阅卡住 错误日志：2018-06-20 13:43:46.730 [ZkClient-EventThread-26-vm153:2181 vm154:2181 vm155:2181] ERROR org.I0Itec.zkclient.ZkEventThread - Error handling event ZkEvent[Data of /otter/canal/destinations/yuantong_order_mysql_10.1.240.87_3307_0/1001/running changed sent to com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor$1@4e7d0621] com.alibaba.otter.canal.protocol.exception.CanalClientException: something goes wrong in initRunning method.  	at com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor.initRunning(ClientRunningMonitor.java:142) 	at com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor$1.handleDataDeleted(ClientRunningMonitor.java:71) 	at org.I0Itec.zkclient.ZkClient$6.run(ZkClient.java:549) 	at org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71) Caused by: com.alibaba.otter.canal.protocol.exception.CanalClientException: java.net.ConnectException: 拒绝连接 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.doConnect(SimpleCanalConnector.java:171) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.access$000(SimpleCanalConnector.java:48) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector$1.processActiveEnter(SimpleCanalConnector.java:396) 	at com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor.processActiveEnter(ClientRunningMonitor.java:207) 	at com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor.initRunning(ClientRunningMonitor.java:118) 	... 3 common frames omitted Caused by: java.net.ConnectException: 拒绝连接 	at sun.nio.ch.Net.connect0(Native Method) 	at sun.nio.ch.Net.connect(Net.java:454) 	at sun.nio.ch.Net.connect(Net.java:446) 	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.doConnect(SimpleCanalConnector.java:132) 	... 7 common frames omitted 有时候就会一直卡在这个错误上，需要重启才正常， 但有时候这个错误会出现，但最后能订阅成功，不需要重启，canal的版本是24的 我有时候也会出现这个问题  请问你解决了吗?
693,监听不到binlog 使用1.0.23监听的时候，mysql的binlog配置log-bin=mysql186-bin这种结构，监听不到。请问是否和binlog文件名称有关系？
692,指定位点进行同步的的时候使用时间戳指定的问题 如果只使用时间戳指定，不使用binlog文件名辅助，代码会去获取头尾位置的binlog文件，这个时候如果dba粗暴管理或者其他情况导致binlog文件被物理删除，会获取不到binlog起始文件导致报错。 关键代码： ``` class MysqlEventParser 。。。 // 根据时间查找binlog位置     private EntryPosition findByStartTimeStamp(MysqlConnection mysqlConnection  Long startTimestamp) {         EntryPosition endPosition = findEndPosition(mysqlConnection);         EntryPosition startPosition = findStartPosition(mysqlConnection); 。。。   /* 查询当前的binlog位置      */     private EntryPosition findStartPosition(MysqlConnection mysqlConnection) {         try {             ResultSetPacket packet = mysqlConnection.query("show binlog events limit 1");             List<String> fields = packet.getFieldValues();             if (CollectionUtils.isEmpty(fields)) {                 throw new CanalParseException("command : 'show binlog events limit 1' has an error! pls check. you need (at least one of) the SUPER REPLICATION CLIENT privilege(s) for this operation");             }             EntryPosition endPosition = new EntryPosition(fields.get(0)  Long.valueOf(fields.get(1)));             return endPosition;         } catch (IOException e) {             throw new CanalParseException("command : 'show binlog events limit 1' has an error!"  e);         }     } ```   show binlog events limit 1 的时候会报错： > MySQL [(none)]> show binlog events limit 1; > ERROR 29 (HY000): File './mysqlmaster-bin.000001' not found (Errcode: 2 - No such file or directory) 是不是使用 show binary logs; 命令获取 size 大于 0 的文件，然后再使用 show binlog events 来获取起始位点信息会好一点？ 可以考虑提交一个PR给我
635,fix #131 cannot parse decimal (MYSQL_TYPE_NEWDECIMAL) 1 bit represents sign such that unset == +  set == - removed 'clear sign' & 'restore sign' steps [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=635) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=635) before we can accept your contribution.<br/>**1** out of **2** committers have signed the CLA.<br/><br/>:white_check_mark: clennpillo<br/>:x: billyu-mymm<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=635) it.</sub> [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=635) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=635) before we can accept your contribution.<br/>**0** out of **2** committers have signed the CLA.<br/><br/>:x: billyu-mymm<br/>:x: clennpillo<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=635) it.</sub> 你这改动有点大，是基于哪个MySQL版本分析的结果？
632,Canal server端报错异常 LEAK: ByteBuf.release() 2018-05-02 17:48:53.869 [nioEventLoopGroup-2-3] ERROR io.netty.util.ResourceLeakDetector -  LEAK: ByteBuf.release() was not called before it's garbage-collected.  Enable advanced leak reporting to find out where the leak occurred.  To enable advanced leak reporting  specify the JVM option '-Dio.netty.leakDetection.level=advanced'  or call ResourceLeakDetector.setLevel() See http://netty.io/wiki/reference-counted-objects.html for more information. canal server在空闲一段时间以后，时间不等，没有任何操作以后，报出异常，请问是什么导致的 对应的版本是？ @agapple  canal.deployer-1.0.25  发布的正式版本 你这个还是第一次遇到，没啥经验
627,canal server经常会挂，而且进程不退出 canal server经常会挂，而且进程不退出，根本监测不到server是不是挂了，本来写了个监控进程脚本挂了就重启，但是没什么用，每次重启都要删掉meta.dat ，否则会报 线程ID 不存在。。 希望能做的人性化点。 补充一下，在亚马逊rds上用这个，域名访问，经常域名对应的IP会自动变动，变化时canal server就是解析不到ip，log里一直报错。只有重启canal server才能解决。维护很麻烦，是否可以加上自动重连功能 一直都有自动重试的机制，怀疑是jvm的dns cache，导致ip解析到的一直是同一个ip，可以尝试关闭jvm dns cache
600,数据一致性问题探讨 不管是canal自带的client程序，还是otter相关设计，高可用的实现都是依赖zookeeper 拿canal举例，假设这样一个场景： 因为网络问题，机器A和zk出现session超时，running节点被remove，机器B watch到节点移除事件，接管同步实例，但该实例其实还在A上运行（并且恰好处于往目标库进行数据写入的阶段，写入完成后才能检测到实例不属于这台机器了），这就出现了A和B同时运行一个实例的情况光，我们假设B恰好执行的稍快一些，一条数据的状态变更轨迹为x->y->z，B执行得快，更新为z，A执行的慢，并且它拿的那批次数据的最后状态是y，这样一来z又被y覆盖了，随后A不再执行，数据的一致性出问题了。 对于上述场景，有没有什么好的方案来规避一下呢？ 赞一个，你对分布式并发产生的一致性问题理解还是比较深刻，说一下我们针对这类问题的常见做法.   1.  目标库双写仲裁，比如在MySQL上使用特殊的信息，来表明唯一身份，如果已经有第二个来做写入操作，再有不同的人来做同步写入，则拒绝后面人的写入 2. 双写监控判断，使用巡检机制去发现存在任务双写的情况，找到出现双写的时间点，停止双写的任务后，全部回拉到产生双写之前的时间点，重发一次 3. 可靠自杀机制，针对A机器出现session超时，可以采用进程定时5秒自杀机制，而B机器拿到任务之后，延迟5秒钟启动 @lulu2panpan ， 你可以私聊发我一下weixin联系方式 ![_179888e0-7d55-44be-9630-b7d7d5462108](https://user-images.githubusercontent.com/8461826/39353383-0ae83a50-4a3a-11e8-96d4-9b1460e6238b.png) 就像@lulu2panpan  说的， zookeeper session超时后(`无论是网络因素还是因为gc问题`)， 接收到节点删除的通知后，为啥不利用mutex作用到一些地方，例如MysqlEventParser，使其停止binglog的parse， 甚至调用System.exit(-1)(`这可以理解为一种为了避免双主的自杀行为`)，当然可能无法屏蔽像@lulu2panpan说的 `恰好处于往目标库进行数据写入的阶段` 这种极端场景  但是我没看到canal中有类似的代码  如果因为gc问题，initRunning无法抢先写入zookeeper(`即使canal的standby节点存在delayTime`)  那么当前canal实例还是会一直解析binlog，并执行接下来的所有操作 我不知道Hbase的Hmaster， Kafka partition的replica在依赖zookeeper选主的时候有没有做类似的细节 => `检测到自己失去lease的时候  第一时间就是取回自己之前获取的所有权限  停止所有的数据修改  直到再次成功拿到lease后  才能恢复一切(就像上面说的，即使这么做了  任然无法屏蔽一些极端场景  但仍然是有意义的)` @spccold 私信weixin或者QQ上交流吧，这块一致性问题如果要做好是个细致活。 我的联系方式在wiki里有mail地址
593,1.0.26 这个版本bug造成的吗? 请问canal的版本兼容mysql5.x所有版本吗?我的是5.1.73 2018-04-01 14:11:50.712 [main] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2018-04-01 14:11:51.147 [main] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties] 2018-04-01 14:11:52.319 [main] WARN  o.s.beans.GenericTypeAwarePropertyDescriptor - Invalid JavaBean property 'connectionCharset' being accessed! Ambiguous write methods found next to actually used [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.lang.String)]: [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.nio.charset.Charset)] 2018-04-01 14:11:52.700 [main] INFO  c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-example  2018-04-01 14:11:52.774 [main] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - start successful.... 2018-04-01 14:11:52.815 [destination = example   address = /192.168.1.110:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /192.168.1.110:3306 has an error  retrying. caused by  java.lang.NoSuchMethodError: method java.net.InetSocketAddress.getHostString with signature ()Ljava.lang.String; was not found.         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.<init>(MysqlConnector.java:53) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.<init>(MysqlConnector.java:63) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.<init>(MysqlConnection.java:70) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.buildMysqlConnection(MysqlEventParser.java:308) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.buildErosaConnection(MysqlEventParser.java:76) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:154) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]         at java.lang.Thread.run(libgcj.so.10) [na:na] 2018-04-01 14:11:52.960 [destination = example   address = /192.168.1.110:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.lang.NoSuchMethodError: method java.net.InetSocketAddress.getHostString with signature ()Ljava.lang.String; was not found.    at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.<init>(MysqlConnector.java:53)    at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.<init>(MysqlConnector.java:63)    at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.<init>(MysqlConnection.java:70)    at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.buildMysqlConnection(MysqlEventParser.java:308)    at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.buildErosaConnection(MysqlEventParser.java:76)    at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:154)    at java.lang.Thread.run(libgcj.so.10) ] 2018-04-01 14:11:52.965 [destination = example   address = /192.168.1.110:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - parse events has an error java.lang.NullPointerException: null         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.afterDump(MysqlEventParser.java:137) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:249) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]         at java.lang.Thread.run(libgcj.so.10) ~[na:na] java.lang.NoSuchMethodError: method java.net.InetSocketAddress.getHostString with signature ()Ljava.lang.String; was not found. 你这是啥jdk版本啊 25版本也有这个问题。jdk1.8 对应的 配置i canal.instance.dbUsername=canal canal.instance.dbPassword=canal canal.instance.defaultDatabaseName=test canal.instance.connectionCharset=UTF-8
578,求问1.0.26的稳定版什么时候发布？谢谢~ 如图，求问，谢谢 目前还有1，2个已知bug，需要进行修复 谢谢回复，另外想问会有一个大概时间吗，比如会在6月份又或者十一前或者年底这样稍微准确的时间点吗，谢谢 预计在近1，2个月之内吧 一年过去了 非常感谢你的回复
569,msyql 5.6.16-log canal server1.0.26 解析varchar字段 出现 > 符号   msyql  阿里云rds 高可用版 5.6.16-log ，canal server 解析varchar字段 中文字符 出现 > 符号   乱码了吧 最好能给到一个可复现的测试方式
531,想问一下如果MemoryEventStoreWithBuffer中bufferMemUnit=16 *1024 会造成程序运行一段时间后卡主的情况吗？ MemoryEventStoreWithBuffer中，bufferSize    = 16 * 1024，bufferMemUnit=16 *1024，maxMemSize = batchSize * bufferMemUnit =  16 * 1024 * 16 *1024 ， 程序在正常运行2个多小时后卡住，会有mysql数据库 kill掉 socket 链接爆出，无其他明显异常 。 请问下是这个原因导致吗？ bufferMemUnit默认值大小为 =1024 server的机器内存多大？ server jvm 内存配了8G，同步的数据库 binlog文件大小一天有80个G左右。 发自网易邮箱大师 在2018年02月22日 09:45，fefine 写道: server的机器内存多大？ — You are receiving this because you authored the thread. Reply to this email directly  view it on GitHub  or mute the thread. 我这边也是出现了这样的问题  但是不确定是什么地方的。 你的bufferMemUnit 配置为多少？ 在2018年02月23日 09:27，fefine 写道: 我这边也是出现了这样的问题  但是不确定是什么地方的。 — You are receiving this because you authored the thread. Reply to this email directly  view it on GitHub  or mute the thread. 默认配置，bufferSize = 16 * 1024，bufferMemUnit= 1024，这两个乘积是16MB，不应该出现内存不够的问题。 你那里卡住 表现为 什么症状？ 你看一下这个issue[Instance假死](https://github.com/alibaba/canal/issues/527)
527,Instance假死 Server端运行正常，但是Instance全部线程无响应，日志等级开启为Info，并未发现异常日志 这是Server日志： ![http://7xo1fz.com1.z0.glb.clouddn.com/server.png](http://7xo1fz.com1.z0.glb.clouddn.com/server.png) 这是Instance日志： ![http://7xo1fz.com1.z0.glb.clouddn.com/server.png](http://7xo1fz.com1.z0.glb.clouddn.com/instance.png) 我在Instance中也开启了另外一个定时的线程，但是在卡死的时间内毫无响应。数据库在今天下午并无其他特殊操作。 问一下这有可能是什么问题？ 没数据变更？ 有，数据一直在变更 我这是canal server 启动时正常，binlog会被client消费掉。但如果几个小时，一直没有binlong产生，再产生binlog话，server就不会读到这些binlog。 @agapple  如果没有数据变更，隔一段时间instance就不会再消费binlog了吧？ 我测试的结果，为了避免这种情况，需要enable detecting，并使用 insert retl.xdual做为心跳sql. 还有其他的解决方法吗？ 你用的是自己打包的1.0.25版本？ 是的，下载的源码打包 @fefine 使用最新的26  alpha 2代码再试试，对于tcp加了so_timeout机制，可以响应长时间无binlog位点被mysql server主动断开的问题
491,log event entry exceeded max_allowed_packet 应该是有大事务时，会出现下面的异常，canal会解析不过去。 2018-01-12 12:45:20.537 [destination =xxxx   address = /xxxxxx   EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = log event entry exceeded max_allowed_packet; Increase max_allowed_packet on master; the first event 'mysql-bin.007177' at 998888207  the last event read from './mysql-bin.007177' at 123  the last byte read from './mysql-bin.007177' at 998888226. 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) ~[canal.parse-1.0.19.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:117) [canal.parse-1.0.19.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) [canal.parse-1.0.19.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.7.0_76] 看到有一个关于max_allowed_packet的issues，https://github.com/alibaba/canal/issues/85   貌似是为了解决这个问题加了组包的逻辑，但是好像没有什么效果，大家有遇到这个问题的么？ 可以先看看binlog-row-event-max-size参数设置是多少，定义如下： https://dev.mysql.com/doc/refman/5.7/en/replication-options-binary-log.html#option_mysqld_binlog-row-event-max-size binlog-row-event-max-size默认是8k，max_allowed_packet默认是1M。binlog不会将总和>binlog-row-event-max-size的rows合到一个event中，除非单row的size已经超过了这个限制，并且>max_allowed_packet，这个时候只能改max_allowed_packet了。 补充一点， com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection#updateSettings 看上去是在这里设置max_allowed_packet(需要作者确认下) updateSettings暂时未做max packet大小的调整 怎么解决啊，我把max_allowed_packet 16mb 加到64还是会有错误
484,希望后续有对 Docker 支持的计划，环境调试特别不方便。 我本来打算自己封装镜像的，我发现需要自己写 `docker entrypoint.sh` 不说，主要是日志还有配置的处理需要在应用层做一些工作果断放弃了，希望官方后续对 Docker 进行支持。 我们自己做了一个镜像，需要的话，可以交流一下，微信gary0526 @freeme 你方便把你的 `dockerfile` 的仓库链接发一下吗？  > BTW: 你封装的镜像是改造过官方的代码吗？ 我这边期望的镜像功能有如下两点主要特性： * 日志可以统一输出并有效通过 `docker logging` 控制收集； * 可以通过 `docker environment` 环境变量控制核心配置； 修改startup.sh文件，变成前台应用就好了。 @pczhaoyun 脚本只是其中一方面，应用本身的日志和配置也要做相应适应性处理的，否则依然管理麻烦的。 @freeme 可以提交一个dockerfile PR 是啊，有这个会方便很多
464,脑裂问题 HA集群模式下： 1.首先启动canal-server1 ，启动成功，相应的destination实例启动成功。 2.启动canal-server2，没有启动成功，相应的destination实例也没有启动成功。 3.断掉canal-server1和zk的连接，比如我添加了一个防火墙，不允许canal-server1连接到zk，但是不断canal-server1和mysql的连接 4.这个时候zk中destination的running节点的信息变成canal-server2，但是canal-server1 中的destination实例仍在运行，仍然可以接收mysql的binlog进行解析。这样就出现了两个canal server的destination都处于运行的状态， 5.此时，我断掉canal-server1 ，这个时候zk中的destination无法自动切换到canal-server2，虽然canal-server2正在运行 第4部不是已经正常切换了吗？老的canal-server1会监听zk的状态变化，如果非当前运行会关闭内存里的destination 在第四步的时候，zk的信息确实已经成功切换到cana-server2   但是如果canal-server1一直连不上zk的情况下。它会不断报连接被拒绝，但是同时和canal-server2一样，也是在解析mysql的binlog canal-server1使用的zk client会和zk server心跳机制，超过一段时间连不上，则自己会认为出现session timeout，然后关闭自己的binlog解析 假死的canal-server1会和zk有心跳，timeout之后会关闭自己的binlog解析，比如我们设置的心跳时间是5秒，在这个5秒的心跳窗口时间之内，应该是canal-server1和canal-server2同时都在解析吧。 当canal-server1假死，canal-server2切换成active状态的时候，应该对canal-server1进行隔离吧
450,canal-1.0.25 引用的druid解析 dcl 语句出错  canal 版本: cannal-1.0.25 druid解析出错信息如下： ` 2017-12-14 14:09:15.304 [destination = example   address = /127.0.0.1:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE USER 'wingerx'@'%' IDENTIFIED WITH 'mysql_native_password' AS '*6BB4837EB74329105EE4568DDA7DC67ED2CA2AD9' com.alibaba.druid.sql.parser.ParserException: syntax error  error in :'ord' AS '*6BB4837EB74329105EE4568DD'  expect AS  actual AS pos 64  line 1  column 62  token AS 	at com.alibaba.druid.sql.parser.SQLParser.printError(SQLParser.java:284) ~[druid-1.1.6.jar:1.1.6] 2017-12-14 14:09:15.309 [destination = example   address = /127.0.0.1:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : ALTER USER 'wingerx'@'%' IDENTIFIED WITH 'mysql_native_password' AS '*6BB4837EB74329105EE4568DDA7DC67ED2CA2AD9' com.alibaba.druid.sql.parser.ParserException: syntax error  expect PASSWORD  actual IDENTIFIED  pos 31  line 1  column 21  token IDENTIFIED 	at com.alibaba.druid.sql.parser.SQLParser.acceptIdentifier(SQLParser.java:60) ~[druid-1.1.6.jar:1.1.6] ` 这个错误暂时不影响，只要不是涉及到表结构变化的DDL解析失败
425,canal binlog 丢失 现网数据变更较大，发现数据存在遗漏的情况，定位在binlog未接收到事件信息。 对比发现binlog 29827255 未知的信息丢失 dump binlog ![image](https://user-images.githubusercontent.com/8357717/33230017-72c7cf0e-d1a0-11e7-94e6-981ab2367e25.png) canal log ![image](https://user-images.githubusercontent.com/8357717/33230032-0865fa2c-d1a1-11e7-90ae-a62fc76fbbe5.png) ![image](https://user-images.githubusercontent.com/8357717/33230062-a4101d04-d1a1-11e7-83d1-c151ca6b771f.png) 日志丢失的问题看源码找到原因了，但是binlog接收处确实未接收到，log每次会将接收到的binlog打印出来，29827255未被接收，处理代码如下 ``` void execute() {         long batchId;         LOGGER.debug("execute destination : " + destination);         while (true) {             try {                 connector.connect();                 connector.subscribe(filter);                 while (true) {                     Message message = connector.getWithoutAck(BATCH_SIZE);                     batchId = message.getId();                     int size = message.getEntries().size();                     if (batchId == -1 || size == 0) {                         try {                             Thread.sleep(SLEEP_TIME);                         } catch (InterruptedException ignored) {                             // ignored                         }                     } else {                         process(message.getEntries());                     }                     connector.ack(batchId);                 }             } catch (Exception e) {                 LOGGER.error("canal connect error  destination : " + destination  e);                 AlarmUtil.dcAlarm(App.DC_ID  "canal_connect_error"                         "canal connect error  destination : " + destination);             } finally {                 connector.disconnect();             }             try {                 Thread.sleep(1000 * 60);             } catch (InterruptedException e) {                 // ignored             }         }     }     private void process(List<CanalEntry.Entry> entryList) {         for (CanalEntry.Entry entry : entryList) {             if (entry.getEntryType() == CanalEntry.EntryType.TRANSACTIONBEGIN                     || entry.getEntryType() == CanalEntry.EntryType.TRANSACTIONEND) {                 continue;             }             if (entry.getEntryType() != CanalEntry.EntryType.ROWDATA) {                 continue;             }             CanalEntry.RowChange rowChange;             try {                 rowChange = CanalEntry.RowChange.parseFrom(entry.getStoreValue());             } catch (Exception e) {                 throw new RuntimeException("parse event has an error   data:" + entry.toString()  e);             }             CanalEntry.EventType eventType = rowChange.getEventType();             if (eventType == CanalEntry.EventType.DELETE) {                 return;             }             if (eventType != CanalEntry.EventType.INSERT && eventType != CanalEntry.EventType.UPDATE) {                 return;             }             List<CanalEntry.RowData> rowDataList = rowChange.getRowDatasList();             if (rowDataList == null || rowDataList.size() == 0) {                 continue;             }             List<BinlogColumnDTO> columns = new ArrayList<>();             try {                 for (CanalEntry.RowData rowData : rowDataList) {                     columns = convertColumnList(rowData.getAfterColumnsList());                     long updatedCount = columns.stream().filter(BinlogColumnDTO::getUpdated).count();                     if (updatedCount < 1) {                         return;                     }                     String binlogFileOffset = entry.getHeader().getLogfileName() + ":"                             + entry.getHeader().getLogfileOffset() + ":"                             + DateUtil.timestamp2DateTime(entry.getHeader().getExecuteTime());                     LOGGER.log(ACCESS  binlogFileOffset + "  eventType : {}  data : {}"                             eventType  toJsonString(columns));                     if (eventType == CanalEntry.EventType.INSERT) {                         syncService.insert(columns  binlogFileOffset);                     }                     else if (eventType == CanalEntry.EventType.UPDATE){                         syncService.update(columns  binlogFileOffset);                     }                 }             }             catch (Exception e) {                 LOGGER.error("binlog handle error  data : " + JSONArray.toJSONString(columns)  e);                 AlarmUtil.dcAlarm(App.DC_ID  "binlog_handle_error"  e.toString());             }         }     } ``` 跟踪源码，canal server打印debug日志，发现client发请求过来的时候是有读取到对应postion的，但就是没有返回事件内容过去。 另一个案例的截图  binlog file ![image](https://user-images.githubusercontent.com/8357717/33230711-53efdf28-d1ae-11e7-87ac-229d440857f0.png) canal server log ![image](https://user-images.githubusercontent.com/8357717/33230713-6129ee18-d1ae-11e7-899c-58cdc8d1e51c.png) 1.  你这最后的日志是在位点定位的时候得，不是数据读取的日志 2.  canal log里记录的位点是以批次为单位，并没有精确到一条记录.   确认过滤条件没问题吧？ 只过滤了一个表，如果是过滤条件的问题，那应该一条记录都获取不到啊，目前的情况时部分binlog丢失 instance.properties # table regex canal.instance.filter.regex = p2p.t_loan # table black regex canal.instance.filter.black.regex = 传递的filter：p2p.t_loan 有结论吗？我这边最新版26也发现更新记录有丢失的问题。 @JasonHuangHuster 请问解决了么，我1.1.0也出现1000条数据丢一两条的问题 建议你们先用标准的example工程打印接收到的数据，如果有能复现的方式最好能提供一下 有解决吗？我这边也遇到binlog丢失的情况 我用的是canal-1.0.24，数据库用的是MariaDB 10.1.19  @agapple 我发现当数据丢失的时候，canal日志会报解析异常 我的部署情况： 1. 2台canal HA模式 2.  2个实例监控两个mysql master 3. 使用mycat分片，每个master上有8个片 4. mysql使用GTID ROW 这是错误日志： 2018-09-12 17:08:12.989 [New I/O server worker #1-7] INFO  c.a.otter.canal.instance.core.AbstractCan alInstance - subscribe filter change to immig([1-8]|1[0-8])\.immig_enc_text 2018-09-12 17:09:37.077 [destination = IMMIG_200   address = /172.16.40.200:3306   EventParser] ERRO R c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address 172.16.40.200/172.16.40.200:3 306 has an error  retrying. caused by  com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception .CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: java.util.ConcurrentModificationException: null 	at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:909) ~[na:1.8.0_171] 	at java.util.ArrayList$Itr.next(ArrayList.java:859) ~[na:1.8.0_171] 	at com.alibaba.otter.canal.parse.driver.mysql.packets.UUIDSet.toString(UUIDSet.java:125) ~[c anal.parse.driver-1.1.0.jar:na] 	at com.alibaba.otter.canal.parse.driver.mysql.packets.MysqlGTIDSet.toString(MysqlGTIDSet.jav a:111) ~[canal.parse.driver-1.1.0.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.createHeader(LogEventC onvert.java:849) ~[canal.parse-1.1.0.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEven tConvert.java:561) ~[canal.parse-1.1.0.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$DmlParserStage.onE vent(MysqlMultiStageCoprocessor.java:302) ~[canal.parse-1.1.0.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$DmlParserStage.onE vent(MysqlMultiStageCoprocessor.java:288) ~[canal.parse-1.1.0.jar:na] 	at com.lmax.disruptor.WorkProcessor.run(WorkProcessor.java:143) ~[disruptor-3.4.2.jar:na] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8. 0_171] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8. 0_171] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171] 2018-09-12 17:09:37.078 [destination = IMMIG_200   address = /172.16.40.200:3306   EventParser] ERRO R com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:IMMIG_200[com.alibaba.otter.can al.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException:  parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: java.util.ConcurrentModificationException 	at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:909) 	at java.util.ArrayList$Itr.next(ArrayList.java:859) 	at com.alibaba.otter.canal.parse.driver.mysql.packets.UUIDSet.toString(UUIDSet.java:125) 	at com.alibaba.otter.canal.parse.driver.mysql.packets.MysqlGTIDSet.toString(MysqlGTIDSet.jav a:111) 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.createHeader(LogEventC onvert.java:849) 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEven tConvert.java:561) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$DmlParserStage.onE vent(MysqlMultiStageCoprocessor.java:302) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$DmlParserStage.onE vent(MysqlMultiStageCoprocessor.java:288) 	at com.lmax.disruptor.WorkProcessor.run(WorkProcessor.java:143) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) 	at java.lang.Thread.run(Thread.java:748) ] 2018-09-12 17:09:51.781 [destination = IMMIG_200   address = /172.16.40.200:3306   EventParser] WARN   c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - find start position : EntryPosition[includ ed=false journalName=mysql-bin.000012 position=46893782 serverId=200 gtid=85506e6a-a0f8-11e8-96b3-00 5056b70bff:1-85344 timestamp=1536743377000] 2018-09-12 17:09:53.682 [New I/O server worker #1-9] INFO  c.a.otter.canal.instance.core.AbstractCan alInstance - subscribe filter change to immig([1-8]|1[0-8])\.immig_enc_text 这是我的binlog：（4757那条是丢失的） ### INSERT INTO `immig2`.`immig_enc_text` ### SET ###   @1='4756' ###   @2='Name-20180912171327-3073F1CC4D611dd0b13565e94969af0b4fe0972d58bf' ###   @3='DETAIL-20180912171327-73F1CC4D611dd0b13565e94969af0b4fe0972d58bf' ###   @4='BIRTHDATE-20180912171327-1CC4D611dd0b13565e94969af0b4fe0972d58bf' ###   @5='1dd0b13565e94969af0b4fe0972d58bf' ###   @6='1' ###   @7='00' ###   @8='1' ###   @9='1' ###   @10='20180912171327' ###   @11='CHN' ###   @12=1536743376 # at 46871947 #180912 17:09:36 server id 200  end_log_pos 46872041 CRC32 0xf43de30e 	Table_map: `immig2`.`immig_enc_text` mapped to number 111 # at 46872041 #180912 17:09:36 server id 200  end_log_pos 46872344 CRC32 0x32dac44c 	Write_rows: table id 111 flags: STMT_END_F ### INSERT INTO `immig2`.`immig_enc_text` ### SET ###   @1='4757' ###   @2='Name-20180912171327-3073F1CC4D615b1e326471284984b522636e96b55fc3' ###   @3='DETAIL-20180912171327-73F1CC4D615b1e326471284984b522636e96b55fc3' ###   @4='BIRTHDATE-20180912171327-1CC4D615b1e326471284984b522636e96b55fc3' ###   @5='5b1e326471284984b522636e96b55fc3' ###   @6='1' ###   @7='00' ###   @8='1' ###   @9='1' ###   @10='20180912171327' ###   @11='CHN' ###   @12=1536743376 # at 46872344 #180912 17:09:36 server id 200  end_log_pos 46872438 CRC32 0x4dd666d7 	Table_map: `immig2`.`immig_enc_text` mapped to number 111 # at 46872438 #180912 17:09:36 server id 200  end_log_pos 46872741 CRC32 0x86092308 	Write_rows: table id 111 flags: STMT_END_F ### INSERT INTO `immig2`.`immig_enc_text` ### SET ###   @1='4758' ###   @2='Name-20180912171327-3073F1CC4D618b85988e6a054822800bf861691f6e13' ###   @3='DETAIL-20180912171327-73F1CC4D618b85988e6a054822800bf861691f6e13' ###   @4='BIRTHDATE-20180912171327-1CC4D618b85988e6a054822800bf861691f6e13' ###   @5='8b85988e6a054822800bf861691f6e13' ###   @6='1' ###   @7='00' ###   @8='1' ###   @9='1' ###   @10='20180912171327' ###   @11='CHN' ###   @12=1536743376 # at 46872741 #180912 17:09:36 server id 200  end_log_pos 46872835 CRC32 0xeb520dc4 	Table_map: `immig2`.`immig_enc_text` mapped to number 111 # at 46872835 #180912 17:09:36 server id 200  end_log_pos 46873138 CRC32 0x31e40985 	Write_rows: table id 111 flags: STMT_END_F @xesygao ConcurrentModificationException的问题: https://github.com/alibaba/canal/pull/902 @lcybo 由于串行的性能对我来说已经够用了，我暂时先用串行解析，测试了十几次没出现丢数据的问题了。tks 可以先尝试用一下1.1.1的alpha版本
385,MariaDB 10.0.25 Datetime 类型无法解析毫秒级 数据库中该字段类型为Datetime(6) 值为2016-12-21 17:57:16.000000  canal解析后，得到的字符串是103946-26-59 06:40:65。 这应该是解析的问题，手头暂时没有MariaDB该版本，方便提供对应的binlog文件，我本地跑一下 你可以直接查看binlog文件，数据应该也是不正确的，这个版本的mariadb在记录秒级精度以上数据的binlog有bug，timestamp和Datetime都有这个问题 https://jira.mariadb.org/browse/MDEV-9567 @chrobin 你的意思就是mariadb写的binlog数据格式有问题咯？
369,canal消费端到某个时间点不消费 用的canal/canal-deployer-1.0.24  最新版本   订阅的mysql版本是mariadb的10.0.22 发现消费端运行到某一时刻 zk的位点和时间戳就不在变化 而且消费端的进程并没有挂掉 也没有发现错误日志 重新启动进程不好使 必须重新置成instance.properties的时间戳 删掉zk里位点 重新消费才好使 盼回复 谢谢!  没日志[允悲] 是不是阻塞了？ 这么解决的？遇到同样问题， 没有日志很恼火。可以确定的是 client和server直接的问题 +1
368,客户端报错no match ack position 你们好，我在使用canal的过程中碰到了个奇怪的问题，描述如下 我的客户端代码是 while(!Thread.currentThread().isInterrupted()) {     Message message = canalService.getWithoutAck(identity  1  1L  TimeUnit.SECONDS);     long batchId = message.getId();     if(batchId != -1)          canalService.ack(identity  batchId); } 具体过程是每次取一条，然后进行ack，日志显示第一条也就是batch id为1的数据ack成功了，第二条数据也就是batch id为2的数据ack失败了，并且碰到以下的异常 com.alibaba.otter.canal.store.CanalStoreException: no match ack positionLogPosition[identity=LogIdentity[sourceAddress=/10.160.246.137:1379 slaveId=-1] postion=EntryPosition[included=false journalName=mysql-bin.000001 position=130 serverId=1379 timestamp=1489048651000]] 请问这个异常可能是什么原因导致的呢 no match ack的意思是未按照batchId的顺序调用ack 哇，还以为这个问题要被忘掉了。。。 实际上是按照顺序ack的，但是仍然出现了这种状况，经过我跟踪代码，我发现如果消费者启动过快，而此时canal server没有取到任何binlog数据后，接下来binlog到来时就会发生这种情况。 我的CanalInstance是CanalInstanceWithManager，MetaManager是FileMixedMetaManager，LogPositionManager是MetaLogPositionManager，版本1.24. 在我这个三个搭配下以及在上述的特殊情况下就会发生这个问题，不知道其他搭配有没有问题
340,canal 支持mysql ssl连接么 外网同步的过程中，传输内容容易被抓包，请问canal支持ssl连接吗。 目前暂时不支持，目前团队内正在调研mysql ssl协议，等调研实施有结果后再借鉴过来
300,binlog 解析出现问题 com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: java.lang.IllegalArgumentException: limit excceed: 191 	at com.taobao.tddl.dbsync.binlog.LogBuffer.getFullString(LogBuffer.java:1140) ~[canal.parse.dbsync-1.0.25-SNAPSHOT.jar:na] 	at com.taobao.tddl.dbsync.binlog.event.RowsLogBuffer.fetchValue(RowsLogBuffer.java:923) ~[canal.parse.dbsync-1.0.25-SNAPSHOT.jar:na] 	at com.taobao.tddl.dbsync.binlog.event.RowsLogBuffer.nextValue(RowsLogBuffer.java:99) ~[canal.parse.dbsync-1.0.25-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseOneRow(LogEventConvert.java:495) ~[canal.parse-1.0.25-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:388) ~[canal.parse-1.0.25-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:111) ~[canal.parse-1.0.25-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:62) ~[canal.parse-1.0.25-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:326) ~[canal.parse-1.0.25-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:176) ~[canal.parse-1.0.25-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:130) [canal.parse-1.0.25-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) [canal.parse-1.0.25-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_111] [mysql-bin.000954.zip](https://github.com/alibaba/canal/files/972734/mysql-bin.000954.zip) 提供了出错的binlog 文件 canal版本为 1.0.24 mysql 版本为 10.0.26-MariaDB-enterprise 同样版本的 另一个mysql  binlog 解析就没有问题 [mysql-bin.005283.zip](https://github.com/alibaba/canal/files/972748/mysql-bin.005283.zip) 加大了  canal.instance.memory.buffer.size 不在报错 但是 也不解析  canal.instance.memory.buffer.size = 268435456  用FileLogFetcherTest本地解析binlog 也同样报错 但是 加大  FileLogFetcher fetcher = new FileLogFetcher(1024 * 1024 * 1024); 参数后 测试通过 很奇怪 我换了个几个mysql 版本 生成的binlog  用这个本地解析类FileLogFetcherTest 都不能正常解析 都必须调整 new FileLogFetcher(1024 * 1024 * 1024); 这里的参数 往大了调才测试通过  最新发现 貌似和 下面的表结构有关系  `create_time` timestamp(6) NOT NULL DEFAULT CURRENT_TIMESTAMP(6) COMMENT '创建时间'   `update_time` timestamp(6) NOT NULL DEFAULT CURRENT_TIMESTAMP(6) ON UPDATE CURRENT_TIMESTAMP(6) COMMENT '最近操作时间' 值都是这种样子的 就会出现问题 2016-05-17 23:33:24.000000 2016-05-18 00:14:50.414649 2016-05-18 10:06:31.269965  只要建表里 有这些列 就直接解析出错 建表不加这些列 就解析没问题 ### INSERT INTO `a1`.`apply_info_222` ### SET ###   @1=***Corrupted replication event was detected. Not printing the value*** 每次我插入一条数据  在binlog里 都会出来这个玩意 感觉是这个的问题 我先关注一下你的测试binlog文件 应该和mariadb的timestamp精度有关，参考下： [MDEV-9567](https://jira.mariadb.org/browse/MDEV-9567) [MDEV-5377](https://jira.mariadb.org/browse/MDEV-5377) 感觉是mariadb在binlog协议里没有记录足够的meta信息，导致解析失败. 试过10.1系列的mariadb ？我是参考10.1的进行协议解析对比
278,canal server cpu 100% @agapple  版本是最新的master，我是从4天前的一个binlog pos点开始同步的，发现同步速度跟之前相比慢很多，top发现canal server cpu一直100%，经过定位找到如下栈： ![cpu](https://cloud.githubusercontent.com/assets/3198806/23615030/0adf5ef2-02c0-11e7-9334-fc3c3e399d6e.png) 内存当时是：              total       used       free     shared    buffers     cached Mem:         32047      24349       7698          0        274      13740 -/+ buffers/cache:      10333      21713 Swap:          511        189        322 这块是有新同学提交的netty4的实现，可能存在一些问题
267,canal收集 由一条update语句更新多行 产生的binlog的速度很慢 什么原因呢？ 我们有一个场景，一条update语句更新3000行（update tablename set x='xxx' where id>0 and id <3001），然后提交。发现canal收集速度明显慢于正常情况。 从mysql中查看binlog，发现一个事务里，分成了47个update_row event（因此一个update_row event包含64个行变化）， 在我们的消费场景中消费时发现一个entry（rowDatas）中有64个BinlogRow。47*64正好对应3000行变化。 做了一些测试，发现当所更新的mysql行的大小比较小的时候，一条语句更新多行时，会有这种情况，收集速度也很慢。 但是mysql行比较大的时候，（更新3000行，binlog原始文件中有3000个update_row event），这个时候收集速度就正常了。 现在想了解： 1.是否是因为更新多行产生的binlog内容（一个entry包含多个行变化信息）导致 后续解析速度慢，从而收集速度变慢 2.具体什么情况下，一个update_row event会包含多个行变化 3.针对一条sql语句更新多行的场景，有没有办法提高处理性能 这个之前没有刻意关注过，你关注到慢有一些具体的数据么？比如评估tps大概是多少？ 相同配置的情况下，一条sql语句只影响一行的情况下，大概每秒能收集7MB原始binlog文件，这种一条sql语句影响多行的会特别慢，我当时测一条sql语句更新3000行，大概每秒只能收集0.5MBbinlog文件。。 我测试的时候发现，只有原先mysql单条记录比较小的时候，这样批量更新会出现这种情况。对于批量更新单条mysql记录比较大的操作（我尝试的是各个字段加起来超过1KB），收集速度和正常情况差不多。 @lan1994 你那个问题解决了吗？我们也遇到类似的问题 @pan289091315 没解决 mysql配置如下参数：innodb_flush_log_at_trx_commit 在提交事务的时候是否提交缓冲? 有3个值： 	0：不会主动触发日志缓冲写入磁盘 	1（默认）：每次提交事务的时候，同时会把日志缓冲刷新到磁盘 	2：每次提交事务的时候，会把日志缓冲刷新到磁盘，但是他不是同时进行的 	   而是每秒钟刷新一次 建议配置成2 ，对性能影响较大。 可以试试这个 @wufengbin 你提的这个参数是解决mysql批量写入慢的问题，但是这里提到的是canal解析慢的问题。在预写入mysql完毕后，才测试解析性能的。 @lan1994 请问解决了吗？ 倒是可以先执行一个大事务，然后canal回溯到事务执行之前进行验证，从我这边的测试来看，基本都是可以满足基本性能 @lan1994 问题原因找到了没？兄弟
192,Canal 权限控制 我在ClusterCanalConnector和SimpleCanalConnector类中发现了username和password的field，而实际上在Canal Server中并没有具体的校验，请问后期会进行权限控制吗？ 这个需要对接一套console系统来运维 @agappl 这个需要对接一套console系统来运维 Canal如何对接一套console系统，有没有案例 可以参考下DevGuide，目前没有现成的console 2016-08-10 19:26 GMT+08:00 GoldenSelfish notifications@github.com: > @agappl 这个需要对接一套console系统来运维 > Canal如何对接一套console系统，有没有案例 >  > — > You are receiving this because you commented. > Reply to this email directly  view it on GitHub > https://github.com/alibaba/canal/issues/192#issuecomment-238839278  or mute > the thread > https://github.com/notifications/unsubscribe-auth/AAy8txrP3a3-yukYkIPd1-dw6QYVzOquks5qebVngaJpZM4Jg5v9 > . 阿里内部是有一套console系统对接的吧，只不过不方便open出来？ 是的，可以对接到otter的配置运维页面 那么这个运维的system内部是不是采用ssh（Jsch）的方式来启动服务器的（sh startup.sh）? 你可以参考otter怎么运维canal，一种策略是嵌入式 canal基本是没有权限管理的，11111端口被扫描到，就可能被连接上，非嵌入式只是在嵌入式的基础上稍微封装下netty，我有一个简单的本公司用的认证改动版，基于客户端的IP和用户名密码验证，不通过断开，可以发给你参考下，或者你参考下netty的简单认证代码 好的，谢谢，麻烦发我一份 在 2017-05-03 10:03:53，"liyongcun" <notifications@github.com> 写道： canal基本是没有权限管理的，11111端口被扫描到，就可能被连接上，非嵌入式只是在嵌入式的基础上稍微封装下netty，我有一个简单的本公司用的认证改动版，基于客户端的IP和用户名密码验证，不通过断开，可以发给你参考下，或者你参考下netty的简单认证代码 — You are receiving this because you authored the thread. Reply to this email directly  view it on GitHub  or mute the thread. @liyongcun 简单的本公司用的认证改动版的代码可以发我一份吗？xyf19920109@163.com邮箱
188,mysql长时间没数据，可能导致canal与mysql的连接读操作永远阻塞 canal从mysql读binlog事件时，建的连接设置了读超时时间：        channel.socket().setSoTimeout(this.soTimeout);  ，soTimeout用了30s。 读binlog事件时，用了SocketChannel.read(buffer)方法，soTimeout对这个SocketChannel的read方法无效，永远不会超时，参考http://bugs.java.com/bugdatabase/view_bug.do?bug_id=4614802。 如果mysql有一段时间没数据（比如一个小时），这个连接失效了（可能因为防火墙问题，参考http://www.tldp.org/HOWTO/TCP-Keepalive-HOWTO/overview.html的2.4.  虽然设置了channel.socket().setKeepAlive(true)，但是linux系统默认的keepalive配置时间太长、不起作用。），但canal还在永远阻塞read。 所以是不是用sock.socket().getInputStream()这个来read比较好，如果在soTimeout内没读到数据，就超时了，强制重新建立连接。 目前设计时依赖于MySQL数据库的空闲链接超时机制，默认是8小时会断开链接，你的链接失效是指半开链接么？这个客户端能获取到EOFException 这个问题在我自己的测试环境中很容易重现，mysql版本:5.5.14 wait_timeout设为8小时，canal版本1.0.22，如果mysql半个小时无数据更新，canal就无法读取新的bin-log，重启canalServer后就正常了。难道跟数据库的配置有关系，还是其他原因？期望能得到指导。 PS：CanalServer没有任何的错误日志，不好分析问题。。。 暂时性解决方案，可以开启下心跳SQL. 这问题还真没遇到过 mysql的slave连接master读数据的默认超时时间（slave_net_timeout）是3600s（时间太长），超时后重建连接，参考http://dev.mysql.com/doc/refman/5.6/en/replication-options-slave.html#sysvar_slave_net_timeout mysql5.5后好像引入了master/slave之间的heartbeat binlog event，master没binlog event空闲一段时间后（超过MASTER_HEARTBEAT_PERIOD）就发送给slave heartbeat事件。参考： https://www.percona.com/blog/2011/12/29/actively-monitoring-replication-connectivity-with-mysqls-heartbeat/ http://dev.mysql.com/doc/refman/5.6/en/change-master-to.html里的MASTER_HEARTBEAT_PERIOD MASTER_HEARTBEAT_PERIOD的默认配置是slave_net_timeout/2，默认是1800s，时间都太长了。 @tianshazzq 可以测试下这些参数试试。 canal程序想要支持默认配置、不出问题，最好还是像开头说的那样自己控制超时时间、连接空闲一段时间（比如最多1min？）后重新建连接。 mysql与canl-server的tcp连接闪断，canl-server并不知情，导致后续的binlog无法通知到canal-server，这个怎么破？ 建议开启canal心跳设置，会定时更新数据库来产生心跳binlog，如果指定时间没收到心跳信息可以尝试重建链接 mysql到canal的连接异常了，比如mysql宕机这种情况，canal是被动接收数据的，自然是不会知道任何链路是否正常的状态；关键是canal是否实现了空闲后有无重连的机制或者mysql异常重连？如若实现了配置哪个参数？ canal基于被动监听式的是没法感知socket被server断开的，最重要还是开启心跳binlog，或者升级mysql5.6(mysql针对空闲链接会定时发送心跳binlog事件来维持tcp链接) 在Mysql宕机后重启这种情况下面的，开启心跳没什么用吧？我们需要在canal中实现mysql这个参数slave_net_timeout所需要实现的功能。 @agapple  把数据库断开时间改成了2分钟，并且开启了心跳检查。长时间（2分钟以上）没有数据后新插入数据，canal server会获取不到 （过不定时间后canal server又能获取到数据10~30分钟不等）。debug看心跳数据都是能正常收发。日志里没有任何异常输出 ## detecing config canal.instance.detecting.enable = true #canal.instance.detecting.sql = insert into retl.xdual values(1 now()) on duplicate key update x=now() canal.instance.detecting.sql = select 1 canal.instance.detecting.interval.time = 3 canal.instance.detecting.retry.threshold = 3 canal.instance.detecting.heartbeatHaEnable = false mysql与canl-server的tcp连接闪断，canl-server并不知情，导致后续的binlog无法通知到canal-server，这个怎么破？ --------------------------------------------------------------------------------------------------- 这个问题你怎么解决的，我现在也遇到了这个问题 @githubcjh  刚刚踩到一个坑，canal.instance.memory.buffer.size设置过小，导致主库执行大事务时，canal堵住 buffer.size建议设置>1024 确实有这个问题，不知道楼主解决了这个问题么？ 改为inputstream是可以的，sotimeout生效了，但是长时间没有数据，inputstream 阻塞在read那里时不会响应thread.interrupt()，event parser那里需要做一些改动先close socket再调用thread.interrupt() 但是总感觉这个方法和canal的nio设计有些矛盾。但是问题是现实存在的。虽然是nio但是读的时候用的是阻塞的模式  buffer size能解决这个问题？？ 解决方案就是在读取mysql binlog变更的地方加个超时就好，不然就只能依赖tcp的keeplive机制；我不太懂java，已经没用这一套了，自己开发了一套。 SocketChannel.read(buffer)此方法的确不会报超时异常（[https://stackoverflow.com/questions/2866557/timeout-for-socketchannel-doesnt-work#answer-9150513](https://stackoverflow.com/questions/2866557/timeout-for-socketchannel-doesnt-work#answer-9150513)），目前本人使用1.0.24版本，暂时将DirectLogFetcher使用的SocketChannel转为ReadableByteChannel，可使socket timeout 生效，抛出SocketTimeoutException。 @vamdt [BioSocketChannelPool.java](https://github.com/alibaba/canal/blob/master/driver/src/main/java/com/alibaba/otter/canal/parse/driver/mysql/socket/BioSocketChannelPool.java)，基于原生Socket操作，soTimeout参数验证也可以生效 @agapple OK，暂时使用体验上感觉1.0.24更稳，等1.0.26正式发布再试试看BIO的。
186,ack 错误了 2016-06-06 19:07:07.416 [New I/O server worker #1-25] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400   Caused by :  something goes wrong with channel:[id: 0x4d32397e  /10.252.158.196:53393 => /10.1.7.109:9930]  exception=com.alibaba.otter.canal.server.exception.CanalServerException: ack error   clientId:1001 batchId:85 is not exist   please check 2016-06-06 19:07:07.418 [New I/O server worker #1-25] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x4d32397e  /10.252.158.196:53393 :> /10.1.7.109:9930]  exception=java.nio.channels.ClosedChannelException     at org.jboss.netty.channel.socket.nio.NioWorker.cleanUpWriteBuffer(NioWorker.java:643)     at org.jboss.netty.channel.socket.nio.NioWorker.writeFromUserCode(NioWorker.java:370)     at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:137)     at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76)     at org.jboss.netty.channel.Channels.write(Channels.java:611)     at org.jboss.netty.channel.Channels.write(Channels.java:578)     at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:28)     at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:144)     at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48)     at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:275)     at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302)     at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndFireMessageReceived(ReplayingDecoder.java:525)     at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:506)     at org.jboss.netty.handler.codec.replay.ReplayingDecoder.cleanup(ReplayingDecoder.java:541)     at org.jboss.netty.handler.codec.replay.ReplayingDecoder.channelDisconnected(ReplayingDecoder.java:449)     at org.jboss.netty.channel.Channels.fireChannelDisconnected(Channels.java:360)     at org.jboss.netty.channel.socket.nio.NioWorker.close(NioWorker.java:593)     at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:119)     at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76)     at org.jboss.netty.channel.Channels.close(Channels.java:720)     at org.jboss.netty.channel.AbstractChannel.close(AbstractChannel.java:200)     at org.jboss.netty.channel.ChannelFutureListener$1.operationComplete(ChannelFutureListener.java:46)     at org.jboss.netty.channel.DefaultChannelFuture.notifyListener(DefaultChannelFuture.java:381)     at org.jboss.netty.channel.DefaultChannelFuture.addListener(DefaultChannelFuture.java:148)     at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:30)     at com.alibaba.otter.canal.server.netty.NettyUtils.error(NettyUtils.java:51)     at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:200)     at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48)     at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:275)     at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302)     at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndFireMessageReceived(ReplayingDecoder.java:525)     at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:506)     at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:443)     at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274)     at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261)     at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:349)     at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:280)     at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:200)     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)     at java.lang.Thread.run(Thread.java:744) 我也遇到过这个问题，是多消费都是消费canal时报的错，你那边是啥情况产生的？ 检查一下server端，是否发生了instance重启
167,canal server如何自适应数据库HA？ 当主db挂了切换到备机，这个时候canal server能自动连接备机获取binlog吗？offset怎么设置呢？ 
147,关于canal高可用的问题 目前canal的高可用只考虑了双机热备，并且是抢占式的(大部分情况下一台机器运行所有instance，另一台机器处于空闲状态) 有没有考虑过canal的集群和负载均衡？比如： 一共有20个instance，10台canal-server，每台canal-server在conf目录下都配置这20个instance，但是通过负载均衡策略，每台server实际只运行2个instance。当出现宕机时，做re-balance，其它server接管宕机server的instance。 没有这种统一的集群策略的话，目前只能以分组的方式实现某种程度的集群。比如： canal_A1和canal_A2为一组，conf目录下配置instance1，instance2，instance3，instance4 canal_B1和canal_B2为一组，conf目录下配置instance5，instance6，instance7，instance8 相应的消费端同理，一组只消费1 2 3 4，另一组只消费5 6 7 8 高可用模式，是针对instance级别，非server级别. 也就是说你10台server，20个instance，每个instance都会是10台server进行互相抢占. ps. 是抢占式可能不会是最平均的状态，比如一台server先启动，就会抢占所有的instance 倒是可以对集群模式下的server，允许设置容量上限，比如最多同时服务多少个instance，避免一个server撑死. 比如你这情况，单server设置最多服务2个，就可以将instance服务平摊到所有server上 单server设置最多服务2个，是基于instance的数目是静态的假设的，如果以后能做成动态的就好了，自动做balance操作。 你假定有10台server，但是在启动第一台server时是不知道还有剩余9台，所以启动第一台server时，要么就人为设置上限不加载所有instance，要么就是认为自己应该全部加载.  一旦集群启动完成后，节点的起停也有类似的问题，别人一旦抢占不会自动做均衡.  恩，这个如果真做的话，可以考虑通过zk做协调，metaq的消费端负载均衡就做到了动态调整。 文档中说道：更新对应instance的running节点内容，将"active"设置为false，对应的running节点收到消息后，会主动释放running节点，让出控制权但自己jvm不退出，gracefully.  经过我的测试后，我发现 还是可以进行数据同步的 所以；在 instance 是高可用的。在 server 层次是备份，但得做好监控。非常完美 理解正确，手工将active设置为false，可以手工干预集群负载 @agapple 要做instance自动均衡，可以在多个server中用zk选个leader，由leader将多个instance均衡分配给不同的server节点（比如可用一致性哈希），分配结果写到zk目录，所有server监听这个zk目录，得到自己要处理的instance。leader在发现server节点有变化时，就重新分配。 @chelubaiq 按照我之前的规划，每个canal节点会有一个server id，会注册到后台manager系统，任务发布都是通过manager系统进行集群分配. 模型和otter有点像，大物理集群，通过配置定义的方式隔离出一堆的逻辑小集群(比如一个任务可以关联到1 2 3这几台server上).   之前偷懒的做法，就是使用了otter manager进行了任务分配，整个canal的集群管理并没有做起来，二期你仔细观察otter manager其实也预留了一些canal的多样性部署的case，比如嵌入式部署和独立式部署.   包括canal自身，也预留了文件队列store的 + 多client的模型，国内已有个别公司已经有实现，但国内开源氛围既如此，一般不太愿意贡献.   
83,增加重置位点的工具 收到反馈来看，重置位点有普遍的需求，并且手工重置的方式误操作或者操作不成功比较多，考虑集成到系统中作为工具提供 这个跟功能实现了嘛？ 还没实现，具体操作还没想清楚，目前可通过人肉重置 2014-09-05 22:57 GMT+08:00 chinaxing notifications@github.com: > 这个跟功能实现了嘛？ >  > — > Reply to this email directly or view it on GitHub > https://github.com/alibaba/canal/issues/83#issuecomment-54635691. 思路是什么样的呢? 再添加一个借口 增加一个消息类型? 或者直接运行 rollback 到 ack 后的 batchId ? 重置是以什么为单位? batchId  binlog position  timestamp ? 应该是binlog position  timestamp，你可以先考虑下 有没有谁实现了？ 重置后必须重启canal server么？ 重启instance，不一定需要server重启 
40,canal支持本地文件持久化的EventStore实现 支持基于本地文件存储的EventStore实现，解决一对多的需求.  需要考虑的几个点： 1.  文件数据的清理模型： 定时清理，ack清理，满清理 2.  位点管理： binlog解析位点，文件位点 3.  位点定位： 时间戳，binlog位置，文件位点 4.  快慢队列： memory + file的混合模式支持，(定时将file数据读入到memory buffer中，提升效率) 所以，这个是什么情况了，现在？ ：） 在store这里持久化的话，似乎很难处理不同client使用不同filter的需求吧，这样的话，就得在client端自己过滤，貌似效率比较低？ 可以考虑提前数据过滤和数据客户端过滤，两种组合 提这样一个想法看行不行 主要是为了支持 Client 与 Canal 在同一服务器上的性能. 将 Event 存入Chronicle-Map  这样支持在 Canal 写的时候  client 同时也能读 没有网络传输 直接的文件并发读取 以 key 作为 binlog 位置 如果能存为有序 map  那么 client 自己也能找到下一个读取 key  这样能使 canal 与 client 相独立出来 主要是避免了一层网络开销.但是只能在 canal 与 client 同 pc 的时候才能做到最高效.这个只是部署上的调整 觉得也是能够接受. canal本身就支持嵌入式模式，允许应用直接内嵌canal server 减少网络传输. CanalServerWithEmbeded 嵌入模式 只能有一个消费者? 
4,mysql自动识别表编码 schema编码识别： SELECT default_character_set_name FROM information_schema.SCHEMATA S WHERE schema_name = "schemaname"; 表编码识别： SELECT CCSA.character_set_name FROM information_schema.`TABLES` T        information_schema.`COLLATION_CHARACTER_SET_APPLICABILITY` CCSA WHERE CCSA.collation_name = T.table_collation   AND T.table_schema = "schemaname"   AND T.table_name = "tablename"; 字段编码识别： SELECT character_set_name FROM information_schema.`COLUMNS` C WHERE table_schema = "schemaname"   AND table_name = "tablename"   AND column_name = "columnname"; 目前手工指定编码存在一个问题，canal无法支持按表或者字段定义编码 针对一个schema下存在多种编码，就会出现编码解析问题 如果一个表存在这不同的字段编码，这个实践本身就有问题吧。 自动识别编码可解决这问题，一个表中有不同的编码。不过遇到一些变态的使用就会有问题，比如varchar，数据库存储的是iso8859-1编码，然后在应用层进行编码转化，类似于以前中文站的oracle 2013年到现在了 还没解决么 业务上暂时无多编码的需求
