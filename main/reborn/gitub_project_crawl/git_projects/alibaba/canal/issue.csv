issue_id,issue_content
1086,application.yml配置打包设置默认模板 
1084,fix #1083 
1083,扁平发送RocketMQ出现发送空包现象kafka应该存在同样问题 [https://github.com/alibaba/canal/blob/master/server/src/main/java/com/alibaba/otter/canal/rocketmq/CanalRocketMQProducer.java](url) 代码 96行处返回数组，数组中值可能为空，但是下面没有判断，会给mq发送空消息，kafka应该也存在同样问题。 另外一个问题还请大神帮忙回答一下 FlatMessage 这个对象 的属性 data length 什么情况下是**大于1**的 有发生dml操作data size就会大于1
1082,cana-kafak1.1.0版本，搭建kafka消费客户端，怎么对ROWDATA类型的storeValue进行解析 2018-10-31 15:49:04.318 [Thread-1] INFO  c.a.o.canal.client.running.kafka.CanalKafkaClientExample - Message[id=12 entries=[header {   version: 1   logfileName: "mysql-bin.000002"   logfileOffset: 1206761   serverId: 1   serverenCode: "UTF-8"   executeTime: 1540972144000   sourceType: MYSQL   schemaName: ""   tableName: ""   eventLength: 79 } entryType: TRANSACTIONBEGIN storeValue: " _"  header {   version: 1   logfileName: "mysql-bin.000002"   logfileOffset: 1206909   serverId: 1   serverenCode: "UTF-8"   executeTime: 1540972144000   sourceType: MYSQL   schemaName: "smart_meter"   tableName: "sys_region_new"   eventLength: 46   eventType: INSERT   props {     key: "rowsCount"     value: "1"   } } entryType: ROWDATA storeValue: "\b\212\001\020\001P\000b\217\001\022\036\b\000\020\004\032\002id \001(\0010\000B\00530026R\aint(11)\022\037\b\001\020\f\032\004name \000(\0010\000B\000R\vvarchar(20)\022!\b\002\020\004\032\tparent_id \000(\0010\000B\0012R\aint(11)\022)\b\003\020\371\377\377\377\377\377\377\377\377\001\032\005level \000(\0010\000B\0011R\ntinyint(1)"  header {   version: 1   logfileName: "mysql-bin.000002"   logfileOffset: 1206955   serverId: 1   serverenCode: "UTF-8"   executeTime: 1540972144000   sourceType: MYSQL   schemaName: ""   tableName: ""   eventLength: 31 } entryType: TRANSACTIONEND storeValue: "\022\0049961" ] raw=true rawEntries=[]] 想知道storeValue怎么解析？ 同问，折腾好久了 可以参考: MessageUtil.parse4Dml
1081,10.1.22-MariaDB版本数据库 journalName乱码 你好，我使用10.1.22-MariaDB和1.1.1版本canal，启动后报如下错误 2018-11-01 09:40:42.663 [destination = cloud   address = /192.168.1.21:3306   EventParser] WARN  c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - prepare to find start position just last position  {"identity":{"slaveId":-1 "sourceAddress":{"address":"192.168.1.21" "port":3306}} "postion":{"gtid":"" "included":false **"journalName":"mysql-bin.000607Æ\u009E´U"** "position":91598031 "serverId":1 "timestamp":1541028888000}} 2018-11-01 09:40:42.674 [destination = cloud   address = /192.168.1.21:3306   EventParser] WARN  c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - find start position : EntryPosition[included=false journalName=mysql-bin.000607Æ´U position=91598031 serverId=1 gtid= timestamp=1541028888000] 2018-11-01 09:40:42.690 [destination = cloud   address = /192.168.1.21:3306   EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Could not find first log file name in binary log index file 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:102) ~[canal.parse-1.1.1.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:216) [canal.parse-1.1.1.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:252) [canal.parse-1.1.1.jar:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-11-01 09:40:42.691 [destination = cloud   address = /192.168.1.21:3306   EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address /192.168.1.21:3306 has an error  retrying. caused by  java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Could not find first log file name in binary log index file 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:102) ~[canal.parse-1.1.1.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:216) ~[canal.parse-1.1.1.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:252) ~[canal.parse-1.1.1.jar:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-11-01 09:40:42.692 [destination = cloud   address = /192.168.1.21:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:cloud[java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Could not find first log file name in binary log index file 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:102) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:216) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:252) 	at java.lang.Thread.run(Thread.java:748) ] 看到是journalName有乱码，请问怎么解决 本地一直没重现，有debug能力可以尝试看下RotateLogEvent的解析过程 我改了EntryPosition类 加了个解析方法    ``` public EntryPosition(String journalName  Long position  Long timestamp){         super(timestamp);         this.journalName = filterLogFileName(journalName);         this.position = position;     }     public EntryPosition(String journalName  Long position  Long timestamp  Long serverId){         this(journalName  position  timestamp);         this.serverId = serverId;     }     public String filterLogFileName(String logfileName) {         Pattern p = Pattern.compile("(mysql-bin\\.[0-9]{6}).");         Matcher m = p.matcher(logfileName);         while(m.find()){             if(m.groupCount() >= 1) {                 return m.group(1);             }         }         return logfileName;     }     public String getJournalName() {         return filterLogFileName(journalName);     }     public void setJournalName(String journalName) {         this.journalName = filterLogFileName(journalName);     } ``` 然后好使了 这个改法都是曲线救国的硬编码处理，没有找到根本原因 @haopangxu 可以试一下我的改法.   主要原因: mariadb在处理checksum行为上和mysql有点不太一致，针对rotate_event这个binlog对象，mariadb会执行checksum，而mysql不会，需要有特殊判断处理
1079, application.yml不打包进jar application.yml不打包进jar  解决jar中的application.yml和config中的会合并bug
1078,canal-server 在积压事件时重启会丢弃内存中的积压事件吗？ ## 问题 canal-server 在配合 ZooKeeperLogPositionManager使用时，重启后读取的 binlog 位点是 server 上一次 parse 的位置还是 client ack 的位置？ 根据 https://github.com/alibaba/canal/wiki/Introduction#eventparser%E8%AE%BE%E8%AE%A1 的描述，EventParser 记录的 binlog 位点是上一次 parse 到的位点，这种行为是不是会导致 server 重启时丢弃事件？ ## 现象 提出这个问题是因为在实践的时候遇到了 binlog 事件丢失的问题，现象如下： 在 canal-server 有事件积压（大约 20 M，未达到设定的内存限制）时重启 canal-server、canal-client、zk（使用 docker-compose 部署，server、client 和 zk 均为单实例，server 使用镜像为 canal/canal-server:v1.1.0），canal 会丢弃一部分事件。 ## binlog 丢失原因猜测 根据 https://github.com/alibaba/canal/wiki/Introduction#eventparser%E8%AE%BE%E8%AE%A1 的描述，EventParser 只记录 parse 到的 binlog 位点，猜测原因是 server 端在积压事件的同时重启，重启后从上次 parse 到的位点进行消费，造成了内存中经过 parse、sink、store 但是还未被 get 的 binlog 丢失。 ## client 代码 client （基于 commit 节点 82f8a9f - fixed issue #483   show slave hosts 修改）使用 com.alibaba.otter.canal.client.ClientLauncher 配合自定义的 CanalOuterAdapter 进行消费。为了记录接收到的 binlog 位点，对 com.alibaba.otter.canal.client.adapter.loader.CanalAdapterWorker 进行了如下修改： ```java while (running) {             try {                 // if (switcher != null) {                 // switcher.get();                 // }                 logger.info("=============> Start to connect destination: {} <============="  this.canalDestination);                 connector.connect();                 logger.info("=============> Start to subscribe destination: {} <============="  this.canalDestination);                 connector.subscribe();                 logger.info("=============> Subscribe destination: {} succeed <============="  this.canalDestination);                 while (running) {                     // try {                     // if (switcher != null) {                     // switcher.get();                     // }                     // } catch (TimeoutException e) {                     // break;                     // }                     // server配置canal.instance.network.soTimeout(默认: 30s)                     // 范围内未与server交互，server将关闭本次socket连接                     // 下面是添加的部分                     Message message = connector.getWithoutAck(BATCH_SIZE); // 获取指定数量的数据                     for (CanalEntry.Entry entry : message.getEntries()) {                         if (!entry.getEntryType().equals(CanalEntry.EntryType.ROWDATA)) {                             continue;                         }                         logger.info("receive binlog event  file {}  offset {}"  entry.getHeader().getLogfileName()  entry.getHeader().getLogfileOffset());                     } ``` 检查 client 日志，发现接收到的 binlog 不连续，范围与 canal 整体丢弃的 binlog 范围相同。由于是在 client 的入口处打印的 binlog 位点，所以我认为 client 没有接收到 binlog 事件，server 处丢弃了这一部分事件。 ## 配置 instance.xml 与 canal/deployer/src/main/resources/spring/default-instance.xml 的 diff 如下： ```xml 112 120c112 113 <                       <bean class="com.alibaba.otter.canal.parse.index.FailbackLogPositionManager"> <                               <constructor-arg> <                                       <bean class="com.alibaba.otter.canal.parse.index.MemoryLogPositionManager" /> <                               </constructor-arg> <                               <constructor-arg> <                                       <bean class="com.alibaba.otter.canal.parse.index.MetaLogPositionManager"> <                                               <constructor-arg ref="metaManager"/> <                                       </bean> <                               </constructor-arg> --- >                       <bean class="com.alibaba.otter.canal.parse.index.ZooKeeperLogPositionManager"> >                               <constructor-arg index="0" ref="zkClientx"/> 129c122 <                       <bean class="com.alibaba.otter.canal.parse.support.AuthenticationInfo" init-method="initPwd"> --- >                       <bean class="com.alibaba.otter.canal.parse.support.AuthenticationInfo"> 133 134d125 <                               <property name="pwdPublicKey" value="${canal.instance.pwdPublicKey:retl}" /> <                               <property name="enableDruid" value="${canal.instance.enableDruid:false}" /> 139c130 <                       <bean class="com.alibaba.otter.canal.parse.support.AuthenticationInfo" init-method="initPwd"> --- >                       <bean class="com.alibaba.otter.canal.parse.support.AuthenticationInfo"> 143 144d133 <                               <property name="pwdPublicKey" value="${canal.instance.pwdPublicKey:retl}" /> <                               <property name="enableDruid" value="${canal.instance.enableDruid:false}" /> ``` ## 请大神指点迷津，是我的配置有误，还是 server 确实会在重启后丢失积压的事件？ canal-server事件的存储是用的基于内存实现的EventStore. 丢失是必然的多。看看wiki吧 wiki都写的很清楚了。 重启读取的是上一次client ack的位点，内存数据虽然会丢失，但会基于最后一次消费成功的位点继续dump binlog，所以架构上不存在消息丢失，只要正确ack位点 你这是配置问题，必须参考file/default-instance.xml里的com.alibaba.otter.canal.parse.index.FailbackLogPositionManager的配置，它在重新启动时会读取到MetaLogPositionManager (ack的位点) > 你这是配置问题，必须参考file/default-instance.xml里的com.alibaba.otter.canal.parse.index.FailbackLogPositionManager的配置，它在重新启动时会读取到MetaLogPositionManager (ack的位点) 感谢大神指导，问题已解决
1076,关于rds oss binlog 离线读取的问题 hi  您好 我在实例配置文件中配置了 #rds oss binlog canal.instance.rds.accesskey= canal.instance.rds.secretkey= canal.instance.rds.instanceId= 重启canal 集群，还是无法获取到oss的binlog 这是什么原因呢？ 看实例日志，也没有关于oss的信息 这是错误信息 2018-10-30 16:33:20.395 [destination = instance   address = 地址    EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:instance[java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Could not find first log file name in binary log index file         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:102)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:153)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:262)         at java.lang.Thread.run(Thread.java:745) canal.instance.rds.accesskey，canal.instance.rds.secretkey运维给申请的，rds.instanceId的实例id也是没有问题的 oss binlog的回溯，目前只支持基于时间戳的定位，它会判定当前mysql binlog是否有该时间戳的数据，没有的话会从oss上去自动下载binlog，然后本地解析直到本地mysql binlog里时间能接上.  ps.  如果给了一个错误的binlog位点，不确定是给的位点错误，还是binlog被删除，后续可以针对RDS情况识别优化一下 @agapple  嗯，感谢你的回复，现在还有一个问题，就是我canal clients正常消费binlog，已经跑了有一段时间了，但是我看着zk中实例被消费的位置一直没有更新，这是什么原因呢？(canal的HA是用三台节点做的)， 如图： ![2121](https://user-images.githubusercontent.com/12511065/47762190-91bdca00-dcf6-11e8-8aaa-aec578ff67d1.jpg) 昨天下午看的也是这样，没有更新位置。 canal ha位点的更新策略是什么呀？我看文档是这么说的 **canal server在接收了客户端的ack后，就会记录客户端提交的最后位点** 但是我的位点没有被更新 1.  你server端没开启canal.instance.filter.transaction.entry=true吧？ 2. 检查下当前run的server节点，对应的meta.log，看下如果有ack成功会记录信息 @agapple  1. 每个节点我设置了canal.instance.filter.transaction.entry=true 2.没有看到运行节点有对应的meta.log   ... 嗯。。。我的集群搭建的有问题？😭 1. canal.instance.filter.transaction.entry=true这个设置的问题，过滤了事务头和尾，就没有可更新的位点 (位点记录以来begin/end事件)，新的v1.1.1有兼容这个参数，filter并不是过滤全部，而是会间隔几秒保留一个事务事件 2. rds binlog位点的问题，做了兼容处理，正常的位点被删除，只要位点里有timestamp，会自动下载oss binlog @agapple  再次感谢🙏， zk位点更新已经可以了，确实是canal.instance.filter.transaction.entry=true的问题引发的。 > oss binlog的回溯，目前只支持基于时间戳的定位，它会判定当前mysql binlog是否有该时间戳的数据，没有的话会从oss上去自动下载binlog，然后本地解析直到本地mysql binlog里时间能接上. >  > ps. 如果给了一个错误的binlog位点，不确定是给的位点错误，还是binlog被删除，后续可以针对RDS情况识别优化一下 您说的RDS优化是怎么做的呢？我现在还是无法确定，它到底有没有去oss上面下载我要的binlog； 优化我已经代码提交了，可以基于master重新打个包
1075,kafka的canal.instance.filter.transaction.entry问题 用的kafka模式，在canal.properties文件设置了canal.instance.filter.transaction.entry:true后 屏蔽了TransactionBegin事件，但是还可以收到Transactionend事件 是用的flatMessage:false么？可以尝试一下flat为true的情况
1071,canal1.1.1运行client中KafkaClientRunningTest报错 在1.1.0版本中是Message message = connector.getWithoutAck(3L  TimeUnit.SECONDS); 在1.1.1版本中是List<Message> messages = connector.getList(3L  TimeUnit.SECONDS); 运行1.1.0版本没有问题，可以看到message数据包，运行1.1.1到这一行代码处报错。 是否还不是稳定版本 把deploy中的配置mq.yml文件中flatMessage设置为false  KafkaClientRunningTest中为原生Message接收 配置mq.yml文件中flatMessage设置为false，KafkaClientRunningTest改为原生Message， connector.getWithoutAck(batchSize 5L  TimeUnit.SECONDS); 执行还是运行不了。 在 2018-10-30 09:45:54，"rewerma" <notifications@github.com> 写道： 把deploy中的配置mq.yml文件中flatMessage设置为false  KafkaClientRunningTest中为原生Message接收 — You are receiving this because you authored the thread. Reply to this email directly  view it on GitHub  or mute the thread. 报com.alibaba.otter.canal.protocol.exception.CanalClientException: mq not support this method connector.getWithoutAck(batchSize 5L  TimeUnit.SECONDS);这个不是kafkaConnector所提供的方法 测试类不要修改  就使用这段代码没有问题 ``` executor.submit(new Runnable() {             @Override             public void run() {                 connector.connect();                 connector.subscribe();                 while (running) {                     try {                         List<Message> messages = connector.getList(3L  TimeUnit.SECONDS);                         if (messages != null) {                             System.out.println(messages);                         }                         connector.ack();                     } catch (WakeupException e) {                         // ignore                     }                 }                 connector.unsubscribe();                 connector.disconnect();             }         }); ``` 使用上述测试类代码，flatMessage设置为false后，运行后没反应，把catch块改为Exception 后，看到报错信息为 `Caused by: com.alibaba.otter.canal.protocol.exception.CanalClientException: deserializer failed 	at com.alibaba.otter.canal.client.CanalMessageDeserializer.deserializer(CanalMessageDeserializer.java:52) 	at com.alibaba.otter.canal.client.CanalMessageDeserializer.deserializer(CanalMessageDeserializer.java:14) 	at com.alibaba.otter.canal.client.kafka.MessageDeserializer.deserialize(MessageDeserializer.java:24) 	at com.alibaba.otter.canal.client.kafka.MessageDeserializer.deserialize(MessageDeserializer.java:1) 	at org.apache.kafka.common.serialization.ExtendedDeserializer$Wrapper.deserialize(ExtendedDeserializer.java:65) 	at org.apache.kafka.common.serialization.ExtendedDeserializer$Wrapper.deserialize(ExtendedDeserializer.java:55) 	at org.apache.kafka.clients.consumer.internals.Fetcher.parseRecord(Fetcher.java:967) 	at org.apache.kafka.clients.consumer.internals.Fetcher.access$3300(Fetcher.java:93) 	at org.apache.kafka.clients.consumer.internals.Fetcher$PartitionRecords.fetchRecords(Fetcher.java:1144) 	at org.apache.kafka.clients.consumer.internals.Fetcher$PartitionRecords.access$1400(Fetcher.java:993) 	at org.apache.kafka.clients.consumer.internals.Fetcher.fetchRecords(Fetcher.java:527) 	at org.apache.kafka.clients.consumer.internals.Fetcher.fetchedRecords(Fetcher.java:488) 	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1155) 	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1115) 	at com.alibaba.otter.canal.client.kafka.KafkaCanalConnector.getListWithoutAck(KafkaCanalConnector.java:173) 	at com.alibaba.otter.canal.client.kafka.KafkaCanalConnector.getList(KafkaCanalConnector.java:159) 	at com.alibaba.otter.canal.client.running.kafka.KafkaClientRunningTest$1.run(KafkaClientRunningTest.java:44) 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) Caused by: com.google.protobuf.InvalidProtocolBufferException: Protocol message end-group tag did not match expected tag. 	at com.google.protobuf.InvalidProtocolBufferException.invalidEndTag(InvalidProtocolBufferException.java:110) 	at com.google.protobuf.CodedInputStream$ArrayDecoder.checkLastTagWas(CodedInputStream.java:660) 	at com.google.protobuf.CodedInputStream$ArrayDecoder.readGroup(CodedInputStream.java:869) 	at com.google.protobuf.UnknownFieldSet$Builder.mergeFieldFrom(UnknownFieldSet.java:541)` groupId 换一个  你消费到的还是老的faltMessage ok了，谢谢大师！
1069,pass useDruidDdlFilter parameter to LogEventConvert 有关tsdb功能变动的说明 1. tsdb应该只关心ddl  dml和dcl应该跳过  像 #450 这样的问题不会再出现，当然fastsql也应该支持dcl 2. 不管ddl是否过滤，tsdb的功能都应该保证正确性 3. ddl history写入放在过滤器后面可以保证在表结构对比的时候不存在权限问题 [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=1069) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=1069) before we can accept your contribution.<br/><hr/>**wuwo** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=1069) it.</sub> [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=1069) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=1069) before we can accept your contribution.<br/><hr/>**wuwo** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=1069) it.</sub> 这样的改法会有一个风险：假如fastsql parse在解析正常的DDL语句是时有异常，DdlResult返回的默认是Query，你这里就会忽略掉，连一点日志都没有 如果ddl解析的异常加上呢，现在ddl解析的异常都是忽略，然后直接返回Query ddl异常加上，效果我觉得和之前也没啥区别吧。 如果能正常解析，druidFilter就已经把不订阅的表给过滤了 我回滚了有关tsdb的变更，但是目前像你说的如果正常的ddl解析失败  会直接返回Query，如果开启了dcl过滤，这次ddl变更还是会丢失，正常的ddl解析，如果开启ddl过滤  ddl变更也会丢失  我还是觉得tsdb功能的正确性不应该和filterQueryDdl，filterQueryDml，filterQueryDcl绑上关系  估计很多用户也无法意识到这个问题 我想想有没有其它优雅一些的调整方式 tks
1066,binlog同步过程中ddl有丢失 mysql版本 5.6.26 canalserver版本 1.0.24 我们在实时同步binlog处理dml的过程中（万条/秒），发送了一条ddl语句，发现client端并没有拿到这条ddl语句，请问是什么原因。 canal.instance.get.ddl.isolation 和这个配置有关系吗？我用的是默认值false 关闭issue？原因自己找到了？ @yxzhang666  > 关闭issue？原因自己找到了？ @yxzhang666  我们的DBA添加表字段的逻辑和我们的处理方式不一致。我们通过CanalEntry.EventType.ALTER 获取ddl 并处理该ddl语句；dba是通过克隆表，更换表名之后添加的字段，然后把之前的表drop，再更改克隆表名为之前的表名，新表名我们没有去关注，所以没拿到该条ddl。严格来说，canal server并没有丢失该条binlog ，是我们没拿到，虚惊一场！ canal.instance.get.ddl.isolation，这个isolation我们要不要打开，有没有建议呢？ 这不是pt-schema-change-online和ghost的做法吗 canal.instance.get.ddl.isolation=true，如果打开的话，它会和其他DML分开多个batch，一个batch一个ddl，没有啥本质区别。可以用在做数据同步上 > 个batch一个ddl，没有啥本质区别。可以用在做数据同步上 明白了。谢谢！ > 这不是pt-schema-change-online和ghost的做法吗 不太了解这块。。
1065,fix: fix NPE  使用CanalServerWithManager时，开启GTID模式下，不传入masterPosition的情… 使用CanalServerWithManager时，开启GTID模式下，不传入masterPosition的情况下，会产生NPE [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=1065) <br/>All committers have signed the CLA. tks 你们CanalServerWithManager对接的是自己的控制台？ @agapple 我们是基于canal做增量和全量数据同步的平台 自己做了一套控制台咯？ > 自己做了一套控制台咯？ 目前还没有做页面，只是做了核心功能
1064,1.1.0版本golang客户端 报错信息：Fatal error: proto: Messages: illegal tag 0 (wire type 0)exit status 1 可以提供一下你是怎么调用的呢？  你可以去 [canal-go-issue](https://github.com/CanalClient/canal-go/issues) 提issue。 看了一下，这个报错发生在get操作proto那边解码的时候，而且当时数据库没有任何操作 我这边测试是pass的喔，现在依然报错吗？ 偶然出现的，不是一直有，而且当时数据库没有操作，应该没有数据才对 已经解决
1063,RocketMQ分区发送 没有commit RocketMQ分区发送 没有commit [https://github.com/alibaba/canal/blob/master/server/src/main/java/com/alibaba/otter/canal/rocketmq/CanalRocketMQProducer.java](url) 
1061,不正常的full gc问题 我们正在使用的是1.10版本，做了简单的二次开发实现数据发送到kafka多分区，通过pinpoint监控工具发现，服务端在堆内存使用到1.5G时就会触发full gc，但默认设置最大可用内存是3G。消费端没有此问题。启动用的start.sh内容没有修改过，具体原因正在排查，请问其他人是否有此问题。
1060,AdminGuide文档未更新 1. 操作系统     a.  纯java开发，windows/linux均可支持     b.  jdk建议使用1.6.25以上的版本，稳定可靠，目前阿里巴巴使用基本为此版本.    2. mysql要求    a. 目前canal支持mysql 5.5版本以下，对mysql5.6暂不支持，(mysql4.x版本没有经过严格测试，理论上是可以兼容)    改好了
1059,CanalConnector接口去掉stopRunning方法 CanalConnector接口去掉stopRunning方法  stopRunning方法仅在内部调用
1058,关于Docker模式的疑惑 1、docker stop container-id停不下来容器； 2、使用docker rm -f 移除容器，然而再次使用镜像启动容器时显示刚才被移除的容易已经存在，有时候还会存在11111端口被占，需要修改启动脚本里面的容器name才能启动容器； 求解释下，谢谢。 docker的基本操作，网上多找找吧 close
1057,kafka canal-adapter 分布式开关的bug 解决分布式同步开关关闭同步状态下  重启 kafka canal-adapter后再开启同步 丢失数据的bug
1056,group 线程池没有初始化的bug 
1055,代码整理 
1054,[ISSUE1027]Add flat message support to rocketmq 增加了对rocketmq对于hash后队列有序性支持，使其可以分发Flat message. tks
1053,canal是否支持事务分组 在数据库操作中包含事务性质的多表操作，在数据输出的时候能不能对这部分数据进行分组或打包为一个数据包。还是自己根据事务id进行分组 数据事务有begin/end进行包裹，业务上可以基于这块进行组合 > 数据事务有begin/end进行包裹，业务上可以基于这块进行组合 canal是否支持python对接，将数据同步到kafka 新的1.1.1版本会支持直接将数据投递到kafka，可用对应kafka的python客户端进行消费 > 新的1.1.1版本会支持直接将数据投递到kafka，可用对应kafka的python客户端进行消费 谢谢，那我直接可以配置好之后通过python从kafka取数据了是吧 是的 > 是的 我看到GitHub上的1.1.1版本中提交记录写道‘去掉kafka server端TRANSACTIONBEGIN和TRANSACTIONEND的判断’，这个版本要处理事务操作的话通过什么方式判断？有事务id吗？
1052,canal启动正常，客户端取不到binlog # mysql版本 5.7 # canal 1.1.1 # logs/example/example.log ``` 2018-10-28 09:20:28.615 [main] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2018-10-28 09:20:28.630 [main] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties] 2018-10-28 09:20:28.860 [main] WARN  o.s.beans.GenericTypeAwarePropertyDescriptor - Invalid JavaBean property 'connectionCharset' being accessed! Ambiguous write methods found next to actually used [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.lang.String)]: [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.nio.charset.Charset)] 2018-10-28 09:20:28.967 [main] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2018-10-28 09:20:28.967 [main] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties] 2018-10-28 09:20:29.289 [main] ERROR com.alibaba.druid.pool.DruidDataSource - testWhileIdle is true  validationQuery not set 2018-10-28 09:20:29.736 [main] INFO  c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-example 2018-10-28 09:20:29.916 [main] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - subscribe filter change to .*\..* 2018-10-28 09:20:29.916 [main] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - start successful.... 2018-10-28 09:20:29.918 [destination = example   address = /192.168.255.129:3306   EventParser] WARN  c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - prepare to find start position just show master status 2018-10-28 09:20:59.976 [New I/O server worker #1-1] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - subscribe filter change to .*\..* 2018-10-28 09:23:20.777 [destination = example   address = /192.168.255.129:3306   EventParser] WARN  c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - find start position : EntryPosition[included=false journalName=mysql-bin.000009 position=383 serverId=1 gtid=<null> timestamp=1540689592000] 2018-10-28 09:25:09.385 [New I/O server worker #1-2] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - subscribe filter change to .*\..* ~            ```     canal/canal.log报错: ``` 2018-10-28 12:13:15.534 [New I/O server worker #1-2] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x26e93058  /192.168.255.1:65515 => /192.168.255.129:11111]  exception=java.io.IOException: Connection reset by peer         at sun.nio.ch.FileDispatcherImpl.read0(Native Method)         at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)         at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)         at sun.nio.ch.IOUtil.read(IOUtil.java:192)         at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)         at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:322)         at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281)         at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201)         at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46)         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)         at java.lang.Thread.run(Thread.java:748) ``` Connection reset by peer  估计空闲链接被关闭了 > Connection reset by peer  估计空闲链接被关闭了 怎么解决？请指教 已解决。
1051,binlog变化client无响应 mysql、canal均使用docker方式部署，二者启动都正常，mysql binlog文件映射在宿主机的/var/lib/mysql/目录，操作数据库client端无响应，始终打印empty count，是否是binlog文件位置有问题？ mysql版本是5.7 一样的问题 检查canal server里对应的日志 我是使用单机模式，需要把parallel参数设置成false，但是错误日志只显示一个空指针，错误信息提示不太友好
1050,修改hbase配置项名 
1049,1.1.1版本配置kafka AbstractKafkaTest中的groupId是什么 
1047,tablemeta tsdb数据增加过期清理能力 TableMetaTSDB介绍：https://github.com/alibaba/canal/wiki/TableMetaTSDB 思考的问题：tablemeta默认会24小时做checkpoint生成一份snapshot数据持久化，随着运行时间越来越久，snapshot数据会持续膨胀，需要增加一个expire的策略，定时清理掉陈旧的数据 设计思路： 1.  增加两个参数：canal.instance.tsdb.snapshot.interval / canal.instance.tsdb.snapshot.expire 2. 在interval的运行频率内，对超过expire时间的snapshot数据做定时清理。注意：这里会保留初始化init的第一份snapshot(第一份的binlog_timestamp=-1) ，避免所有的snapshot数据被清理掉
1046,列类型是tinyint(1) unsigned，但实际数据值大于127时，canal获取的列值错误 列类型是tinyint(1) unsigned，但实际数据值大于127，在128～255之间，canal是按照boolean来处理的，直接转化成string，值变成了负数。在LogEventConvert.java的674行，建议修改如下(未充分测试)             // if (isSingleBit && javaType == Types.TINYINT) {             //     javaType = Types.BIT;             // }             if (buffer.isNull()) {                 columnBuilder.setIsNull(true);             } else {                 final Serializable value = buffer.getValue();                 if (isSingleBit && javaType == Types.TINYINT && ((Number) value).intValue() >= 0) {                     javaType = Types.BIT;                 }                 //... https://github.com/alibaba/canal/blob/master/parse/src/main/java/com/alibaba/otter/canal/parse/inbound/mysql/dbsync/LogEventConvert.java 方便提供一下测试的SQL？
1045,修改打包配置 tks
1044,canal 过滤的表达式 Expression is too large. Caused by: com.alibaba.otter.canal.filter.exception.CanalFilterException: org.apache.oro.text.regex.MalformedPatternException: Expression is too large. Caused by: org.apache.oro.text.regex.MalformedPatternException: Expression is too large. 	at org.apache.oro.text.regex.Perl5Compiler.compile(Unknown Source) 	at org.apache.oro.text.regex.Perl5Compiler.compile(Unknown Source) 	at com.alibaba.otter.canal.filter.PatternUtils$1.apply(PatternUtils.java:30) 	at com.alibaba.otter.canal.filter.PatternUtils$1.apply(PatternUtils.java:25) 	at com.google.common.collect.ComputingConcurrentHashMap$ComputingValueReference.compute(ComputingConcurrentHashMap.java:356) 	at com.google.common.collect.ComputingConcurrentHashMap$ComputingSegment.compute(ComputingConcurrentHashMap.java:182) 	at com.google.common.collect.ComputingConcurrentHashMap$ComputingSegment.getOrCompute(ComputingConcurrentHashMap.java:151) 	at com.google.common.collect.ComputingConcurrentHashMap.getOrCompute(ComputingConcurrentHashMap.java:67) 	at com.google.common.collect.MapMaker$ComputingMapAdapter.get(MapMaker.java:885) 	at com.alibaba.otter.canal.filter.PatternUtils.getPattern(PatternUtils.java:41) 	at com.alibaba.otter.canal.filter.aviater.RegexFunction.call(RegexFunction.java:24) 	at Script_1540529734651_0.execute0(Unknown Source) 	at com.googlecode.aviator.ClassExpression.execute(ClassExpression.java:53) 	at com.alibaba.otter.canal.filter.aviater.AviaterRegexFilter.filter(AviaterRegexFilter.java:74) 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEventForTableMeta(LogEventConvert.java:456) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$SimpleParserStage.onEvent(MysqlMultiStageCoprocessor.java:247) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$SimpleParserStage.onEvent(MysqlMultiStageCoprocessor.java:222) 	at com.lmax.disruptor.BatchEventProcessor.processEvents(BatchEventProcessor.java:168) 	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:125) 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) 这个问题很奇怪，我后面再执行的话，没有问题了。 你的表达式有多长啊？ 等等哈。我再仔细重现一下。 我debug一下。在 canal 表达式计算模块中: pattern="^pub_operation_log2.tb_operation_log_429$|^pub_operation_log2.tb_operation_log_428$|^pub_operation_log0.tb_operation_log_201$|^pub_operation_log2.tb_operation_log_427$|^pub_operation_log0.tb_operation_log_200$|^pub_operation_log2.tb_operation_log_426$|^pub_operation_log2.tb_operation_log_425$|^pub_operation_log2.tb_operation_log_424$|^pub_operation_log0.tb_operation_log_205$|^pub_operation_log2.tb_operation_log_423$|^pub_operation_log0.tb_operation_log_204$|^pub_operation_log2.tb_operation_log_422$|^pub_operation_log0.tb_operation_log_203$|^pub_operation_log2.tb_operation_log_421$|^pub_operation_log0.tb_operation_log_202$|^pub_operation_log2.tb_operation_log_420$|^pub_operation_log0.tb_operation_log_209$|^pub_operation_log0.tb_operation_log_208$|^pub_operation_log0.tb_operation_log_207$|^pub_operation_log0.tb_operation_log_206$|^pub_operation_log2.tb_operation_log_419$|^pub_operation_log2.tb_operation_log_418$|^pub_operation_log2.tb_operation_log_417$|^pub_operation_log2.tb_operation_log_41 。。。。。。" target= "tts_operation_log0.tb_operation_log_244" 我在 otter 中配置了即使用了分库也使用了分表： pub_operation_log[0-4] | tb_operation_log_[0-499]   我看了一下，确实是 。表名太长了，2500 个^pub_operation_log2.tb_operation_log_417$ 类似的这种表达式，我在跑demo 的时候，都跑不下去了。 idea 直接说：Error:(109  60) java: 常量字符串过长 在使用分库分表的时候，需要注意下：字符串不能太长。 你这是分库分表的吧  可以用正则匹配一下
1043,Canal无法解析mysql5.6的binlog求大神帮忙 操作系统：Centos6.5  数据库版本：mysql5.6 首先我在mysql的/etc/my.cnf文件中添加在这三条 [mysqld] server-id=1 log-bin=mysql-bin binlog-format=ROW 配置完查看了一下，如图： ![image](https://user-images.githubusercontent.com/32409300/47547957-f4dfe300-d929-11e8-9cfb-c6bac9679f58.png) 给我的mysql新增的了一个canal账户 开启了所有的权限。Repl_slave_priv=Y ![image](https://user-images.githubusercontent.com/32409300/47548081-6750c300-d92a-11e8-942b-9b5bba436b97.png) 监测canal的日志tail -f logs/example/example.log ，开始启动canal  日志内容如下图： ![image](https://user-images.githubusercontent.com/32409300/47548180-bdbe0180-d92a-11e8-9a64-7608dccd144f.png) 我的canal配置项： ![image](https://user-images.githubusercontent.com/32409300/47548240-05dd2400-d92b-11e8-8dc2-d7a1bced6de3.png) 这个参数不在截图中：canal.instance.filter.regex=.*\\..* 执行sql查看binlog日志中的文件是有变化的： ![image](https://user-images.githubusercontent.com/32409300/47548333-5785ae80-d92b-11e8-9d0f-5ed4882955b9.png) 上图中查看binlog的姿势有点问题，正确的姿势是：/usr/bin/mysqlbinlog -v mysql-bin.000003 ![image](https://user-images.githubusercontent.com/32409300/47548446-a7647580-d92b-11e8-9bd1-69c495fb00b7.png) test是我的数据库名，user是我的表名。 我执行完sql之后 canal日志没有发生任何改变。 canal客户端可以正常连接到canal服务端。但是canal服务端无法正常解析mysql的binlog，求大神帮忙解决。 如果哪里描述的不清晰请大神指正。 example工程是打印到另外的目录的，仔细找找 打印到另外的目录的？大神的意思是如果产生了binlog日志，在logs/example/example.log这个日志文件中是看不到的是吗 @agapple  ![image](https://user-images.githubusercontent.com/32409300/47554077-1dbca400-d93b-11e8-88ce-4cf6082b9646.png) 
1042,canal adapter readme.md 
1041,canal.example-1.1.0.tar.gz包下的startup.bat脚本提示命令语法不正确 我目前是在windows7环境下进行测试的 语法报错的内容，或者提交一个修复的PR给我 ![2018-10-29_134009](https://user-images.githubusercontent.com/12081348/47631335-a710e800-db80-11e8-8825-06467c7045da.png) 很低级的问题，基本的bat用法你先google一下
1040,canal集群当挂掉一个后zk节点会被删除，另一个sever起来后数据会重复吗 canal集群当挂掉一个后zk节点会被删除，另一个sever起来后数据会重复吗 数据会重复 > 数据会重复 重复是正常的？ @agapple  无法避免数据不重复，多看wiki
1039,规范注释 tks
1038,Canal 发送到Kafka Topic 消息的同时，实现Partition 分区 Canal 1.1.0 版本消息发送到Kafka的同时，支持指定消息按Partition Key 分发到不同的分区吗？ https://github.com/alibaba/canal/wiki/Canal-Kafka-RocketMQ-QuickStart，即将发布的1.1.1支持这样的散列功能
1037,完善adapter-launcher及hbase-adaper相关功能 完善adapter-launcher 完善hbase-adaper 新增hbase etl功能 增加客户端同步开关  etl同步锁等功能 tks
1036,Docker部署canal后客户端连接不了 首先canal在docker已经部署启动完毕，服务端口11112 然后docker是有宿主IP 和 虚拟IP，本机电脑访问是通过虚拟IP：192.168.102.111:6633 然后客户端使用192.168.102.111:6633连接canal，启动服务之后客户端接收不到信息，而canal服务端一直输出c.a.otter.canal.server.netty.handler.SessionHandler - message receives in session handler... 请问IP问题怎么解决？ 按照Docker QuickStart里的脚本，启动为host模式，并做端口映射
1034,如何将canal 客户端接收到的CanalEntry 解析成可执行的sql 语句 Master Mysql A 中执行的sql  然后 Slave  canal  接收到CanalEntry 的对象，如何解析该对象 成为 可执行的sql  然后同步到数据库 mysql B中呢？费了很大的劲拼装sql 发现自己拼装的要不情景不全，要不各种瑕疵，有没有方便的方法呢？ 1.1.0 版本中 DDL操作可以直接拿到sql  那DML操作是否可以直接拿到sql呢？ canal.example工程里，看下同步到DB的样例代码：https://github.com/alibaba/canal/tree/master/example/src/main/java/com/alibaba/otter/canal/example/db
1032,v1.1.1-alpha 1 版本没有添加可配置的row参数，默认是true。 v1.1.1-alpha 1 版本没有添加可配置的row参数，默认是true。 使用CanalServerWithEmbedded的还是存在Entry解析性能问题 ![Uploading image.png…]() ![image](https://user-images.githubusercontent.com/34024756/47475509-474bd180-d84e-11e8-88f7-bc45e2df4483.png) ![image](https://user-images.githubusercontent.com/34024756/47475517-4f0b7600-d84e-11e8-8266-425e7162b102.png) 
1031,新增adapter-launcher工程 新增adapter-launcher springboot工程
1030,启动一直不动也不报错，日志有这个 2018-10-24 11:08:47.331 [destination = canal-car   address = rm-m5e5o46l2f13j7an8.mysql.rds.aliyuncs.com/172.31.19.222:3306   EventParser] WARN  c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - prepare to find start position just show master status 2018-10-24 11:08:52.651 [destination = canal-car   address = rm-m5e5o46l2f13j7an8.mysql.rds.aliyuncs.com/172.31.19.222:3306   EventParser] WARN  c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - find start position : EntryPosition[included=false journalName=mysql- bin.000570 position=1244577 serverId=1797136470 gtid=<null> timestamp=1540350517000] 这个不影响。
1029,[v1.1.0]在mysql上更新t_canal_table表约30w条记录的同一字段(容量:755B/条 字段数量：79)，canal解析bin log报错：column size is not match for table:  database_canal.t_canal_table 79 vs 78 更新语句：sql = "UPDATE t_canal_table SET update_time= " + "'" + str(datetime.datetime.now()) + "'"  异常信息： Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: column size is not match for table:glocalme_css_tcanal.t_css_vsim 79 vs 78 ] 2018-10-10 11:37:10.242 [destination = canalEsInstance   address = /192.168.0.135:3306   EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address 192.168.0.135/192.168.0.135:3306 has an error  retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. 表存在ddl列删除操作，需要开启TSDB才可以支持 @agapple 此操作没有进行DDL删除操作
1028,Fix Kafka FlatMessage Model Null Value set to "" canal 使用kafka model 以及flatMessage 的情况下， 数据库null值被设置成"". 现象如下 ![image](https://user-images.githubusercontent.com/7855069/47401555-6d04a800-d774-11e8-99a6-5b89b0338123.png) description 为null 情况下，kafka message body 值为"" 建议返回null  否则varchar 类型null无法区分 ![image](https://user-images.githubusercontent.com/7855069/47401616-a9d09f00-d774-11e8-9305-e2ba1c94d932.png) [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=1028) <br/>All committers have signed the CLA. [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=1028) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=1028) before we can accept your contribution.<br/><hr/>**titeng.jiang** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=1028) it.</sub>
1027,RocketMQ 增加对FlatMessage的支持 为了提升rocketmq binlog分发性能，需要增加数据分片支持。 如果增加了分片数据一致性就不好保证了 这个是否分片应该是根据用的选择，做到表级别是否开启。针对主键不会发生修改的或者只是监听一下事件本身不关注顺序的，应该是比较适合 目前我们这边想针对每个mysql数据库实例做一个topic，不同的数据库实例不同的topic，然后Tag使用库名+表名做区分。针对个别需要顺序的table做指定队列发送，如果正好本库就有这样的表那就整个库都是发送到一个队列的，需要将对应配置转移到expmle中进行配置 代码已提交
1026,canal varchar null处理 使用canal-kafka，收到消息，column值为null被设置成""   int   date 类型收到"" 可以默认理解为null 对于varchar  text类型如何区分 null和"" 单个column对象有个getisNull的方法来判断这个列对应的value是不是null CanalEntry.Column 这个类 LS正解
1025,destination:user_cards[com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.TableIdNotFoundException: not found tableId:21005 环境：Ubuntu 16.04 jdk：1.8 canal环境：一个canal server，两个instance， canal版本：canal.kafka-1.1.0 出现问题原因： 1、删除zk上/otter znode  目的：重置instance的binlog file和Position 详细日志： 2018-10-23 16:05:48.880 [pool-6-thread-2] ERROR com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - process error! java.lang.NullPointerException: null 	at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.calculateSize(MemoryEventStoreWithBuffer.java:555) ~[canal.store-1.1.0.jar:na] 	at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.doGet(MemoryEventStoreWithBuffer.java:322) ~[canal.store-1.1.0.jar:na] 	at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.tryGet(MemoryEventStoreWithBuffer.java:261) ~[canal.store-1.1.0.jar:na] 	at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getEvents(CanalServerWithEmbedded.java:478) ~[canal.server-1.1.0.jar:na] 	at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:310) ~[canal.server-1.1.0.jar:na] 	at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:273) ~[canal.server-1.1.0.jar:na] 	at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.worker(CanalKafkaStarter.java:121) [canal.kafka-1.1.0.jar:na] 	at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.access$000(CanalKafkaStarter.java:26) [canal.kafka-1.1.0.jar:na] 	at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter$1.run(CanalKafkaStarter.java:70) [canal.kafka-1.1.0.jar:na] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_181] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_181] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_181] 2018-10-23 16:05:48.881 [pool-6-thread-2] INFO  com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - ## the canal consumer user_cards is running now ...... 2018-10-23 16:05:48.881 [pool-6-thread-2] ERROR com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - process error! java.lang.NullPointerException: null 	at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.calculateSize(MemoryEventStoreWithBuffer.java:555) ~[canal.store-1.1.0.jar:na] 	at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.doGet(MemoryEventStoreWithBuffer.java:322) ~[canal.store-1.1.0.jar:na] 	at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.tryGet(MemoryEventStoreWithBuffer.java:261) ~[canal.store-1.1.0.jar:na] 	at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getEvents(CanalServerWithEmbedded.java:478) ~[canal.server-1.1.0.jar:na] 	at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:310) ~[canal.server-1.1.0.jar:na] 	at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:273) ~[canal.server-1.1.0.jar:na] 	at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.worker(CanalKafkaStarter.java:121) [canal.kafka-1.1.0.jar:na] 	at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.access$000(CanalKafkaStarter.java:26) [canal.kafka-1.1.0.jar:na] 	at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter$1.run(CanalKafkaStarter.java:70) [canal.kafka-1.1.0.jar:na] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_181] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_181] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_181] 我暴力的删除的了zk中的znode，重启canal的时候他会重新加载instance和canal server的配置么？ 停止canal server，再删除zknode，最后重启，这样可以生效instance里的位点定义。  ps.  为什么需要停止server再删除，因为server如果不停止会有定时刷新zknode，所以提前删除zknode没任何意义 btw，我停止canal server之后，删除znode，然后再从上游数据库show master status拿出来binlog file和Position，重启canal server没问题，canal server和instance都不报错； 但是，一旦上游库有update操作，canal server会一直报这个错，instance不报错 。 用最新的v1.1.1的版本  canal.deployer部署一下kafka的同步   已知问题修复了.  ps.  新版本已经合并了canal.kafka工程到canal.deployer中  主要修改一下canal.properties里的canal.serverMode=kafka即可 这个问题，谢谢 2018-10-23 22:02:58.642 [pool-4-thread-1] ERROR com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - process error! java.lang.NullPointerException: null 	at com.alibaba.otter.canal.store.helper.CanalEventUtils.createPosition(CanalEventUtils.java:69) ~[canal.store-1.1.0.jar:na] 	at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:375) ~[canal.store-1.1.0.jar:na] 	at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:36) ~[canal.store-1.1.0.jar:na] 	at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:307) ~[canal.server-1.1.0.jar:na] 	at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:273) ~[canal.server-1.1.0.jar:na] 	at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.worker(CanalKafkaStarter.java:121) [canal.kafka-1.1.0.jar:na] 	at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.access$000(CanalKafkaStarter.java:26) [canal.kafka-1.1.0.jar:na] 	at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter$1.run(CanalKafkaStarter.java:70) [canal.kafka-1.1.0.jar:na] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_131] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_131] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131]
1024,canal 怎么支持DRDS数据库 [destination = example1   address = *************************:3306   EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 1047  sqlstate = HY000 errmsg = [dc26f11c5801000][172.16.22.194:3306][zbtsteam_test]Unknown command @daoshunli canal是模拟mysql的binlog dump协议，DRDS目前没有对binlog协议有直接支持，可以直连DRDS底下的RDS来直接订阅
1023,c.a.o.canal.parse.inbound.mysql.dbsync.LogEventConvert - table parser error WARN  c.a.o.canal.parse.inbound.mysql.dbsync.LogEventConvert - table parser error : header{} 今天突然遇到这样一个异常，导致这张表同步不成功。 之前还是正常的，今天突然发现有这个异常，是怎么回事呢？ 估计是binlog里对应的表被删除，重置位点跳过，后续可以开启TSDB来规避
1022,FlatMessage直接生产到kafka，然后kafka消费端解析效率不是更高么，感觉probuf比json解析效率高啊 对象序列化会有很多类信息消息体大概比json大了3倍  吞吐量下降明显
1021,1.0.24 canal hv模式下解析mysql timestamp问题 单机情况下解析mysql timestamp 数据返回是正确 实例： mysql 时间 2018-10-19 19:09:09 单机解析过来，canal获取到是 2018-10-19 19:09:09            hv模式下 解析获取到为2018-10-19 07:02:09 相差为12小时 单机和hv为同一个服务器 hv三台服务器时间均为CTC时区 时间也是正确的，有没有大佬知道是什么问题导致            hv是啥？canal是以binlog里的数据为准，可以先通过mysqlbinlog看一下原始binlog数据 打错了 HA模式，昨天邮箱一直验证不了，就没回复。昨天因为生产环境问题，我在测试环境重新安装了canal环境，HA模式下解析timestamp类型是没问题的。
1020,canal-kafka 数据同步到kafka之后，kafka topic乱码 ![default](https://user-images.githubusercontent.com/6134206/47210704-34587d80-d3c6-11e8-805c-0c7ed3772058.png) canal启动之后，数据能同步到kafka 但是同步的数据乱码 canal版本是：1.1.0，单节点部署的，本地查看topic乱码 kafka默认传输的是序列化Message字节码 不是可见字符 需用client中的kafka模块反序列化为Message 喔，好的 在下个版本会支持String和Message 你可以改动canal-server的包中kafka相关发送代码(参照主分支的代码 有提供) 先用着 等新版本出来后就好了. 在下个版本会支持String和Message 你可以改动canal-server的包中kafka相关发送代码(参照主分支的代码 有提供) 先用着 等新版本出来后就好了. 我用的python客户端消费kafka，希望能支持明文发送json数据导kafka，方便消费，不用考虑反序列化的问题。
1019,canal 1.1.0 的Entry解析性能问题 这里entry.toByteString()是基于什么考虑？我们从canal 1.0.24升级到1.1.0，发现性能差了很多。 我看到这个地方，已经拿到Entry对象了，应该可以缓存起来，这样业务上就不需要再次做一次parse了。 ```     public Event(LogIdentity logIdentity  CanalEntry.Entry entry){         this.logIdentity = logIdentity;         this.entryType = entry.getEntryType();         this.executeTime = entry.getHeader().getExecuteTime();         this.journalName = entry.getHeader().getLogfileName();         this.position = entry.getHeader().getLogfileOffset();         this.serverId = entry.getHeader().getServerId();         this.gtid = entry.getHeader().getGtid();         this.eventType = entry.getHeader().getEventType();         // build raw         this.rawEntry = entry.toByteString();         this.rawLength = rawEntry.size();         if (entryType == EntryType.ROWDATA) {             List<CanalEntry.Pair> props = entry.getHeader().getPropsList();             if (props != null) {                 for (CanalEntry.Pair p : props) {                     if ("rowsCount".equals(p.getKey())) {                         rowsCount = Integer.parseInt(p.getValue());                         break;                     }                 }             }         }     } ``` 类似的还有这里，其实已经有RowChange了，但是都被序列化成bytes了。 ```             RowChange rowChange = rowChangeBuider.build();             if (tableError) {                 Entry entry = createEntry(header  EntryType.ROWDATA  ByteString.EMPTY);                 logger.warn("table parser error : {}storeValue: {}"  entry.toString()  rowChange.toString());                 return null;             } else {                 Entry entry = createEntry(header  EntryType.ROWDATA  rowChange.toByteString());                 return entry;             } ``` 我们用的是 CanalServerWithEmbedded entry.toByteString()主要是针对server/client这样的部署模式，避免在client.get操作时序列化周期过长导致client tps上不去 可以考虑增加一个配置，允许保留entry对象，这样会带来内存上的额外开销
1017,1.1.0版本的canal的RDS配置属性   支持使用manager的方法么？ canalInstanceWithManager 这种形式的配置 是不是还不支持RDS新加的那3个配置 可以考虑提交一个PR给我 已修复
1016,每天遇到一次 should execute connector.connect() first 异常，是什么原因。 otter每天遇到一次下面的异常，看堆栈是连接中断，需要重联。 但是遇到这种错误，有时同步迅速就恢复了，有时同步会中断10分钟左右。是什么原因，如何解决？ pid:4 nid:3 exception:canal:hd_canal_236:com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first     at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4832)     at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:160)     at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:759)     at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:428)     at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:114)     at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:66)     at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:337)     at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:184)     at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:145)     at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220)     at java.lang.Thread.run(Thread.java:724) Caused by: java.io.IOException: should execute connector.connect() first     at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.(MysqlQueryExecutor.java:30)     at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:87)     at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMetaByDB(TableMetaCache.java:80)     at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:30)     at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:55)     at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:50)     at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527)     at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319)     at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282)     at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197)     at com.google.common.cache.LocalCache.get(LocalCache.java:3937)     at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941)     at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824)     at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830) 对应的otter版本是啥？ 建议升级最新版，已有处理
1015,求指导：canal获取的事务号(tranactionID)和mysql中的事务号(transactionID)不一致，如何解决？  1.canal的transactionID获取方法： TransactionEnd txEnd = TransactionEnd.parseFrom(entry.getStoreValue()); String txID = txEnd.getTransactionId(); 结果：46 2. mysql中查询结果： Trx id counter 88363                          你mysql中怎么一个查询办法？是从mysqlbinlog命令中获取？ 谢谢您的回复！不好意思，之前查询的方法错了。canal获取的transactionID没有问题。 现使用的查询方法： sudo mysqlbinlog --no-defaults  /usr/local/mysql/data/mysql-bin.000149 只是MySQL的DDL语句是非事务的，不能对DLL语句进行回滚操作，请问canal是如何处理ddl的呢？ canal只是解析binlog里的DDL，binlog里记录的内容已经事务提交成功的
1014,修复通过`show master status`获取到多个GTID 时，回车符导致UUID 解析失败的bug as title tks
1013,canal.kafka 用bin/kafka-console-consumer.sh命令收到乱码 在mysql数据库中插入一条记录后，用命令bin/kafka-console-consumer.sh --zookeeper 192.168.206.128:2181 --topic canal1 --from-beginning，看到消费端输出乱码： `*: mysql-bin.000027*UTF-80BJP229 *7 mysql-bin.000027*UTF-80BJPF mysql-bin.000027*UTF-80Bcan_dbJ      user_testP+Xb         _-+_C-+++1Pb_id (0B6Ri++(11)                                                        +a+e (0BdafdR                                                                         +a_cha_(40)age (0B15Ri++(11): +y_-+-bi+.000027 *UTF-80 8BJP261 *7 +y_-+-bi+.000027 *UTF-80 8BJPF  +y_-+-bi+.000027 *UTF-80 8Bca+_dbJ      +_e__+e_+P*Xb         _-+_C-+++1Pb^id (0B7Ri++(11)                                                        +a+e (0BeeeR                                                                        +a_cha_(40)age (0B11Ri++(11): +y_-+-bi+.000027 *UTF-80 8BJP262` 然后看了kafka-client中的代码，改了AbstractKafkaTest.java相应的配置参数，如 `public static String  topic     = "canal1";     public static Integer partition = 0;     public static String  groupId   = "";     public static String  servers   = "192.168.206.128:6667";     public static String  zkServers = "192.168.206.128:2181";` 后，当往mysql中插入新记录后，但运行KafkaClientRunningTest.java没有收到数据包。各位有没有遇到过这种情况。 canal kafka 模式默认是发送Message对象的序列化字节不是可见字符串 client接收到会反序列化. 如要发送可见字符串 请用 flatMessage模式 将会发送json格式数据 @brightsong 我这边也碰到一样的问题。请问你的问题解决了吗？ > 在mysql数据库中插入一条记录后，用命令bin/kafka-console-consumer.sh --zookeeper 192.168.206.128:2181 --topic canal1 --from-beginning，看到消费端输出乱码： > `�*��: ���mysql-bin.000027�*UTF-80BJP����229 �*�7 ���mysql-bin.000027�*UTF-80BJPF���� �����mysql-bin.000027�*UTF-80B�can_dbJ user_testP+X�b _-+_C-+++��1������Pb_�����id �(�0B�6Ri++(11)��� ��+a+e (�0B�dafdR +a_cha_(40)�����age (�0B�15Ri++(11)�: ���+y_-+-bi+.000027�� �*UTF-80 8�BJP����261 �*��7 ���+y_-+-bi+.000027�� �*UTF-80 8�BJPF���� ��� ���+y_-+-bi+.000027�� �*UTF-80 8�B�ca+_dbJ +_e__+e_+P*X�b _-+_C-+++��1������Pb^�����id �(�0B�7Ri++(11)��� ��+a+e (�0B�eeeR +a_cha_(40)�����age (�0B�11Ri++(11)�: ���+y_-+-bi+.000027�� �*UTF-80 8�BJP����262` > 然后看了kafka-client中的代码，改了AbstractKafkaTest.java相应的配置参数，如 > `public static String topic = "canal1"; public static Integer partition = 0; public static String groupId = ""; public static String servers = "192.168.206.128:6667"; public static String zkServers = "192.168.206.128:2181";` > 后，当往mysql中插入新记录后，但运行KafkaClientRunningTest.java没有收到数据包。各位有没有遇到过这种情况。 我和你一样遇到相同的问题。当把接收格式改为ConsumerRecords<String  String> 可以接收到输出，但是原生的ConsumerRecords<String  Message> 没有消息。请问你的问题解决了吗？ > canal kafka 模式默认是发送Message对象的序列化字节不是可见字符串 client接收到会反序列化. > 如要发送可见字符串 请用 flatMessage模式 将会发送json格式数据 请问对应的是哪个配置文件中的哪个配置项的呢？ 解决了，把虚拟机cpu改的大一点就可以了。如果不行再修改canal.instance.parser.parallel 参数。 在 2018-10-22 11:06:28，"jkl0898" <notifications@github.com> 写道： @brightsong 我这边也碰到一样的问题。请问你的问题解决了吗？ 在mysql数据库中插入一条记录后，用命令bin/kafka-console-consumer.sh --zookeeper 192.168.206.128:2181 --topic canal1 --from-beginning，看到消费端输出乱码： �*��: ���mysql-bin.000027�*UTF-80BJP����229 �*�7 ���mysql-bin.000027�*UTF-80BJPF���� �����mysql-bin.000027�*UTF-80B�can_dbJ user_testP+X�b _-+_C-+++��1������Pb_�����id �(�0B�6Ri++(11)��� ��+a+e (�0B�dafdR +a_cha_(40)�����age (�0B�15Ri++(11)�: ���+y_-+-bi+.000027�� �*UTF-80 8�BJP����261 �*��7 ���+y_-+-bi+.000027�� �*UTF-80 8�BJPF���� ��� ���+y_-+-bi+.000027�� �*UTF-80 8�B�ca+_dbJ +_e__+e_+P*X�b _-+_C-+++��1������Pb^�����id �(�0B�7Ri++(11)��� ��+a+e (�0B�eeeR +a_cha_(40)�����age (�0B�11Ri++(11)�: ���+y_-+-bi+.000027�� �*UTF-80 8�BJP����262 然后看了kafka-client中的代码，改了AbstractKafkaTest.java相应的配置参数，如 public static String topic = "canal1"; public static Integer partition = 0; public static String groupId = ""; public static String servers = "192.168.206.128:6667"; public static String zkServers = "192.168.206.128:2181"; 后，当往mysql中插入新记录后，但运行KafkaClientRunningTest.java没有收到数据包。各位有没有遇到过这种情况。 我和你一样遇到相同的问题。当把接收格式改为ConsumerRecords<String  String> 可以接收到输出，但是原生的ConsumerRecords<String  Message> 没有消息。请问你的问题解决了吗？ — You are receiving this because you were mentioned. Reply to this email directly  view it on GitHub  or mute the thread. 不好意思，看错了，暂时还没有解决 在 2018-10-22 11:06:28，"jkl0898" <notifications@github.com> 写道： @brightsong 我这边也碰到一样的问题。请问你的问题解决了吗？ 在mysql数据库中插入一条记录后，用命令bin/kafka-console-consumer.sh --zookeeper 192.168.206.128:2181 --topic canal1 --from-beginning，看到消费端输出乱码： �*��: ���mysql-bin.000027�*UTF-80BJP����229 �*�7 ���mysql-bin.000027�*UTF-80BJPF���� �����mysql-bin.000027�*UTF-80B�can_dbJ user_testP+X�b _-+_C-+++��1������Pb_�����id �(�0B�6Ri++(11)��� ��+a+e (�0B�dafdR +a_cha_(40)�����age (�0B�15Ri++(11)�: ���+y_-+-bi+.000027�� �*UTF-80 8�BJP����261 �*��7 ���+y_-+-bi+.000027�� �*UTF-80 8�BJPF���� ��� ���+y_-+-bi+.000027�� �*UTF-80 8�B�ca+_dbJ +_e__+e_+P*X�b _-+_C-+++��1������Pb^�����id �(�0B�7Ri++(11)��� ��+a+e (�0B�eeeR +a_cha_(40)�����age (�0B�11Ri++(11)�: ���+y_-+-bi+.000027�� �*UTF-80 8�BJP����262 然后看了kafka-client中的代码，改了AbstractKafkaTest.java相应的配置参数，如 public static String topic = "canal1"; public static Integer partition = 0; public static String groupId = ""; public static String servers = "192.168.206.128:6667"; public static String zkServers = "192.168.206.128:2181"; 后，当往mysql中插入新记录后，但运行KafkaClientRunningTest.java没有收到数据包。各位有没有遇到过这种情况。 我和你一样遇到相同的问题。当把接收格式改为ConsumerRecords<String  String> 可以接收到输出，但是原生的ConsumerRecords<String  Message> 没有消息。请问你的问题解决了吗？ — You are receiving this because you were mentioned. Reply to this email directly  view it on GitHub  or mute the thread. > canal kafka 模式默认是发送Message对象的序列化字节不是可见字符串 client接收到会反序列化. > 如要发送可见字符串 请用 flatMessage模式 将会发送json格式数据 请问在哪里可以配置呢？ 我也没有配置，我是这样的，接受到Message对象后，解析里面的内容，然后把内容放在json中发送到另一个kafka中。 在 2018-10-23 17:12:59，"liugaozy" <notifications@github.com> 写道： canal kafka 模式默认是发送Message对象的序列化字节不是可见字符串 client接收到会反序列化. 如要发送可见字符串 请用 flatMessage模式 将会发送json格式数据 请问在哪里可以配置呢？ — You are receiving this because you were mentioned. Reply to this email directly  view it on GitHub  or mute the thread.
1012,canal能获取mysql中LSN（Log sequence number）吗？ binlog没有这 这样啊，谢谢
1011,Kafka模式，client offset不前进 Kafka模式，client offset不前进，来来回回就消费那么几条数据，没报错
1010,最新1.1.1-SNAPSHOT启用gtid模式，还是会报错errno = 1236 1.1.1-SNAPSHOT版本启用gtid模式。源数据库gtid_purged有值，启动后报错。无法同步 ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = The slave is connecting using CHANGE MASTER TO MASTER_AUTO_POSITION = 1  but the master has purged binary logs containing GTIDs that the slave requires. at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:102) 拿着这异常区google，mysql配置问题 > 拿着这异常区google，mysql配置问题 mysql是配置的gtid模式，gtid_purged也有值。网上搜索此错误得到的解决方案都是：停止slave同步然后在slave上设置gtid_purged。 但对于模拟slave的canal来说要如何设置gtid_purged呢？ https://dev.mysql.com/doc/refman/5.6/en/replication-mode-change-online-enable-gtids.html 参考这个操作设置mysql server
1009,meta.dat not update [canal 1.1.0]  Canal server and client consumes data normally  but the timestamp and content of meta.dat not change  which will cause an exception 'Could not find first log file name in binary log index file' when restart canal server. Pls check the configurations in attachment. [instance.log](https://github.com/alibaba/canal/files/2486428/instance.log) [canal.log](https://github.com/alibaba/canal/files/2486429/canal.log) Could not find first log file name in binary log index file     代表mysql主机上binlog已被删除 I got more information with adding log in canal server. something unexpected occurs the updateCursor does not execute. ![image](https://user-images.githubusercontent.com/9636488/47079731-b9ae2780-d238-11e8-8fe6-41fedf6ac8df.png) finally  I compile a new canal server with source code and deploy  the meta.dat was updated normally.
1008,对于mysql enum类型 canal默认返回int（即1，2，3，...），如何返回enum中的value？ 对于mysql enum类型 canal默认返回int（即1，2，3，...），现需要canal返回enum类型字段的具体值value，如何实现呢？ // *********** 我之前的做法： 1. 修改parser模块的rowslogbuffer的mysqlToJavaType方法，对于enum返回Types.CHAR;  2. 修改rowslogbuffer的fetchValue方法，对于enum类型按照string类型修改，即返回string，但是报错： Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: java.lang.IllegalArgumentException: limit excceed: 63 	at com.taobao.tddl.dbsync.binlog.LogBuffer.getFullString(LogBuffer.java:1122) <img width="1125" alt="enum1" src="https://user-images.githubusercontent.com/28953872/47005872-44beed00-d167-11e8-83a9-bb56a2cae283.png"> <img width="1121" alt="enum2" src="https://user-images.githubusercontent.com/28953872/47005873-45578380-d167-11e8-8512-1b0eee9b05cc.png"> <img width="981" alt="enum3" src="https://user-images.githubusercontent.com/28953872/47005875-45578380-d167-11e8-84a4-b942af2f2816.png"> 请教大神该如何修改呢？ 不要在binlog解析里修改，可以在外围拿到表结构，基于int下表找到具体的文本进行替换，比如LogEventConvert 大神，您说的“找到具体的文本进行替换”是不是指“根据enum的下标索引从fieldMeta.getColumnType()中解析对应的内容”？如下所示： fieldMeta.getColumnType() is: enum('100' '200' '300' '400' '500' '600') >>>>>> columnType.startsWith("enum") >>>>>> String.valueOf(value): 3 从'100' '200' '300' '400' '500' '600'中解析第3个元素300？ 但这样万一enum中的元素包含特殊字符比如逗号 或者单引号'怎么办？ 还是说有其他的办法？ 求指教！ @agapple  枚举特殊字符的各种情况，实际测试一下 好的，谢谢指导！
1007,restart canal-server  canal stuck at at sun.nio.ch.Net.poll(Native Method) Server and client works normally  restart server   client will disconnect and try to connect and subscribe  while I found client thread stuck. Do someone have got this  or come up with some ideas about this. Callstack and information as follow: 2018-10-16 10:01:41  ERROR [CustomCanalClient]  Main Loop outter Exception.com.alibaba.otter.canal.protocol.exception.CanalClientException: java.io.IOException: end of stream when reading header 2018-10-16 10:01:41  WARN [HttpConnectionPool] object == null 2018-10-16 10:01:41  TRACEBACK [CustomCanalClient]com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:317)  com.alibaba.otter.canal.custom.CustomCanalClient.process(CustomCanalClient.java:106)  com.alibaba.otter.canal.custom.CustomCanalClient$1.run(CustomCanalClient.java:67)  java.lang.Thread.run(Thread.java:748)  2018-10-16 10:01:43  ERROR [CustomCanalClient]  Main Loop outter Exception.com.alibaba.otter.canal.protocol.exception.CanalClientException: java.net.ConnectException: Connection refused 2018-10-16 10:01:43  WARN [HttpConnectionPool] object == null 2018-10-16 10:01:43  TRACEBACK [CustomCanalClient]com.alibaba.otter.canal.client.impl.SimpleCanalConnector.doConnect(SimpleCanalConnector.java:190)  com.alibaba.otter.canal.client.impl.SimpleCanalConnector.connect(SimpleCanalConnector.java:114)  com.alibaba.otter.canal.custom.CustomCanalClient.process(CustomCanalClient.java:101)  com.alibaba.otter.canal.custom.CustomCanalClient$1.run(CustomCanalClient.java:67)  java.lang.Thread.run(Thread.java:748)    java.lang.Thread.State: RUNNABLE         at sun.nio.ch.Net.poll(Native Method)         at sun.nio.ch.SocketChannelImpl.poll(SocketChannelImpl.java:954)         - locked <0x0000000094054d90> (a java.lang.Object)         at sun.nio.ch.SocketAdaptor$SocketInputStream.read(SocketAdaptor.java:204)         - locked <0x0000000094054d80> (a java.lang.Object)         at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)         - locked <0x0000000094054e98> (a sun.nio.ch.SocketAdaptor$SocketInputStream)         at java.nio.channels.Channels$ReadableByteChannelImpl.read(Channels.java:385)         - locked <0x0000000094054f08> (a java.lang.Object)         at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.read(SimpleCanalConnector.java:429)         at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.readNextPacket(SimpleCanalConnector.java:419)         - locked <0x00000000c038e900> (a java.lang.Object)         at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.readNextPacket(SimpleCanalConnector.java:403)         at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.receiveMessages(SimpleCanalConnector.java:322)         at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:315)         at com.alibaba.otter.canal.custom.CustomCanalClient.process(CustomCanalClient.java:106)         at com.alibaba.otter.canal.custom.CustomCanalClient$1.run(CustomCanalClient.java:67)         at java.lang.Thread.run(Thread.java:748) client example : https://github.com/alibaba/canal/blob/master/example/src/main/java/com/alibaba/otter/canal/example/SimpleCanalClientTest.java [jstack_log.log](https://github.com/alibaba/canal/files/2481661/jstack_log.log) I add a shell script 'test.sh' to start canal.example.SimpleCanalClientTest class  but the same result comes out. ![image](https://user-images.githubusercontent.com/834743/46998952-b3e01580-d156-11e8-89a9-5f964f6e4eb9.png) if no binlog message    you can add sleep time . ex : Thread.sleep(1000) Thank you
1006,support  golang canal client  address  link as title tks
1005,binlog-rows-query-log-events=1 中的queryevent事件少库名，row数据少GTID 5.6数据库中含有配置 binlog-rows-query-log-events=1 中的queryevent事件少库名，row数据少GTID。我看了binlog，binlog中确实也少这俩，canal能帮把这俩属性补全了吗？ header {   version: 1   logfileName: "mysql-bin.000945"   logfileOffset: 430773470   serverId: 1346306   serverenCode: "UTF-8"   executeTime: 1539165635000   sourceType: MYSQL   schemaName: ""    ----------------------------少库名   tableName: "product_sc_id"   eventLength: 374   eventType: QUERY   gtid: "816fb525-8e3c-11e7-b7b0-525400730122:1-212612172 73aa9f66-8e18-11e7-b6c5-525400730120:1-358980 b064380d-0559-11e6-b5be-005056996b87:1-1593948705" } entryType: ROWDATA storeValue: "xxx"  header {------------------------少gtid   version: 1   logfileName: "mysql-bin.000945"   logfileOffset: 430773918   serverId: 1346306   serverenCode: "UTF-8"   executeTime: 1539165635000   sourceType: MYSQL   schemaName: "gome_jiaoyu"   tableName: "product_sc_id"   eventLength: 126   eventType: UPDATE   props {     key: "rowsCount"     value: "1"   } } entryType: ROWDATA storeValue: "xxx" binlog-rows-query-log-events schema没法获取到，binlog里就没有 ROWDATA里增加gtid里已添加，不过需要理解gtid的一个语义，如果是事务中的一行记录gtid为位点，会造成发送给mysql之后，丢弃当前事务的剩余记录.  
1004,支持Mysql8么 有测过支持Mysql8么 目前不支持 mysql8 那支持的mysql最高版本是多少？ 多看wiki
1003,Canal写kafka的逻辑没有考虑kafka的message size限制? https://github.com/alibaba/canal/blob/e4b6385dcc439fcf495c81b0c0f89da858dd377b/server/src/main/java/com/alibaba/otter/canal/kafka/CanalKafkaProducer.java#L104-L113 如果需要发送的record大小超过kafka限制(默认1MB)  是不是无法继续处理? 我是自己处理的 canal batchSize配置不要超过1M ``` servers: localhost:9092 #for rocketmq: means the nameserver retries: 0 batchSize: 16384 lingerMs: 1 bufferMemory: 33554432 # Canal的batch size  默认50K  由于kafka最大消息体限制请勿超过1M(900K以下) canalBatchSize: 50 ``` @rewerma 你好，还是有些疑惑需要请教一下。 canalBatchSize意义是什么呢， 控制的粒度是事务级别 or event 级别 or row级别 or 字段级别？如果某个字段就超过了1M， 如何处理呢。 canalBatchSize是拉取canal拉取的发送的批大小 具体请参考canal 的配置说明 如果有单个字段大于1M的情况暂时可以先调整kafka的消息体限制上限
1002,1. AbstractEventParser 中应该对multiStageCoprocess每次在while最后进行stop而不是rese… 1. AbstractEventParser 中应该对multiStageCoprocess每次在while最后进行stop而不是reset， reset会在mysqlMultiStageCoprocessor中start()重新创建两个线程池， 如果在dump出错的情况下， 会无限多的创建线程池而且不stop不terminal， 导致系统资源被占用完毕     注意while循环每次都会创建新的mysqlMultiStageCoprocessor 2. MySqlMultiStageComprocessor中在stop时线程池需要判断是否已经shutdown， 实际情况中存在线程池shutdown为true而terminal一直为false的情况 [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=1002) <br/>All committers have signed the CLA. tks
1001,使用 docker 部署 canal-server 无法使用 docker-restart 命令 - 现象 当使用 docker 部署官方 canal-server 镜像（canal/canal-server:v1.1.0）并成功启动后，调用 ```shell docker restart ``` 会有以下错误信息： ```log mv: cannot stat `/home/admin/canal-server/conf/example': No such file or directory ``` - 原因 检查 docker 启动脚本，发现 https://github.com/alibaba/canal/blob/master/docker/image/admin/app.sh 的 84-92 行引发的错误： ```shell destination=`perl -le 'print $ENV{"canal.destinations"}'` if [[ "$destination" =~ ' ' ]]; then     echo "multi destination:$destination is not support"     exit 1; else     if [ "$destination" != "" ] && [ "$destination" != "example" ] ; then         mv /home/admin/canal-server/conf/example /home/admin/canal-server/conf/$destination     fi  fi ``` 这里的行为是每次启动时，如果指定了 destination 且 destination 的值不是 example，会把 ```path /home/admin/canal-server/conf/example ``` 目录重命名为指定的 destination 值。 由于我指定了 destination 且值不是 example，所以这个行为导致了执行 ```shell docker restart ``` 时，由于容器内已经没有 example 文件夹，所以容器无法启动。 - 问题 各位大佬是如何解决这个问题的？自己打镜像还是有其他姿势？ 你可以写一个 docker-compose的yml，映射一下即可，可以参考下我写的[docker-compose.yml](https://github.com/CanalSharp/CanalSharp/blob/692672b5e7ecec9c91019cd254a1e73adfea7289/docker/docker-compose.yml) mv操作做了下判断目录存在处理 > 你可以写一个 docker-compose的yml，映射一下即可，可以参考下我写的[docker-compose.yml](https://github.com/CanalSharp/CanalSharp/blob/692672b5e7ecec9c91019cd254a1e73adfea7289/docker/docker-compose.yml) @WithLin 试过这种方法，但是重启时会报其他错误： ```log server_1  | mv: cannot move `/home/admin/canal-server/conf/example' to `/home/admin/canal-server/conf/uc': Device or resource busy ```
1000,解析位点记录使用FileMixedLogPositionManager时，启动不了 版本：1.1.0 mysql：5.6.36 `所有配置只修改两处，一个是example中配置的mysql的ip：port，另外一处就是spring/file-instance.xml ，如下：` <!-- 解析位点记录 -->         <property name="logPositionManager">             <bean class="com.alibaba.otter.canal.parse.index.FailbackLogPositionManager">                 <constructor-arg>                     <bean class="com.alibaba.otter.canal.parse.index.FileMixedLogPositionManager" >                         <constructor-arg index="0" value="/tmp/parse.dat" />                         <constructor-arg index="1" value="2000" />                         <constructor-arg index="2" ref="memoryLogPositionManager"/>                     </bean>                 </constructor-arg>                 <constructor-arg>                     <bean class="com.alibaba.otter.canal.parse.index.MetaLogPositionManager">                         <constructor-arg ref="metaManager"/>                     </bean>                 </constructor-arg>             </bean>         </property> `memoryLogPositionManager定义：`   `<bean id="memoryLogPositionManager" class="com.alibaba.otter.canal.parse.index.MemoryLogPositionManager" />` 报错如下： 2018-10-12 15:46:57.245 [destination = example   address = /127.0.0.1:3307   EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address /127.0.0.1:3307 has an error  retrying. caused by  java.lang.NullPointerException: null         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPositionInternal(MysqlEventParser.java:480) ~[canal.parse-1.1.0.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPosition(MysqlEventParser.java:362) ~[canal.parse-1.1.0.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:182) ~[canal.parse-1.1.0.jar:na]         at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171] 2018-10-12 15:46:57.258 [destination = example   address = /127.0.0.1:3307   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.lang.NullPointerException         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPositionInternal(MysqlEventParser.java:480)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPosition(MysqlEventParser.java:362)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:182)         at java.lang.Thread.run(Thread.java:748) ] `spring的原本配置：(保持原本配置，此时启动一切正常)` <!-- 解析位点记录 -->         <property name="logPositionManager">             <bean class="com.alibaba.otter.canal.parse.index.FailbackLogPositionManager">                 <constructor-arg>                     <bean class="com.alibaba.otter.canal.parse.index.MemoryLogPositionManager" />                 </constructor-arg>                 <constructor-arg>                     <bean class="com.alibaba.otter.canal.parse.index.MetaLogPositionManager">                         <constructor-arg ref="metaManager"/>                     </bean>                 </constructor-arg>             </bean>         </property> com.alibaba.otter.canal.parse.index.FileMixedLogPositionManager这里要求dataDir是一个根路径，查找dat文件时，会是这样的一个路径 dataDir + /$destination$/ + parse.dat，你的路径构造错了 @agapple  thx
999,ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.lang.IllegalArgumentException 参照Canal Kafka QuickStart中的说明，配置了canal到kafka数据同步，启动canal-kafka后，报 `2018-10-12 15:26:32.189 [destination = example   address = /192.168.206.136:3306   EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address /192.168.206.136:3306 has an error  retrying. caused by  java.lang.IllegalArgumentException: null         at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1310) ~[na:1.7.0_79]         at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1233) ~[na:1.7.0_79]         at java.util.concurrent.Executors.newFixedThreadPool(Executors.java:114) ~[na:1.7.0_79]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor.start(MysqlMultiStageCoprocessor.java:84) ~[canal.parse-1.1.0.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:238) ~[canal.parse-1.1.0.jar:na]         at java.lang.Thread.run(Thread.java:745) [na:1.7.0_79] 2018-10-12 15:26:32.189 [destination = example   address = /192.168.206.136:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.lang.IllegalArgumentException         at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1310)         at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1233)         at java.util.concurrent.Executors.newFixedThreadPool(Executors.java:114)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor.start(MysqlMultiStageCoprocessor.java:84)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:238)         at java.lang.Thread.run(Thread.java:745) ]` 有没有遇到这个问题的？ 使用环境：单机zookeeper-3.4.12，kafka_2.10-0.10.0.1，canal-kafka-1.1.0 ； 我也遇到一模一样的的问题，你解决了吗？ 此问题已经解决，将服务端配置文件canal.properties 中的并行线程数关掉即可，单例单线程运行
998,RowChange解析出来中文乱码 CanalEntry.RowChange rowChange = CanalEntry.RowChange.parseFrom(canalEntry.getStoreValue()); 得到的数据中文乱码，而且ddl的sql包含“\r\n" 数据库和canal的配置都是utf8，怎么没回事，求大佬解答 ![image](https://user-images.githubusercontent.com/26699764/46853678-60f21f80-ce31-11e8-8201-c7e87c996bfe.png) ![image](https://user-images.githubusercontent.com/26699764/46853718-77987680-ce31-11e8-8f17-e21e953c837a.png) 不好意思，toString()害死人，光看到toString打印的信息，没有直接获取值，傻逼了。。。。
996,聚石塔内的RDS部署无效 问题描述     我wget 了项目代码 编译成功 ，并且 更改了 ```conf/example/instance.properties``` 内容 ，运行/bin/startup.sh后报错 ``` 2018-10-11 20:12:49.383 [Thread-5] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - stop CannalInstance for null-example 2018-10-11 20:12:49.391 [Thread-5] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - stop successful.... 2018-10-11 20:12:57.090 [main] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2018-10-11 20:12:57.095 [main] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties] 2018-10-11 20:12:57.289 [main] WARN  o.s.beans.GenericTypeAwarePropertyDescriptor - Invalid JavaBean property 'connectionCharset' being accessed! Ambiguous write methods found next to actually used [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.nio.charset.Charset)]: [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.lang.String)] 2018-10-11 20:12:57.343 [main] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2018-10-11 20:12:57.343 [main] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties] 2018-10-11 20:12:57.553 [main] ERROR com.alibaba.druid.pool.DruidDataSource - testWhileIdle is true  validationQuery not set 2018-10-11 20:12:57.865 [main] INFO  c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-example 2018-10-11 20:12:57.881 [main] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - start successful.... 2018-10-11 20:12:57.918 [destination = example   address = xxxxxxxx:3333   EventParser] WARN  c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - prepare to find start position just show master status 2018-10-11 20:12:59.013 [destination = example   address = xxxxxxxx:3333   EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address xxxxxxxx:3333 has an error  retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: ErrorPacket [errorNumber=1142  fieldCount=-1  message=SHOW command denied to user 'xxxxxx' for table 'slow_log'  sqlState=42000  sqlStateMarker=#]  with command: show create table `mysql`.`event`;show create table `mysql`.`func`;show create table `mysql`.`general_log`;show create table `mysql`.`help_category`;show create table `mysql`.`help_keyword`;show create table `mysql`.`help_relation`;show create table `mysql`.`help_topic`;show create table `mysql`.`proc`;show create table `mysql`.`slow_log`;show create table `mysql`.`time_zone`;show create table `mysql`.`time_zone_leap_second`;show create table `mysql`.`time_zone_name`;show create table `mysql`.`time_zone_transition`;show create table `mysql`.`time_zone_transition_type`; Caused by: java.io.IOException: ErrorPacket [errorNumber=1142  fieldCount=-1  message=SHOW command denied to user 'xxxxxxx' for table 'slow_log'  sqlState=42000  sqlStateMarker=#]  with command: show create table `mysql`.`event`;show create table `mysql`.`func`;show create table `mysql`.`general_log`;show create table `mysql`.`help_category`;show create table `mysql`.`help_keyword`;show create table `mysql`.`help_relation`;show create table `mysql`.`help_topic`;show create table `mysql`.`proc`;show create table `mysql`.`slow_log`;show create table `mysql`.`time_zone`;show create table `mysql`.`time_zone_leap_second`;show create table `mysql`.`time_zone_name`;show create table `mysql`.`time_zone_transition`;show create table `mysql`.`time_zone_transition_type`;         at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.queryMulti(MysqlQueryExecutor.java:109) ~[canal.parse.driver-1.1.0.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.queryMulti(MysqlConnection.java:107) ~[canal.parse-1.1.0.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:175) ~[canal.parse-1.1.0.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:129) ~[canal.parse-1.1.0.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:91) ~[canal.parse-1.1.0.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:188) ~[canal.parse-1.1.0.jar:na]         at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171 ``` 去掉mysql库的订阅
995,canal怎么处理源表delete的数据 如标题 delete在binlog中就是一个只有before image的event，和insert/update没啥区别。你说的怎么处理是指？ > delete在binlog中就是一个只有before image的event，和insert/update没啥区别。你说的怎么处理是指？ 好吧，其实是我表达的不清楚，应该是说，canal能解析binlog肯定能获取delete的语句。 我的问题应该是，获取到delete数据后，从canal抽取数据-->hive仓库。那hive仓库怎么处理这个删除的数据？ 注意： 1、hive没有启用事务，不能delete操作 这个是你hive要处理的啊，和canal有什么关系， 你可是试试 apache orc > 这个是你hive要处理的啊，和canal有什么关系， 你可是试试 apache orc 好的，我试试。谢谢！
994,canal是否支持多个instance指向同一database 只是对应的白名单表不同? 试了下设置多个instance 其中的instance.properties的数据库地址相同 仅仅是白名单指向不同的表(主要是为了一张表对应一个kafka的topic) 但是在启动的时候报错实例化Datasource冲突? 可以设置 但是对于zookeeper上cursor的position没有更新 
993,对特定的instance指定xxx-instance.xml 请问 假如现在有两个实例: `conf/instance1/instance.properties`   `conf/instance2/instance.properties ` 那么如何为它们分别指定`conf/spring/` 中不同的xml 配置? 应该是没有办法，只能开两个canal @littleneko 在一个server中开启两个instance. 为一个instance指定配置. 在应用扩展貌似中给出了答案: https://github.com/alibaba/canal/wiki/DevGuide#%E5%BA%94%E7%94%A8%E6%89%A9%E5%B1%95 1. 在`conf` 中创建新实例属性文件 `new-instance/instance.properties` 2. 新建 xml 配置文件 `spring/custom-instance.xml` 3. canal.properties 中指定 `canal.instance.new-instance.spring.xml = classpath:spring/custom-instance.xml` 如果 canal.properties 没有设置自动扫描(canal.auto.scan = true  开启自动扫描)  需在 canal.properties 中指定: canal.destinations= example  new-instance --------------------------------------- 这里的"通道名称" 是指的instance名称? 还是什么? 但是`canal.instance.destination` 这个占位符的作用是什么? 如何使用? 谢谢大佬! @yudianer  这种方法不能指定两个不同的xml啊，xml是在`canal.properties`中配置的。 通道名称就是`new-instance`，每个instance需要在`${canal.conf.dir}`(默认为conf目录下，在`canal.properties`中配置)中新建一个目录，里面写上`instance.properties`配置文件。 `canal.instance.destination`实际上就是每个instance的目录。 在`https://github.com/alibaba/canal/blob/master/deployer/src/main/java/com/alibaba/otter/canal/deployer/CanalController.java`中初始化： ```java // 设置当前正在加载的通道，加载spring查找文件时会用到该变量 System.setProperty(CanalConstants.CANAL_DESTINATION_PROPERTY  destination); ``` @littleneko  我感觉咱俩说的一个意思啊. :smile: ![image](https://user-images.githubusercontent.com/12033023/46842989-50c34b80-ce03-11e8-8224-bf0a5aa593e0.png) > // 设置当前正在加载的通道，加载spring查找文件时会用到该变量 System.setProperty(CanalConstants.CANAL_DESTINATION_PROPERTY  destination); 你说的这句话倒是提醒了我 `canal.instance.destination` 并不倾向于使用者设置的. 而是加载配置文件的时候 设置为每一个instance名称 然后对properties 配置文件中占位符进行替换. 谢谢! 
992,使用CanalKafkaClientExample storeValue是乱码 2018-10-10 17:25:19.304 [Thread-2] INFO  c.a.o.canal.client.running.kafka.CanalKafkaClientExample - Message[id=16 entries=[header {   version: 1   logfileName: "mysql-bin.000156"   logfileOffset: 298158292   serverId: 1   serverenCode: "UTF-8"   executeTime: 1539163519000   sourceType: MYSQL   schemaName: "payment"   tableName: "tmp_canal_test"   eventLength: 69   eventType: UPDATE   props {     key: "rowsCount"     value: "1"   } } entryType: ROWDATA storeValue: "\b\242\006\020\002P\000b\341\001\n\033\b\000\020\004\032\002id \001(\0000\000B\00245R\aint(11)\n\'\b\001\020\f\032\busername \000(\0000\000B\003111R\fvarchar(255)\n*\b\002\020\f\032\bpassword \000(\0000\000B\006123123R\fvarchar(255)\022\033\b\000\020\004\032\002id \001(\0000\000B\00245R\aint(11)\022\'\b\001\020\f\032\busername \000(\0000\000B\003111R\fvarchar(255)\022\'\b\002\020\f\032\bpassword \000(\0010\000B\003123R\fvarchar(255)" ] raw=true rawEntries=[]] 请问一下怎么处理 `storeValue`是`com.alibaba.otter.canal.protocol.RowChange`类型，需要进一步解析出来，具体可以看EntryProtocol.proto中的定义 LS正解
991,canal实现HA 并且发送消息到kafka集群 主备切换导致消息重复发送 canal实现HA后 binlog的position记录到了zookeeper中 但将主应用暂停掉后 备用会将之前一段时间没有经由它处理的binlog重新投递到kafka中.是什么情况??? 在HA模式下 停止A 切换到B 再启动A 再停止B由A操作的zk上的cursor的position不更新 不知道是因为集成了Kafka还是其他原因?? 已知问题，建议升级1.1.1新版本
990,增加 目标数据库密码加密解密，采用druid方式进行加密解密，并设置canal.instance.enableDruid=true时，请使… 增加 目标数据库密码加密解密，采用druid方式进行加密解密，并设置canal.instance.enableDruid=true时，请使… [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=990) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=990) before we can accept your contribution.<br/><hr/>**shichengming** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=990) it.</sub> [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=990) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=990) before we can accept your contribution.<br/><hr/>**shichengming** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=990) it.</sub> tks
989,修改canal dbsync模块的代码未生效 在定制canal的过程中，已经完成了canal的安装部署且运行正常。 现需要对canal的dbsync模块的代码（com.alibaba.otter.canal.parse.inbound.mysql.dbsync包下面的类LogEventConvert的方法parseOneRow）进行改写（下载了canal source，并在intellij idea中打开），但是一直不生效。不知道什么原因？ 哪位大神能帮忙指点下，谢谢啦！ <img width="1232" alt="dbsync" src="https://user-images.githubusercontent.com/28953872/46657304-89380f00-cbe2-11e8-973f-3c13658b8e04.png"> @agapple @lcybo 求大神有空指点下 不生效？debug断点能走到这里么？ 在idea中设置断点调试过了，没有走到这里。难道是要改canal安装包中的lib/canal.parse.dbsync-1.0.24.jar？ 断点能走到其他地方么？先确保在IDEA里面debug能走到你添加的代码块，测试没问题了，在把工程打成jar包。再部署 谢谢回复！ 不好意思，之前写错了，应该是修改parser模块（不是dbsync模块）的LogEventConvert代码没生效。 另外我尝试在parser模块的其他代码设置断点调试，但都没有跑到该断点。 ********************** 具体情况是这样的：我正在定制canal来获取mysql的增量数据并经转换后得到自定义的json string（包含1条修改的record），json格式如下（insert 1条record）： { "table":"mysql.mysql.all_types" "transactionID":"29" "scn":"29.0" "ts":"1539333745000" "data":[{"opType":"i" "after":{"smallint1":"111" "timestamp1":"2018-10-12 16:42:25.000000" "float1":"345.102" "inta":"108" "tinyint1":"11" "varchar1":"a" "decimal1":"-1235666.666668899987000000000000000000" "double1":"123.134577501986" "datetime1":"2018-10-12 16:42:25.000000"}}]} 但是原生canal(1.0.24版本)，“client端（example模块的ClusterCanalClientTest.java）使用column.getvalue()获取的double和decimal类型的数据” 和 “mysql shell中插入/查询数据的精度和小数点位数(precision scale)”不一致。 为了解决这个数据一致性问题，我尝试对canal的parser模块的代码（com.alibaba.otter.canal.parse.inbound.mysql.dbsync包下面的类LogEventConvert的方法parseOneRow）进行改写。但是不起作用，于是我在intellij idea中开启了断点调试，但是程序显示不会执行到parseOneRow。 我觉得原因可能是：我是在intellij idea中修改了canal parser模块（canal 源码）的代码，但是canal server （即canal首页下载的 deployer（二进制）并解压缩得到lib、bin、conf以及logs文件夹，其中lib中包含各种jar）负责解析mysql的二进制文件。所以真正需要更改的是canal server 的lib下面的jar。 因此我需要在intellij idea中修改好parser源码后，需要重新打成新parser jar包，并用该新jar包替换canal server 的lib下面的parser jar。 猜想待验证。 测试结果显示，需要在intellij idea中修改好parser源码后，需要重新打成新parser jar包，并用该新jar包替换canal server 的lib下面的parser jar。修改已经生效了。 谢谢 @theonesmx 
988,canal.instance.filter.regex不生效 尝试了两个mysql服务器，同样的配置，一个生效，另一个不生效。 已经检查bin log格式，是ROW格式。 用的是KafkaCanalConnector subscribe()方法里没有用参数。 请问是什么原因？ 服务器1 不生效 ![image](https://user-images.githubusercontent.com/24933564/46660266-44fc3d00-cbe9-11e8-93f3-2f7218564f10.png) ![image](https://user-images.githubusercontent.com/24933564/46660815-79bcc400-cbea-11e8-9085-ee90f1eb5933.png) 服务器2 生效 ![image](https://user-images.githubusercontent.com/24933564/46660299-534a5900-cbe9-11e8-9aca-ae68e98b27ed.png) ![image](https://user-images.githubusercontent.com/24933564/46660833-82ad9580-cbea-11e8-80d8-9ce49dbbf46b.png) 这两个服务器binlog的配置的不同会影响吗 看一下wiki里的FAQ，有几种常见的filter未生效的case
987,解析create schema日志后，调用不断抛异常 > 疑问1 场景：启动canal，执行create schema语句，通过 Message getWithoutAck(int batchSize)获得日志数据。当下一次再调用该方法时，抛出以下异常。 如果canal解析create schema日志前，解析了其他语句（crud）的日志，则不会出现这样的错误。 ### 异常信息栈： 2018-10-09 10:19:20.755 [main] ERROR syncLogger - !!!!增量同步异常|com.alibaba.otter.canal.protocol.exception.CanalClientException: something goes wrong with reason: something goes wrong with channel:[id: 0xcf561b9e  /192.168.1.179:50803 => /192.168.1.179:2017]  exception=java.lang.NullPointerException 	at com.alibaba.otter.canal.store.helper.CanalEventUtils.createPosition(CanalEventUtils.java:69) 	at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:375) 	at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:36) 	at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:307) 	at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:273) 	at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:124) 	at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88) 	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) 	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791) 	at org.jboss.netty.channel.SimpleChannelHandler.messageReceived(SimpleChannelHandler.java:142) 	at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88) 	at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:36) 	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) 	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791) 	at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:294) 	at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70) 	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) 	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296) 	at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:536) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:435) 	at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70) 	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) 	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255) 	at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88) 	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:109) 	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312) 	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90) 	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178) 	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) 	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) 	at java.lang.Thread.run(Thread.java:748) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.receiveMessages(SimpleCanalConnector.java:344) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:315) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:287) > 疑问2 另外，create schema语句对应的event type是Query，isDdl=false，感觉不太对。 ![image](https://user-images.githubusercontent.com/22972651/46643311-2167d100-cbae-11e8-8695-a9a2559f92b3.png) 1.  升级一下1.1.11版本 2.  create schema 语法没有识别，ddl语法主要识别了create/alter/drop table等操作
986,idle timeout exceeds  close channel to save server resources canal1.1.1版本错误  2018-10-08 18:09:07.144 [Hashed wheel timer #1] WARN  c.a.o.c.server.netty.handler.ClientAuthenticationHandler - channel:[id: 0x2d3c7336  /10.8.32.241:54621 => /10.8.32.241:11111] idle timeout exceeds  close channel to save server resources... 2018-10-08 18:09:13.279 [New I/O server worker #1-14] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x4863b38b  /10.8.32.241:54622 :> /10.8.32.241:11111]  exception=java.nio.channels.ClosedChannelException         at org.jboss.netty.channel.socket.nio.NioWorker.cleanUpWriteBuffer(NioWorker.java:629)         at org.jboss.netty.channel.socket.nio.NioWorker.close(NioWorker.java:605)         at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:356)         at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281)         at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201)         at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46)         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)         at java.lang.Thread.run(Thread.java:745) client长时间没有和canal server有交互导致，默认超时时间1小时
985,server配置了多个实例，某个实例报错后如何恢复 server监听了多个库，有个库由于网络问题中断了一会，后面一直连不上，报timeout。 要重新接的话是不是只能重启server，还是说可以只重启出错的实例？ ![default](https://user-images.githubusercontent.com/22339074/46599888-0e57f100-cb1b-11e8-81e4-38203ac9e231.PNG) 建议直接测试一下最新的1.1.1版本
984,问一个canal代码数据结构的问题 RowChange rowChange中的getRowDatasList()可能 会返回多条吗？如果会 什么情况下会 批量情况
983,[1.1.0]是否支持MariaDB-10.3.6 支持mariadb 5.x和10.x
982,请问，客户端如何能获取 TableMeta 对象，谢谢！ 如题，想在客户端获取 表中的主键或者唯一键。 但不知从何下手。 现在我们用的 canal 是1.0.22；据说1.0.26以后，canal增加tsdb来实时存储表结构信息；那客户端如何获取呢？直接connect到druid中拿？还是在批量拉取时，从返回的message或者其他对象中能获取到？ 请不吝赐教，谢谢各位！ 表的主键在返回的protobuf对象里已经有标识，isKey.  如果要获取唯一键，需要拿到表名去反查一下数据库表结构 明白了，谢谢！
981, client模块中rocketmq-client包scope provide，外部使用单独依赖 tks
980,table meta 错误  ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - pls submit github issue  show create table ddl:CREATE TABLE `partner_organization_data` (   `id` int(11) NOT NULL AUTO_INCREMENT   `organizationName` varchar(250) DEFAULT NULL   `organizationId` varchar(250) NOT NULL   `organizationLogo` varchar(500) DEFAULT NULL COMMENT '机构logo'   `organizationUrl` varchar(500) DEFAULT NULL COMMENT '机构地址'   `isDelete` tinyint(4) NOT NULL DEFAULT '0' COMMENT '是否删除0是删除，1是不删除'   `isOnline` tinyint(4) NOT NULL DEFAULT '0' COMMENT '是否上线，0是不上线，1是上线'   `organizationLogoh5_2` varchar(500) DEFAULT NULL COMMENT 'h5  第二个logo'   `organizationLogoh5` varchar(500) DEFAULT NULL COMMENT 'h5机构logo'   `organizationUrlh5` varchar(500) DEFAULT NULL COMMENT 'h5机构url'   PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=11 DEFAULT CHARSET=utf8   compare failed .   db : TableMeta [schema=operation  table=partner_organization_data  fileds=         FieldMeta [columnName=id  columnType=int(11)  nullable=false  key=true  defaultValue=null  extra=null  unique=false]         FieldMeta [columnName=organizationName  columnType=varchar(250)  nullable=true  key=false  defaultValue=null  extra=null  unique=false]         FieldMeta [columnName=organizationId  columnType=varchar(250)  nullable=false  key=false  defaultValue=null  extra=null  unique=false]         FieldMeta [columnName=organizationLogo  columnType=varchar(500)  nullable=true  key=false  defaultValue=null  extra=null  unique=false]         FieldMeta [columnName=organizationUrl  columnType=varchar(500)  nullable=true  key=false  defaultValue=null  extra=null  unique=false]         FieldMeta [columnName=isDelete  columnType=tinyint(4)  nullable=false  key=false  defaultValue=0  extra=null  unique=false]         FieldMeta [columnName=isOnline  columnType=tinyint(4)  nullable=false  key=false  defaultValue=0  extra=null  unique=false]         FieldMeta [columnName=organizationLogoh5_2  columnType=varchar(500)  nullable=true  key=false  defaultValue=null  extra=null  unique=false]         FieldMeta [columnName=organizationLogoh5  columnType=varchar(500)  nullable=true  key=false  defaultValue=null  extra=null  unique=false]         FieldMeta [columnName=organizationUrlh5  columnType=varchar(500)  nullable=true  key=false  defaultValue=null  extra=null  unique=false] ]   mem : TableMeta [schema=operation  table=partner_organization_data  fileds=         FieldMeta [columnName=id  columnType=int(11)  nullable=false  key=true  defaultValue=null  extra=null  unique=false]         FieldMeta [columnName=organizationName  columnType=varchar(250)  nullable=true  key=false  defaultValue=null  extra=null  unique=false]         FieldMeta [columnName=organizationId  columnType=varchar(250)  nullable=false  key=false  defaultValue=null  extra=null  unique=false]         FieldMeta [columnName=organizationLogo  columnType=varchar(500)  nullable=true  key=false  defaultValue=null  extra=null  unique=false]         FieldMeta [columnName=organizationUrl  columnType=varchar(500)  nullable=true  key=false  defaultValue=null  extra=null  unique=false]         FieldMeta [columnName=isDelete  columnType=tinyint(4)  nullable=false  key=false  defaultValue=0  extra=null  unique=false]         FieldMeta [columnName=isOnline  columnType=tinyint(4)  nullable=false  key=false  defaultValue=0  extra=null  unique=false]         FieldMeta [columnName=organizationLogoh5_2  columnType=varchar(500)  nullable=true  key=false  defaultValue=null  extra=null  unique=false]         FieldMeta [columnName=organizationLogoh5_2  columnType=varchar(500)  nullable=true  key=false  defaultValue=null  extra=null  unique=false]         FieldMeta [columnName=organizationLogoh5  columnType=varchar(500)  nullable=true  key=false  defaultValue=null  extra=null  unique=false]         FieldMeta [columnName=organizationUrlh5  columnType=varchar(500)  nullable=true  key=false  defaultValue=null  extra=null  unique=false] ] 2018-09-30 15:53:41.988 [[scheduler-table-meta-snapshot]] ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - compare failed   check log 以上是错误日志  于两天前增加了一个数据列 organizationLogoh5_2 今天发生的错误.mem中出现了两个organizationLogoh5_2 1. 对应版本是？ 2. 你能从本地h2文件里，找到对应该表上操作的DDL，我在本地复现一下 内存表结构是没有判重的处理，判重主要是meta_history里会基于serverId+位点做去重过滤，如果是正常的主备是在复制时会保留serverId，但对应的位点会不同，此时meta_history会有主备库各自的DDL，此时如果再回溯一次的话，就会出现内存重复的列 国庆之后支持一下tsdb内存表结构的幂等处理，规避主备切换的回溯出现重复的DDL 2天前的master版本 一个canal实例  instance配置里面连接的是主库 未设置standby select * from PUBLIC.meta_snapshot 以下为查询结果 只有一条 1	2018-09-29 15:53:47.086000000	2018-09-29 15:53:47.086000000	partner	0	0	-1	-2	{"operation":"CREATE TABLE `application` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT \n\t`name` varchar(100) NOT NULL DEFAULT '' \n\t`type` varchar(50) NOT NULL DEFAULT '' \n\tPRIMARY KEY (`id`) \n\tUNIQUE `应用名词索引` (`name`)\n) ENGINE = InnoDB AUTO_INCREMENT = 3 CHARSET = utf8; \nCREATE TABLE `op_account_history` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT \n\t`hexun_user_id` int(11) NOT NULL \n\t`hexun_user_nick` varchar(255) NOT NULL \n\t`add_time` datetime NOT NULL \n\t`type` varchar(255) NOT NULL \n\t`content` varchar(255) NOT NULL \n\t`success` int(1) NOT NULL \n\t`params` text \n\t`access_type` varchar(45) DEFAULT NULL \n\t`access_log_param` text \n\t`access_log_detail` text \n\t`ip` varchar(45) DEFAULT NULL \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 217066 CHARSET = utf8 COMMENT '管理员账号访问记录'; \nCREATE TABLE `op_account_url_permission` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT COMMENT '自增permission id\n' \n\t`permission_url` varchar(255) NOT NULL COMMENT '权限根据url控制' \n\t`permission_desc` varchar(255) DEFAULT NULL COMMENT '权限的描述，如“审核管理”' \n\t`allowed_roles` varchar(512) NOT NULL COMMENT '允许的角色ID' \n\t`log_message_format` varchar(6000) DEFAULT NULL \n\t`access_type` varchar(45) DEFAULT NULL \n\t`permission_api_id` varchar(128) DEFAULT NULL \n\tPRIMARY KEY (`id`) \n\tUNIQUE `id_UNIQUE` (`id`) \n\tUNIQUE `permission_url_UNIQUE` (`permission_url`)\n) ENGINE = InnoDB AUTO_INCREMENT = 353 CHARSET = utf8 COMMENT '角色可以行使的权限'; \nCREATE TABLE `op_account_user` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT \n\t`hexun_user_id` int(11) NOT NULL \n\t`hexun_user_name` varchar(255) NOT NULL \n\t`hexun_user_nick` varchar(255) NOT NULL \n\t`email_notification` varchar(255) NOT NULL \n\t`user_role_id` int(11) NOT NULL \n\t`create_time` datetime NOT NULL \n\t`creator_user_id` int(11) NOT NULL \n\t`creator_user_name` varchar(255) NOT NULL \n\t`creator_user_nick` varchar(255) NOT NULL \n\t`true_name` varchar(255) DEFAULT NULL \n\tPRIMARY KEY (`id`) \n\tUNIQUE `id_UNIQUE` (`id`) \n\tUNIQUE `hexun_user_id_UNIQUE` (`hexun_user_id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 85 CHARSET = utf8 COMMENT '运营后台管理员表'; \nCREATE TABLE `op_account_user_role` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT \n\t`role_id` tinyint(4) NOT NULL \n\t`role_name` varchar(255) NOT NULL \n\t`role_desc` text \n\tPRIMARY KEY (`id`) \n\tUNIQUE `id_UNIQUE` (`id`) \n\tUNIQUE `roleName_UNIQUE` (`role_name`) \n\tUNIQUE `roleId_UNIQUE` (`role_id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 26 CHARSET = utf8; \nCREATE TABLE `op_check_flow_right` (\n\t`id` tinyint(8) NOT NULL \n\t`check_level` tinyint(4) DEFAULT NULL \n\t`role_id` varchar(16) DEFAULT NULL \n\t`modular_id` tinyint(4) DEFAULT NULL \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB CHARSET = utf8; \nCREATE TABLE `op_check_partner_eventflow` (\n\t`id` int(8) NOT NULL AUTO_INCREMENT \n\t`partner_id` int(8) DEFAULT NULL \n\t`check_level` tinyint(2) DEFAULT '1' COMMENT '分为1级和2级审核，3级为驳回状态' \n\t`check_status` tinyint(2) DEFAULT '1' COMMENT '1待审，2审核通过' \n\t`check_time` datetime DEFAULT NULL \n\t`check_reason` varchar(512) DEFAULT NULL \n\t`partner_submit_time` datetime DEFAULT NULL \n\t`adviser_status` tinyint(2) DEFAULT '0' COMMENT '投顾身份审核 0直播人审核，1投顾审核，2投顾审核完成 3投顾降级为直播人' \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 7034 CHARSET = utf8; \nCREATE TABLE `op_check_partner_log` (\n\t`id` int(8) NOT NULL AUTO_INCREMENT \n\t`operateName` varchar(32) DEFAULT '' COMMENT '操作员姓名' \n\t`operateType` varchar(32) DEFAULT '' COMMENT '操作员角色' \n\t`partnerId` int(8) DEFAULT NULL COMMENT '事件ID' \n\t`operateTime` datetime DEFAULT NULL COMMENT '操作时间' \n\t`operateContent` varchar(256) DEFAULT '' COMMENT '操作内容' \n\t`checkType` varchar(64) DEFAULT '' COMMENT '操作类型（申请，驳回，通过审核）' \n\t`checkUserId` int(11) DEFAULT NULL \n\t`reason` mediumtext \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 2534 CHARSET = utf8; \nCREATE TABLE `op_check_partner_situation_detail` (\n\t`id` int(8) NOT NULL AUTO_INCREMENT \n\t`check_user_id` int(11) DEFAULT NULL \n\t`partner_id` int(11) DEFAULT NULL \n\t`check_time` datetime DEFAULT NULL \n\t`check_status` tinyint(2) DEFAULT NULL \n\t`check_level` tinyint(2) DEFAULT NULL \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 104 CHARSET = utf8; \nCREATE TABLE `op_menu_management` (\n\t`id` int(8) NOT NULL AUTO_INCREMENT \n\t`parent_id` int(8) DEFAULT '0' COMMENT '菜单父id' \n\t`menu_name` varchar(64) NOT NULL COMMENT '菜单名称' \n\t`menu_flag` varchar(32) NOT NULL COMMENT '菜单标识' \n\t`menu_link_address` varchar(128) DEFAULT NULL COMMENT '二级菜单链接地址' \n\t`menu_permission` varchar(32) DEFAULT NULL COMMENT '菜单权限' \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 104 CHARSET = utf8; \nCREATE TABLE `op_operate_log` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT \n\t`operaterId` int(11) DEFAULT NULL \n\t`operaterName` varchar(64) DEFAULT NULL \n\t`model` varchar(64) DEFAULT NULL \n\t`userId` int(11) DEFAULT NULL \n\t`operateContent` varchar(512) DEFAULT NULL \n\t`timeStamp` datetime DEFAULT NULL \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 1949 CHARSET = utf8; \nCREATE TABLE `op_partner_comment_statistics` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT \n\t`dataDate` datetime(4) DEFAULT NULL COMMENT '日期' \n\t`articleCommentCount` int(4) DEFAULT '0' COMMENT '文章评论数' \n\t`videoCommentCount` int(4) DEFAULT '0' COMMENT '视频直播评论数' \n\t`systemClassCommentCount` int(4) DEFAULT '0' COMMENT '系统课评论数' \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 422 CHARSET = utf8; \nCREATE TABLE `op_partner_data_statistics` (\n\t`id` int(8) NOT NULL AUTO_INCREMENT \n\t`dataDate` datetime DEFAULT NULL \n\t`partnerId` int(11) DEFAULT NULL \n\t`caiquanCount` int(4) DEFAULT '0' COMMENT '财圈数' \n\t`articleCount` int(4) DEFAULT '0' COMMENT '文章总数' \n\t`articleMessageCount` int(4) DEFAULT '0' COMMENT '文章留言数' \n\t`videoDemandClassCount` int(4) DEFAULT '0' COMMENT '视频点播课数' \n\t`liveClassCount` int(4) DEFAULT '0' COMMENT '视频直播课数' \n\t`caiquanReplyCount` int(4) DEFAULT '0' COMMENT '财圈回复数' \n\t`newAttentionCount` int(4) DEFAULT '0' COMMENT '新增关注数' \n\t`cancelAttentionCount` int(4) DEFAULT '0' COMMENT '取消关注数' \n\t`increaseAttentionCount` int(4) DEFAULT '0' COMMENT '净增关注数' \n\t`totalAttentionCount` int(4) DEFAULT '0' COMMENT '累计关注数' \n\t`caiboCount` int(4) DEFAULT '0' COMMENT '财播数' \n\t`sales` int(8) DEFAULT '0' COMMENT '销售额' \n\t`aSharesCount` int(4) DEFAULT '0' COMMENT 'A股雷达数' \n\t`tuguReplyCount` int(4) DEFAULT '0' COMMENT '投顾回答数' \n\t`remarkCount` int(4) DEFAULT '0' COMMENT '直播室老师发言总数' \n\t`studentSpeakCount` int(4) DEFAULT '0' COMMENT '直播室中学员发言总数' \n\t`openReplyCount` int(4) DEFAULT '0' COMMENT '老师在直播室中公开回复总数' \n\t`studentSecretAskCount` int(4) DEFAULT '0' COMMENT '学员私信提问总数' \n\t`secretReplyCount` int(4) DEFAULT '0' COMMENT '老师私信回复总数' \n\t`freeStudentSpeaks` int(4) DEFAULT '0' COMMENT '学员发言（免费）' \n\t`nonFreeStudentSpeaks` int(4) DEFAULT '0' COMMENT '学员发言（收费' \n\t`freeStudentAsks` int(4) DEFAULT '0' COMMENT '学员私信提问（免费）' \n\t`nonFreeStudentAsks` int(4) DEFAULT '0' COMMENT '学员私信提问（收费）' \n\t`freeTeacherSpeaks` int(4) DEFAULT '0' COMMENT '老师公开发言（免费）' \n\t`nonFreeTeacherSpeaks` int(4) DEFAULT '0' COMMENT '老师公开发言（收费）' \n\t`freeTeacherRepliesPublic` int(4) DEFAULT '0' COMMENT '老师公开回复（免费）' \n\t`nonFreeTeacherRepliesPublic` int(4) DEFAULT '0' COMMENT '老师公开回复（收费）' \n\t`freeTeacherRepliesPrivate` int(4) DEFAULT '0' COMMENT '老师私密回复（免费）' \n\t`nonFreeTeacherRepliesPrivate` int(4) DEFAULT '0' COMMENT '老师私密回复（收费）' \n\t`freeArticleCount` int(4) DEFAULT '0' COMMENT '免费文章数' \n\t`nonfreeArticleCount` int(4) DEFAULT '0' COMMENT '收费文章数' \n\t`onlineTime` int(4) DEFAULT '0' COMMENT '老师直播室在线时长' \n\t`freeStudentCount` int(4) DEFAULT '0' COMMENT '免费直播室在线人数' \n\t`nonFreeStudentCount` int(4) DEFAULT '0' COMMENT '收费直播室在线人数' \n\t`freeTeacherSecretSpeaks` int(4) DEFAULT '0' COMMENT '隐私观点数（免费直播室）' \n\t`nonFreeTeacherSecretSpeaks` int(4) DEFAULT '0' COMMENT '隐私观点数（收费直播室暂无数据）' \n\t`totalTeacherSecretSpeaks` int(4) DEFAULT '0' COMMENT '老师隐私观点（所有）' \n\tPRIMARY KEY (`id`) \n\tUNIQUE `index_data_id` USING BTREE (`dataDate`  `partnerId`) \n\tKEY `index_date` USING BTREE (`dataDate`)\n) ENGINE = InnoDB AUTO_INCREMENT = 1412753 CHARSET = utf8; \nCREATE TABLE `op_partner_data_statistics_log` (\n\t`id` int(8) NOT NULL AUTO_INCREMENT \n\t`partnerId` int(11) DEFAULT NULL \n\t`dataDate` datetime DEFAULT NULL \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 943575 CHARSET = utf8; \nCREATE TABLE `op_product_data` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT \n\t`productId` int(11) DEFAULT NULL \n\t`productName` varchar(100) DEFAULT NULL \n\t`partnerId` int(11) DEFAULT NULL \n\t`checkType` varchar(20) DEFAULT NULL COMMENT '审核类型' \n\t`productType` varchar(30) DEFAULT NULL \n\t`createTime` datetime DEFAULT NULL \n\t`bussinessProId` varchar(50) DEFAULT NULL \n\t`bussinessCheck` varchar(10) DEFAULT NULL \n\t`bussinessInfo` varchar(1000) DEFAULT NULL \n\t`bussinessLog` varchar(1000) DEFAULT NULL \n\t`modifiTime` datetime DEFAULT NULL \n\t`isClose` int(8) DEFAULT '0' \n\t`verifyCode` varchar(100) DEFAULT NULL \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 137 CHARSET = utf8; \nCREATE TABLE `op_req_external_service_log` (\n\t`id` int(8) NOT NULL AUTO_INCREMENT \n\t`partner_id` int(11) DEFAULT NULL COMMENT '合作者id' \n\t`service_name` varchar(64) DEFAULT NULL COMMENT '请求接口名称' \n\t`req_param` text COMMENT '请求参数' \n\t`req_time` datetime DEFAULT NULL COMMENT '请求时间' \n\t`resp_content` text COMMENT '接口返回内容' \n\t`resp_status` tinyint(2) DEFAULT NULL COMMENT '请求是否成功 0成功 1失败' \n\t`manual_sync_status` tinyint(2) DEFAULT '0' COMMENT '1 代表为失败的这条记录已经手工同步操作' \n\tPRIMARY KEY (`id`) \n\tKEY `index_partner_id` USING BTREE (`partner_id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 1484 CHARSET = utf8; \nCREATE TABLE `op_violation_forbidden_words` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT \n\t`forbidden_word` varchar(255) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL DEFAULT '' \n\t`add_time` datetime NOT NULL \n\t`creator_user_id` int(11) NOT NULL \n\t`creator_user_nick` varchar(255) NOT NULL \n\tPRIMARY KEY (`id`) \n\tUNIQUE `id_UNIQUE` (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 503987 CHARSET = utf8 COMMENT '敏感词'; \nCREATE TABLE `partner_advertising_data` (\n\t`id` int(8) NOT NULL AUTO_INCREMENT \n\t`partnerId` int(11) DEFAULT NULL COMMENT '老师id' \n\t`content` text COMMENT '广告内容' \n\t`content_h` text COMMENT '拓展字段' \n\t`createTime` datetime DEFAULT NULL COMMENT '创建时间' \n\t`createUserId` int(11) DEFAULT NULL COMMENT '创建用户Id' \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 5224 CHARSET = utf8; \nCREATE TABLE `partner_circle_data` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT \n\t`partnerId` int(11) NOT NULL \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 3 CHARSET = utf8; \nCREATE TABLE `partner_config` (\n\t`id` int(4) NOT NULL AUTO_INCREMENT \n\t`partnerId` bigint(20) NOT NULL COMMENT '合作者id' \n\t`isCombined` tinyint(4) DEFAULT '0' COMMENT '是否隐藏，1是组合账号，0不是组合账号' \n\t`isHide` tinyint(4) NOT NULL DEFAULT '0' COMMENT '是否隐藏，1是隐藏，0是不隐藏' \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 12 CHARSET = utf8; \nCREATE TABLE `partner_data` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT COMMENT '自增id' \n\t`partnerId` int(11) NOT NULL COMMENT '用户id' \n\t`truename` varchar(25) DEFAULT NULL COMMENT '真实姓名' \n\t`sex` varchar(5) DEFAULT NULL COMMENT '性别' \n\t`province` varchar(25) DEFAULT NULL COMMENT '省份' \n\t`city` varchar(25) DEFAULT NULL COMMENT '城市' \n\t`qq` varchar(25) DEFAULT NULL COMMENT 'qq号' \n\t`blog` varchar(250) DEFAULT NULL COMMENT '博客' \n\t`weibo` varchar(250) DEFAULT NULL COMMENT '微博' \n\t`orgname` varchar(100) DEFAULT NULL COMMENT '组织名称' \n\t`intro` text COMMENT '简介' \n\t`noticeType` varchar(10) DEFAULT NULL COMMENT '标签类型' \n\t`noticeInfo` text COMMENT '内容' \n\t`level` varchar(32) DEFAULT NULL COMMENT '内容' \n\t`modifitime` datetime DEFAULT NULL COMMENT '最后修改时间' \n\t`createtime` datetime DEFAULT NULL COMMENT '创建时间' \n\t`cooperationState` tinyint(2) DEFAULT '0' COMMENT '0未合作，1合作，2暂停，3拉黑' \n\t`aplanstate` tinyint(2) DEFAULT '0' COMMENT '0 代表禁用，1开启' \n\t`zhibostate` tinyint(2) DEFAULT '1' COMMENT '文字直播服务0 代表禁用，1开启' \n\t`busDepartment` varchar(500) DEFAULT NULL \n\t`jobCode` varchar(250) DEFAULT NULL \n\t`adviserCooperationState` tinyint(2) DEFAULT '0' COMMENT '0未合作，1合作，2暂停，3拉黑' \n\t`openWeChatPlatform` tinyint(2) DEFAULT '0' COMMENT '0未开通微信平台，1开通微信平台' \n\t`adviserHide` tinyint(2) DEFAULT '0' COMMENT '1隐藏' \n\t`openEntrust` tinyint(2) DEFAULT '0' COMMENT '0是没有权限，1是有权限' \n\tPRIMARY KEY (`id`) \n\tUNIQUE `partnerId` (`partnerId`) \n\tKEY `level` USING BTREE (`level`)\n) ENGINE = InnoDB AUTO_INCREMENT = 7171 CHARSET = utf8; \nCREATE TABLE `partner_navigation` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT \n\t`partnerId` int(11) NOT NULL COMMENT '合作者id' \n\t`navigation` varchar(250) DEFAULT NULL \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 882 CHARSET = utf8; \nCREATE TABLE `partner_notice` (\n\t`id` int(8) NOT NULL AUTO_INCREMENT \n\t`title` varchar(64) DEFAULT NULL \n\t`content` varchar(512) DEFAULT NULL \n\t`createTime` datetime DEFAULT NULL \n\t`createUserName` varchar(64) DEFAULT NULL \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 68 CHARSET = utf8; \nCREATE TABLE `partner_notice_number` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT \n\t`partnerId` int(11) DEFAULT NULL \n\t`readNumber` int(11) DEFAULT NULL \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 464 CHARSET = utf8; \nCREATE TABLE `partner_organization` (\n\t`id` int(4) NOT NULL AUTO_INCREMENT \n\t`partnerId` int(8) DEFAULT NULL COMMENT '合作者id' \n\t`organization` varchar(25) DEFAULT NULL \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 95 CHARSET = utf8; \nCREATE TABLE `partner_organization_data` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT \n\t`organizationName` varchar(250) DEFAULT NULL \n\t`organizationId` varchar(250) NOT NULL \n\t`organizationLogo` varchar(500) DEFAULT NULL COMMENT '机构logo' \n\t`organizationUrl` varchar(500) DEFAULT NULL COMMENT '机构地址' \n\t`isDelete` tinyint(4) NOT NULL DEFAULT '0' COMMENT '是否删除0是删除，1是不删除' \n\t`isOnline` tinyint(4) NOT NULL DEFAULT '0' COMMENT '是否上线，0是不上线，1是上线' \n\t`organizationLogoh5_2` varchar(500) DEFAULT NULL COMMENT 'h5  第二个logo' \n\t`organizationLogoh5` varchar(500) DEFAULT NULL COMMENT 'h5机构logo' \n\t`organizationUrlh5` varchar(500) DEFAULT NULL COMMENT 'h5机构url' \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 11 CHARSET = utf8; \nCREATE TABLE `partner_recommend` (\n\t`partner_id` int(11) NOT NULL COMMENT '老师id' \n\t`partner_order` int(11) NOT NULL COMMENT '排序' \n\t`open_recommend` tinyint(2) DEFAULT NULL COMMENT '是否打开推荐' \n\tPRIMARY KEY (`partner_id`) \n\tUNIQUE `index_partner_id` USING BTREE (`partner_id`)\n) ENGINE = InnoDB CHARSET = utf8; \nCREATE TABLE `partner_recommend_copy` (\n\t`partner_id` int(11) NOT NULL COMMENT '老师id' \n\t`partner_order` int(11) NOT NULL COMMENT '排序' \n\t`open_recommend` tinyint(2) DEFAULT NULL COMMENT '是否打开推荐' \n\tPRIMARY KEY (`partner_id`) \n\tUNIQUE `index_partner_id` USING BTREE (`partner_id`)\n) ENGINE = InnoDB CHARSET = utf8; \nCREATE TABLE `partner_service_data` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT \n\t`partnerId` int(11) NOT NULL COMMENT '合作者id' \n\t`serviceStatus` tinyint(4) NOT NULL COMMENT '是否显示服务按钮' \n\t`aplanStatus` tinyint(4) NOT NULL COMMENT '是否显示A计划' \n\t`articleStatus` tinyint(4) NOT NULL COMMENT '是否显示文章' \n\t`circleStatus` tinyint(4) NOT NULL COMMENT '是否显示圈子' \n\t`classesStatus` tinyint(4) NOT NULL COMMENT '是否显示课程' \n\t`blogStatus` tinyint(4) DEFAULT NULL COMMENT '是否显示博客' \n\t`aplanUrl` varchar(500) DEFAULT NULL COMMENT 'a计划链接' \n\t`articleUrl` varchar(500) DEFAULT NULL COMMENT '文章url' \n\t`circleUrl` varchar(500) DEFAULT NULL COMMENT '圈子url' \n\t`classUrl` varchar(500) DEFAULT NULL COMMENT '课程url' \n\t`blogUrl` varchar(500) DEFAULT NULL COMMENT '博客url' \n\tPRIMARY KEY (`id`) \n\tKEY `partnerId` USING BTREE (`partnerId`)\n) ENGINE = InnoDB AUTO_INCREMENT = 15 CHARSET = utf8; \nCREATE TABLE `partner_signelectric` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT COMMENT '自增主键' \n\t`partnerId` int(11) NOT NULL COMMENT '合作者id' \n\t`partnerName` varchar(50) DEFAULT NULL COMMENT '合作者名称' \n\t`trueName` varchar(20) DEFAULT NULL COMMENT '真实姓名' \n\t`idCard` varchar(50) DEFAULT NULL COMMENT '身份证号' \n\t`address` varchar(200) DEFAULT NULL COMMENT '地址' \n\t`createtime` datetime DEFAULT NULL COMMENT '创建时间' \n\t`modifitime` datetime DEFAULT NULL COMMENT '最后修改时间' \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 1279 CHARSET = utf8; \nCREATE TABLE `partner_tag_category` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT COMMENT '自增主键' \n\t`categoryId` int(11) NOT NULL DEFAULT '0' COMMENT '栏目id' \n\t`parentId` int(11) DEFAULT NULL COMMENT '父栏目id' \n\t`categoryName` varchar(50) DEFAULT NULL COMMENT '栏目名称' \n\t`categoryType` int(11) DEFAULT NULL COMMENT '栏目类型' \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 16 CHARSET = utf8; \nCREATE TABLE `partner_tag_data` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT COMMENT '自增主键' \n\t`tagId` int(11) NOT NULL DEFAULT '0' COMMENT '标的id' \n\t`tagName` varchar(50) DEFAULT NULL COMMENT '标的名称' \n\t`tagType` int(11) DEFAULT NULL COMMENT '标的类型' \n\t`categoryId` int(11) DEFAULT NULL COMMENT '标的栏目' \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 49 CHARSET = utf8; \nCREATE TABLE `partner_user_tag` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT COMMENT '自增主键' \n\t`partnerId` int(11) DEFAULT NULL COMMENT '合作者id' \n\t`tag` varchar(200) DEFAULT NULL COMMENT '标的' \n\t`tagType` int(4) DEFAULT NULL COMMENT '类型' \n\t`createtime` datetime DEFAULT NULL COMMENT '时间' \n\t`userDefined` tinyint(1) DEFAULT NULL \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 64455 CHARSET = utf8; \nCREATE TABLE `statistics_px-operation-app` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT \n\t`timestamp` bigint(1) NOT NULL DEFAULT '0' COMMENT '时间戳' \n\t`serviceInterface` varchar(255) NOT NULL DEFAULT '' COMMENT '接口名' \n\t`method` varchar(255) NOT NULL DEFAULT '' COMMENT '方法名' \n\t`type` varchar(10) DEFAULT NULL COMMENT '当前调用的应用类型' \n\t`tps` float(11  2) NOT NULL DEFAULT '0.00' COMMENT 'TPS值' \n\t`kbps` float(11  2) DEFAULT NULL COMMENT '流量' \n\t`host` varchar(50) DEFAULT NULL COMMENT 'ip地址' \n\t`elapsed` int(11) DEFAULT NULL COMMENT '耗时' \n\t`concurrent` int(11) DEFAULT NULL COMMENT '并发数' \n\t`input` int(11) DEFAULT NULL COMMENT '输入值' \n\t`output` int(11) DEFAULT NULL COMMENT '输出大小' \n\t`successCount` int(11) DEFAULT NULL COMMENT '成功次数' \n\t`failureCount` int(11) DEFAULT NULL COMMENT '失败次数' \n\t`remoteAddress` varchar(50) DEFAULT NULL COMMENT '远程地址' \n\t`remoteType` varchar(20) DEFAULT NULL COMMENT '远程应用类型' \n\tPRIMARY KEY (`id`) \n\tKEY `time-index` (`timestamp`) \n\tKEY `method-index` (`method`) \n\tKEY `service-index` (`serviceInterface`)\n) ENGINE = InnoDB AUTO_INCREMENT = 168704 CHARSET = utf8; \nCREATE TABLE `statistics_zhibo-app` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT \n\t`timestamp` bigint(1) NOT NULL DEFAULT '0' COMMENT '时间戳' \n\t`serviceInterface` varchar(255) NOT NULL DEFAULT '' COMMENT '接口名' \n\t`method` varchar(255) NOT NULL DEFAULT '' COMMENT '方法名' \n\t`type` varchar(10) DEFAULT NULL COMMENT '当前调用的应用类型' \n\t`tps` float(11  2) NOT NULL DEFAULT '0.00' COMMENT 'TPS值' \n\t`kbps` float(11  2) DEFAULT NULL COMMENT '流量' \n\t`host` varchar(50) DEFAULT NULL COMMENT 'ip地址' \n\t`elapsed` int(11) DEFAULT NULL COMMENT '耗时' \n\t`concurrent` int(11) DEFAULT NULL COMMENT '并发数' \n\t`input` int(11) DEFAULT NULL COMMENT '输入值' \n\t`output` int(11) DEFAULT NULL COMMENT '输出大小' \n\t`successCount` int(11) DEFAULT NULL COMMENT '成功次数' \n\t`failureCount` int(11) DEFAULT NULL COMMENT '失败次数' \n\t`remoteAddress` varchar(50) DEFAULT NULL COMMENT '远程地址' \n\t`remoteType` varchar(20) DEFAULT NULL COMMENT '远程应用类型' \n\tPRIMARY KEY (`id`) \n\tKEY `time-index` (`timestamp`) \n\tKEY `method-index` (`method`) \n\tKEY `service-index` (`serviceInterface`)\n) ENGINE = InnoDB AUTO_INCREMENT = 1657961 CHARSET = utf8; \n"}	 
979,reset last position when apply snapshot to database applySnapshotToDB里做reset的意义是？ ![image](https://user-images.githubusercontent.com/8461826/46252914-816dc180-c4a2-11e8-8eef-39f8a859b2aa.png) 如果不reset  即使没有ddl变更   这里的position仍然指向上一次ddl的位点 applySnapshotToDB里的position传入的就是最后一次DDL变更应用到内存结构的位点，还是没明白为啥要做reset
978,解决kafka 客户端消息无法ack的bug 解决kafka 客户端消息无法ack的bug
977,同步位点问题 canal 版本：1.0.24 源实例：RDS 问题：canal server在启动之后，配置了多个intancaes，如果在订阅的过程发生了rds主备切换或者实例迁移，怎么修复？(经过试过，修改配置文件只是将intances进行reload，修改信息不会同步至zk中，那么这个时候应该怎么做) 停止canal server，删除zookeeper中/otter/canal/destinations/${your_instance}/1001 整个znode，再重启canal server @1241407808 如果是对应的多个intances，重启server会对其他的intances有影响吗？ 当然会，其他的instance会停掉； canal server会把其他instance 的位点记录到zk中，下一次启动会按照对应的位点继续。 多谢了 
976,canal-kafka-1.1.0版本，binlog没有变化的时候，cpu占用跑满一个核心99.99% cpu (50.00% of 2 core) 使用canal-kafka-1.1.0版本，canal.properties使用的默认配置，启动后cpu占用就很高，binlog没有变化 服务器CPU信息 [cpuinfo.txt](https://github.com/alibaba/canal/files/2430253/cpuinfo.txt) 线程调用信息为： ![123](https://user-images.githubusercontent.com/16751056/46240746-a862bf80-c3de-11e8-9707-8a6bb6771a6e.png) 火焰图 ![image](https://user-images.githubusercontent.com/16751056/46240795-5bcbb400-c3df-11e8-9e51-874821bc76fa.png) 是因为机器配置低还是其他原因 请跟新最新版本 #904
975,数据一直在同步但数据量不对  ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x127a1b89  /5.39.221.52:60106 => 我的canal ip 每隔几秒就报一次这个错，大佬，是有人偷连我的canal吗 ，这个ip好像是荷兰的 5.39.221.52:60106 你自己不要暴露到公网上啊 -_-#
973,[ISSUE 800]提供RocketMQ原生接入canal 提供了rocketmq Connector对接canal，但是目前仅支持单分区有序，待上层canal message可以按照业务字段hash拆分聚合后，再做多分区有序，但是这样对canal的改动较大； 后续按照表明做多分区有序处理，但是在表之间出现关联时，会存在问题。 [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=973) <br/>All committers have signed the CLA. [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=973) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=973) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=973) it.</sub> 客户端的canal-client.yml配置格式有点问题，应该改成： ``` mqTopics: - mqMode: kafka   topic: example   groups:   - groupId: example_g1     outAdapters:     - name: logger ``` rocketMQ可以考虑增加对FlatMessage的支持，做数据hash分片
972,canal1.1.0 ZK模式运行一段时间就停止消费ClosedChannelException 重启客户端之后继续消费，但是位点不更新了 以下是canal服务端出现的的错误日志 2018-09-26 17:14:32.336 [New I/O server worker #1-16] ERROR c.a.otter.canal.server.netty.handler.Ses sionHandler - something goes wrong with channel:[id: 0x4b49367f  /172.16.40.202:46512 :> /172.16.40. 200:11111]  exception=java.nio.channels.ClosedChannelException 	at org.jboss.netty.channel.socket.nio.NioWorker.cleanUpWriteBuffer(NioWorker.java:629) 	at org.jboss.netty.channel.socket.nio.NioWorker.close(NioWorker.java:605) 	at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioSe rverSocketPipelineSink.java:119) 	at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketP ipelineSink.java:76) 	at org.jboss.netty.channel.Channels.close(Channels.java:720) 	at org.jboss.netty.channel.AbstractChannel.close(AbstractChannel.java:208) 	at com.alibaba.otter.canal.server.netty.handler.SessionHandler.exceptionCaught(SessionHandle r.java:272) 	at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwar eChannelHandler.java:48) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.exceptionCaught(ReplayingDecoder.ja va:462) 	at org.jboss.netty.channel.Channels.fireExceptionCaught(Channels.java:432) 	at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:332) 	at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) 	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) 	at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) 	at java.lang.Thread.run(Thread.java:748) 2018-09-26 17:15:30.046 [New I/O server worker #1-1] ERROR c.a.otter.canal.server.netty.handler.Sess ionHandler - something goes wrong with channel:[id: 0x7ac76894  /172.16.40.202:51872 => /172.16.40.2 00:11111]  exception=java.io.IOException: Connection reset by peer 	at sun.nio.ch.FileDispatcherImpl.read0(Native Method) 	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39) 	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223) 	at sun.nio.ch.IOUtil.read(IOUtil.java:192) 	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380) 	at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:322) 	at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) 	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) 	at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) 	at java.lang.Thread.run(Thread.java:748) 重启服务端，位点开始更新了
971,Question: canal v1.1.0  为什么存在DatabaseTableMeta#compareTableMetaDbAndMemory这样的一个比较过程？ 如题 为了确保基于内存tsdb维护的表结构正确性，在做snapshot之前和当前数据库做一下对比 在内存tsdb维护的表结构正确的情况下，下面两中情况仍然会导致DatabaseTableMeta#compareTableMetaDbAndMemory返回false 1. canal消费存在比较大的延迟，此时内存中的表结构和mysql真实的表结构存在差异 2. 回退位点，重复消费binlog，消费很慢，24小时内仍未追上最新的位点 snapshot会每间隔24小时定时发起，如果一次中对比失败，可以在下一次对比成功后记录 3Q  我清楚这块的设计意图了
970, fix bug #968 bug 重现: 1. 启用并行解析 2. 正常连接源端数据库 - 打印线程数 3. 关闭源端数据库 - 打印线程数 通过频繁打印当前的线程数可发现线程数在不断递增，直至OOM 赞一个
969,Fix link error as title tks
968,并行解析下，数据库一直连不上导致OOM异常 版本 v1.1.1-alpha 1 heapdump: ![image](https://user-images.githubusercontent.com/33280738/46004767-435d5000-c0e6-11e8-942d-1b283879a799.png) 
967,Canal 运行一段时间 服务端报错? [New I/O server worker #1-2] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x5c5493e4  /10.152.0.121:18964 => /10.193. 16.48:11111]  exception=java.io.IOException: Connection reset by peer canal 版本 1.1.0 看其他issue 说是idleTimeout 这个空闲超时时间的问题 我看了下报错的点  好像就是在1个小时之后报错。我想问下。我可以设置这个idleTimeout 参数的时间么。谢谢！ SimpleCanalConnector.idleTimeout
966,源码问题，关于MemoryEventStoreWithBuffer#checkFreeSlotAt实现的疑问。 MemoryEventStoreWithBuffer的put操作会调用checkFreeSlotAt检查是否有空位。源码如下： `private boolean checkFreeSlotAt(final long sequence) {          final long wrapPoint = sequence - bufferSize;         final long minPoint = getMinimumGetOrAck();         if (wrapPoint > minPoint) { // 刚好追上一轮             return false;         } else {            //...         }     }` 我想知道，为什么调用getMinimumGetOrAck()方法来获得getSequence或ackSequence的较小值，ackSequence不应该总是小于等于getSequence吗？直接与ackSequence比较不就可以了？ 是的，ack < get  < put，基本满足这个关系
965,tsdb持久化存储的方案使用mysql时可以正常监听到数据，改成h2之后就监听不到数据了 版本是1.1.0 canal.properties中的h2配置 #table meta tsdb info canal.instance.tsdb.enable=true canal.instance.tsdb.dir=${canal.file.data.dir:../conf}/${canal.instance.destination:} canal.instance.tsdb.url=jdbc:h2:${canal.instance.tsdb.dir}/h2;CACHE_SIZE=1000;MODE=MYSQL; canal.instance.tsdb.dbUsername=canal canal.instance.tsdb.dbPassword=canal instance.properties 中的h2配置 #table meta tsdb info  #记录按时间序列记录表结构  canal.instance.tsdb.enable=true #将表结构数据保存在h2 嵌入式数据库中 canal.instance.tsdb.url=jdbc:h2:${canal.instance.tsdb.dir}/h2;CACHE_SIZE=1000;MODE=MYSQL; canal.instance.tsdb.dbUsername=canal canal.instance.tsdb.dbPassword=canal 打开h2 的数据库文件发现meta_history 和 meta_snapshot 中都没有数据。 ![h2](https://user-images.githubusercontent.com/10652165/45876725-0cc5c380-bdce-11e8-8d64-dd5814762051.png) 问题太笼统，关注server异常日志
964,建立tsdb的meta_history表的建表语句中的字段与sqlmap_history中查询语句的字段不一样，导致更改表结构时报错 使用的版本是1.1.0 conf\spring\tsdb\sql\create_table.sql 中建立meta_history表的建表语句如下: ``` CREATE TABLE IF NOT EXISTS `meta_history` (   `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT COMMENT '主键'   `gmt_create` datetime NOT NULL COMMENT '创建时间'   `gmt_modified` datetime NOT NULL COMMENT '修改时间'   `destination` varchar(128) DEFAULT NULL COMMENT '通道名称'   `binlog_file` varchar(64) DEFAULT NULL COMMENT 'binlog文件名'   `binlog_offest` bigint(20) DEFAULT NULL COMMENT 'binlog偏移量'   `binlog_master_id` varchar(64) DEFAULT NULL COMMENT 'binlog节点id'   `binlog_timestamp` bigint(20) DEFAULT NULL COMMENT 'binlog应用的时间戳'   `use_schema` varchar(1024) DEFAULT NULL COMMENT '执行sql时对应的schema'   `schema` varchar(1024) DEFAULT NULL COMMENT '对应的schema'   `table` varchar(1024) DEFAULT NULL COMMENT '对应的table'   `sql` longtext DEFAULT NULL COMMENT '执行的sql'   `type` varchar(256) DEFAULT NULL COMMENT 'sql类型'   `extra` text DEFAULT NULL COMMENT '额外的扩展信息'   PRIMARY KEY (`id`)   UNIQUE KEY binlog_file_offest(`destination` `binlog_master_id` `binlog_file` `binlog_offest`)   KEY `destination` (`destination`)   KEY `destination_timestamp` (`destination` `binlog_timestamp`)   KEY `gmt_modified` (`gmt_modified`) ) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8 COMMENT='表结构变化明细表'; ``` conf\spring\tsdb\sql-map\sqlmap_history.xml 中查询的字段如下: ```xml   <sql id="allVOColumns">         <![CDATA[ 		a.id as id a.gmt_create as gmtCreate a.gmt_modified as gmtModified 		a.destination as destination a.binlog_file as binlogFile a.binlog_offest as binlogOffest a.binlog_master_id as binlogMasterId a.binlog_timestamp as binlogTimestamp 		a.use_schema as useSchema a.sql_schema as sqlSchema a.sql_table as sqlTable a.sql_text as sqlText a.sql_type as sqlType a.extra as extra         ]]>     </sql> ``` 不一致的字段: shema -> sql_shema table ->sql_table sql ->sql_text type ->sql_type 最新主干已经修复
963,Canal ClientIdentity 中的destination 和 clientId 含义是什么? 请帮忙解释一下。 多看wiki
962,canal client如何自定义client端获取到的字段内容的数据类型？ canal client端获取到的字段内容默认为string类型，请大神指教如何才能够自定义client端获取到的字段内容的数据类型？ 多看wiki，根据业务类型自行将字符串转换为具体type
961,canal client端获取的float类型字段的“精度”与mysql中的不一致 现象： 1. mysql中源表的字段类型： float(36 6)  2. shell中查询到的该字段的值：  345.678894  （insert后查询） 3. insert操作触发canal之后，canal中获取到的值：345.6789   （获取方法：client端column.getValue()） 问题： canal client端获取的float类型字段的“精度”与mysql中的不一致。 我曾经尝试改写 com.alibaba.otter.canal.common.utils.CanalToStringStyle.appendDetail方法，如下： <img width="1230" alt="mysql float" src="https://user-images.githubusercontent.com/28953872/45869265-98355980-bdba-11e8-900c-fd9bebe6fcf0.png"> 但是结果现实方法改写不起作用。 请问大神如何修正这个问题？ 祝好！ 告诉我一下重现的办法，mysql版本、canal版本、建表语句、测试SQL等等 已修复，修改已生效，之前不生效的原因是：我没有在intellij idea中将修改的源码重新打包jar并替换所安装的canal server中的lib下的对应jar。精度不一致的修复方法：parser模块下的logeventconvert的parseOneRow方法（ case Types.REAL:）将float转换为double再处理。 是你自己改动了代码导致？ 「现象： mysql中源表的字段类型： float(36 6) shell中查询到的该字段的值： 345.678894 （insert后查询） insert操作触发canal之后，canal中获取到的值：345.6789 （获取方法：client端column.getValue()） 问题： canal client端获取的float类型字段的“精度”与mysql中的不一致。」 这个”精度问题不一致问题“不是由于更改源码导致的。 ******************************************** mysql版本： Server version:		5.7.22-log MySQL Community Server (GPL) canal版本：1.0.24 建表语句： CREATE TABLE `all_types` (   `varchar1` varchar(20) DEFAULT NULL   `inta` int(10) unsigned NOT NULL AUTO_INCREMENT   `tinyint1` tinyint(4) DEFAULT NULL   `smallint1` smallint(6) DEFAULT NULL   `bigint1` bigint(20) DEFAULT NULL   `decimal1` decimal(65 30) DEFAULT NULL   `double1` double DEFAULT NULL   `date1` date DEFAULT NULL   `time1` time DEFAULT NULL   `year1` year(4) DEFAULT NULL   `datetime1` datetime DEFAULT NULL   `timestamp1` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP   `float1` float(36 6) DEFAULT NULL   PRIMARY KEY (`inta`) ) ENGINE=InnoDB AUTO_INCREMENT=111118 DEFAULT CHARSET=latin1 测试SQL： insert into all_types values('a' 111112 11 111 1111111 -1235666.6666688999870 123.1345775019860 sysdate() sysdate() '1999' sysdate() sysdate() 345.67890110); 经过测试发现可能是因为在logeventconvert的parseOneRow方法中做了： case Types.REAL:                          // 对象为number类型，直接valueof即可                         columnBuilder.setValue(String.valueOf(value));                         break; String.valueOf(value)中如果value是float，则会导致精度丢失。 这个还不一定好改 1.  mysql自带的mysqlbinlog工具解析345.67890110直接变为345.679，canal解析为345.6789，如果强转为double则为345.67889404296875 2.  自己插入的是345.67890110，实际mysql存入的是345.678894 3.  mysql对于float的描述，存储上本身存在不精准的行为：https://dev.mysql.com/doc/refman/5.7/en/problems-with-float.html，考虑0.0001的精度误差 我是这样处理的，不知道是否正确： float先转double，mysql存入的为345.678894，double为345.67889404296875，数据类型为float(36 6)，因此scale为6，double中小数点scale+1位即345.6788940，对double中小数点scale位进行4舍5入： 当小数点scale+1位数字>=5  则小数店第scale位+1；否则不变化。因此345.6788940取6位是：345.678894。 经过了几次测试，目前发现经处理后的结果和mysql中存入结果一致。 没有可依托的可靠case，mysql自身的binlog解析代码也是直接float对象处理 insert语句直接插入345.6789，mysql里select出来的结果也是345.678894 是啊，这样的话确实是不好处理。 float这种类型本身决定的啦 mysql里不建议使用float
960,有没有工具可以把 Message 转为 SQL语句？ 我需要把Message 转换成从库可执行的语句，Canal 是否提供了相关接口？ 看一下client里的example
959,kafka消息顺序问题 kafka只能保证partition 级消息的顺序， 对于一张表的某个记录需要保证严格的顺序，这个怎么处理？ canalDestinations 并没有对于kafka key做什么特殊处理 默认全部发送打partition 0  配置了pk hash的会发送到对应partition #958
958,kafka生产端增加按pk hash到对应partition功能 kafka生产端增加按pk hash到对应partition功能，未设置分区hash的或则默认状态全部发送到分区0 server端kafka增加如下配置： ```` canalDestinations:   - canalDestination: example     topic: expample     #对应topic分区数量     partitionsNum: 3     partitionHash:       #库名.表名: 唯一主键  指定库表中的唯一主键用其值作为hash分发partition       mytest.person: id ```` 此功能只适用于flatMessage模式 建议可以优化一下表的hash字段定义，方便在大量表的配置问题： 1.  增加通配符匹配表，同时支持pk这样的默认关键字 2. 未命中通配符的，按照 schema.table : columngA 进行匹配 迫不及待
957,canal.kafka的一些相关问题 目前用canal.kafka实现了一套数据增量到elasticsearch的项目。有一些问题请教？ 1. 当client挂掉，这段时间内的增量数据，在client重启后，这些增量数据不会通知到client。会出现数据丢失问题吗？ 2. 有没有关于cannal的监控方案。类似jmxtrans+InfluxDb+Grafana `对于kafka的监控。 多看一下文档，你想要的答案都有，监控有直接对接prometheus
956,有没有canal未来一年的roadmap 可关注issue和wiki
955,canal v1.1.0开启GTID会出现 CanalParseException: java.util.ConcurrentModificationException异常 配置canal.instance.gtidon=true如下： # enable gtid use true/false canal.instance.gtidon=true Caused by: java.util.ConcurrentModificationException 	at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:901) 	at java.util.ArrayList$Itr.next(ArrayList.java:851) 	at com.alibaba.otter.canal.parse.driver.mysql.packets.UUIDSet.toString(UUIDSet.java:125) 	at com.alibaba.otter.canal.parse.driver.mysql.packets.MysqlGTIDSet.toString(MysqlGTIDSet.java:111) 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.createHeader(LogEventConvert.java:849) 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.createHeader(LogEventConvert.java:823) 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.buildQueryEntry(LogEventConvert.java:807) 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsQueryEvent(LogEventConvert.java:394) 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:137) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$DmlParserStage.onEvent(MysqlMultiStageCoprocessor.java:298) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$DmlParserStage.onEvent(MysqlMultiStageCoprocessor.java:288) 	at com.lmax.disruptor.WorkProcessor.run(WorkProcessor.java:143) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) 使用最新的1.1.1-alpha 版本
954,canal1.1.1在RDS上遇到的几个问题 使用的是RDS mysql5.6高可用版，2个RDS都申请了高权限账号，按照常规instance.properties配置，一个能够启动，一个报错。 2018-09-19 11:30:38.912 [destination = xxx  address = xxx/xxx:3306   EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump addressxxx/xxx:3306 has an error  retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: ErrorPacket [errorNumber=1142  fieldCount=-1  message=SHOW VIEW command denied to user 'canal'@'xxx' for table 'db_view'  sqlState=42000  sqlStateMarker=#] 好像高权限账号也没有Mysql库的db_view表的权限。 也没有这个权限，但是启动正常。 咨询了下阿里云工程师，说是canal.instance.filter.regex配置的问题，需要剔除mysql库的监听，然后启动正常，但是客户端收不到数据。 第二个问题，在服务端删除节点了，但是每次启动报错，can't find destination:{} 用源码跟踪调试，发现还在启动原来删除的节点，太奇怪了。我把整个deployer包删除重新部署还是会启动那个删除的节点。 我用的是单机部署，没有使用zk，望有空解答下，谢谢。 老实说没太看懂你的问题 > 老实说没太看懂你的问题 背景是这样的，在ECS上单机部署了deploy server端，然后连了4个RDS。第一个问题是我们访问不了mysql下面的一些表，高权限账号也是不行，然后这块好像只能通过配置filter去掉mysql schema的监控。（当时收不到binlog数据应该是filter正则配置错误的问题）。第二个问题就是在deploy server的conf文件夹下删除了一个RDS的配置，但是重启了之后，canal 服务端还是去加载这个删除的配置，很奇怪，我把deploy工程删除重新部署还是会去加载，这个是我debug看到的。然后只能重启ECS了，重启之后就好了。所以这里怀疑是不是哪里缓存了conf下的节点。 缓存应该是没有的，都在conf目录下 > 缓存应该是没有的，都在conf目录下 另外，如果单机模式，server端的log数据如果不消费掉的话，会不会爆内存？这几天ECS莫名爆满了，内存优化这块有什么好的建议吗?或者换zk？ 多看wiki，不会爆内存
953,单词拼写修正 MemoryEventStoreWithBuffer 内 INIT_SQEUENCE 改为 INIT_SEQUENCE [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=953) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=953) before we can accept your contribution.<br/><hr/>**zhikuodai** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=953) it.</sub> tks
952,canal 1.1.0 不打印数据rowdata信息  ![default](https://user-images.githubusercontent.com/16894071/45675324-b3595c80-bb61-11e8-9d31-103f0386ba22.png) 类信息参考 https://github.com/alibaba/canal/blob/master/example/src/main/java/com/alibaba/otter/canal/example/AbstractCanalClientTest.java 写的  ` protected void printEntry(List<Entry> entrys) {         for (Entry entry : entrys) {             long executeTime = entry.getHeader().getExecuteTime();             long delayTime = System.currentTimeMillis() - executeTime;             Date date = new Date(entry.getHeader().getExecuteTime());             SimpleDateFormat simpleDateFormat = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");             if (entry.getEntryType() == EntryType.TRANSACTIONBEGIN || entry.getEntryType() == EntryType.TRANSACTIONEND) {                 if (entry.getEntryType() == EntryType.TRANSACTIONBEGIN) {                     TransactionBegin begin = null;                     try {                         begin = TransactionBegin.parseFrom(entry.getStoreValue());                     } catch (InvalidProtocolBufferException e) {                         throw new RuntimeException("parse event has an error   data:" + entry.toString()  e);                     }                     // 打印事务头信息，执行的线程id，事务耗时                     logger.info(transaction_format                             new Object[] { entry.getHeader().getLogfileName()                                     String.valueOf(entry.getHeader().getLogfileOffset())                                     String.valueOf(entry.getHeader().getExecuteTime())  simpleDateFormat.format(date)                                     entry.getHeader().getGtid()  String.valueOf(delayTime) });                     logger.info(" BEGIN ----> Thread id: {}"  begin.getThreadId());                     printXAInfo(begin.getPropsList());                 } else if (entry.getEntryType() == EntryType.TRANSACTIONEND) {                     TransactionEnd end = null;                     try {                         end = TransactionEnd.parseFrom(entry.getStoreValue());                     } catch (InvalidProtocolBufferException e) {                         throw new RuntimeException("parse event has an error   data:" + entry.toString()  e);                     }                     // 打印事务提交信息，事务id                     logger.info("----------------\n");                     logger.info(" END ----> transaction id: {}"  end.getTransactionId());                     printXAInfo(end.getPropsList());                     logger.info(transaction_format                             new Object[] { entry.getHeader().getLogfileName()                                     String.valueOf(entry.getHeader().getLogfileOffset())                                     String.valueOf(entry.getHeader().getExecuteTime())  simpleDateFormat.format(date)                                     entry.getHeader().getGtid()  String.valueOf(delayTime) });                 }                 continue;             }             if (entry.getEntryType() == EntryType.ROWDATA) {                 RowChange rowChage = null;                 try {                     rowChage = RowChange.parseFrom(entry.getStoreValue());                 } catch (Exception e) {                     throw new RuntimeException("parse event has an error   data:" + entry.toString()  e);                 }                 EventType eventType = rowChage.getEventType();                 String tableName = entry.getHeader().getTableName();                 for (CanalEntry.RowData rowData : rowChage.getRowDatasList()) {                     Map map = new HashMap<>();                     map.put("tableName" tableName);                     map.put("eventType" eventType);                     map.put("eventDate"  org.apache.http.client.utils.DateUtils.formatDate(new Date() "yyyy-MM-dd"));                     if (eventType == CanalEntry.EventType.DELETE) {                         printColumn(map rowData.getBeforeColumnsList());                         //发送删除命令                     } else if (eventType == CanalEntry.EventType.INSERT) {                         //发送插入命令                         printColumn(map rowData.getAfterColumnsList());                     } else {                         //发送修改命令                         printColumn(map rowData.getAfterColumnsList());                     }                 }                 if (eventType == EventType.QUERY || rowChage.getIsDdl()) {                     logger.info(" sql ----> " + rowChage.getSql() + SEP);                     continue;                 } //                printXAInfo(rowChage.getPropsList());             }         }     } ` 检查过滤条件
951,V1.1.0版本，源端没有数据时，dump日志的socket2.5s就timeout抛异常，导致重连 启动Canal，如果持续有数据，不报错。当没有数据时，2.5s就SocketTimeoutException异常，导致重连。 报错位于BioSocketChannel.read(BioSocketChannel.java:123) 查看代码，BioSocketChannel中的SO_TIMEOUT=1000，即1s，如果read()不到数据，1s就会抛出SocketTimeoutException。其中累加到入参的2.5s，就会向上抛出异常，导致重连。 首先累加的timeout是25s。 其次canal开启了master heartbeat，默认没数据时每15s会获得一个heartbeat包重置超时时间。 可以从这里着手检查一下系统。 @lcybo 我描述错了，是25s，不是2.5s。 在文件DirectLogFetcher中定义了master hearbeat=15；然后READ_TIMEOUT_MILLISECONDS=(MASTER_HEARTBEAT_PERIOD_SECONDS + 10) * 1000;即read()的时候timeout=25s。 现在现象就是BioSocketChannel中的read(25)方法传入的timeout时间是25s，如果没有增量数据，超过25s，就抛出SocketTimeoutException异常了。 如你所说： 如果不开启master heartbeat的话，是不是25s就会sockettimeout异常，进行重连。 开启master heartbeat是设置配置文件中的canal.instance.detecting.enable =true 和canal.instance.detecting.heartbeatHaEnable = true么？我现在设置了没有起作用呢？是哪里没设置对么 canal.instance.detecting.enable和binlog master heartbeat不是一个概念。 canal.instance.detecting.enable是fork出另一个connection由canal server发起的heartbeat。 binlog master heartbeat则是MySQL dump协议的一部分，由MySQL master发起直接在dump connection中传输。canal server 无需配置直接开启的（为了解决半连接问题）。 https://dev.mysql.com/doc/refman/8.0/en/change-master-to.html 可以看MASTER_HEARTBEAT_PERIOD 这一段，检查下master端是否生效. @lcybo 感谢。我看了下介绍，这个MASTER_HEARTBEAT_PERIOD 是slave端设置的吧。 canal server做为slave直接开启了已经，应该不需要Canal端设置什么了。 你让我检查master是否生效，意思是在master要设置什么？怎么检查？ 在master端执行CHANGE MASTER TO MASTER_HEARTBEAT_PERIOD=20么？我命令执行成功了，但是还是25s断开，重连。 我的Canal server连接的是master. 或者说canal server为了解决半连接问题，没数据时，25s自动重连一次，是正常的。 2018-09-18 17:40:44.165 [destination = example   address = /127.0.0.1:3306   EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - Socket timeout expired  closing connection java.net.SocketTimeoutException: Timeout occurred  failed to read 4 bytes in 25000 milliseconds. 	at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.read(BioSocketChannel.java:123) ~[classes/:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:174) ~[classes/:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:77) ~[classes/:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:213) [classes/:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:248) [classes/:na] 	at java.lang.Thread.run(Unknown Source) [na:1.7.0_45] @lcybo 感谢，我又试了下，可以了。之前连接的master是mysql5.1，它不支持MASTER_HEARTBEAT_PERIOD。现在换成mysql5.7了，使得MASTER_HEARTBEAT_PERIOD生效。不在25s断了。那如果连接5.6以下版本，不支持MASTER_HEARTBEAT_PERIOD的，就会25s断开重连吧 @theonesmx 嗯，这个feature是5.5加入的。
950,GTID模式同步下报gtid_purged的错误 1.1.0版本启用gtid模式。源数据库gtid_purged有值，启动后报错。无法同步 ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = The slave is connecting using CHANGE MASTER TO MASTER_AUTO_POSITION = 1  but the master has purged binary logs containing GTIDs that the slave requires. 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:102) see #902。着急用的话可以先将主干代码编译打包后试下 google这个errmsg
949,Fix simpleCanalConnector bug as title tks
948,序列化的问题 更新版本后出现反序列化错误 ERROR com.alibaba.otter.canal.client.kafka.MessageDeserializer - Error when deserializing byte[] to message 
947,修复group模式报错 修复 https://github.com/alibaba/canal/issues/943 1. counter 统一到abstractMySQLEventParser层。 2. 对于gourp中每个parser单独统计。 tks
945,收到非订阅表的DML语句 Mysql 5.7 binlog-rows-query-log-events ON Canal 1.1.0 canal.instance.filter.query.dml false 会收到非订阅表的DML语句（不正常），不会收到非订阅表数据（正常） 设置 canal.instance.filter.query.dml = true吧，目前不会对5.7的DML query语句进行过滤，因为无法获取对应的dbname @agapple ok，本来想记录下SQL语句，无法过滤的话就把binlog-rows-query-log-events也关掉了。tks
944,请问CanalAdapterKafkaWorker的ack是否存在问题？ com.alibaba.otter.canal.client.adapter.loader.CanalAdapterKafkaWorker类以下代码，意思是只有poll到null或者执行时间大于一分钟才会执行ack操作，试了下一直正常消费数据，永远不会执行connector.ack()，这样是不是有问题呢，请问为什么要这样写？ while (running) {                     try {                         // switcher.get(); //等待开关开启                         final Message message = connector.getWithoutAck(100L  TimeUnit.MILLISECONDS timeFlag startTime);                         timeFlag = false;                         executing.set(true);                         if (message != null) {                            .......                             // 间隔一段时间ack一次  防止因超时未响应切换到另外台客户端                             long currentTS = System.currentTimeMillis();                             while (executing.get()) {                                 // 大于1分钟未消费完ack一次keep alive                                 if (System.currentTimeMillis() - currentTS >  60000) {                                     connector.ack();                                     currentTS = System.currentTimeMillis();                                 }                             }                         } else {                             connector.ack();                         }                     } catch (CommitFailedException e) {                         logger.warn(e.getMessage());                     } catch (Exception e) {                         logger.error(e.getMessage()  e);                         TimeUnit.SECONDS.sleep(1L);                     }                 } 确实有bug，在最新pr中会修复。这里的间隔一段时间ack是为了防止消费超时kafka Consumer会切换到另外一台 @rewerma 好的，谢谢
943,v1.1.0版本goup模式问题 启动时 goup的实例会报错 其它的实例不会，但是也会正常消费。 错误如下: com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.lang.String)]: [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.nio.charset.Charset)] 2018-09-12 14:51:57.847 [main] WARN  com.alibaba.otter.canal.prometheus.PrometheusService - Unable to register instance exports for coupon. java.lang.IllegalArgumentException: CanalEventParser must be MysqlEventParser 	at com.alibaba.otter.canal.prometheus.impl.ParserCollector.register(ParserCollector.java:86) ~[canal.prometheus-1.1.0.jar:na] 	at com.alibaba.otter.canal.prometheus.CanalInstanceExports.register(CanalInstanceExports.java:65) ~[canal.prometheus-1.1.0.jar:na] 	at com.alibaba.otter.canal.prometheus.PrometheusService.register(PrometheusService.java:96) ~[canal.prometheus-1.1.0.jar:na] 	at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.start(CanalServerWithEmbedded.java:109) [canal.server-1.1.0.jar:na] 	at com.alibaba.otter.canal.deployer.CanalController$2$1.processActiveEnter(CanalController.java:140) [canal.deployer-1.1.0.jar:na] 	at com.alibaba.otter.canal.common.zookeeper.running.ServerRunningMonitor.processActiveEnter(ServerRunningMonitor.java:244) [canal.common-1.1.0.jar:na] 	at com.alibaba.otter.canal.common.zookeeper.running.ServerRunningMonitor.initRunning(ServerRunningMonitor.java:149) [canal.common-1.1.0.jar:na] 	at com.alibaba.otter.canal.common.zookeeper.running.ServerRunningMonitor.start(ServerRunningMonitor.java:103) [canal.common-1.1.0.jar:na] 	at com.alibaba.otter.canal.deployer.CanalController.start(CanalController.java:438) [canal.deployer-1.1.0.jar:na] 	at com.alibaba.otter.canal.deployer.CanalLauncher.main(CanalLauncher.java:38) [canal.deployer-1.1.0.jar:na] 2018-09-12 14:51:57.847 [main] INFO  c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-coupon  2018-09-12 14:51:57.858 [main] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - start successful.... 好像是说真正用的setConnectionCharset(Charset connectionCharset)这个方法 但是确设置成使用 setConnectionCharset(String connectionCharset)这个方法，但是为什么其它模式不报错 https://github.com/alibaba/canal/pull/947 @shizhengchao 可以的话，请打补丁测试一下，万分感谢。 > #947 > @shizhengchao 可以的话，请打补丁测试一下，万分感谢。 @lcybo  嗯，现在已经不报错了。()  @shizhengchao thx
942,增加当前的gtid相关信息至Entry Header  1. 将当前的gtid，sequence no 和last committed信息增加至Entry(trx begin/end  rowdata) Header 中 客户端可从property中获取相关值. 2. example 增加获取对应gtid相关信息示例 protected Map<String  String> gtidMap = new HashMap<>();  put和get时会存在一个并发性吧 LogDecoder 会为每个event 事件实例化LogHeader 对象，当GTID事件到来时，对logHeader 的map.put操作，后续无论是串行还是并行解析，都是对这个event事件的私有header中map进行get操作. 我的理解: ``` 串行解析顺序应该是: put |-> get-> get -> get 并行解析顺序应该是: put |->get     |-> get      |-> get ``` tks
941,增加扁平Message用于kafka消息投递 增加FlatMessage类，用于kafka的消息投递，Message的每个Entry会被拆分成对应FlatMessage，使用json序列化发送至kafka消费。 后续将实现对FlatMessage按pk hash 和 partition进行拆分的需求 **增加配置** server端： conf/kafka.yml 增加新属性： flatMessage: true  true：代表使用FlatMessage消息投递，false：代表使用原生Message消息投递 client端： canal_client/conf/canal-client.yml 增加属性： flatMessage: true true：代表使用FlatMessage消息投递，false：代表使用原生Message消息投递 **注：** FlatMessage只对kafka消息投递有效，不影响tcp的原生Message消息传输 ![image](https://user-images.githubusercontent.com/33280738/45535386-e08bcf00-b830-11e8-8349-37378d5470bb.png) 感觉这个最大差距应该在: 一个是同步提交到Kafka，一个使用异步提交吧?否则从你的封装来看，这个差异不应该这么大呀。有尝试过都改成异步测试吗？ 改成异步tps并没有区别很大 OK
940,1.1.0生产环境4个destination点位信息有2个没有写zk2个不更新 从zk上看01和04destinaton没有cursor目录 02和03有cursor目录但是不更新，重启了2次都是如此 我部署另外一个canal server实例的时候，没有报 TableIdNotFoundException，点位信息全部正常 启动的时候canal.canal.log有报错信息： 2018-09-13 23:46:59.156 [destination = ordercanalprd02   address = /***:3321   EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address /***:3321 has an error  retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.TableIdNotFoundException: not found tableId:110 Caused by: com.alibaba.otter.canal.parse.exception.TableIdNotFoundException: not found tableId:110 2018-09-13 23:46:59.109 [destination = ordercanalprd01   address = /***:3320   EventParser] WARN  c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - prepare to find start position ::1536827400000 **2018-09-13 23:46:59.151 [destination = ordercanalprd03   address = /***:3322   EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address /***:3322 has an error  retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.TableIdNotFoundException: not found tableId:118 Caused by: com.alibaba.otter.canal.parse.exception.TableIdNotFoundException: not found tableId:118** 2018-09-13 23:46:59.156 [destination = ordercanalprd02   address = /***:3321   EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address /***:3321 has an error  retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.TableIdNotFoundException: not found tableId:110 Caused by: com.alibaba.otter.canal.parse.exception.TableIdNotFoundException: not found tableId:110 2018-09-13 23:46:59.203 [destination = ordercanalprd03   address = /---:3322   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:ordercanalprd03[com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.TableIdNotFoundException: not found tableId:118 Caused by: com.alibaba.otter.canal.parse.exception.TableIdNotFoundException: not found tableId:118 ] 2018-09-13 23:46:59.204 [destination = ordercanalprd02   address = /---:3321   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:ordercanalprd02[com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.TableIdNotFoundException: not found tableId:110 Caused by: com.alibaba.otter.canal.parse.exception.TableIdNotFoundException: not found tableId:110 ] 配置信息： # canal.properties binlog filter config canal.instance.filter.druid.ddl = true canal.instance.filter.query.dcl = true canal.instance.filter.query.dml = true canal.instance.filter.query.ddl = true canal.instance.filter.table.error = true canal.instance.filter.rows = false canal.instance.filter.transaction.entry = false table meta tsdb info **canal.instance.tsdb.enable=false** canal.instance.tsdb.dir=${canal.file.data.dir:../conf}/${canal.instance.destination:} canal.instance.tsdb.url=jdbc:h2:${canal.instance.tsdb.dir}/h2;CACHE_SIZE=1000;MODE=MYSQL; canal.instance.tsdb.dbUsername=canal canal.instance.tsdb.dbPassword=canal canal.instance.global.spring.xml = classpath:spring/default-instance.xml # instance.properties canal.instance.gtidon=false position info canal.instance.master.address=***:3322 canal.instance.master.journal.name= canal.instance.master.position= canal.instance.master.timestamp=1536827400000 canal.instance.master.gtid=  rds oss binlog canal.instance.rds.accesskey= canal.instance.rds.secretkey= canal.instance.rds.instanceId= table meta tsdb info **canal.instance.tsdb.enable=false** # kafka.yml配置信息： servers: *** retries: 0 batchSize: 16384 lingerMs: 1 bufferMemory: 33554432 canal的批次大小，单位 k，量大建议改为1M canalBatchSize: 50 filterTransactionEntry: true canalDestinations:   - canalDestination: ordercanalprd01     topic: ordercanalprd01     partition:   - canalDestination: ordercanalprd02     topic: ordercanalprd01     partition:   - canalDestination: ordercanalprd03     topic: ordercanalprd01     partition:   - canalDestination: ordercanalprd04     topic: ordercanalprd01     partition: 我自查了一遍 估计这个错误是发生在 canal.destinations= 这个配置直接给空了 我把这个地方配上，然后slaveid按实例自增部署了其他28个destination都没有发生问题 我猜应该是启动过程中扫描destination和其他代码的先后顺序引发的问题 
939,canal1.1.0往kafka发送数据量和mysql产生的量不一致 我的kafka.yml是这样配置的： retries: 0 batchSize: 1024 lingerMs: 0 bufferMemory: 33554432 # canal的批次大小，单位 k，量大建议改为1M canalBatchSize: 50 filterTransactionEntry: true canalDestinations:   - canalDestination: elample     topic: elample     partition: mysql用存储过程插入10000条数据，每次都只能在kafka消费到6千多条 batchSize不能改太大，kafka消息体默认最大不能超过1M数据，所以canalBatchSize必须1M一下建议500K，batchSize可以按默认不需要修改 @rewerma  他这个单位不是字节的吗？1024按字节算的话不到1M的 我改成默认的也是一样的情况 retries: 0 batchSize: 16384 lingerMs: 1 bufferMemory: 33554432 # canal的批次大小，单位 k canalBatchSize: 50 filterTransactionEntry: true 试过了，这几个参数，都是一样的情况，每次都是只能同步六千六百多条数据
938,Fix upgrade proto2 to proto3 Fix upgrade proto2 to proto3  And  that test is already compatible with previous versions of the client.
937,canal.instance.filter.transaction.entry不起作用 mysql版本 5.6.26 canalserver版本 1.0.24 问题1： 在canal.properties 里配置 canal.instance.filter.transaction.entry = true 之后 在canal client 里仍然可以看到entryType 有 transactionbegin 和 transactionend  请问，怎么处理可以在server端去除掉 transcation 问题二： 在instance.properties里配置了canal.instance.filter.regex 之后 在canal client 里仍然可以看到未关注表的  transactionbegin 和 transactionend ，相比关注表，未关注表只是少了 query的eventType 和 dml的eventType 请问，canalserver能不能做到只去同步我关注表的binlog ，以此减少canalserver的负载 问题2是由问题1未生效导致的，建议升级至1.1.1再试一下 ps:  1. 开启canal.instance.filter.transaction.entry =true后，会导致无法ack，一旦同步出现问题，将有可能导致丢数据甚至批量数据重发的可能。 2. canal server 在1.1.0做了大量性能优化，之前版本的性能问题在1.1.0上应该得到大幅度缓解甚至解决。 感谢回答！目前的应用场景还不允许脱离ack ; 之后尝试一下1.1 版本
936,canal1.1.0 并发解析下多线程对GTID更新操作 导致java.util.ConcurrentModificatioinException 具体错误如下: 2018-09-12 18:38:25 776||destination = 1002   address = /*********:3306   EventParser|?|ERROR|c.a.o.c.p.i.mysql.MysqlEventParser - dump address ************:3306 has an error  retrying. caused by  com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: java.util.ConcurrentModificationException: null 	at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:901) ~[na:1.8.0_121] 	at java.util.ArrayList$Itr.next(ArrayList.java:851) ~[na:1.8.0_121] 	at com.alibaba.otter.canal.parse.driver.mysql.packets.UUIDSet.toString(UUIDSet.java:125) ~[canal.parse.driver-1.1.0-20180912.102450-4.jar:na] 	at com.alibaba.otter.canal.parse.driver.mysql.packets.MysqlGTIDSet.toString(MysqlGTIDSet.java:111) ~[canal.parse.driver-1.1.0-20180912.102450-4.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.createHeader(LogEventConvert.java:849) ~[canal.parse-1.1.0-20180912.102500-4.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:561) ~[canal.parse-1.1.0-20180912.102500-4.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$DmlParserStage.onEvent(MysqlMultiStageCoprocessor.java:302) ~[canal.parse-1.1.0-20180912.102500-4.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$DmlParserStage.onEvent(MysqlMultiStageCoprocessor.java:288) ~[canal.parse-1.1.0-20180912.102500-4.jar:na] 	at com.lmax.disruptor.WorkProcessor.run(WorkProcessor.java:143) ~[disruptor-3.4.2.jar:na] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_121] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_121] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_121] 2018-09-12 18:38:25 776||destination = 1002   address = /172.16.2.71:3306   EventParser|?|ERROR|c.a.o.c.common.alarm.LogAlarmHandler - destination:1002[com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: java.util.ConcurrentModificationException 	at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:901) 	at java.util.ArrayList$Itr.next(ArrayList.java:851) 	at com.alibaba.otter.canal.parse.driver.mysql.packets.UUIDSet.toString(UUIDSet.java:125) 	at com.alibaba.otter.canal.parse.driver.mysql.packets.MysqlGTIDSet.toString(MysqlGTIDSet.java:111) 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.createHeader(LogEventConvert.java:849) 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:561) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$DmlParserStage.onEvent(MysqlMultiStageCoprocessor.java:302) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$DmlParserStage.onEvent(MysqlMultiStageCoprocessor.java:288) 	at com.lmax.disruptor.WorkProcessor.run(WorkProcessor.java:143) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) ]st.java:901) 	at java.util.ArrayList$Itr.next(ArrayList.java:851) 	at com.alibaba.otter.canal.parse.driver.mysql.packets.UUIDSet.toString(UUIDSet.java:125) 	at com.alibaba.otter.canal.parse.driver.mysql.packets.MysqlGTIDSet.toString(MysqlGTIDSet.java:111) 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.createHeader(LogEventConvert.java:849) 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:561) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$DmlParserStage.onEvent(MysqlMu JDK: 1.8.0_21 配置信息: preallell=true parallelthreadSize=4(计算结果) 分析原因(不一定正确): 在LogEventConvert的180行 对gtid_set进行了更新 其实是更新了MysqlGTIDSet的34行 sets.get(sid).intervals.addAll(us.intervals) 这个地方更新了list的modCount++ 在UUIDSet的tostring方法中 遍历了intervals 结果触发了ArrayList的901行的checkForComodification方法 由于addAll增加了modCount 但是execptedModCount没有增加 跑出了异常. 解决方法: 方法1:preallell=false 方法2:多线程情况下 是不是需要加锁. https://github.com/alibaba/canal/pull/902 可以先尝试用一下1.1.1的alpha版本 遇到了同样的问题，分析原因跟mycat-lulin一致，现先采用preallell=false的方法测试一下，后续考虑加锁
935,如何指定一个表的数据到一个partition，而不是分散到多个partition里 或者说如何确认？ Kafka创建topic的时候partition只给一个不就ok了 #958 
934,如何设置一张表到一个topic的指定partition 这个把这个表名作为message的key就行了吧。 #958 
933,canal 修改instance配置文件，重新启动实例，这个信息会同步至zk中吗 版本是1.0.24 不会
932,请问能拿到单条数据吗，否则往kafka发送只能是单分区，多分区顺序无法保证 请问1.10发送到kafka的话，现在只能是单分区发送，多分区顺序不能保证，我们希望能发送到不同分区，并且按照key进行hash分区，我的做法是把entrys拿出来遍历，再把rowchange拿出来遍历，最后得到一条rowdata，用库名加主键的值拼出key，按照key发送这条数据，就可以保证一条数据总是会到一个分区，但是觉着这样效率很低，请问之后会有能直接拿到单条数据的方法吗，还是现在就有呢？ 修改后的代码如下： public void send(KafkaProperties.Topic topic  Message message) throws IOException {                  //获取entrys         List<ByteString> rawEntries = message.getRawEntries();         for (ByteString rawEntry : rawEntries) {             message.addEntry(CanalEntry.Entry.parseFrom(rawEntry));         }         //遍历entrys         for (CanalEntry.Entry entry : message.getEntries()) {             String tableName = entry.getHeader().getTableName();             CanalEntry.RowChange rowChage = CanalEntry.RowChange.parseFrom(entry.getStoreValue());             CanalEntry.EventType eventType = rowChage.getEventType(); //            遍历rowchange             for (CanalEntry.RowData rowData : rowChage.getRowDatasList()) {                 String key = "";                 List<CanalEntry.Column> list = null;                 if (eventType == CanalEntry.EventType.DELETE) {                     list = rowData.getBeforeColumnsList();                 } else {                     list = rowData.getAfterColumnsList();                 }                 //重新封装                 CanalEntry.RowData.Builder rowDataBuilder = CanalEntry.RowData.newBuilder();                 for (CanalEntry.Column column : list) {                     if (column.getIsKey()) {                         key = tableName + "_" + column.getValue();                     }                     if (eventType == CanalEntry.EventType.DELETE) {                         rowDataBuilder.addBeforeColumns(column);                     } else {                         rowDataBuilder.addAfterColumns(column);                     }                 }                 CanalEntry.RowChange.Builder rowChangeBuilder = CanalEntry.RowChange.newBuilder();                 rowChangeBuilder.addRowDatas(rowDataBuilder);                 CanalEntry.RowChange rowChangeNew = rowChangeBuilder.build();                 CanalEntry.Entry.Builder entryBuilder = CanalEntry.Entry.newBuilder();                 entryBuilder.setHeader(entry.getHeader())                         .setEntryType(entry.getEntryType())                         .setStoreValue(rowChangeNew.toByteString());                 List<CanalEntry.Entry> listEntry = new ArrayList<>();                 listEntry.add(entryBuilder.build());                 Message messageNew = new Message(message.getId());                 messageNew.setEntries(listEntry);                 //按照rowkey进行hash分区发送                 ProducerRecord<String  Message> record;                 record = new ProducerRecord<String  Message>(topic.getTopic()  key  messageNew);                 producer.send(record);                 if (logger.isDebugEnabled()) {                     logger.debug("send message to kafka topic: {} \n {}"  topic.getTopic()  message.toString());                 }             }         }     } 针对数据hash到不同分区，如果高效需要在构建protobuf之前就完成计算，否则就会有解压反序列化的问题 感谢回复，但是看代码只能按批获取数据，不反序列化出来没法直接拆成单条吧 其实也可以这样处理：canal 写入kafka的数据作为贴源的数据包存在，在Kafka之后可以做个sink进行分发、合并等预处理工作，因为无论如何必须要有个处理端对数据包进行解析，还有这部分贴源的数据一直是以Message包的形式存在于kafka中，无论后续的数据需要如何处理，贴源的数据会作为基础数据存，服务于不同的数据流应用场景中，扩展性也会很好。 还有几个场景也需要考虑并处理: 1.  多个key，组合key 2. 对包数据进行拆解后，幂等处理 @wingerx 嗯，感谢提供思路，分层能更好的进行解耦，是个不错的方法，但贴源层也只能是单分区，预处理分发部分也只能是单线程，整体性能跟现在是一样的。其实现在量也达不到不够用的程度，只是想看看是不是有更好的方式，所以来跟大家交流一下 @undeadwing 把并发打开试了没？不知你目前数据库的tps是多少，延迟是多大？ @wingerx 我本来用group模式，接了5个mysql，发现很吃力，现在拆了，每个mysql接一个default模式的实例，单个实例过滤后tps正常也就几十到一百多吧，没有延迟，业务有时候会全表update一个字段，那时候会卡很久，tps也很高。之前没用kafka的时候，数据有错server端需要回退，然后client只能单线程追数，追的比较慢，所以现在想使用kafka多分区，之后出问题或者全表update多线程消费就很快了 client如果消费逻辑组装数据很慢，这个消费性能太低了，client完全可以多线程消费，事务提交后记录一个offset到zk，如果kill-9，下次从这个offset重新开始消费，消费输出需要保证幂等；实际上消费线程池实践中都是出于饥饿状态，阻塞队列没有任务等待，刚开始上线的时候可以不考虑这个场景；如果需要保证顺序问题，在client消费的时候做一个hash后线程池绑定即可(初始化N个单线程的线程池)，我现在就是按照这个思路处理的，8个 canal server回溯历史binlog25分钟拉1100W消息，消费8个节点每个节点50线程池需要40分钟消费完，如果是单线程消费遇到大量更新就傻眼了。 如果能够在源头入kafka的时候就做好数据分发，client端就不用考虑并发的顺序问题了 @yin007008 感谢提供思路，之前没想过线程池绑定。我现在的做法是在kafka发送的时候将数据拆分成单条，按照key发送到kafka多分区，这样的好处一是消费时候顺序可以保证，二是数据回溯时server端不需要操作，client端设置下kafka消费起始时间即可，三是消费者可以是单实例多线程，也可以是在不同机器上的多实例多线程，消费性能和可用性得到保证，缺点就是数据拆分重组部分需要反序列化再序列化，可能会成为性能瓶颈。目前接入使用正常，最大并发数没测试。 多partition分区，最大的问题在于pk/uk的变更.  如果业务场景上能规避，那是比较perfect的.  ps.  已经有人提交了多分区写出到kafka的方案，下一个版本就会带上 #958
931,fix #904: 空记录时sleep 1s防止循环get数据过快 1000ms长了点  如果控制binlog低延迟  可以考虑10~100ms tks
930,canal服务异常 [New I/O server worker #1-1] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x6b1bba32  /192.10.0.46:57388 => /192.10.16.48:11111]  exception=java.io.IOException: Connection reset by peer 考虑升级canal版本 估计是idle超时了
929,Message 总是返回 RawEntries 如何解析？ 请问如何将 `RawEntries` 中的`ByteString` 转为正常的业务对象？ 搞清楚了，先解析出 `Entry` 再解析出 `RowChange`。     Entry changeEntry = Entry.parseFrom(rawEntry);     RowChange.parseFrom(changeEntry.getStoreValue()); 
928,Consider add filed executeTime at canal-adapter-common class Dml? Consider add filed executeTime?  That's sql exactly execute time  more useful sometimes. solved on v1.1.1
927,fix #912 解决guava冲突问题 通过maven插件将guava修改包路径打包进canal.client tks
926,在版本1.1.0中，默认开启了tsdb，报错信息如下 2018-09-11 11:10:08.724 [destination = example   address = /127.0.0.1:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: apply failed Caused by: org.springframework.jdbc.BadSqlGrammarException: SqlMapClient operation; bad SQL grammar []; nested exception is com.ibatis.common.jdbc.exception.NestedSQLException: --- The error occurred in spring/tsdb/sql-map/sqlmap_history.xml. --- The error occurred while executing query. --- Check the          select                     a.id as id a.gmt_create as gmtCreate a.gmt_modified as gmtModified    a.destination as destination a.binlog_file as binlogFile a.binlog_offest as binlogOffest a.binlog_master_id as binlogMasterId a.binlog_timestamp as binlogTimestamp    a.use_schema as useSchema a.sql_schema as sqlSchema a.sql_table as sqlTable a.sql_text as sqlText a.sql_type as sqlType a.extra as extra                       from meta_history a         where destination = ? and binlog_timestamp >= ? and binlog_timestamp <= ?         order by binlog_timestamp asc id asc              . --- Check the SQL Statement (preparation failed). --- Cause: org.h2.jdbc.JdbcSQLException: Table "META_HISTORY" not found; SQL statement:          select                     a.id as id a.gmt_create as gmtCreate a.gmt_modified as gmtModified    a.destination as destination a.binlog_file as binlogFile a.binlog_offest as binlogOffest a.binlog_master_id as binlogMasterId a.binlog_timestamp as binlogTimestamp    a.use_schema as useSchema a.sql_schema as sqlSchema a.sql_table as sqlTable a.sql_text as sqlText a.sql_type as sqlType a.extra as extra                       from meta_history a                  where destination = ? and binlog_timestamp >= ? and binlog_timestamp <= ?         order by binlog_timestamp asc id asc               [42102-196]         at org.springframework.jdbc.support.SQLErrorCodeSQLExceptionTranslator.doTranslate(SQLErrorCodeSQLExceptionTranslator.java:237)         at org.springframework.jdbc.support.AbstractFallbackSQLExceptionTranslator.translate(AbstractFallbackSQLExceptionTranslator.java:72)         at org.springframework.orm.ibatis.SqlMapClientTemplate.execute(SqlMapClientTemplate.java:206)         at org.springframework.orm.ibatis.SqlMapClientTemplate.queryForList(SqlMapClientTemplate.java:296)         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.dao.MetaHistoryDAO.findByTimestamp(MetaHistoryDAO.java:28)         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.applyHistoryOnMemory(DatabaseTableMeta.java:367)         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:123)         at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:91)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:188)         at java.lang.Thread.run(Thread.java:748) Caused by: com.ibatis.common.jdbc.exception.NestedSQLException: --- The error occurred in spring/tsdb/sql-map/sqlmap_history.xml. --- The error occurred while executing query. --- Check the          select                     a.id as id a.gmt_create as gmtCreate a.gmt_modified as gmtModified    a.destination as destination a.binlog_file as binlogFile a.binlog_offest as binlogOffest a.binlog_master_id as binlogMasterId a.binlog_timestamp as binlogTimestamp    a.use_schema as useSchema a.sql_schema as sqlSchema a.sql_table as sqlTable a.sql_text as sqlText a.sql_type as sqlType a.extra as extra                       from meta_history a                  where destination = ? and binlog_timestamp >= ? and binlog_timestamp <= ?         order by binlog_timestamp asc id asc              . --- Check the SQL Statement (preparation failed). --- Cause: org.h2.jdbc.JdbcSQLException: Table "META_HISTORY" not found; SQL statement:          select                     a.id as id a.gmt_create as gmtCreate a.gmt_modified as gmtModified    a.destination as destination a.binlog_file as binlogFile a.binlog_offest as binlogOffest a.binlog_master_id as binlogMasterId a.binlog_timestamp as binlogTimestamp    a.use_schema as useSchema a.sql_schema as sqlSchema a.sql_table as sqlTable a.sql_text as sqlText a.sql_type as sqlType a.extra as extra                       from meta_history a                  where destination = ? and binlog_timestamp >= ? and binlog_timestamp <= ?         order by binlog_timestamp asc id asc               [42102-196]         at com.ibatis.sqlmap.engine.mapping.statement.MappedStatement.executeQueryWithCallback(MappedStatement.java:201)         at com.ibatis.sqlmap.engine.mapping.statement.MappedStatement.executeQueryForList(MappedStatement.java:139)         at com.ibatis.sqlmap.engine.impl.SqlMapExecutorDelegate.queryForList(SqlMapExecutorDelegate.java:567)         at com.ibatis.sqlmap.engine.impl.SqlMapExecutorDelegate.queryForList(SqlMapExecutorDelegate.java:541)         at com.ibatis.sqlmap.engine.impl.SqlMapSessionImpl.queryForList(SqlMapSessionImpl.java:118)         at org.springframework.orm.ibatis.SqlMapClientTemplate$3.doInSqlMapClient(SqlMapClientTemplate.java:298)         at org.springframework.orm.ibatis.SqlMapClientTemplate$3.doInSqlMapClient(SqlMapClientTemplate.java:296)         at org.springframework.orm.ibatis.SqlMapClientTemplate.execute(SqlMapClientTemplate.java:203)         ... 7 more Caused by: org.h2.jdbc.JdbcSQLException: Table "META_HISTORY" not found; SQL statement:          select                     a.id as id a.gmt_create as gmtCreate a.gmt_modified as gmtModified    a.destination as destination a.binlog_file as binlogFile a.binlog_offest as binlogOffest a.binlog_master_id as binlogMasterId a.binlog_timestamp as binlogTimestamp    a.use_schema as useSchema a.sql_schema as sqlSchema a.sql_table as sqlTable a.sql_text as sqlText a.sql_type as sqlType a.extra as extra                       from meta_history a                  where destination = ? and binlog_timestamp >= ? and binlog_timestamp <= ?         order by binlog_timestamp asc id asc               [42102-196]         at org.h2.message.DbException.getJdbcSQLException(DbException.java:345)         at org.h2.message.DbException.get(DbException.java:179)         at org.h2.message.DbException.get(DbException.java:155)         at org.h2.command.Parser.readTableOrView(Parser.java:5552)         at org.h2.command.Parser.readTableFilter(Parser.java:1266)         at org.h2.command.Parser.parseSelectSimpleFromPart(Parser.java:1946)         at org.h2.command.Parser.parseSelectSimple(Parser.java:2095)         at org.h2.command.Parser.parseSelectSub(Parser.java:1940)         at org.h2.command.Parser.parseSelectUnion(Parser.java:1755)         at org.h2.command.Parser.parseSelect(Parser.java:1743)         at org.h2.command.Parser.parsePrepared(Parser.java:449)         at org.h2.command.Parser.parse(Parser.java:321)         at org.h2.command.Parser.parse(Parser.java:293)         at org.h2.command.Parser.prepareCommand(Parser.java:258)         at org.h2.engine.Session.prepareLocal(Session.java:578)         at org.h2.engine.Session.prepareCommand(Session.java:519)         at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1204)         at org.h2.jdbc.JdbcPreparedStatement.<init>(JdbcPreparedStatement.java:73)         at org.h2.jdbc.JdbcConnection.prepareStatement(JdbcConnection.java:288)         at com.alibaba.druid.pool.DruidPooledConnection.prepareStatement(DruidPooledConnection.java:349)         at com.ibatis.sqlmap.engine.execution.SqlExecutor.prepareStatement(SqlExecutor.java:497)         at com.ibatis.sqlmap.engine.execution.SqlExecutor.executeQuery(SqlExecutor.java:175)         at com.ibatis.sqlmap.engine.mapping.statement.MappedStatement.sqlExecuteQuery(MappedStatement.java:221)         at com.ibatis.sqlmap.engine.mapping.statement.MappedStatement.executeQueryWithCallback(MappedStatement.java:189)  Table "META_HISTORY" not found   conf下有一些初始化的ddl文件，检查一下 @agapple 在canal-1.1.0/conf/spring/tsdb/sql/create_table.sql 有这个文件 尝试debug看一下吧  为啥initTable的操作没有生效  MetaHistoryDAO里的initTable 好的，谢谢 @agapple 
925,如何确定同步完成？ 使用示例程序example会持续获取发送dump协议，获取master端的binlog数据，然后进行更新处理。但发现，似乎程序在同步完数据后没有结束或处于待机状态，而是持续不断地重复上一次的同步数据。例如向主库新增10条记录，按处理流程备库增加10条，按照一般逻辑此时客户端已完成同步，应处于待机状态，直到主库有新的更新，再进行同步。而示例程序展示的是持续否断地重复之前已同步的数据，请问是需要进行什么参数设置吗？ 另外，1.1.0版本的canal-example对DDL操作仅输出eventype和sql语句，似乎没有对备库进行相应的DDL操作，不知情况是否属实？ 先谢谢同行解答。 2018-09-10 17:06:07.896 [Thread-2] WARN  com.alibaba.otter.canal.example.db.CanalConnectorClient - parse event : QUERY  sql: create table xgeom3(id int(20) not null auto_increment location point not null x timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP primary key(id) spatial key sp_index(location)) ENGINE=MyISAM AUTO_INCREMENT=3 DEFAULT CHARSET=utf8 . ignored! 2018-09-10 17:06:07.899 [Thread-2] INFO  com.alibaba.otter.canal.example.db.CanalConnectorClient -  ===========Transaction begin : =======>>>binlog[mysql-bin.000001:8519]   executeTime : 1536567819000   delay : 2548899ms close this issue.
924,update 整张表后无限循环报错 no match ack positionLogPosition 错误信息如下： 2018-09-10 12:09:57.669 [pool-7-thread-1] ERROR com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - no match ack positionLogPosition[identity=LogIdentity[sourceAddress=/47.97.169.27:3317 slaveId=-1] postion=EntryPosition[included=false journalName=mysql-bin.000003 position=7080270 serverId=20183317 gtid= timestamp=1536552101000]] sql: update t_order_shop_0024 set  modify_time = now() ; # binlog filter config canal.instance.filter.druid.ddl = true canal.instance.filter.query.dcl = true canal.instance.filter.query.dml = true canal.instance.filter.query.ddl = true canal.instance.filter.table.error = true canal.instance.filter.rows = false canal.instance.filter.transaction.entry = true 解析的binlog信息: * Batch Id: [149136]  count : [1]   memsize : [8064]   Time : 2018-09-10 12:30:11 * Start : [mysql-bin.000003:7080270:1536552101000(2018-09-10 12:01:41)]  * End : [mysql-bin.000003:7080270:1536552101000(2018-09-10 12:01:41)]  kafka的消息一直在增长，看日志binlog全部是上面这个mysql-bin.000003:7080270:1536552101000 问题基本上查明: canal.instance.filter.transaction.entry = true 过滤事务头尾设置为true之后，如果zk上没有点位信息，起始我给的一个5天前的timestamp，刚好又做了大批量更新就会无限循环报错 问题的关键点应该是canal.instance.filter.transaction.entry = true 这个设置为true了 需要位点更新的使用的，不能将canal.instance.filter.transaction.entry设置为true LS正解
923,怎么解决宿主机访问docker 绑定的ip啊 使用docker 模式时，canal向zookeeper注册的ip是docker容器的网络的ip：172.17.0.3 比如docker里的canal-server绑定的地址是/172.17.0.3:11111 那么我的客户端访问不了的啊 com.alibaba.otter.canal.protocol.exception.CanalClientException: java.net.ConnectException: Connection timed out: connect 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.doConnect(SimpleCanalConnector.java:171) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.access$000(SimpleCanalConnector.java:48) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector$1.processActiveEnter(SimpleCanalConnector.java:396) 	at com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor.processActiveEnter(ClientRunningMonitor.java:207) 	at com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor.initRunning(ClientRunningMonitor.java:118) 	at com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor.start(ClientRunningMonitor.java:92) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.connect(SimpleCanalConnector.java:93) 	at com.alibaba.otter.canal.client.impl.ClusterCanalConnector.connect(ClusterCanalConnector.java:63) 	at com.fcbox.canal.scheduling.SchedulerTask.run(SchedulerTask.java:39) Caused by: java.net.ConnectException: Connection timed out: connect 	at sun.nio.ch.Net.connect0(Native Method) 	at sun.nio.ch.Net.connect(Net.java:454) 	at sun.nio.ch.Net.connect(Net.java:446) 	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.doConnect(SimpleCanalConnector.java:132) 	... 8 common frames omitted 2018-09-10 11:03:05 454 WARN (ClusterCanalConnector.java:66)- failed to connect to:**### /172.17.0.3:11111** after retry 0 times 而且canal.ip也不能直接写成宿主机的ip，会报错的。 可以考虑docker的host模式
922,关于 canal.instance.filter.regex 设置的问题 版本 1.0.24 @agapple  问题现象： 在同一个mysql中 我的canal.instance.filter.regex=cms\\.rc_.*，意思是只订阅cms数据库下的rc_开头的表，这样是没问题的。 但是我在debug代码时，我修改其它数据库下的表 比如 我修改 test数据库下的表，我的客户端还是能接收到消息，消息的 entryType 只有TRANSACTIONBEGIN和TRANSACTIONEND；虽然这两个消息并不是我想要的，但是我还是希望不要收到其它数据库修改后的“垃圾”消息: 垃圾消息如下： header {   version: 1   logfileName: "mysql-bin.000001"   logfileOffset: 6882   serverId: 1   serverenCode: "UTF-8"   executeTime: 1536320325000   sourceType: MYSQL   schemaName: ""   tableName: ""   eventLength: 75 } entryType: TRANSACTIONBEGIN storeValue: " \036" 我想在表达 下我的问题： 1.canal.instance.filter.regex=cms\.rc_.*的确能达到我预期的效果。 2.但是其它的数据库(schema)的消息我也收到了(增删改就会收到消息)，entryType 虽然没有增删改的消息，只TRANSACTIONBEGIN和TRANSACTIONEND，但我不想收到这两个消息。 3.有什么办法能解决。 有参数可以关闭  过滤掉空事务的头和尾 在canal.properteis 里面只有一个是true 其他全部可以改为false https://github.com/alibaba/canal/wiki/AdminGuide这里可以看 canal.instance.filter.transaction.entry = true  这个意思就是把事务头尾过滤 canal.instance.filter.query.dcl = true canal.instance.filter.query.dml = true canal.instance.filter.query.ddl = true canal.instance.filter.table.error = true 这个配置就是把默认产生的sql语句给过滤掉，也就是binlog日志改为row模式后，还是会有一条EntryType为QUERY的sql语句 @yin007008 @agapple 谢谢
921,canal.instance.filter.transaction.entry 设置true后  zk cusor不更新 canal.instance.filter.transaction.entry = true {"@type":"com.alibaba.otter.canal.protocol.position.LogPosition" "identity":{"slaveId":-1 "sourceAddress":{"address":"10.35.165.15" "port":3306}} "postion":{"gtid":"" "included":false "journalName":"mysql-bin.000023" "**position**":43893 "serverId":1 "timestamp":1536312619000}} canal.instance.filter.transaction.entry不要随意设置  会导致zk状态不更新
920,canan kafka 数据过滤问题 配置了 白名单 canal.instance.filter.regex=easy4ip.civil_user_phone  还是能收到其他表的信息 见如下打印 ``` table:sys_user logfile:mysql-bin.000023 entry:ROWDATA eventType:QUERY sql:UPDATE `sys_user` SET `LOGIN_DATE`='2018-09-08 15:36:02' WHERE (`ID`='21') ``` 看FAQ
919,Fix upgrade protobuf version upgrade  protobuf  version [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=919) <br/>All committers have signed the CLA. [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=919) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=919) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=919) it.</sub> 升级了protobuf 3.6.1之后  使用老版本protobuf 2.6.1的客户端是否可以读取到数据?  估计是不行的，这个升级不兼容。 可以读取到数据，我用的是1.1.0的cliet测试可以连 tks
918,新旧版本是否兼容 v1.1.0出来之后打算升级（老版本1.0.26-snapshot），由于对实时性要求较高，不能停服，考虑后想要平滑的升级： 1、新版本配置相同的config，和老版本的共同注册在同一个zk上。 2、关掉老版本节点，让新版本抢占处理权。 3、再上一个新版本的server。 有一个疑问，在步骤1的时候新老版本能兼容吗？ 文档上说可以完全兼容。
917,canal.instance.dbPassword = 数据库密码支持加密嘛？ 可以考虑根据自己的需求，扩展下 propertiesResolver覆盖掉
916,zookeeper集群搭建canal启动报错? - Session 0x0 for server null  unexpected error  closing socket connection and attempting reconnect java.net.ConnectException: Connection refused 	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[na:1.8.0_181] 	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) ~[na:1.8.0_181] 	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350) ~[zookeeper-3.4.5.jar:3.4.5-1392090] 	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1068) ~[zookeeper-3.4.5.jar:3.4.5-1392090] `canal.properties:` canal.id= 1 canal.zkServers=10.153.1.208:2888 10.153.0.183:2888 10.153.0.38:2888 canal.instance.global.spring.xml = classpath:spring/default-instance.xml 是我canal连接zookeeper连接的端口号弄错了。
915,关于mysql主备切换 两个问题，望指教 1、canal中设置备用库，主库宕机，切到备用库。之后主库恢复，备用库宕机，能切回主库么 2、canal是否支持vip，从而不关注物理节点的切换 1.  不会主动主库 2.  支持rds vip的模式 @agapple tks 切换到备库以后，回退60s，数据重复问题怎么解决？靠主键么？如果修改了主键怎么办？例如：源端操作 1、insert id=1; 2、update id=2 where id=1; 源端目标端数据库都是一条数据id=2了现在。此时主库切换到备库，回退60s日志。又重做了步骤1和2，1成功，2失败（主键冲突）。此时目标库就变成id=1和id=2两条数据。数据不一致了和源端。 1.  insert可以考虑使用merge sql 2. update遇到主键冲突，拆为delete before pk + merge insert new pk
914,canal.instance.dbPassword = 数据库密码支持加密的密码？ 这个密码支持加密的密码？ 目前不支持  可以考虑提交一个PR给我
913,[v1.1.0]异常场景测试导致操作系统cannot allocate memory内存泄漏 模拟MySQL-binlong文件内容跌断，对binlog文件删除一部分数据，canal在发送dump解析到此binlog文件时异常，异常持续长时间会导致内存泄漏。 操作系统日志：Fatal error : pthread_create() failed 最终导致操作系统cannot allocate memory 最后只能重启系统 @agapple  你自己检查一下jvm所需要的内存参数和你机器的内存是否match @agapple    机器内存16G canal使用默认内存 @agapple canal会不停的创建线程，数量达到30000+时，系统就没法分配内存了，canal会抛异常java.lang.OutOfMemoryError： unable to create new native thread，最终导致整个系统不可用，jvm参数使用默认值，创建的大量线程如下： MultiStageCoprocessor-other-example-0 .... MultiStageCoprocessor-other-example-8 MultiStageCoprocessor-Parse-example-0 .... MultiStageCoprocessor-Parse-example-8 最新的master已经修复 https://github.com/alibaba/canal/pull/1002
912,[1.1.0]canal与springboot.2.0.4集成异常，guava-18 版本太低，springboot 集成guava-20 ` java.lang.NoSuchMethodError: com.google.common.collect.MapMaker.makeComputingMap(Lcom/google/common/base/Function;)Ljava/util/concurrent/ConcurrentMap; 	at com.google.common.collect.MigrateMap.makeComputingMap(MigrateMap.java:17) ~[canal.common-1.1.0.jar:na] 	at com.alibaba.otter.canal.common.zookeeper.ZkClientx.<clinit>(ZkClientx.java:26) ~[canal.common-1.1.0.jar:na] 	at com.alibaba.otter.canal.client.kafka.KafkaCanalConnector.<init>(KafkaCanalConnector.java:52) ~[classes/:na] 	at com.alibaba.otter.canal.client.kafka.KafkaCanalConnectors.newKafkaConnector(KafkaCanalConnectors.java:47) ~[classes/:na] 	at com.louxun.search.service.SyncDataToESJob.syncHouseDataToES(SyncDataToESJob.java:41) ~[classes/:na] 	at com.louxun.search.listener.StartupListener.onApplicationEvent(StartupListener.java:18) ~[classes/:na] 	at com.louxun.search.listener.StartupListener.onApplicationEvent(StartupListener.java:10) ~[classes/:na] 	at org.springframework.context.event.SimpleApplicationEventMulticaster.doInvokeListener(SimpleApplicationEventMulticaster.java:172) ~[spring-context-5.0.8.RELEASE.jar:5.0.8.RELEASE] 	at org.springframework.context.event.SimpleApplicationEventMulticaster.invokeListener(SimpleApplicationEventMulticaster.java:165) ~[spring-context-5.0.8.RELEASE.jar:5.0.8.RELEASE] 	at org.springframework.context.event.SimpleApplicationEventMulticaster.multicastEvent(SimpleApplicationEventMulticaster.java:139) ~[spring-context-5.0.8.RELEASE.jar:5.0.8.RELEASE] 	at org.springframework.context.support.AbstractApplicationContext.publishEvent(AbstractApplicationContext.java:400) ~[spring-context-5.0.8.RELEASE.jar:5.0.8.RELEASE] 	at org.springframework.context.support.AbstractApplicationContext.publishEvent(AbstractApplicationContext.java:354) ~[spring-context-5.0.8.RELEASE.jar:5.0.8.RELEASE] 	at org.springframework.context.support.AbstractApplicationContext.finishRefresh(AbstractApplicationContext.java:888) ~[spring-context-5.0.8.RELEASE.jar:5.0.8.RELEASE] 	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.finishRefresh(ServletWebServerApplicationContext.java:161) ~[spring-boot-2.0.4.RELEASE.jar:2.0.4.RELEASE] 	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:553) ~[spring-context-5.0.8.RELEASE.jar:5.0.8.RELEASE] 	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:140) ~[spring-boot-2.0.4.RELEASE.jar:2.0.4.RELEASE] 	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:762) [spring-boot-2.0.4.RELEASE.jar:2.0.4.RELEASE] 	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:398) [spring-boot-2.0.4.RELEASE.jar:2.0.4.RELEASE] 	at org.springframework.boot.SpringApplication.run(SpringApplication.java:330) [spring-boot-2.0.4.RELEASE.jar:2.0.4.RELEASE] 	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1258) [spring-boot-2.0.4.RELEASE.jar:2.0.4.RELEASE] 	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1246) [spring-boot-2.0.4.RELEASE.jar:2.0.4.RELEASE] 	at com.louxun.search.SearchApplication.main(SearchApplication.java:10) [classes/:na] ` <exclusion>貌似可以解决把？ 我是修改了原代码解决的 ` package com.alibaba.otter.canal.common.zookeeper; import java.util.Map; import com.google.common.cache.CacheBuilder; import com.google.common.cache.CacheLoader; import com.google.common.cache.LoadingCache; import org.I0Itec.zkclient.IZkConnection; import org.I0Itec.zkclient.ZkClient; import org.I0Itec.zkclient.exception.ZkException; import org.I0Itec.zkclient.exception.ZkInterruptedException; import org.I0Itec.zkclient.exception.ZkNoNodeException; import org.I0Itec.zkclient.exception.ZkNodeExistsException; import org.I0Itec.zkclient.serialize.ZkSerializer; import org.apache.zookeeper.CreateMode; import com.google.common.base.Function; import com.google.common.collect.MigrateMap; /**  * 使用自定义的ZooKeeperx for zk connection  *  * @author jianghang 2012-7-10 下午02:31:15  * @version 1.0.0  */ public class ZkClientx extends ZkClient {     /* BUG  对于zkclient进行一次缓存，避免一个jvm内部使用多个zk connection     private static Map<String  ZkClientx> clients = MigrateMap.makeComputingMap(new Function<String  ZkClientx>() {         public ZkClientx apply(String servers) {             return new ZkClientx(servers);         }     });     */     //使用 guava 新的方式 替代以前旧的（旧的与springboot2.0集成有问题）     private static LoadingCache<String  ZkClientx> clients = CacheBuilder.newBuilder().build(             new CacheLoader<String  ZkClientx>() {                 public ZkClientx load(String servers) {                     return new ZkClientx(servers);                 }             });    // 其他原代码省略，未做修改 } ` 我说的是你在pom里面 都使用到guava的地方 其中一个旧依赖的地方加上exclusion标签，看看能不能解决 不行啊，canal依赖的是旧版本guava-18的，用上新guava-20的话其中一个类里过时的方法已经被移除了，想统一用新版本行不通啊，所以我现在修改了canal里面源码，将过期的guava方法替换成新的方式 canal使用spring版本为3.2.6，而spring-boot2.0.4依赖spring版本为5.x  在版本上差异比较大，谨慎使用 有替换成功的，可以提交一个PR给我 @panjianping 兄弟你后来用的是哪个版本的canal 和哪个版本的springboot @gezhiwei8899 就用了canal-1.1.0 和 springboot-2.0.4 的，修改了canal 的源码 ZkClientx.java 我还引入了dubbo根本起不来了 @panjianping  @panjianping 能不能重新canal的一个方法 避免修改源码嘛 @gezhiwei8899 现在主要是问题是，高版本的spring-boot使用的是guava-20，而当前canal中的ZkClientx.java依赖提guava-18中的方法，但这个方法在guava-20中已经被移除了，所以不想修改原码的话，需要使用spring-3.x的版本。我大概想到的解决方法就这个了 直接在你的pom里面显示依赖guava-18不就可以了么？ 我就是这么解决的         <dependency>             <groupId>com.google.guava</groupId>             <artifactId>guava</artifactId>             <version>18.0</version>         </dependency>
911,最新canal.kafka-1.1.0.tar部署无法启动，调试源码也是同样的异常，求关注 2018-09-04 16:10:09.223 [main] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2018-09-04 16:10:09.226 [main] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties] 2018-09-04 16:10:09.400 [main] WARN  o.s.beans.GenericTypeAwarePropertyDescriptor - Invalid JavaBean property 'connectionCharset' being accessed! Ambiguous write methods found next to actually use d [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.lang.String)]: [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlE ventParser.setConnectionCharset(java.nio.charset.Charset)] 2018-09-04 16:10:09.435 [main] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2018-09-04 16:10:09.435 [main] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties] 2018-09-04 16:10:09.667 [main] ERROR com.alibaba.druid.pool.DruidDataSource - testWhileIdle is true  validationQuery not set 2018-09-04 16:10:09.891 [main] INFO  c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-example 2018-09-04 16:10:09.905 [main] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - start successful.... 2018-09-04 16:10:09.948 [destination = example   address = /192.168.200.30:3306   EventParser] WARN  c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - prepare to find start position just las t position  {"identity":{"slaveId":-1 "sourceAddress":{"address":"mySlave" "port":3306}} "postion":{"gtid":"" "included":false "journalName":"mysql-bin.000029" "position":132895982 "serverId":2 "timestamp":1 536044055000}} 2018-09-04 16:10:09.993 [Thread-7] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - stop CannalInstance for null-example 2018-09-04 16:10:09.996 [destination = example   address = /192.168.200.30:3306   EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - parse events has an error com.alibaba.otter.canal.parse.exception.CanalParseException: dump address /192.168.200.30:3306 has an error  retrying. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: should execute connector.connect() first Caused by: java.io.IOException: should execute connector.connect() first         at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.<init>(MysqlQueryExecutor.java:30) ~[canal.parse.driver-1.1.0.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.queryMulti(MysqlConnection.java:106) ~[canal.parse-1.1.0.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:175) ~[canal.parse-1.1.0.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:129) ~[canal.parse-1.1.0.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:91) ~[canal.parse-1.1.0.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:188) ~[canal.parse-1.1.0.jar:na]         at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_171] 2018-09-04 16:10:09.997 [Thread-7] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - stop successful.... 在我们环境里无法重试，你尝试debug看一下MysqlConnection什么条件下被disconnect
910,canal如何把mysql的某些数据导入到redis的集群？ 目前已经有redis集群6台 canal客户端一台 在数据不更新的情况下 如何能把mysql中现有的某些表的数据导入到redis的集群中呢? 谢谢 "在数据不更新的情况下“  这个前提不符合canal的工作原理，可能无法满足你的需求。可以看看yugong或者dataX是不是可以满足? 明白了。谢谢
909,[v1.1.0]canal如何在Zookeeper中处理云服务器IP问题（大小网IP问题）？ canal在Zookeeper中注册云服务器的内网IP，如192.168.4.x，而对应大网服务器地址是其他IP，如10.100.x.x 这样导致canal以及Zookeeper、client必须部署到同一云服务器小网段中，无法使用大网段进行访问 @agapple  这种特殊网络没在设计考虑的范畴 @agapple 能否计划支持，Zuul也有类似情况进行网络段位过滤机制 每个canal server自己人肉设置对应的ip? 你可以考虑提交一个pr给我
908,meta.bat的问题 为什么我启动服务不是生成的meta.bat文件而是生成的h2.mv.db文件？有没有大神解释一下！ h2.mv.db文件默认会有，canal.instance.global.spring.xml = classpath:spring/default-instance.xml是写到zk上的，canal.instance.global.spring.xml = classpath:spring/file-instance.xml这个才是写本地的meta.bat LS正解
907,example里面的报错 2018-09-04 11:39:04.244 [WrapperSimpleAppMain] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2018-09-04 11:39:04.244 [WrapperSimpleAppMain] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties] 2018-09-04 11:39:04.400 [WrapperSimpleAppMain] WARN  o.s.beans.GenericTypeAwarePropertyDescriptor - Invalid JavaBean property 'connectionCharset' being accessed! Ambiguous write methods found next to actually used [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.nio.charset.Charset)]: [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.lang.String)] 2018-09-04 11:39:04.400 [WrapperSimpleAppMain] INFO  c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-example  2018-09-04 11:39:04.416 [WrapperSimpleAppMain] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - start successful.... 2018-09-04 11:39:04.416 [canal-instance-scan-0] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - stop CannalInstance for null-example  2018-09-04 11:39:04.432 [destination = example   address = /127.0.0.1:3308   EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - parse events has an error com.alibaba.otter.canal.parse.exception.CanalParseException: dump address /127.0.0.1:3308 has an error  retrying.  Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: java.net.SocketException: Socket Closed Caused by: java.net.SocketException: Socket Closed 	at java.net.SocketInputStream.socketRead0(Native Method) ~[na:1.8.0_77] 	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116) ~[na:1.8.0_77] 	at java.net.SocketInputStream.read(SocketInputStream.java:170) ~[na:1.8.0_77] 	at java.net.SocketInputStream.read(SocketInputStream.java:141) ~[na:1.8.0_77] 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246) ~[na:1.8.0_77] 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286) ~[na:1.8.0_77] 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345) ~[na:1.8.0_77] 	at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.read(BioSocketChannel.java:52) ~[canal.parse.driver-1.1.0.jar:na] 	at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.readHeader(PacketManager.java:12) ~[canal.parse.driver-1.1.0.jar:na] 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.readNextPacket(MysqlQueryExecutor.java:159) ~[canal.parse.driver-1.1.0.jar:na] 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.query(MysqlQueryExecutor.java:56) ~[canal.parse.driver-1.1.0.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:102) ~[canal.parse-1.1.0.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.loadBinlogFormat(MysqlConnection.java:433) ~[canal.parse-1.1.0.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.getBinlogFormat(MysqlConnection.java:582) ~[canal.parse-1.1.0.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:95) ~[canal.parse-1.1.0.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) ~[canal.parse-1.1.0.jar:na] 	at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_77] 2018-09-04 11:39:04.432 [canal-instance-scan-0] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - stop successful.... Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: java.net.SocketException: Socket Closed Caused by: java.net.SocketException: Socket Closed 感觉是主动关闭
906,1.1.0版本启动异常 无法检测到数据库变动 [destination = example   address = /10.19.1.80:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: EOF encountered. Caused by: java.io.IOException: EOF encountered. 	at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.read(BioSocketChannel.java:56) 	at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.readHeader(PacketManager.java:12) 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.readNextPacket(MysqlQueryExecutor.java:159) 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.query(MysqlQueryExecutor.java:56) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:102) 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:148) 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:129) 	at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:91) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:188) 	at java.lang.Thread.run(Thread.java:748) ] Caused by: java.io.IOException: EOF encountered.  检查一下数据库
905,是否支持多订阅者模式？ 请问canal是否支持多订阅者模式，为每个订阅者维持不同的mark 和 ack? 不支持，需要多订阅者可以将数据写入到MQ
904,1.1.0 版本canal server 启动后8个destination cpu一直800%，死循环new Events对象 1.1.0 版本canal server 启动后cpu一直100%甚至160%，这个属于这个版本存在的问题还是使用姿势问题？ 配置方面用的canal.instance.global.spring.xml = classpath:spring/default-instance.xml ，使用kafka集成的方式 不管有没有binlog下来，canal server 进程的cpu消耗基本上一直100%的样子 top -Hp pid看看哪个线程占cpu高，然后用jstack看看是哪个类和方法引起的 现在部署到压测环境了，没有任何binlog下载的情况下，部署了4台云机，8核心的机器，8个destination，cpu800%，求大佬解决啊 "pool-12-thread-1" #29 prio=5 os_prio=0 tid=0x00007fdee4afa000 nid=0x2394 runnable [0x00007fde6cce4000]    java.lang.Thread.State: RUNNABLE 	at com.alibaba.otter.canal.store.model.Events.<init>(Events.java:23) 	at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.doGet(MemoryEventStoreWithBuffer.java:280) 	at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.tryGet(MemoryEventStoreWithBuffer.java:261) 	at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getEvents(CanalServerWithEmbedded.java:478) 	at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:310) 	- locked <0x0000000740492388> (a com.alibaba.otter.canal.instance.spring.CanalInstanceWithSpring) 	at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:273) 	at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.worker(CanalKafkaStarter.java:121) 	at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.access$000(CanalKafkaStarter.java:26) 	at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter$1.run(CanalKafkaStarter.java:70) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) 从线程dump来看就是进入死循环new  Events对象了 从jstat来看 500M eden区，启动15分钟 YGC 500次了 昨晚十点启动到现在的YGC64000次，FGC只有启动初始化因为STW的2次记录，也就是运行中没有YGC # binlog filter config canal.instance.filter.druid.ddl = true canal.instance.filter.query.dcl = true canal.instance.filter.query.dml = true canal.instance.filter.query.ddl = true canal.instance.filter.table.error = true canal.instance.filter.rows = false canal.instance.filter.transaction.entry = false canal.instance.global.spring.xml = classpath:spring/default-instance.xml 正常有binlog拉取时候的线程栈信息如下： # "pool-11-thread-3" #73 prio=5 os_prio=0 tid=0x00007f7ae8d88800 nid=0x3e5e runnable [0x00007f7a70927000]    java.lang.Thread.State: RUNNABLE 	at com.alibaba.otter.canal.store.helper.CanalEventUtils.createPosition(CanalEventUtils.java:76) 	at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:375) 	at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:36) 	at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:307) 	- locked <0x00000007406361f0> (a com.alibaba.otter.canal.instance.spring.CanalInstanceWithSpring) 	at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:273) 	at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.worker(CanalKafkaStarter.java:121) 	at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.access$000(CanalKafkaStarter.java:26) 	at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter$1.run(CanalKafkaStarter.java:70) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) # "pool-11-thread-2" #72 prio=5 os_prio=0 tid=0x00007f7ae8d86800 nid=0x3e5d runnable [0x00007f7a70968000]    java.lang.Thread.State: RUNNABLE 	at com.google.common.collect.MapMakerInternalMap$Segment.getEntry(MapMakerInternalMap.java:2402) 	at com.google.common.collect.ComputingConcurrentHashMap$ComputingSegment.getOrCompute(ComputingConcurrentHashMap.java:81) 	at com.google.common.collect.ComputingConcurrentHashMap.getOrCompute(ComputingConcurrentHashMap.java:67) 	at com.google.common.collect.MapMaker$ComputingMapAdapter.get(MapMaker.java:885) 	at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:296) 	at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:273) 	at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.worker(CanalKafkaStarter.java:121) 	at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.access$000(CanalKafkaStarter.java:26) 	at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter$1.run(CanalKafkaStarter.java:70) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) # "pool-11-thread-1" #71 prio=5 os_prio=0 tid=0x00007f7ae8d85800 nid=0x3e5c runnable [0x00007f7a709a9000]    java.lang.Thread.State: RUNNABLE 	at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:317) 	- locked <0x000000074004a4d0> (a com.alibaba.otter.canal.instance.spring.CanalInstanceWithSpring) 	at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:273) 	at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.worker(CanalKafkaStarter.java:121) 	at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.access$000(CanalKafkaStarter.java:26) 	at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter$1.run(CanalKafkaStarter.java:70) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) @rewerma 大佬这个问题帮忙看看啊  CanalKafkaStarter.worker估计未设置sleep时间 针对空结果时相当于无限循环在跑了  可以优化一下 修改提交了，不过有点奇怪，我这边不设sleep一直无线循环取数据没问题的，CPU正常。我这边用的是1.8的jdk，G1的收集器
903,Could not find first log file name in binary log index file 错误信息： 2018-09-03 17:06:05.707 [main] INFO  c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-example 2018-09-03 17:06:05.709 [main] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - start successful.... 2018-09-03 17:06:05.781 [destination = example   address = xxxxx:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position  {"identity":{"slaveId":-1 "sourceAddress":{"address":"xxxx" "port":3306}} "postion":{"included":false "journalName":"mysql-bin.002736" "position":30948432 "serverId":2118896143 "timestamp":1535632015000}} 2018-09-03 17:06:05.886 [destination = example   address = xxxx:3306   EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Could not find first log file name in binary log index file         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) ~[canal.parse-1.0.24.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122) [canal.parse-1.0.24.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) [canal.parse-1.0.24.jar:na]         at java.lang.Thread.run(Thread.java:748) [na:1.7.0_141] 2018-09-03 17:06:05.887 [destination = example   address = xxxx:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address xxxxx:3306 has an error  retrying. caused by java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Could not find first log file name in binary log index file         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) ~[canal.parse-1.0.24.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122) ~[canal.parse-1.0.24.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) ~[canal.parse-1.0.24.jar:na]         at java.lang.Thread.run(Thread.java:748) [na:1.7.0_141] 2018-09-03 17:06:05.893 [destination = example   address = xxxx:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Could not find first log file name in binary log index file         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209)         at java.lang.Thread.run(Thread.java:748) ] 问题应该是meta.dat中的binlog文件跟不上show master STATUS中的日志文件。 meta.dat中的文件是mysql-bin.002736: {"clientDatas":[{"clientIdentity":{"clientId":1001 "destination":"example" "filter":""} "cursor":{"identity":{"slaveId":-1 "sourceAddress":{"address":"":"xxxx" "p" "port":3306}} "postion":{"included":false "journalName":"mysql-bin.002736" "position":30948432 "serverId":2118896143 "timestamp":1535632015000}}}] "destination":"example"} show master STATUS最新的文件; mysql-bin.002752	24891270	 之前的做法是直接删除meta.dat文件，重新开始同步，但是会丢失数据。运行一段时间后又会出现该问题。请问大佬们怎么解决。 mysql master会基于一定的策略去sweep binlog(根据size或时间)，如果消费的慢，meta.dat里的log就被清理了。 这种情况考虑下提升整体吞吐量吧。 2018-09-04 09:35:33.645 [destination = example   address = xxx:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: column size is not match for table:`skstandard`.`tbl_20180824090548lyqbbcqqjdt68u5m` 103 vs 15 应该是由于表结构改变导致服务一直卡在这里，请问有什么好的方法可以跳过去吗，针对这种表结构改变的情况怎么处理的呢？我现在是手动修改meta.dat中的position跳过的。 升级新版本吧 https://github.com/alibaba/canal/wiki/TableMetaTSDB 大佬，你这个column size is not match for table问题是怎么解决的 望赐教！ 本地使用的是1.1.0版本，但是还会出现column size is not match for table，望大佬赐教解决办法
902,fix bug #890: 并行解析 + Gtid .没有初始化LogContext中的gtidSet tks
901,使用kafka客户端消费binlog出现乱码 *B> 0mysql-bin.000102�0 �*UTF-80踬�� 8B 我也遇到了相同的问题。 kafka发送的是message数据包不是可见字符串，客户端接收到后需要反序列化解包，参考client.kafka
900,kafka server端代码调整 
899,数据消费一段后，canal-client无法拉取新的数据，确定db是在不断更新的。 ![image](https://user-images.githubusercontent.com/5965173/44947782-0e424280-ae45-11e8-973a-b2c102d7d704.png) 使用最新版本。1.10 检查一下server端是否有报错 ![image](https://user-images.githubusercontent.com/5965173/45035342-57d09e80-b08c-11e8-9bf8-bba18de5ae43.png) server端，出现一个异常后，看到client自动重新subscribe的信息。然后client 就读不到数据了。 又一次，出现异常后，消费停止了。 ![image](https://user-images.githubusercontent.com/5965173/45036036-fad5e800-b08d-11e8-8b33-a956e293438d.png) mysql版本 5.6.28 getWithoutAck 使用无需确认的模式，出现上面问题。 使用get Ack的模式就没有问题了。 先考虑升级一下canal版本
898,canal.kafka输出乱码 CentOS Linux release 7.3.1611 (Core)  3.10.0-514.el7.x86_64 JDK：jdk-8u161-linux-x64.tar.gz zookeeper：zookeeper-3.4.13.tar.gz kafka：kafka_2.11-2.0.0.tgz canal.kafka：canal.kafka-1.1.0.tar.gz MySQL：5.7.22-log 按照https://github.com/alibaba/canal/wiki/Canal-Kafka-QuickStart配置 kafka中全是乱码 请问需要在那里调整  bin/kafka-console-consumer.sh --bootstrap-server 192.168.10.110:9092 --topic example *D mysql-binlog.000012*UTF-80BJP401663 *? mysql-binlog.000012*UTF-80BJPH  mysql-binlog.000012*UTF-80BtestJtP+Xb         _-+_C-+++1Pb? id (0B1Ri++(11)    +e_+ (0B1R                +a_cha_(255)D +y_-+-bi++-g.000012 *UTF-80 8BJP401877 *? +y_-+-bi++-g.000012 *UTF-80 8BJPI > +y_-+-bi++-g.000012 *UTF-80 8Beca_dJ+_c+a___ca_d__e-+e_+_+-gPTXb         _-+_C-+++1Pb  kafka数据投递传输的是数据包哦，收到数据后还要解包成对应的message，可参考client中的kafka实现 我看了kafka-client中的AbstractKafkaTest.java，改了相应的配置参数后，当往mysql中插入新记录后，用命令`bin/kafka-console-consumer.sh --zookeeper 192.168.206.128:2181 --topic canal1 --from-beginning`可以看到投递过来的数据包，是乱码，但运行KafkaClientRunningTest.java没有收到数据包。
897,canal-1.1.0源码中canal-kafka入口启动一直报异常。 2018-08-31 09:05:51.141 [destination = example   address = /192.168.200.42:3306   EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address /192.168.200.42:3306 has an error  retrying. caused by  com.alibaba.otter.canal.parse.exception.CanalParseException: java.net.SocketException: Socket closed Caused by: java.net.SocketException: Socket closed 	at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:116) ~[na:1.8.0_20] 	at java.net.SocketOutputStream.write(SocketOutputStream.java:141) ~[na:1.8.0_20] 	at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.write(BioSocketChannel.java:36) ~[classes/:na] 	at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.writeBody0(PacketManager.java:42) ~[classes/:na] 	at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.writeBody(PacketManager.java:35) ~[classes/:na] 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.query(MysqlQueryExecutor.java:55) ~[classes/:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:102) ~[classes/:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.loadBinlogFormat(MysqlConnection.java:433) ~[classes/:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.getBinlogFormat(MysqlConnection.java:582) ~[classes/:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:95) ~[classes/:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) ~[classes/:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_20] 2018-08-31 09:05:51.146 [destination = example   address = /192.168.200.42:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: java.net.SocketException: Socket closed Caused by: java.net.SocketException: Socket closed 	at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:116) 	at java.net.SocketOutputStream.write(SocketOutputStream.java:141) 	at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.write(BioSocketChannel.java:36) 	at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.writeBody0(PacketManager.java:42) 	at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.writeBody(PacketManager.java:35) 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.query(MysqlQueryExecutor.java:55) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:102) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.loadBinlogFormat(MysqlConnection.java:433) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.getBinlogFormat(MysqlConnection.java:582) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:95) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) 	at java.lang.Thread.run(Thread.java:745) ] mysql连接没有问题，同样的mysql配置用canal入口启动没有问题 Socket closed Caused by: java.net.SocketException: Socket closed
896,修复execTime快于canal当前时间，delay无data points @wingerx ，从代码来看，没有data points的情况，基本就是mysql host和canal server ntp不同步，mysql时间较快。 现在修改为：在这种情况下，delay为0；wiki里也会着重修改说明一下。 tks
895,canal 到kafka 数据一致性的问题 如果我设置多个partition，canal如何保证数据的一致性，有序性 无论kafka还是rocketmq都只能是单分区全局有序，或单分区有序。 如果表之间要没有强关联，是可以单个表hash到同一个分区保证单表有序的 好像听说canal的项目作者，可能正在实现多个分区，一个表，并保证顺序。不知道是否是真的（理论上，我觉得，如果有张索引表--按时间排序，先读取索引表，再读真正的数据，就可以实现读取放在多个分区上的binglog数据了） #958
894,fix #893 index name contains "on" keyword [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=894) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=894) before we can accept your contribution.<br/><hr/>**张鑫** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=894) it.</sub> [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=894) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=894) before we can accept your contribution.<br/><hr/>**张鑫** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=894) it.</sub> tks
893,创建或者删除index时，如果index名称中包含on字符串，那么解析table的名称会出现错误 比如： CREATE INDEX `schema_new_index_version_s_idx` ON `tb_email_auth` (`status`) 由于version中包含on，此时解析处理的table名为_s_idx 修正方法： com.alibaba.otter.canal.parse.inbound.mysql.ddl.SimpleDdlParser中的  public static final String CREATE_INDEX_PATTERN   = "^\\s*CREATE\\s*(UNIQUE)?(FULLTEXT)?(SPATIAL)?\\s*INDEX\\s*(.*?)\\s+ON\\s+(.*?)$";     public static final String DROP_INDEX_PATTERN     = "^\\s*DROP\\s*INDEX\\s*(.*?)\\s+ON\\s+(.*?)$"; ON前后的\s必须出现1次到多次（源码中是* 改为+） 其实simpparser应该deprecate移除掉了吧，这个支持不了其它复杂的场景
892,kafka client keep alive间隔时间 
891,canal.deployer-1.1.0 cursor持久化频率问题 测试1：client重连时，meta.dat的修改时间发生更新，但数据还是没变。 测试2：由于测试1的问题，导致position没有更新。导致如果此时server重启，读取的还是“旧”的持久化cursor，client在“旧”的position消费。 issue canal版本：1.0.26 alpha5，1.1.0 1.0.26 alpha2不存在这个问题：每次client消费都更新curosr [#882](https://github.com/alibaba/canal/issues/882)
890,并行解析 + Gtid .没有初始化LogContext中的gtidSet 没有初始化LogContext中的gtidSet     public final void putGtid(GtidLogEvent logEvent) {         if (logEvent != null) {             String gtid = logEvent.getSid().toString() + ":" + logEvent.getGno();             if (gtidSet == null) {                 gtid = logEvent.getSid().toString() + ":1-" + logEvent.getGno();                 gtidSet = MysqlGTIDSet.parse(gtid);             }             gtidSet.update(gtid);         }     } event中的当前server的gtid会覆盖掉zk cursor中的历史gtidSet（若干个server:start-end 组合） 下次再重启，dump报错。 是基于当前的主干代码么？ 基于1.1.0 release 这个commit .  commit dd8b1ce9551b59b719516615e43193c467214ade (tag: canal-1.1.0) Author: 七锋 <jianghang.loujh@alibaba-inc.com> Date:   Mon Aug 20 13:28:57 2018 +0800     [maven-release-plugin] prepare release canal-1.1.0 然后cherry-pick 了并行解析的gtid 的两个bug fix  commit 0ca9fa129975ccb4884d8ca9c02c63b066461e6a Author: winger <winger2049@gmail.com> Date:   Sat Aug 25 09:17:13 2018 +0800     fix bug: 并行解析模式下，开启gtid 会导致解析错误 commit 186b58396862cbea82781458625608400fe3e5c5 Author: winger <winger2049@gmail.com> Date:   Sat Aug 25 03:54:38 2018 +0800     fix bug: 并行解析模式下，开启gtid 会导致解析错误 如果之前zk上存储的position有多个gtid集合，并且有的已经被mysql master purged掉。 很容易复现出来。
889,canal1.1.0启动example.log报错 2018-08-27 17:22:51.560 [destination = example   address = /192.168.155.35:3306   EventParser] WARN  c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - prepare to find start position just show master status 2018-08-27 17:22:51.562 [destination = example   address = /192.168.155.35:3306   EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address /192.168.100.249:3306 has an error  retrying. caused by  com.alibaba.otter.canal.parse.exception.CanalParseException: command : 'show master status' has an error! pls check. you need (at least one of) the SUPER REPLICATION CLIENT privilege(s) for this operation 2018-08-27 17:22:51.562 [destination = example   address = /192.168.155.35:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: command : 'show master status' has an error! pls check. you need (at least one of) the SUPER REPLICATION CLIENT privilege(s) for this operation ] 权限问题 以解决，感谢大佬！ 发自网易邮箱大师 在2018年8月28日 17:47，agapple<notifications@github.com> 写道： Closed #889. — You are receiving this because you authored the thread. Reply to this email directly  view it on GitHub  or mute the thread. 大佬 canal修改表结构后，出现一个异常：column size is not match table xx  将ddl打开还是不行 麻烦大佬给予赐教！ 看一下文档TableMetaTSDB
888,connector.subscribe("xxx") 不起作用 canal.instance.filter.regex=fid_standard_product\\..*  我在server端定义监测这一整个库，在一个客户端connector.subscribe("fid_standard_product.fid_stock_report") 其中某个表，但是这个库里的其他表还是会同步过来，这个connector.subscribe为什么没起到过滤的作用呢 多看FAQ
887,canal 运行发生ack错误 canal运行发生ack错误： ack error   clientId:1001 batchId:261365 is not exist   please check。 这个应该是服务器端instance重启导致。现在服务器端的配置为： canal.instance.get.ddl.isolation = false ################################################# #########               destinations            ############# ################################################# canal.destinations= db9002 # conf root dir canal.conf.dir = ../conf # auto scan instance dir add/remove and start/stop instance canal.auto.scan = false canal.auto.scan.interval = 5 canal.instance.global.mode = spring canal.instance.global.lazy = false 请问instance为什么会发生重启？ 怎样能控制服务端的instance不发生重启？ 遇到ack异常时，重新rollback一下再来一次就可以了，看一下example的样例工程
886,canal 
885,关于kafka topic设置 请问目前是否支持对应每张mysql表设置一个对应的kafka topic？ 初步看代码，目前似乎只支持对应一个destination设置多个topic，数据会发往设置的所有topic。 目前只支持一个destination一个topic
884,通过canal.instance.filter.regex= 过滤 和通过java程序connector.subscribe(" ")有什么不同的吗？ 实际使用中，只让canal监听一个库中的某几张表，怎么配性能会更好一点，望大佬指教 多看下wiki
883,canal1.1.0当监听到第一次变化时报错，后来都没问题 logEventParserProxy - dump address /106.12.14.74:3306 has an error  retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: java.net.SocketException: Connection reset Caused by: java.net.SocketException: Connection reset         at java.net.SocketInputStream.read(SocketInputStream.java:210) ~[na:1.8.0_181]         at java.net.SocketInputStream.read(SocketInputStream.java:141) ~[na:1.8.0_181]         at java.io.BufferedInputStream.fill(BufferedInputStream.java:246) ~[na:1.8.0_181]         at java.io.BufferedInputStream.read1(BufferedInputStream.java:286) ~[na:1.8.0_181]         at java.io.BufferedInputStream.read(BufferedInputStream.java:345) ~[na:1.8.0_181]         at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.read(BioSocketChannel.java:52) ~[canal.parse.driver-1.1.0.jar:na]         at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.readHeader(PacketManager.java:12) ~[canal.parse.driver-1.1.0.jar:na]         at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.readNextPacket(MysqlQueryExecutor.java:159) ~[canal.parse.driver-1.1.0.jar:na]         at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.query(MysqlQueryExecutor.java:56) ~[canal.parse.driver-1.1.0.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:102) ~[canal.parse-1.1.0.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:148) ~[canal.parse-1.1.0.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:129) ~[canal.parse-1.1.0.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:91) ~[canal.parse-1.1.0.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:188) ~[canal.parse-1.1.0.jar:na]         at java.lang.Thread.run(Thread.java:748) [na:1.8.0_181] 2018-08-26 19:08:07.309 [destination = example   address = /106.12.14.74:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: java.net.SocketException: Connection reset Caused by: java.net.SocketException: Connection reset  网络异常  会自动重试
882,1.0.26-alpha 2: canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield canal.instance.tsdb.enable=false canal.instance.gtidon=false 错误日志1： ``` 2018-08-25 10:04:02.917 [destination = platform_data   address = ip:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTa bleMeta - parse faield : CREATE TABLE `t_bill_details` (   `bill_id` bigint(20) NOT NULL   ....   KEY `IDX_BD_CITY` (`store_city`) KEY_BLOCK_SIZE=8   KEY `IDX_BD_DETAIL_ID` (`detail_id`) KEY_BLOCK_SIZE=8   KEY `IDX_BD_SD` (`bill_sale_date`) KEY_BLOCK_SIZE=8   KEY `IDX_BD_SSS` (`bill_status` `store_city` `bill_sale_date` `store_id`) KEY_BLOCK_SIZE=8   KEY `IDX_STORE_ID` (`store_id`) KEY_BLOCK_SIZE=8   KEY `IDX_BD_STATUS` (`bill_status` `bill_sale_date`) KEY_BLOCK_SIZE=8 ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 ROW_FORMAT=COMPRESSED KEY_BLOCK_SIZE=4 com.alibaba.fastsql.sql.parser.ParserException: syntax error  error in :'id`) KEY_BLOCK_SIZE=8   KEY `IDX_B'  expect RPAREN  actual IDENTIFIER pos 1834  line 43  column 32  token IDENTIFIER KEY_BLOCK_SIZE         at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:305) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186]         at com.alibaba.fastsql.sql.parser.SQLParser.accept(SQLParser.java:313) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186]         at com.alibaba.fastsql.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:260) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186]         at com.alibaba.fastsql.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:239) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186]         at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:165) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186]         at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:76) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186]         at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:469) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186]         at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:331) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:71) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.parseTableMeta(TableMetaCache.java:101) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMetaByDB(TableMetaCache.java:87) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:32) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:57) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:52) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) [guava-18.0.jar:na]         at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) [guava-18.0.jar:na]         at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) [guava-18.0.jar:na]         at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) [guava-18.0.jar:na]         at com.google.common.cache.LocalCache.get(LocalCache.java:3937) [guava-18.0.jar:na]         at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941) [guava-18.0.jar:na]         at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824) [guava-18.0.jar:na]         at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830) [guava-18.0.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:185) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:793) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:457) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:133) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:68) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:345) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:187) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:154) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at java.lang.Thread.run(Thread.java:745) [na:1.8.0_60] ``` 错误日志2:  ``` 2018-08-25 09:59:04.733 [destination = platform_data   address = ip:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHand ler - destination:platform_data[com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.util.concurrent.UncheckedExecutionException: com.alibaba.otter.canal.parse.exception.CanalParseException: fetch  failed by table meta:`platform_data`.`tmp_presto_0e13f1bc14574ec4ae4950cade1bc360` Caused by: com.google.common.util.concurrent.UncheckedExecutionException: com.alibaba.otter.canal.parse.exception.CanalParseException: fetch failed by table meta:`platform_data`.`tmp_presto_0e13f1bc145 74ec4ae4950cade1bc360`         at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2203)         at com.google.common.cache.LocalCache.get(LocalCache.java:3937)         at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941)         at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824)         at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:185)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:793)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:457)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:127)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:68)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:345)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:187)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:154)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227)         at java.lang.Thread.run(Thread.java:745) Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: fetch failed by table meta:`platform_data`.`tmp_presto_0e13f1bc14574ec4ae4950cade1bc360` Caused by: java.io.IOException: ErrorPacket [errorNumber=1146  fieldCount=-1  message=Table 'platform_data.tmp_presto_0e13f1bc14574ec4ae4950cade1bc360' doesn't exist  sqlState=42S02  sqlStateMarker=#]  with command: desc `platform_data`.`tmp_presto_0e13f1bc14574ec4ae4950cade1bc360`         at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.query(MysqlQueryExecutor.java:61)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:96)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMetaByDB(TableMetaCache.java:89)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:32)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:62)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:52)         at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527)         at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319)         at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282)         at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197)         at com.google.common.cache.LocalCache.get(LocalCache.java:3937)         at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941)         at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824)         at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:185)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:793)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:457)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:127)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:68)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:345)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:187)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:154)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227)         at java.lang.Thread.run(Thread.java:745) ``` 其实我只需要监听dml，如果我配置过滤要怎么做？配置下面的吗？ ``` canal.instance.filter.druid.ddl = true canal.instance.filter.query.dcl = true canal.instance.filter.query.dml = false canal.instance.filter.query.ddl = true canal.instance.filter.table.error = true canal.instance.filter.rows = false ``` canal.instance.filter.rows具体做什么的，wiki没看懂 先升级最新的1.1.0进行测试
881,fix bug: 并行解析模式下，开启gtid 会导致解析错误 tks
880,aliyun  rds 日志解析失败 Could not find first log file name in binary log index file 不是说全面支持rds了吗，为啥还有这个错，canal.instance.rds.accesskey 那几个参数也配了，但是貌似没起作用啊 这是binlog被删除，清理掉offest，使用binlog时间戳来定位
879,为什么我指定里的一张表，但是从kafka里面就看不到schema名和表名 我设置了这个 canal.instance.filter.regex=db_shopdog_test.deli_order 但是kafka里面解析不到schema名和表名 先看下wiki，确认下filter是否生效
878,instance.properties对表进行过滤canal.instance.filter.regex= 不起作用，如何配置监视数据库中指定某张表的变化 多看wiki里的FAQ 我也有这个需求不知道怎么做？我需要监控一个库里面的三张表 ，github里面的wiki里的FAQ打不开了。 > 多看wiki里的FAQ 我也有这个需求不知道怎么做？我需要监控一个库里面的三张表 ，github里面的wiki里的FAQ打不开了。
877,canal如何配置只监听更新状态，插入，删除都不监听 可以在根据CanalEntry.EventType自己过滤 多谢大佬指点。 发自网易邮箱大师 在2018年8月24日 11:28，xiaokangzhao<notifications@github.com> 写道： 可以在根据CanalEntry.EventType自己过滤 — You are receiving this because you authored the thread. Reply to this email directly  view it on GitHub  or mute the thread.
876,整合 kafka 到 server 和 client tks
875,canal.log启动canal报错 2018-08-24 06:25:04.409 [New I/O server worker #1-1] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x599715d9  /192.168.254.1:57576 => /192.168.254.128:11111]  exception=java.io.IOException: Connection reset by peer         at sun.nio.ch.FileDispatcherImpl.read0(Native Method)         at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)         at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)         at sun.nio.ch.IOUtil.read(IOUtil.java:192)         at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)         at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:322)         at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281)         at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201)         at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46)         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)         at java.lang.Thread.run(Thread.java:745) 望大佬们指点。 升级到最新的1.1.0
874,主从同步延时大概多久 有没有相关的性能测试报告啊，比如数据量多大的时候，延迟比较大 多看wiki的perfomance和监控
873,单核环境下创建线程池会出错。 as title [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=873) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=873) before we can accept your contribution.<br/><hr/>**Chuanyi Li L** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=873) it.</sub> [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=873) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=873) before we can accept your contribution.<br/><hr/>**Chuanyi Li L** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=873) it.</sub> tks，难怪最近有比较多的反馈线程池创建失败的问题，这么多用小规格虚拟机的啊
872,canal server、client 和 canal-kafka server、canal-kafka client 合并 canal server、client 和 canal-kafka server、canal-kafka client 合并
871,为什么我传输到kafka那里显示的全是乱码 配置都是按照文档来的，mysql的字符集是utf8 mysql-bin.000002« *UTF-80¨£¯Ԭ8BJP58025 XshellXshellXshellXshell8 mysql-bin.000002 *UTF-80򯰖 8BJPK w mysql-bin.000002 *UTF-80򯰖 8Btest_heJbP'Xb 	rowsCount1򂎁Pa (0BdR	char(200)= mysql-bin.000002® *UTF-80򯰖 8BJP58052 XshellXshellXshellXshell8 mysql-bin.000002 *UTF-80ȹ񱔬8BJPK w mysql-bin.000002 *UTF-80ȹ񱔬8Btest_heJbP'Xb 	rowsCount1򂎁Pa (0BdR	char(200)= mysql-bin.000002± *UTF-80ȹ񱔬8BJP58481 XshellXshellXshellXshell8 mysql-bin.000002 *UTF-80񺷰Ԭ8BJPK w mysql-bin.000002 *UTF-80񺷰Ԭ8Btest_heJbP'Xb 	rowsCount1򂎁Pa (0BdR	char(200)= mysql-bin.000002´ *UTF-80񺷰Ԭ8BJP58486 XshellXshellXshellXshellXshell8 mysql-bin.000002 *UTF-80º񔪸BJPK w mysql-bin.000002 *UTF-80º񔪸Btest_heJbP'Xb 	rowsCount1򂎁Pa (0BdR	char(200)= mysql-bin.000002· *UTF-80º񔪸BJP58491 XshellXshellXshellXshell8 mysql-bin.000002 *UTF-80񼱖 8BJPK w mysql-bin.000002 *UTF-80񼱖 8Btest_heJbP'Xb 	rowsCount1򂎁Pa (0BdR	char(200)= mysql-bin.000002º *UTF-80񼱖 8BJP58498 XshellXshellXshellXshel8* mysql-bin.000002 *UTF-80тÿ°Ԭ8BJPK w mysql-bin.000002 *UTF-80тÿ°Ԭ8Btest_heJbP'Xb 	rowsCount1򂎁Pa (0BdR	char(200)= mysql-bin.000002½ *UTF-80тÿ°Ԭ8BJP58507 XshellXshellXshellXshell*       8 mysql-bin.000002 *UTF-80Ȇ±Ԭ8BJPK w mysql-bin.000002  *UTF-80Ȇ±Ԭ8Btest_heJbP'Xb 	rowsCount1򂎁Pa (0BdR	char(200)= mysql-bin.000002  *UTF-80Ȇ±Ԭ8BJP58520 XshellXshellXshellXshell* 8 mysql-bin.000002 ¡ *UTF-80¸²±Ԭ8BJPK w mysql-bin.000002¢ *UTF-80¸²±Ԭ8Btest_heJbP'X 	rowsCount1򂎁Pa (0BdR	char(200)= mysql-bin.000002â *UTF-80¸²±Ԭ8BJP58525 XshellXshellXshellXshell*                         8 mysql-bin.000002££ *UTF-80׍±Ԭ8BJPK w mysql-bin.000002¤ *UTF-80׍±Ԭ8Btest_heJbP'Xb 	rowsCount1򂎁Pa (0BdR	char(200)= mysql-bin.000002Ƥ *UTF-80׍±Ԭ8BJP58538 传到kafka的是message的序列化数据，客户端收到后会反序列化为message的
870,canal监听多个mysql数据库应该如何配置？ 多看wiki
869,canal-kafka-1.0.26-preview-4: CanalKafkaStarter  CPU占用率高 CanalKafkaStarter.worker 线程中调用getWithoutAck未使用timeout参数。 ``` "pool-8-thread-1" #21 prio=5 os_prio=0 tid=0x00007facb47b1000 nid=0x5fe runnable [0x00007fac8d16b000]    java.lang.Thread.State: RUNNABLE         at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.isStart(CanalServerWithEmbedded.java:126)         at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.checkStart(CanalServerWithEmbedded.java:484)         at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:279)         at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:259)         at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.worker(CanalKafkaStarter.java:118)         at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.access$000(CanalKafkaStarter.java:26)         at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter$1.run(CanalKafkaStarter.java:67)         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)         at java.lang.Thread.run(Thread.java:748) ``` 最新的1.1.1已经修复
868,canal-kafka-1.1.0: 无限循环CanalKafkaStarter - process error! kafka: n1:9092 n2:9092 zk: n3:2181 n4:2181 n5:2181 canal: n1 错误日志： ``` 2018-08-23 16:25:08.274 [destination = example   address = n1/192.168.4.11:3306   EventParser] WARN  c.a.o.c.p.inboun d.mysql.rds.RdsBinlogEventParserProxy - find start position : EntryPosition[included=false journalName=mysql-bin.0000 01 position=3024 serverId=<null> gtid=<null> timestamp=<null>] 2018-08-23 16:25:08.299 [main] INFO  com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - ## start the kafka wo rkers. 2018-08-23 16:25:08.300 [main] INFO  com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - ## the kafka workers  is running now ...... 2018-08-23 16:25:08.301 [pool-4-thread-1] INFO  com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - ## start t he canal consumer: example. 2018-08-23 16:25:08.305 [pool-4-thread-1] INFO  com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - ## the can al consumer example is running now ...... 2018-08-23 16:25:15.105 [pool-4-thread-1] ERROR com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - process er ror! java.lang.NullPointerException: null         at com.alibaba.otter.canal.store.helper.CanalEventUtils.createPosition(CanalEventUtils.java:69) ~[canal.store -1.1.0.jar:na]         at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffe r.java:375) ~[canal.store-1.1.0.jar:na]         at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffe r.java:36) ~[canal.store-1.1.0.jar:na]         at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java :307) ~[canal.server-1.1.0.jar:na]         at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java :273) ~[canal.server-1.1.0.jar:na]         at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.worker(CanalKafkaStarter.java:121) [canal.kafka-1 .1.0.jar:na]         at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.access$000(CanalKafkaStarter.java:26) [canal.kafk a-1.1.0.jar:na]         at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter$1.run(CanalKafkaStarter.java:70) [canal.kafka-1.1 .0.jar:na]         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_161]         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_161]         at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-08-23 16:25:15.106 [pool-4-thread-1] ERROR com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - process er ror! java.lang.NullPointerException: null         at com.alibaba.otter.canal.store.helper.CanalEventUtils.createPosition(CanalEventUtils.java:69) ~[canal.store -1.1.0.jar:na]         at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffe : ``` 配置： canal.properties ```################################################# ######### 		common argument		#############  ################################################# canal.id= 1 canal.ip= canal.port=11111 canal.metrics.pull.port=11112 canal.zkServers=n3:2181 n4:2181 n5:2181 # flush data to zk canal.zookeeper.flush.period = 1000 canal.withoutNetty = true # flush meta cursor/parse position to file canal.file.data.dir = ${canal.conf.dir} canal.file.flush.period = 1000 ## memory store RingBuffer size  should be Math.pow(2 n) canal.instance.memory.buffer.size = 16384 ## memory store RingBuffer used memory unit size   default 1kb canal.instance.memory.buffer.memunit = 1024  ## meory store gets mode used MEMSIZE or ITEMSIZE canal.instance.memory.batch.mode = MEMSIZE ## detecing config canal.instance.detecting.enable = false #canal.instance.detecting.sql = insert into retl.xdual values(1 now()) on duplicate key update x=now() canal.instance.detecting.sql = select 1 canal.instance.detecting.interval.time = 3 canal.instance.detecting.retry.threshold = 3 canal.instance.detecting.heartbeatHaEnable = false # support maximum transaction size  more than the size of the transaction will be cut into multiple transactions delivery canal.instance.transaction.size =  1024 # mysql fallback connected to new master should fallback times canal.instance.fallbackIntervalInSeconds = 60 # network config canal.instance.network.receiveBufferSize = 16384 canal.instance.network.sendBufferSize = 16384 canal.instance.network.soTimeout = 30 # binlog filter config canal.instance.filter.druid.ddl = true canal.instance.filter.query.dcl = true canal.instance.filter.query.dml = false canal.instance.filter.query.ddl = true canal.instance.filter.table.error = false canal.instance.filter.rows = false canal.instance.filter.transaction.entry = true # binlog format/image check canal.instance.binlog.format = ROW STATEMENT MIXED  canal.instance.binlog.image = FULL MINIMAL NOBLOB # binlog ddl isolation canal.instance.get.ddl.isolation = false # parallel parser config canal.instance.parser.parallel = true ## concurrent thread number  default 60% available processors  suggest not to exceed Runtime.getRuntime().availableProcessors() #canal.instance.parser.parallelThreadSize = 16 ## disruptor ringbuffer size  must be power of 2 canal.instance.parser.parallelBufferSize = 256 # table meta tsdb info canal.instance.tsdb.enable=true canal.instance.tsdb.dir=${canal.file.data.dir:../conf}/${canal.instance.destination:} canal.instance.tsdb.url=jdbc:h2:${canal.instance.tsdb.dir}/h2;CACHE_SIZE=1000;MODE=MYSQL; canal.instance.tsdb.dbUsername=canal canal.instance.tsdb.dbPassword=canal # rds oss binlog account canal.instance.rds.accesskey = canal.instance.rds.secretkey = ################################################# ######### 		destinations		#############  ################################################# canal.destinations= example # conf root dir canal.conf.dir = ../conf # auto scan instance dir add/remove and start/stop instance canal.auto.scan = true canal.auto.scan.interval = 5 canal.instance.tsdb.spring.xml=classpath:spring/tsdb/h2-tsdb.xml #canal.instance.tsdb.spring.xml=classpath:spring/tsdb/mysql-tsdb.xml canal.instance.global.mode = spring  canal.instance.global.lazy = false #canal.instance.global.manager.address = 127.0.0.1:1099 #canal.instance.global.spring.xml = classpath:spring/memory-instance.xml #canal.instance.global.spring.xml = classpath:spring/file-instance.xml canal.instance.global.spring.xml = classpath:spring/default-instance.xml ``` kafka.yml ```servers: n1:9092 n2:9092 retries: 0 batchSize: 16384 lingerMs: 1 bufferMemory: 33554432 # canal的批次大小，单位 k，量大建议改为1M canalBatchSize: 50 filterTransactionEntry: true canalDestinations:   - canalDestination: example     topic: example     partition: ``` example/instance.properties ```################################################# ## mysql serverId   v1.0.26+ will autoGen  # canal.instance.mysql.slaveId=0 # enable gtid use true/false canal.instance.gtidon=false # position info canal.instance.master.address=n1:3306 canal.instance.master.journal.name= canal.instance.master.position= canal.instance.master.timestamp= canal.instance.master.gtid= # rds oss binlog canal.instance.rds.accesskey= canal.instance.rds.secretkey= canal.instance.rds.instanceId= # table meta tsdb info canal.instance.tsdb.enable=false #canal.instance.tsdb.url=jdbc:mysql://127.0.0.1:3306/canal_tsdb #canal.instance.tsdb.dbUsername=canal #canal.instance.tsdb.dbPassword=canal #canal.instance.standby.address = #canal.instance.standby.journal.name = #canal.instance.standby.position =  #canal.instance.standby.timestamp = #canal.instance.standby.gtid= # username/password canal.instance.dbUsername=root canal.instance.dbPassword=root canal.instance.connectionCharset=UTF-8 # table regex canal.instance.filter.regex=test\\..* # table black regex canal.instance.filter.black.regex= ################################################# ``` 第一次insert into table 成功解析binlog，然后就出现这个问题 使用官方canal的example里的SimpleCanalClientTest读取。第一次插入的数据能解析，之后的就报错，日志如下： 一直在换端口？ ``` **************************************************** * Batch Id: [1]  count : [1]   memsize : [55]   Time : 2018-08-25 15:56:55 * Start : [mysql-bin.000005:642:1535212613000(2018-08-25 23:56:53)]  * End : [mysql-bin.000005:642:1535212613000(2018-08-25 23:56:53)]  **************************************************** ----------------> binlog[mysql-bin.000005:642]   name[test ar_tmp]   eventType : INSERT   executeTime : 1535212613000(2018-08-25 23:56:53)   gtid : ()   delay : -28797618 ms id : 1    type=int(11)    update=true name : a    type=varchar(32)    update=true price : 1.1    type=double    update=true time : 2018-08-25 23:56:53    type=datetime    update=true process error!com.alibaba.otter.canal.protocol.exception.CanalClientException: something goes wrong with reason: something goes wrong with channel:[id: 0x12e25627  /192.168.4.1:9230 => /192.168.4.21:11111]  exception=java.lang.NullPointerException 	at com.alibaba.otter.canal.store.helper.CanalEventUtils.createPosition(CanalEventUtils.java:69) 	at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:375) 	at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:36) 	at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:307) 	at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:273) 	at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:124) 	at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:100) 	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) 	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754) 	at org.jboss.netty.channel.SimpleChannelHandler.messageReceived(SimpleChannelHandler.java:154) 	at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:100) 	at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) 	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) 	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754) 	at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) 	at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80) 	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) 	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:444) 	at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80) 	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) 	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:540) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261) 	at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:350) 	at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) 	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) 	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) 	at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) 	at java.lang.Thread.run(Thread.java:748) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.receiveMessages(SimpleCanalConnector.java:344) ~[classes/:na] 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:315) ~[classes/:na] 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:287) ~[classes/:na] 	at com.alibaba.otter.canal.example.AbstractCanalClientTest.process(AbstractCanalClientTest.java:110) ~[classes/:na] 	at com.alibaba.otter.canal.example.AbstractCanalClientTest$2.run(AbstractCanalClientTest.java:76) [classes/:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171] process error!com.alibaba.otter.canal.protocol.exception.CanalClientException: failed to subscribe with reason: something goes wrong with channel:[id: 0x1fda2da3  /192.168.4.1:9232 => /192.168.4.21:11111]  exception=java.lang.NullPointerException 	at com.alibaba.otter.canal.store.helper.CanalEventUtils.createPosition(CanalEventUtils.java:69) 	at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:375) 	at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:36) 	at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.subscribe(CanalServerWithEmbedded.java:157) 	at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:77) 	at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:100) 	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) 	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754) 	at org.jboss.netty.channel.SimpleChannelHandler.messageReceived(SimpleChannelHandler.java:154) 	at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:100) 	at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) 	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) 	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754) 	at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) 	at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80) 	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) 	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:444) 	at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80) 	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) 	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:540) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261) 	at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:350) 	at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) 	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) 	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) 	at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) 	at java.lang.Thread.run(Thread.java:748) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.subscribe(SimpleCanalConnector.java:241) ~[classes/:na] 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.subscribe(SimpleCanalConnector.java:218) ~[classes/:na] 	at com.alibaba.otter.canal.example.AbstractCanalClientTest.process(AbstractCanalClientTest.java:108) ~[classes/:na] 	at com.alibaba.otter.canal.example.AbstractCanalClientTest$2.run(AbstractCanalClientTest.java:76) [classes/:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171] process error!com.alibaba.otter.canal.protocol.exception.CanalClientException: failed to subscribe with reason: something goes wrong with channel:[id: 0x1c0b056f  /192.168.4.1:9233 => /192.168.4.21:11111]  exception=java.lang.NullPointerException 	at com.alibaba.otter.canal.store.helper.CanalEventUtils.createPosition(CanalEventUtils.java:69) 	at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:375) 	at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:36) 	at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.subscribe(CanalServerWithEmbedded.java:157) 	at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:77) 	at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:100) 	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) 	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754) 	at org.jboss.netty.channel.SimpleChannelHandler.messageReceived(SimpleChannelHandler.java:154) 	at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:100) 	at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) 	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) 	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754) 	at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) 	at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80) 	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) 	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:444) 	at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80) 	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) 	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:540) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261) 	at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:350) 	at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) 	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) 	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) 	at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) 	at java.lang.Thread.run(Thread.java:748) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.subscribe(SimpleCanalConnector.java:241) ~[classes/:na] 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.subscribe(SimpleCanalConnector.java:218) ~[classes/:na] 	at com.alibaba.otter.canal.example.AbstractCanalClientTest.process(AbstractCanalClientTest.java:108) ~[classes/:na] 	at com.alibaba.otter.canal.example.AbstractCanalClientTest$2.run(AbstractCanalClientTest.java:76) [classes/:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171] process error!com.alibaba.otter.canal.protocol.exception.CanalClientException: failed to subscribe with reason: something goes wrong with channel:[id: 0x6db99e95  /192.168.4.1:9234 => /192.168.4.21:11111]  exception=java.lang.NullPointerException 	at com.alibaba.otter.canal.store.helper.CanalEventUtils.createPosition(CanalEventUtils.java:69) 	at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:375) 	at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:36) 	at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.subscribe(CanalServerWithEmbedded.java:157) 	at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:77) ``` 我使用的是spring/file-instance.xml。初步估计，应该是${destination_dir}/meta.dat的问题 alpha5版本写入的meta.dat ``` {"clientDatas":[{"clientIdentity":{"clientId":1001 "destination":"local_mysql" "filter":""} "cursor":{"identity":{"slaveId":-1 "sourceAddress":{"address":"192.168.4.101" "port":3306}} "postion":{"gtid":"" "included":false "journalName":"mysql-bin.000005" "position":14016 "serverId":1 "timestamp":1535189721000}}}] "destination":"local_mysql"} ``` 而1.1.0写入的meta.dat ``` {"clientDatas":[{"clientIdentity":{"clientId":1001 "destination":"local_mysql" "filter":""}}] "destination":"local_mysql"} ``` 如果我把alpha5的meta.dat覆盖1.1.0的，那1.1.0的就没有楼上的问题了 NPE的问题  我们关注一下 2018-09-03 17:21:00.046 [main] INFO  com.alibaba.otter.canal.kafka.CanalServerStarter - ## the canal server is running now ...... 2018-09-03 17:21:00.048 [main] INFO  com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - ## load kafka configurations 2018-09-03 17:21:00.169 [main] INFO  com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - ## start the kafka workers. 2018-09-03 17:21:00.169 [main] INFO  com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - ## the kafka workers is running now ...... 2018-09-03 17:21:00.169 [pool-5-thread-1] INFO  com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - ## start the canal consumer: ordertest3317. 2018-09-03 17:21:53.135 [canal-instance-scan-0] WARN  o.s.beans.GenericTypeAwarePropertyDescriptor - Invalid JavaBean property 'connectionCharset' being accessed! Ambiguous write methods found next to actually used [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.lang.String)]: [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.nio.charset.Charset)] 2018-09-03 17:21:53.157 [canal-instance-scan-0] INFO  c.a.o.canal.deployer.monitor.SpringInstanceConfigMonitor - auto notify reload ordertest3317 successful. 2018-09-03 17:21:54.161 [destination = ordertest3317   address = yunjitest.mysql.rds.aliyuncs.com/47.98.70.247:3306   EventParser] WARN  c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - prepare to find start position just show master status 2018-09-03 17:21:54.204 [destination = ordertest3317   address = yunjitest.mysql.rds.aliyuncs.com/47.98.70.247:3306   EventParser] WARN  c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - find start position : EntryPosition[included=false journalName=mysql-bin.001277 position=264457500 serverId=<null> gtid=<null> timestamp=<null>] 2018-09-03 17:21:54.230 [pool-5-thread-1] INFO  com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - ## the canal consumer ordertest3317 is running now ...... **2018-09-03 17:21:55.892 [pool-5-thread-1] ERROR com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - process error! java.lang.NullPointerException: null 	at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.calculateSize(MemoryEventStoreWithBuffer.java:555) ~[canal.store-1.1.0.jar:na] 	at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.doGet(MemoryEventStoreWithBuffer.java:322) ~[canal.store-1.1.0.jar:na] 	at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.tryGet(MemoryEventStoreWithBuffer.java:261) ~[canal.store-1.1.0.jar:na] 	at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getEvents(CanalServerWithEmbedded.java:478) ~[canal.server-1.1.0.jar:na] 	at** com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:310) ~[canal.server-1.1.0.jar:na] 	at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:273) ~[canal.server-1.1.0.jar:na] 	at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.worker(CanalKafkaStarter.java:121) [canal.kafka-1.1.0.jar:na] 	at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.access$000(CanalKafkaStarter.java:26) [canal.kafka-1.1.0.jar:na] 	at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter$1.run(CanalKafkaStarter.java:70) [canal.kafka-1.1.0.jar:na] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_181] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_181] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_181] 2018-09-03 17:21:55.896 [pool-5-thread-1] INFO  com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - ## the canal consumer ordertest3317 is running now ...... 2018-09-03 17:21:55.896 [pool-5-thread-1] ERROR com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - process error! java.lang.NullPointerException: null 	at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.calculateSize(MemoryEventStoreWithBuffer.java:555) ~[canal.store-1.1.0.jar:na] 	at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.doGet(MemoryEventStoreWithBuffer.java:322) ~[canal.store-1.1.0.jar:na] 	at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.tryGet(MemoryEventStoreWithBuffer.java:261) ~[canal.store-1.1.0.jar:na] 	at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getEvents(CanalServerWithEmbedded.java:478) ~[canal.server-1.1.0.jar:na] 	at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:310) ~[canal.server-1.1.0.jar:na] 	at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:273) ~[canal.server-1.1.0.jar:na] 	at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.worker(CanalKafkaStarter.java:121) [canal.kafka-1.1.0.jar:na] 	at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.access$000(CanalKafkaStarter.java:26) [canal.kafka-1.1.0.jar:na] 	at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter$1.run(CanalKafkaStarter.java:70) [canal.kafka-1.1.0.jar:na] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_181] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_181] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_181] 版本：1.1.0 spring.xml /default-instance.xml 我这里也遇到无限NPE错误的问题了，错误的关键日志见上面加粗部分，canal server 上次调试好之后，有1周多没有使用了，这次启动就报错误了，我把zk清空后重启问题依旧 at com.alibaba.otter.canal.store.helper.CanalEventUtils.createPosition(CanalEventUtils.java:69) 这个错误我也看到过，不过找不到对应的日志了 我把数据库实例给换了一个其他的测试库，清空zk，启动的时候就没有出现这个错误了，然后我重新把数据库实例换到原来出问题的这个 把timestamp设置为当前时间(我怀疑是binlog有问题引起，设置当前时间是为了跳过有问题的binlog)，启动后果然没有报错，我想让错误复现，把timestamp改回到出问题之前的时间点清空zk，但是错误也没有再次出现了 我是想定位出导致这个NPE错误的原因到底是什么，以后在生产中才能够避免或者解决掉 请大佬赐教，这个到底是什么问题啊，我很担心上线后出现问题hold不住就完蛋了，求大佬指点！ 2018-10-19 03:08:14.957 [pool-4-thread-1] INFO  com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - ## the canal consumer example is running now ...... 2018-10-19 03:08:14.957 [pool-4-thread-1] ERROR com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - process error! java.lang.NullPointerException: null   ![image](https://user-images.githubusercontent.com/14846522/47178034-9a082380-d34c-11e8-9b2a-d9079e6c5485.png) 我的版本是 canal.kafka-1.1.0 这是上面的报错 你试着 cat canal.properties 不要参考wiki 上的，使用默认的就好，我的是这样解决的 #canal.instance.global.spring.xml = classpath:spring/memory-instance.xml canal.instance.global.spring.xml = classpath:spring/file-instance.xml #canal.instance.global.spring.xml = classpath:spring/default-instance.xml ![image](https://user-images.githubusercontent.com/14846522/47182391-e311a500-d357-11e8-8d21-929ddd83e007.png) reopened but closed again just because Github was down yesterday which I think is MS 'stealling user's data
867,一遇到mysql 进行大批量操作的时候，客户端就会报错 com.alibaba.otter.canal.protocol.exception.CanalClientException: java.io.IOException: Broken pipe 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.ack(SimpleCanalConnector.java:339) 	at com.adups.canal.CanalHandler.handler(CanalHandler.java:72) 	at com.adups.canal.CanalClient$2.run(CanalClient.java:52) 	at java.lang.Thread.run(Thread.java:748) Caused by: java.io.IOException: Broken pipe 	at sun.nio.ch.FileDispatcherImpl.write0(Native Method) 	at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47) 	at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93) 	at sun.nio.ch.IOUtil.write(IOUtil.java:65) 	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:471) 	at java.nio.channels.Channels.writeFullyImpl(Channels.java:78) 	at java.nio.channels.Channels.writeFully(Channels.java:98) 	at java.nio.channels.Channels.access$000(Channels.java:61) 	at java.nio.channels.Channels$1.write(Channels.java:174) 	at java.nio.channels.Channels$WritableByteChannelImpl.write(Channels.java:458) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.writeWithHeader(SimpleCanalConnector.java:382) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.writeWithHeader(SimpleCanalConnector.java:369) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.ack(SimpleCanalConnector.java:333) 	... 3 common frames omitted 	 客户端处理速度慢了，超过超时时间了？升级1.1.0试试 是不是客户端ack太慢了，导致超时报错？
866,canal.deployer-1.1.0版本，当监听到数据库变动时，server端报异常，望大佬指教 2018-08-23 22:52:32.366 [destination = example   address = /106.12.14.74:3306   EventParser] WARN  c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - find start position : EntryPosition[included=false journalName=mysql-bin.000004 position=5377 serverId=1 gtid=<null> timestamp=1535008711000] 2018-08-23 22:52:32.582 [destination = example   address = /106.12.14.74:3306   EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address /106.12.14.74:3306 has an error  retrying. caused by  java.lang.IllegalArgumentException: null         at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1314) ~[na:1.8.0_181]         at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1237) ~[na:1.8.0_181]         at java.util.concurrent.Executors.newFixedThreadPool(Executors.java:151) ~[na:1.8.0_181]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor.start(MysqlMultiStageCoprocessor.java:84) ~[canal.parse-1.1.0.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:238) ~[canal.parse-1.1.0.jar:na]         at java.lang.Thread.run(Thread.java:748) [na:1.8.0_181] 2018-08-23 22:52:32.582 [destination = example   address = /106.12.14.74:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.lang.IllegalArgumentException         at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1314)         at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1237)         at java.util.concurrent.Executors.newFixedThreadPool(Executors.java:151)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor.start(MysqlMultiStageCoprocessor.java:84)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:238)         at java.lang.Thread.run(Thread.java:748) ] 遇到同样的问题了，不知道楼主有没有解决 我刚才还在给开发大佬发邮件 他说看看是否canal.properties里修改过参数配置。我并没修改 还在等待他回复。 --- from Dcein. 在2018年8月23日 17:23，platypus0127<notifications@github.com> 写道： 遇到同样的问题了，不知道楼主有没有解决 — You are receiving this because you authored the thread. Reply to this email directly  view it on GitHub  or mute the thread. 问个问题，是不是在单核环境下跑的，可能有个bug，稍后就改。canal.instance.parser.parallel设成false可以先绕过去 @lcybo 感谢大佬，成功了。如果多核环境下这个是不是就不用设置了？ 多核不用 感谢大佬！膜拜~！如果canal监视多个mysql服务器变化，是不是需要配置多个canal.instance.standby.address = 被监视mysql地址，还是配置多个canal.instance.master.address呢？ 在2018年8月23日 18:20，lcybo<notifications@github.com> 写道： 多核不用 — You are receiving this because you authored the thread. Reply to this email directly  view it on GitHub  or mute the thread. 大佬：      你好，真是打扰你了。🙏有时间麻烦帮我解决个问题。我在我们linux中mysql配置文件已开启row模式，而且让运维师也开启的canal相关操作权限，但是启动后查新example日志，仍然报错，提示没权限，麻烦大佬给予指点。     2018-08-27 18:27:16.849 [destination = example   address = /192.168.100.249:3306   EventParser] WARN  c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - prepare to find start position just show master status 2018-08-27 18:27:16.851 [destination = example   address = /192.168.100.249:3306   EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address /192.168.100.249:3306 has an error  retrying. caused by  com.alibaba.otter.canal.parse.exception.CanalParseException: command : 'show master status' has an error! pls check. you need (at least one of) the SUPER REPLICATION CLIENT privilege(s) for this operation 2018-08-27 18:27:16.851 [destination = example   address = /192.168.100.249:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: command : 'show master status' has an error! pls check. you need (at least one of) the SUPER REPLICATION CLIENT privilege(s) for this operation ]     多谢大佬赐教！ | | Dcein520 | | 邮箱：Dcein520@163.com | 签名由 网易邮箱大师 定制 在2018年08月23日 18:26，Dcein520 写道： 感谢大佬！膜拜~！如果canal监视多个mysql服务器变化，是不是需要配置多个canal.instance.standby.address = 被监视mysql地址，还是配置多个canal.instance.master.address呢？ 在2018年8月23日 18:20，lcybo<notifications@github.com> 写道： 多核不用 — You are receiving this because you authored the thread. Reply to this email directly  view it on GitHub  or mute the thread. you need (at least one of) the SUPER REPLICATION CLIENT privilege(s) for this 检查mysql grant  > 大佬： 你好，真是打扰你了。有时间麻烦帮我解决个问题。我在我们linux中mysql配置文件已开启row模式，而且让运维师也开启的canal相关操作权限，但是启动后查新example日志，仍然报错，提示没权限，麻烦大佬给予指点。 2018-08-27 18:27:16.849 [destination = example   address = /192.168.100.249:3306   EventParser] WARN c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - prepare to find start position just show master status 2018-08-27 18:27:16.851 [destination = example   address = /192.168.100.249:3306   EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address /192.168.100.249:3306 has an error  retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: command : 'show master status' has an error! pls check. you need (at least one of) the SUPER REPLICATION CLIENT privilege(s) for this operation 2018-08-27 18:27:16.851 [destination = example   address = /192.168.100.249:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: command : 'show master status' has an error! pls check. you need (at least one of) the SUPER REPLICATION CLIENT privilege(s) for this operation ] 多谢大佬赐教！ | | Dcein520 | | 邮箱：Dcein520@163.com | 签名由 网易邮箱大师 定制 在2018年08月23日 18:26，Dcein520 写道： 感谢大佬！膜拜~！如果canal监视多个mysql服务器变化，是不是需要配置多个canal.instance.standby.address = 被监视mysql地址，还是配置多个canal.instance.master.address呢？ 在2018年8月23日 18:20，lcybo<notifications@github.com> 写道： 多核不用 — You are receiving this because you authored the thread. Reply to this email directly  view it on GitHub  or mute the thread. 建议在example/instance.properties 里面修改 canal.instance.filter.regex=.*\\..*   只监控自己这个instance要的数据库。 我发现用rds的时候即使超级用户也提示权限不足 估计内建库权限不足 rds的超级账号还是有对mysql库的部分表无权限，需要过滤掉mysql库
865,fix #849: HBase数据同步适配 HBase数据同步外部适配器，会打包到client-launcher下 tks 
864,canal-1.1.0 BioSocketChannel Timeout `2018-08-21 05:51:48.895 [destination = zaful   address = /127.0.0.1:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position  {"identity":{"slaveId":-1 "sourceAddress":{"address":"127.0.0.1" "port":3306}} "postion":{"gtid":"" "included":false "journalName":"binlog.002535" "position":157229530 "serverId":97153 "timestamp":1534833018000}} 2018-08-21 05:51:48.895 [destination = zaful   address = /127.0.0.1:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=binlog.002535 position=157229530 serverId=97153 gtid= timestamp=1534833018000] 2018-08-21 05:52:36.469 [destination = zaful   address = /127.0.0.1:3306   EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - Socket timeout expired  closing connection java.net.SocketTimeoutException: Timeout occurred  failed to read 9471 bytes in 25000 milliseconds. 	at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.read(BioSocketChannel.java:123) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:174) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:85) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:206) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:240) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_91] 2018-08-21 05:52:36.470 [destination = zaful   address = /127.0.0.1:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address 127.0.0.1/127.0.0.1:3306 has an error  retrying. **caused by  _### **###  > java.net.SocketTimeoutException: Timeout occurred  failed to read 9471 bytes in 25000 milliseconds **_.** 	at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.read(BioSocketChannel.java:123) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:174) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:85) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:206) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:240) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_91] 2018-08-21 05:52:36.470 [destination = zaful   address = /127.0.0.1:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:zaful[java.net.SocketTimeoutException: Timeout occurred  failed to read 9471 bytes in 25000 milliseconds. 	at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.read(BioSocketChannel.java:123) 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:174) 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:85) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:206) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:240) 	at java.lang.Thread.run(Thread.java:745) ]` 程序启动 运行一段时间后，就一直报这个错！每次fetch超时后，就会kill掉和Mysql的connection，然后又重试，接着又超时，一直这样子死循环，重启也没有用，但是删除掉meta.dat后，能够正常运行一段时间，之后又会报超时的错误。 实在不能理解在有很多binlog文件没有消费的情况下，为什么读几kb数据花25s都读不完！！ 有遇到相同问题的同仁吗？  换1.1.0版本试试 @fangchunsheng ，如果你的环境能经常reproduce，建议在   BioSocketChannel.read(BioSocketChannel.java:123) 里面debug下，是不是 input.read(data  off + n  len - n); 这个调用真的timeout了。 PS: 看了下openjdk实现，timeout通过poll或select查看返回值是否>0。因此就算只能读到1个byte，都不算超时。 @agapple  不好意思  笔误写成了2.1.1 实际用的就是1.1.0，但是我换到1.0.25就不会存在有这种情况 @lcybo  ok 我试一下  不过我们线上用的不是openjdk哈 谢谢 我这里读4个byte都会timeout，启动example也会报这个错 java.net.SocketTimeoutException: Timeout occurred  failed to read 4 bytes in 25000 milliseconds.         at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.read(BioSocketChannel.java:123) ~[canal.parse.driver-1.1.0.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:174) ~[canal.parse-1.1.0.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:77) ~[canal.parse-1.1.0.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:213) [canal.parse-1.1.0.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:248) [canal.parse-1.1.0.jar:na]         at java.lang.Thread.run(Thread.java:748) [na:1.7.0_191] 2018-08-22 05:24:35.190 [destination = centos1   address = /127.0.0.1:3306   EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address /127.0.0.1:3306 has an error  retrying. caused by  java.net.SocketTimeoutException: Timeout occurred  failed to read 4 bytes in 25000 milliseconds.         at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.read(BioSocketChannel.java:123) ~[canal.parse.driver-1.1.0.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:174) ~[canal.parse-1.1.0.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:77) ~[canal.parse-1.1.0.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:213) ~[canal.parse-1.1.0.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:248) ~[canal.parse-1.1.0.jar:na]         at java.lang.Thread.run(Thread.java:748) [na:1.7.0_191] 2018-08-22 05:24:35.194 [destination = centos1   address = /127.0.0.1:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:centos1[java.net.SocketTimeoutException: Timeout occurred  failed to read 4 bytes in 25000 milliseconds.         at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.read(BioSocketChannel.java:123)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:174)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:77)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:213)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:248)         at java.lang.Thread.run(Thread.java:748) @nerverbe  是的 我这边也会有这种情况 就是4个字节也会报错，我看了下代码，应该是读他的协议的head的时候会是4个字节。我还试过把25000ms改成75000ms过，但是一样的报错 如果确认socketInputStream的read调用有timeout，可以考虑下网络问题。 查看dump connection用的网卡有无挤占的情况。 @lcybo  我用canal-1.1.0 里面的dump的测试用例去到线上去跑过，用相同的binglog文件，然后相同的position 跑的时候完全没有问题，所以应该不是网络原因 @fangchunsheng 你说的测试用例是MysqlDumpTest吗？ 测试用例里的MysqlEventParser是线上使用的parser，用的socketChannel其实也是一致的。 看来也要考虑下MysqlEventParser外的因素， @agapple 。
863,在程序中设置master position报错。 我是直接extends MysqlEventParser ```java  /**    * 查询当前的binlog位置    * @param mysqlConnection    * @return    */   private EntryPosition findEndPosition(MysqlConnection mysqlConnection) {     try {   // ... EntryPosition endPosition = new EntryPosition(           "my-bin.000001"  Long.valueOf(1L));       return endPosition; ``` 运行直接报错 ```cmd 18:06:43.222 [destination = example   address = /117.107.241.79:3306   EventParser] INFO  c.a.o.c.p.d.mysql.MysqlConnector - KILL DUMP 1110 failure java.io.IOException: ErrorPacket [errorNumber=1094  fieldCount=-1  message=Unknown thread id: 1110  sqlState=HY000  sqlStateMarker=#]  with command: KILL CONNECTION 1110 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlUpdateExecutor.update(MysqlUpdateExecutor.java:49) ~[classes/:na] 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.disconnect(MysqlConnector.java:107) ~[classes/:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.disconnect(MysqlConnection.java:93) [classes/:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:309) [classes/:na] ``` 是我太low了，应该是从4L，开始。
862,kafka 版本wiki配置说明不详 canal_kafka的deploy版本，按照wiki配置方法，配置kafka.xml，其他的参考canal独立配置，kafka console消费没有输出，canal这边也没有日志反馈，有需要特别的配置吗 canal_server不是直接从deploy模块下的target/canal/bin里启动的，而是从kafka模块下的target/canal/bin下启动，启动server后先观察有没有往kafka发送数据 release里面的，刚看了，文件目录都是一样的。canal启动的时候有kafka workers is running now  就是不会输出到kafka client看不到数据 同问，怎么解决，也不报错，也没有日志输出，kafka就是消费不到数据 参考文档：https://github.com/alibaba/canal/wiki/Canal-Kafka-RocketMQ-QuickStart 如有文档不明确，可以修改和反馈
861,Client QPS指标的datasource配置项由"Prometheus"改为取$datasource参数 升级Canal到1.1.0后配置了监控，在Grafana的监控页面中发现名为"Client QPS"的panel不可用，原因在于"Canal instances"面板的配置文件Canal_instances_tmpl.json中，名为"Client QPS"的panel的"datasource"配置项写成了固定值"Prometheus"，修改为取"$datasource"参数后，panel就可以正常显示了 [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=861) <br/>All committers have signed the CLA. [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=861) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=861) before we can accept your contribution.<br/><hr/>**lixiang** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=861) it.</sub> 多谢
860,集群模式下如何共享zk?canal client端又 如何指定zk呢? 有篇文章是提供了下面的方法: ##Production场景，HA模式下，比如使用ZK作为服务管理，此处至少指定“多数派ZK Node”的IP列表   ##如果你的多个Canal Cluster共享ZK，那么每个Canal还需要使用唯一的“rootpath”。   canal.zkServers = 10.0.1.21:2818 10.0.1.22 10.0.2.21:2818/canal/g1  我本机测试 canal.zkServers = localhost:2181/canal/g1 启动就直接报错了 Something goes wrong when starting up the canal Server: java.lang.IllegalArgumentException: Path length must be > 0 如果原生不支持，是否就只有通过部署多套zk来隔离多个canal server 集群呢？  /canal/g1 这个不要带 我也遇到这个问题  现在的设计貌似只能一套zk支持一套canal server HA集群  同一套canal server在zk的注册路径不能修改吗? canal本身就支持集群模式，在A canal server 启动完成后会在zk的destination的running有该节点的信息，B canal server启动后，如果是跟A canal server的destination有相同的名称的，会在zk的destination目录的cluster里面有该节点的信息，因为A已经running中了，所以这个节点是不会拉取binlog的，只有当A canal server 挂了后，会由ZK通知B去获取binlog，running信息变为B canal server，cluster里面A的信息也会没了 client端直管连接对应的destination的zk和destination name即可，当有多个client时也是通过zk调度实现HA的 问题已经解决了，之前问这个问题属于对canal不熟悉引起 
859,能提供些1.1.0的相关的内部设计文档吗？ 能提供些1.1.0的相关的内部设计文档吗？ 都在wiki里了 wiki里的貌似比较旧， 没有1.1.0的 如prometheus. https://github.com/alibaba/canal/wiki   这里 收到多谢
858,在1.1.0中，LogEventConvert.java中的tableMetaCache属性是null。 tableMetaCache是什么时候，初始化的呢？ 具体复现的办法?  还有你是自己改造过代码么?  是改到过代码。 直接继承 ```java extends MysqlEventParser ``` 多谢，这样tableMetaCache就初始化了
857,新增client-launcher模块，外部数据落地适配器的总入口 外部数据落地适配器的总入口，后续可适配HBase和ES的数据同步
856,ErrotCode:400 canal 1.0.26-SNAPSHOT-2 2018-08-20 09:46:27.050 [New I/O server worker #1-1] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400   Caused by :  something goes wrong with channel:[id: 0x2ae8ccbc  /10.31.152.38:34114 => /10.31.152.38:11111]  exception=com.alibaba.otter.canal.server.exception.CanalServerException: ack error   clientId:1001 batchId:36171 is not exist   please check 2018-08-20 09:48:12.626 [New I/O server worker #1-3] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400   Caused by :  something goes wrong with channel:[id: 0x568191e2  /10.31.152.38:34184 => /10.31.152.38:11111]  exception=com.alibaba.otter.canal.server.exception.CanalServerException: ack error   clientId:1001 batchId:36172 is not exist   please check 2018-08-20 09:50:13.754 [New I/O server worker #1-1] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400   Caused by :  something goes wrong with channel:[id: 0x4176eecb  /10.31.152.38:34258 => /10.31.152.38:11111]  exception=com.alibaba.otter.canal.server.exception.CanalServerException: ack error   clientId:1001 batchId:36173 is not exist   please check 2018-08-20 09:52:11.962 [New I/O server worker #1-3] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400   Caused by :  something goes wrong with channel:[id: 0x49e5e3d0  /10.31.152.38:34332 => /10.31.152.38:11111]  exception=com.alibaba.otter.canal.server.exception.CanalServerException: ack error   clientId:1001 batchId:36174 is not exist   please check 2018-08-20 09:54:03.646 [New I/O server worker #1-1] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400   Caused by :  something goes wrong with channel:[id: 0x74b1d81d  /10.31.152.38:34400 => /10.31.152.38:11111]  exception=com.alibaba.otter.canal.server.exception.CanalServerException: ack error   clientId:1001 batchId:36175 is not exist   please check 2018-08-20 09:55:57.774 [New I/O server worker #1-3] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400   Caused by :  something goes wrong with channel:[id: 0x2b72e577  /10.31.152.38:34460 => /10.31.152.38:11111]  exception=com.alibaba.otter.canal.server.exception.CanalServerException: ack error   clientId:1001 batchId:36176 is not exist   please check 2018-08-20 09:57:52.922 [New I/O server worker #1-1] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400   Caused by :  something goes wrong with channel:[id: 0x61134480  /10.31.152.38:34530 => /10.31.152.38:11111]  exception=com.alibaba.otter.canal.server.exception.CanalServerException: ack error   clientId:1001 batchId:36177 is not exist   please check server端instance肯定是有rollback或者重启的行为，按照example工程重新subscibe/get/ack
855,Remove TPS(events) in tmpl. 展示层先去掉TPS(events)。
854,根据comments的改进 该进基本上与comments一致，有一个不同： delay分为：master  put  get  ack，仔细考虑了一下，还是保留了master（server与mysql master）的delay。 master delay与 put  get  ack的主要区别在于： master delay专注于server与mysql，当mysql处于idle状态时（无delta binlog）也可以通过heartbeat刷新delay；而put  get  ack尤其是后两者，偏向于client delay。 文档稍后更新。 我理解一下master delay 主要是指最后一次接收binlog和当前时间的差值么? 针对无binlog时 通过heartbeat来更新最后一次接收数据的时间戳 ? "expr": "rate(canal_instance_store_consume_seq{destination=~\"$destination\"}[2m])" 这个binlog events的TPS计算  其实是以我们对进入eventStore的binlog计数  并不是原始mysql的binlog计数.   举个例子来说: mysql的DML都会有一个TableLogEvent +WriteLogEvent两条  但这个consume_seq只会记录WriteLogEvent一条 1. master delay，对的。通过目前15s一次的master heartbeat。 2. 我理解下，TableLogEvent是table map元数据吧。那这个TPS(events)看来意义不大，我把他去了，thx。
853,修复启动时，没读到带execTime时，出现48年delay。 1. 修复bug 2. 增加template 3. 更新一些图
852,canal解析到的字段和值不匹配，大佬给看看呗 ![image](https://user-images.githubusercontent.com/18712087/44260217-51e25d00-a246-11e8-9622-32655791da82.png) 这一行取到的rowchange 里边可以看到解析到的字段 和值 与原库的不符，甚至类型都不对，原库int  解析后是date了 是不是发生ddl变更了? @wingerx   比如说 原表 有一条insert，但是canal没有消费，如果原表又做了ddl，那么canal再消费之前的数据就会发生这种情况么 是的，这种情况可以开启tsdb来避免后面类似的问题，当前只能跳过了 . tsdb是指什么呢。。不了解 @wingerx  之前我记得也有这种情况啊。改了表结构的。。也没出现过这个问题
851,canal sever-client heartbeat Server端的Idle检测，只是检测Socket读写通道的阻塞时间 Client与Server订阅关系建立后，CanalConnector没有提供纯粹的心跳检测方法，只能向Server端只能发送get请求才能证明自己活着，如果消费速度过慢，超时就会被Server端关闭Socket连接。 可否增加一个单纯的心跳检测实现？ 短期内先调大超时时间吧
850,alter语句无法解析 正常的mysql alter语句，server端解析出错。 无法跳过，目前想到的暂时解决办法是手动在目标库执行，然后把offset往后移。 2018-08-14 16:53:22.500 [destination =xxxx   address = /xxxx   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : ALTER TABLE `loan_withdraw_record` ADD COLUMN `remark` varchar(255) DEFAULT NULL COMMENT '备注信息' AFTER `is_remind_limit` ALGORITHM=inplace LOCK=NONE com.alibaba.fastsql.sql.parser.ParserException: syntax error  expect TABLES or TABLE  actual EQ  pos 143  line 1  column 143  token =         at com.alibaba.fastsql.sql.dialect.mysql.parser.MySqlStatementParser.parseStatementListDialect(MySqlStatementParser.java:863) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371]         at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:483) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371]         at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371]         at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.apply(DatabaseTableMeta.java:104) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.apply(TableMetaCache.java:228) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseQueryEvent(LogEventConvert.java:265) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:126) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:68) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:345) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:187) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:154) [canal.parse-1.0.26-SNAPSHOT.jar:na] SHTERM: session timeoutotter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at java.lang.Thread.run(Thread.java:745) [na:1.8.0_51] fastsql解析的问题
849,canal支持一下同步到hbase 目前hbase作为大数据领域比较广泛的方案之一，期望能提供一个基于canal增量同步到hbase的能力
847,我在开启server后，报一个 dump address /127.0.0.1:3306 has an error ，这个是什么原因啊     [destination = example   address = /127.0.0.1:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /127.0.0.1:3306 has an error  retrying. caused by  java.lang.IllegalArgumentException: null 	at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1314) ~[na:1.8.0_144] 	at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1237) ~[na:1.8.0_144] 	at java.util.concurrent.Executors.newFixedThreadPool(Executors.java:151) ~[na:1.8.0_144] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor.start(MysqlMultiStageCoprocessor.java:84) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:230) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144] 2018-08-16 12:21:45.933 [destination = example   address = /127.0.0.1:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.lang.IllegalArgumentException 	at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1314) 	at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1237) 	at java.util.concurrent.Executors.newFixedThreadPool(Executors.java:151) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor.start(MysqlMultiStageCoprocessor.java:84) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:230) 	at java.lang.Thread.run(Thread.java:748) 是1.0.26 alpha的那个版本的错误？ 是这个版本。 alpha好几个版本，alpha3还是alpha5？不要使用alpha4，alpha4有问题 alpha5 这个版本报的错误。 alpha5 遇到同样的问题 canal.instance.parser.parallelThreadSize 需要设置 你当时canal.instance.parser.parallelThreadSize 这个参数是配置成为多少的？ 我用默认的值，原配置文档这行注释掉的 也遇到此问题，求帮助 最新的1.1.1版本已解决
846,[v1.0.26-alpha5]客户端偶尔频发报错：no alive canal server com.alibaba.otter.canal.protocol.exception.CanalClientException:no alive canal server  at com.alibaba.otter.canal.client.impl.ClusterNodeAccessStrategy.nextNode(ClusterNodeAccessStrategy.java:76)~ 客户端偶尔频发报以上错误 在Zookeeper中检查 get /otter/canal/destinations/example/running {"active":true "address":"xxx.xxx.xxx.xxx" "cid":1} 服务端没有报错也是正常的，但客户端会频发报错 如果client自我恢复的话  检查一下当时canal server是否发生过切换或者退出?  @agapple 怎么检查canal server呢？ @agapple   在Zookeeper中检查 get /otter/canal/destinations/example/running 结果： Node does not exist : /otter/canal/destinations/example/running get /otter/canal/destinations/sample/running 结果： Node does not exist : /otter/canal/destinations/sample/running 这种情况频率有些多，虽然可以重试成功，但出现这种问题是不是canal server有问题啊 目前场景： 两个实例example、sample，两个实例中配置一模一样 canal.deployer中log日志中并没有报错，只是Zookeeper中检查不到节点信息 出现no alive canal server，肯定是canal server有退出的行为，需要检查server的日志
845,Add header size for each packet. client流量补上包头长度，MD直接放到wiki。
844,加上 if (logger.isDebugEnabled()) 防止message.toString每次都会被运行，message过大会卡cpu tks 邮件能联系我一下 jianghang115@gmail.com   邀请你加入canal Collaborators   一起共建 @rewerma 
843,canal + rocksmq 能确保数据不丢失么？  如果canal把数据推送到rocksmq后，rmq崩溃，另外mq没有及时flush落盘，数据丢失？  canal + rocksmq是否有丢失binlog的可能。 如果rmq奔溃前没有flush落盘，那是MQ设计问题，肯定会丢失数据，需要手工重新回来数据进行补充数据 ok @agapple 
842,v1.0.26正式版什么时候发布呢？ v1.1.0-alapa版本都在进行了，还不见v1.0.26的release版本啊 1.0.26直接转为1.1.0，预计这一两周会正式release
841,Create canal prometheus docs. 起始的文档，等全部完成了，会放到wiki里面。 每天晚上都会更新一部分，确保这周能搞定。 由于目前工作上与canal已经脱钩，只能晚上回家抽时间弄，进度上还请见谅，谢谢。 你可以直接写到wiki里啊    这里：https://github.com/alibaba/canal/wiki
840,升级Kafka 版本至 2.11_1.1.1 赞
839,Debug Memory Leak [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=839) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=839) before we can accept your contribution.<br/>**1** out of **2** committers have signed the CLA.<br/><br/>:white_check_mark: payonxp<br/>:x: yfpeng<br/><hr/>**yfpeng** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=839) it.</sub>
838,canal-1.0.26-preview-5嵌入式Server无法获取Entries 之前1.0.25版本是可以的 https://github.com/alibaba/canal/releases/tag/canal-1.0.26-preview-5 1.0.26 对性能进行了优化，对Entries 的处理方式上进行了修改. 嵌入式server 可以参考https://github.com/alibaba/canal/blob/master/client/src/main/java/com/alibaba/otter/canal/client/impl/SimpleCanalConnector.java#L332 中对数据的处理方式 恩恩，就是新版的Message信息都放到List<ByteString> rawEntries = message.getRawEntries();里了，再通过result.addEntry(Entry.parseFrom(byteString));可以获取完整信息，谢谢！
837,MemoryEventStoreWithBuffer修复内存泄漏，增加Max batch size 修复了MemoryEventStoreWithBuffer中的一个内存泄漏问题： public void cleanUntil(Position position) throws CanalStoreException line 430: for (long next = sequence + 1; next <= maxSequence; next++) 在CanalEventStore接口中增加了最大Batch长度参数，使得otter能够控制获取的Batch最大长度，优化RPC使用场景中Dubbo请求超出Max payload的问题。 [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=837) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=837) before we can accept your contribution.<br/>**1** out of **2** committers have signed the CLA.<br/><br/>:white_check_mark: payonxp<br/>:x: yfpeng<br/><hr/>**yfpeng** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=837) it.</sub> [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=837) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=837) before we can accept your contribution.<br/>**1** out of **2** committers have signed the CLA.<br/><br/>:white_check_mark: payonxp<br/>:x: yfpeng<br/><hr/>**yfpeng** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=837) it.</sub> 1.  代码先合并一下master，无效的分支改动有点多 2.  otter payload的问题，canal master代码已经可以精确控制batch的内存大小，也不需要额外的batchSize来控制，其次业务otter的payload可以选择auto模式，会根据数据大小判断是否走rpc还是HTTP的文件下载 #839  我们业务不太适合使用HTTP文件下载的方式，所以对基于RPC PayLoad过大的问题进行了改造。 内存泄漏的问题你们看下吧。
836,merge [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=836) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=836) before we can accept your contribution.<br/>**1** out of **2** committers have signed the CLA.<br/><br/>:white_check_mark: payonxp<br/>:x: yfpeng<br/><hr/>**yfpeng** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=836) it.</sub>
835,MemoryEventStoreWithBuffer修复内存泄漏，增加Max batch size 修复了MemoryEventStoreWithBuffer中的一个内存泄漏问题： public void cleanUntil(Position position) throws CanalStoreException line 430: for (long next = sequence + 1; next <= maxSequence; next++) 在CanalEventStore接口中增加了最大Batch长度参数，使得otter能够控制获取的Batch最大长度，优化RPC使用场景中Dubbo请求超出Max payload的问题。 [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=835) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=835) before we can accept your contribution.<br/>**1** out of **2** committers have signed the CLA.<br/><br/>:white_check_mark: payonxp<br/>:x: yfpeng<br/><hr/>**yfpeng** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=835) it.</sub> [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=835) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=835) before we can accept your contribution.<br/>**1** out of **2** committers have signed the CLA.<br/><br/>:white_check_mark: payonxp<br/>:x: yfpeng<br/><hr/>**yfpeng** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=835) it.</sub>
834,关于master - slave 的 bin-log 问题 文档中指出，Canal 是通过模拟自己为一个 “slave” 去和 master 交互，进行 bin-log 的订阅和解析。 那假如 slave 也开启了bin-log，那 Canal 是否能与 slave 交互，从而实现相关的功能？ 提出这个的原因是，考虑到目前我们 master 的物理机子性能比较差，然后之前也没有的 bin-log 模式也不是 ROW，所以想咨询是否能在 slave 开 bin-log，然后 Canal 直接通过 slave 进行订阅和解析。 谢谢。 可以的，对canal来说，你的slave就是master @lcybo thx
833,store增加bufferSize，metrics server端口可配置 如题。
832,大佬，这个以后会支持条件筛选吗 比如说同步一张表的数据，但是我不想全部同步过去想做一些筛选，类似于在where 后面定义一些条件，符合条件的才同步过去 client里面写代码做数据过滤
831,有没有打算升级下Google Protocol Buffer? 想做一个c#的客户端连接 CanalServer。 你可以提交一个PR上来  可以做升级.   got it
830,Entry header 中 executeTime 字段能做到毫秒精度？ binlog里面timestamp 4个字节，精度是秒
829,请问下，example下instance.properties和rds_properties里面的数据库配置有啥区别啊 我记得1.0.24是配置在instance.properties里面的，rds_properties是干嘛用的啊，还有这个canal.instance.tsdb.dbUsername 是啥啊，帮帮忙啊老哥 这块新版本会换成新方案，可以看一下 : https://github.com/alibaba/canal/issues/727
828,canal 性能指标采集 指标列表： canal_instance_traffic_delay   与master的delay，单位毫秒，精度毫秒 canal_instance_transactions   接收的transaction数量 canal_instance_row_events    接收的rowdata entry数量 canal_instance_rows_counter   变更的行数 canal_instance     instance列表 canal_instance_subscriptions      instance订阅数量 canal_instance_publish_blocking_time      parallel模式下pushlish阻塞时间，单位毫秒，精度纳秒 canal_instance_received_binlog_bytes       接收的binlog字节数 canal_instance_parser_mode        parser是否parallel模式 canal_instance_client_packets      client instances请求packets数量 canal_instance_client_bytes         向client instances发送字节数 canal_instance_client_empty_batches       向client instances发送空包数量 canal_instance_client_request_error     client instances错误请求数量 canal_instance_client_request_latency    请求latency情况 canal_instance_sink_blocking_time     sink线程put store的阻塞时间，单位毫秒，精度纳秒 canal_instance_store_produce_seq      生产entry的序号 canal_instance_store_consume_seq     消费entry的序号，与canal_instance_store_produce_seq配合使用 canal_instance_store                      store基本信息 canal_instance_store_produce_mem      生产的entry占用mem总量 canal_instance_store_consume_mem     消费的entry占用mem总量 待补充... httpserver端口目前是11112 晚上和rds的提交撞车了，所以有些冲突文件merge。 protoc用的2.6.1版本。 后续会补充Prometheus表达式，可视化相关文档，test cases；将端口等参数可配置等等。 请review一下，谢谢。 大赞!!   问一下 METRICS_OPTS 这个注释了  目前监控信息的获取方式是JMX还是HTTP ? 建议你把监控这块的指标的概念  以及基于指标来定位问题的case能罗列一下   后续大家都可以共享了 rds的提交比较大  和我这边的一些提交也有撞车  需要留意看看是否被覆盖.   这次RDS的提交  主要为解决阿里云上RDS binlog被删的问题  会自动衔接oss的binlog 和 mysql binlog.  有了这套机制， 以后对接各类云RDS基本无障碍了  METRICS_OPTS 本来是用于-javaagent的，现在去掉了。 目前监控信息的获取方式是http： - job_name: 'canal'     static_configs:     - targets: ['localhost:11112']     //端口或其他配置后续会配置到文件里 指标的概念及case会连同表达式，写一个MD，再commit，包括一些develop guide和可视化的内容。 赞赞赞
827,Merge master updates [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=827) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=827) before we can accept your contribution.<br/>**2** out of **3** committers have signed the CLA.<br/><br/>:white_check_mark: agapple<br/>:white_check_mark: lcybo<br/>:x: lin848497337<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=827) it.</sub>
826,Metrics support 增加heartbeat刷新delay，增加MHEARTBEAT event type。 使用version 2.6.1 protoc编译CanalEntry。 使用pair中的rowsCount统计变更行数。 [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=826) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=826) before we can accept your contribution.<br/>**4** out of **5** committers have signed the CLA.<br/><br/>:white_check_mark: payonxp<br/>:white_check_mark: agapple<br/>:white_check_mark: wingerx<br/>:white_check_mark: lcybo<br/>:x: yfpeng<br/><hr/>**yfpeng** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=826) it.</sub>
825,Remove dirty test case source下面com.alibaba.otter.canal.parse.inbound.mysql.tablemeta已经删掉了，test case还在，编译有问题。 tks
824,client如何控制同一个事物内的insert  update消费顺序？ client如何控制同一个事物内的insert  update消费顺序？有时候client往往先拿到了update的事件，后拿到insert事件。 client拿到的顺序就是数据库里binlog记录的顺序
823,TableMetaCache.parseTableMetaByDesc 解析desc table 的结果 NullPointer mysql版本：5.6 desc table的packets解析中key错误。     public static final String              COLUMN_NAME    = "COLUMN_NAME";     public static final String              COLUMN_TYPE    = "COLUMN_TYPE";     public static final String              IS_NULLABLE    = "IS_NULLABLE";     public static final String              COLUMN_KEY     = "COLUMN_KEY";     public static final String              COLUMN_DEFAULT = "COLUMN_DEFAULT";     public static final String              EXTRA          = "EXTRA"; 应该改为：     public static final String              COLUMN_NAME    = "Field";     public static final String              COLUMN_TYPE    = "Type";     public static final String              IS_NULLABLE    = "Null";     public static final String              COLUMN_KEY     = "Key";     public static final String              COLUMN_DEFAULT = "Default";     public static final String              EXTRA          = "Extra"; 否则会在这一行报NullPointer  meta.setColumnName(packet.getFieldValues().get(nameMaps.get(COLUMN_NAME) + i * size).intern()); mysql> select @@version ; +--------------------+ | @@version          | +--------------------+ | 5.6.28-cdb2016-log | +--------------------+ 1 row in set (0.01 sec) mysql> desc test.test ; +-------+---------+------+-----+---------+-------+ | Field | Type    | Null | Key | Default | Extra | +-------+---------+------+-----+---------+-------+ | id    | int(11) | YES  |     | NULL    |       | +-------+---------+------+-----+---------+-------+ 1 row in set (0.01 sec) ![image](https://user-images.githubusercontent.com/33280738/43878223-88545b3a-9bd0-11e8-9f31-6dccbc75e37a.png) 取的是originalName，而非name. NPE是测试出来的结果吗? ![image](https://user-images.githubusercontent.com/7187362/43878339-234ac804-9bd1-11e8-99de-a4646bdb9c61.png) 哥们~~我这里把packet打出来是这样。是不是我的mysql版本和你那边的不一致         System.out.println(packet); UT可以测出来。 有可能. https://dev.mysql.com/doc/refman/8.0/en/columns-table.html ![image](https://user-images.githubusercontent.com/33280738/43881977-9a81a8ac-9be0-11e8-9126-c6711b98eb16.png) ![image](https://user-images.githubusercontent.com/33280738/43881992-ad3b1fbe-9be0-11e8-97e7-11bad3d560c9.png) 我这边用5.6.28测出来的也是一致的。你那边的版本从哪里获取到的？ 哥。。我弄错了。。mysqlConnection连到另外一个数据库去了额。。。是个8.0.11版本的。 占用你这么久时间了，不好意思了。 ok. 8.0版本目前暂不支持。 打扰了~~issue关了吧。
822,fix tableMetaStorage NPE 
821, NPE：MysqlEventParser TableMetaCacheWithStorage 如图，tableMetaStorageFactory为NULL时，storage值为null，后续就NullPointerException了 	at com.alibaba.otter.canal.parse.inbound.mysql.tablemeta.TableMetaCacheWithStorage.<init>(TableMetaCacheWithStorage.java:21) ![image](https://user-images.githubusercontent.com/5847660/43820893-1ccadbde-9b1a-11e8-9979-2ab94346dfdb.png) #822  看我最新的master提交，已经去掉了tableMetaStorageFactory ok
820,canal是支持到MySQL5.7.18吗？ 5.7.18以上的版本是完全不支持，还是说某些功能的支持有问题？有没有在更高版本上使用过的童鞋哇？ 我们现在在 5.7.20 上使用，没发现问题.
819,canal 性能监控改动版 代码我先merge到开发分支里（metrics_support）。 最近有些别的事要忙，进度有点慢，抱歉。 目前进度是简单debug过，后面几天回家后我会抓紧自己先测试下。 一些TODO list备忘： 1. 扩展EntryProtocol，增加master的heartbeat，用来在idle状态刷新delay。 2. 扩展EntryProtocol，在header头增加rowData计数（否则只能通过反序列化拿到），关于这一条，也想询问 @agapple 的意见。这条划掉，已经在Pair里面了。 [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=819) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=819) before we can accept your contribution.<br/>**4** out of **5** committers have signed the CLA.<br/><br/>:white_check_mark: wingerx<br/>:white_check_mark: lcybo<br/>:white_check_mark: agapple<br/>:white_check_mark: qmz<br/>:x: Chuanyi Li L<br/><hr/>**Chuanyi Li L** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=819) it.</sub>
818,HA模式下，canal.instance.tsdb.enable=true，数据库进行主从切换后，canal的master与standby 切换后，会报如下异常。目前我使用的解决方式是将canal.instance.tsdb.enable=false，就可以进行正常的切换了。 HA模式下，canal.instance.tsdb.enable=true，数据库进行主从切换后，canal的master与standby 切换后，会报如下异常。目前我使用的解决方式是将canal.instance.tsdb.enable=false，就可以进行正常的切换了。 2018-08-07 10:44:48.772 [destination = orderfailover   address = /10.8.132.135:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position by switch ::1532919362000 2018-08-07 10:44:48.780 [destination = orderfailover   address = /10.8.132.135:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - Didn't find the corresponding binlog files from mysql-bin.000020 to mysql-bin.000021 2018-08-07 10:44:48.782 [destination = orderfailover   address = /10.8.132.135:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /10.8.132.135:3306 has an error  retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: can't find start position for orderfailover 2018-08-07 10:44:48.782 [Druid-ConnectionPool-Create-1877078260] ERROR com.alibaba.druid.pool.DruidDataSource - create connection SQLException  url:   errorCode 0  state null java.sql.SQLException: connect error  url   driverClass org.h2.Driver         at com.alibaba.druid.pool.DruidAbstractDataSource.createPhysicalConnection(DruidAbstractDataSource.java:1582) ~[druid-1.1.9.jar:1.1.9]         at com.alibaba.druid.pool.DruidDataSource$CreateConnectionThread.run(DruidDataSource.java:2466) ~[druid-1.1.9.jar:1.1.9] 2018-08-07 10:44:48.782 [destination = orderfailover   address = /10.8.132.135:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:orderfailover[com.alibaba.otter.canal.parse.exception.CanalParseException: can't find start position for orderfailover ] 2018-08-07 10:44:49.282 [Druid-ConnectionPool-Create-1877078260] ERROR com.alibaba.druid.pool.DruidDataSource - create connection SQLException  url:   errorCode 0  state null java.sql.SQLException: connect error  url   driverClass org.h2.Driver         at com.alibaba.druid.pool.DruidAbstractDataSource.createPhysicalConnection(DruidAbstractDataSource.java:1582) ~[druid-1.1.9.jar:1.1.9]         at com.alibaba.druid.pool.DruidDataSource$CreateConnectionThread.run(DruidDataSource.java:2466) ~[druid-1.1.9.jar:1.1.9] 2018-08-07 10:44:49.783 [Druid-ConnectionPool-Create-1877078260] ERROR com.alibaba.druid.pool.DruidDataSource - create connection SQLException  url:   errorCode 0  state null java.sql.SQLException: connect error  url   driverClass org.h2.Driver         at com.alibaba.druid.pool.DruidAbstractDataSource.createPhysicalConnection(DruidAbstractDataSource.java:1582) ~[druid-1.1.9.jar:1.1.9]         at com.alibaba.druid.pool.DruidDataSource$CreateConnectionThread.run(DruidDataSource.java:2466) ~[druid-1.1.9.jar:1.1.9] 2018-08-07 10:44:50.283 [Druid-ConnectionPool-Create-1877078260] ERROR com.alibaba.druid.pool.DruidDataSource - create connection SQLException  url:   errorCode 0  state null java.sql.SQLException: connect error  url   driverClass org.h2.Driver         at com.alibaba.druid.pool.DruidAbstractDataSource.createPhysicalConnection(DruidAbstractDataSource.java:1582) ~[druid-1.1.9.jar:1.1.9]         at com.alibaba.druid.pool.DruidDataSource$CreateConnectionThread.run(DruidDataSource.java:2466) ~[druid-1.1.9.jar:1.1.9] 2018-08-07 10:44:50.783 [Druid-ConnectionPool-Create-1877078260] ERROR com.alibaba.druid.pool.DruidDataSource - create connection SQLException  url:   errorCode 0  state null java.sql.SQLException: connect error  url   driverClass org.h2.Driver         at com.alibaba.druid.pool.DruidAbstractDataSource.createPhysicalConnection(DruidAbstractDataSource.java:1582) ~[druid-1.1.9.jar:1.1.9]         at com.alibaba.druid.pool.DruidDataSource$CreateConnectionThread.run(DruidDataSource.java:2466) ~[druid-1.1.9.jar:1.1.9] 2018-08-07 10:44:51.283 [Druid-ConnectionPool-Create-1877078260] ERROR com.alibaba.druid.pool.DruidDataSource - create connection SQLException  url:   errorCode 0  state null 主要原因为h2-tsdb.xml里依赖了System里的canal.instance.destination变量定义，在主备切换时是有一个异步现场操作，该值为空导致
817,canal-1.026-alpha4 启动失败 OS: CentOS Linux release 7.4.1708 canal : canal-1.026-alpha4 在mac中验证没有问题。linux环境 启动失败. java stack: `Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: java.lang.IllegalArgumentException: limit excceed: 44 Caused by: java.lang.IllegalArgumentException: limit excceed: 44         at com.taobao.tddl.dbsync.binlog.LogBuffer.getUint8(LogBuffer.java:235) ~[canal.parse.dbsync-1.0.26-SNAPSHOT.jar:na]         at com.taobao.tddl.dbsync.binlog.event.GtidLogEvent.<init>(GtidLogEvent.java:48) ~[canal.parse.dbsync-1.0.26-SNAPSHOT.jar:na]         at com.taobao.tddl.dbsync.binlog.LogDecoder.decode(LogDecoder.java:364) ~[canal.parse.dbsync-1.0.26-SNAPSHOT.jar:na]         at com.taobao.tddl.dbsync.binlog.LogDecoder.decode(LogDecoder.java:110) ~[canal.parse.dbsync-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$SimpleParserStage.onEvent(MysqlMultiStageCoprocessor.java:210) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]` 已知问题，请下载最新的alpha-5
816,fix ci: fixed compiler java 1.7 fix ci: fixed compiler java 1.7 1. 移除canal.instance.spring 依赖后，需要加入druid与mysql connector的依赖  2. PropertyPlaceholderConfigurer由canal.instance.spring移至db tks
815,canal HA 模式，查看position canal server 启了两台，开启了HA高可用。配置如下： 但是zk不小心被删除了，现在active canal 还是正常接收binlog解析，但是已经没法更新zk中的position信息了。请问一下，zk不小心误删了，除了zk还能在哪查到当前instance的position数据？ ` ################################################# ######### 		common argument		############# ################################################# canal.id= 1 canal.ip= canal.port= 11111 canal.zkServers=10.100.1.10:2181 10.100.1.11:2181 10.100.1.12:2181 canal.zookeeper.flush.period = 1000 canal.file.data.dir = ${canal.conf.dir} canal.file.flush.period = 1000 canal.instance.memory.buffer.size = 16384 canal.instance.memory.buffer.memunit = 1024 ` logs下的meta.log会有记录每次的消费 HA模式下，logs下的meta.log 没有记录，只有非HA的时候才会记录meta.log。 是我HA下，配置有问题吗？
814,schema history storage interface 为了解决Otter回追数据时Rowdata与当前schema不匹配 对TableMeta提供持久化接口，根据timestamp匹配历史schema 在我们的mysql储存实现中，由Otter Manager进行数据库操作，CanalEmbedSelector组装Factory并放入CanalProperties。 没有storage实现则退化为原始TableMetaCache。 [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=814) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=814) before we can accept your contribution.<br/>**1** out of **2** committers have signed the CLA.<br/><br/>:white_check_mark: payonxp<br/>:x: yfpeng<br/><hr/>**yfpeng** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=814) it.</sub> [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=814) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=814) before we can accept your contribution.<br/><hr/>**yfpeng** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=814) it.</sub> @payonxp 代码又冲突，麻烦先合并一下主干代码 这个功能和tsdb 有什么区别？ 看了下实现，是在解析到当时的DDL日志时反查一下数据库表结构，做一下timestamp的映射记录。相比于TableMeta tsdb有一个问题在于，拿到binlog DDL时进行反查表结构，此时的表结构和当时的DDL对应的结构是有时间GAP的，会有不一致性.  可以canal1.0.26最新的实现：https://github.com/alibaba/canal/wiki/TableMetaTSDB @agapple 我没细看实现逻辑，但从描述上看感觉就是TSDB实现的功能 拿到binlog DDL时尝试调用了TSDB的解析功能，其实不是每次都往数据库查。 主要的功能和TSDB差不多，是想把持久化功能放到otter，canal本身不实现持久化。 建议把otter里实现的storage能力，也提交一个PR 我重新修改支持了otter使用TableMetaTSDB https://github.com/alibaba/canal/commit/9e816bc48f9955f9e2839873993cef9627c2f389 对应的otter commit : https://github.com/alibaba/otter/commit/7cc897131da65aab2c1aa31b7b0b8aa7a6f65745 Thx
813,fix #802: 从otter中剥离MySQL 相关同步实现，作为example的参考 非常赞
812,链接失效 ### wiki的home + **相关资料**：与阿里巴巴的rocketMQ配合使用 连接失效 ### wiki的Introduction + **知识科普**：mysql的Binlay Log介绍的第二个链接失效 + **EventParser设计**：binlog event structure，详细信息请参考的连接 Page Not Found taobaodba的blog 已经不存在了.其他连接已经修复，tks. @wingerx 可以说十分的效率了，赞一个。 赞一个 原谅我的强迫症 README.md 里面关于RocketMQ的连接未更改。 https://github.com/apache/rocketmq 已修复
811,change com.alibaba.fastsql version to the last on http://central.mave… Solve the bug of issue #808  [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=811) <br/>All committers have signed the CLA. [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=811) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=811) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=811) it.</sub> 后面会提交fastsql的最新版 好，那尽快deploy一下哈，我就把这个先关了
810,fix bug: kafka get row data for performance tks
809,canal ack error 2018-08-03 15:17:45.414 [New I/O server worker #1-4] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x7cec5107  /10.111.61.32:47512 :> /10.111.61.32:11111]  exception=java.nio.channels.ClosedChannelException         at org.jboss.netty.channel.socket.nio.NioWorker.cleanUpWriteBuffer(NioWorker.java:649)         at org.jboss.netty.channel.socket.nio.NioWorker.writeFromUserCode(NioWorker.java:370)         at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:137)         at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76)         at org.jboss.netty.channel.Channels.write(Channels.java:611)         at org.jboss.netty.channel.Channels.write(Channels.java:578)         at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:46)         at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:174)         at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48)         at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276)         at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302)         at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526)         at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507)         at org.jboss.netty.handler.codec.replay.ReplayingDecoder.cleanup(ReplayingDecoder.java:542)         at org.jboss.netty.handler.codec.replay.ReplayingDecoder.channelDisconnected(ReplayingDecoder.java:450)         at org.jboss.netty.channel.Channels.fireChannelDisconnected(Channels.java:360)         at org.jboss.netty.channel.socket.nio.NioWorker.close(NioWorker.java:599)         at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:119)         at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76)         at org.jboss.netty.channel.Channels.close(Channels.java:720)         at org.jboss.netty.channel.AbstractChannel.close(AbstractChannel.java:208)         at org.jboss.netty.channel.ChannelFutureListener$1.operationComplete(ChannelFutureListener.java:46)         at org.jboss.netty.channel.DefaultChannelFuture.notifyListener(DefaultChannelFuture.java:381)         at org.jboss.netty.channel.DefaultChannelFuture.addListener(DefaultChannelFuture.java:148)         at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:48)         at com.alibaba.otter.canal.server.netty.NettyUtils.error(NettyUtils.java:69)         at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:253)         at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48)         at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276)         at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302)         at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526)         at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507)         at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:444)         at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274)         at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261)         at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:350)         at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281)         at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201)         at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46)         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)         at java.lang.Thread.run(Thread.java:748) 按照example代码做一下重试处理
808,com.alibaba.fastsql:fastsql:jar:2.0.0_preview_520 mvn clean install的时候报错 [ERROR] Failed to execute goal on project canal.parse: Could not resolve dependencies for project com.alibaba.otter:canal.parse:jar:1.0.26-SNAPSHOT: Could not find artifact com.alibaba.fastsql:fastsql:jar:2.0.0_preview_520 in central (http://repo1.maven.org/maven2) -> [Help 1] 请问下这个怎么解决。谢谢 你用2.0.0_preview_186吧，2.0.0_preview_520 并没有对外开放，另外配置maven仓库中心为：http://central.maven.org/maven2/ 2.0.0_preview_520 在release 里面有jar包
807,instance with manager模式一些微调 1、CanalParameter#MetaMode增加本地文件方式适配manager模式是instance使用本地meta.dat存储位点信息； 2、typo修正; 3、BatchMode默认值修正; CanalParameter的参数名调整会有一个历史版本兼容性的问题，单词拼写的问题以前发现了但不能做变动了，除非兼容老的set/get方法 明白，我重置了一下提交，目前这个pr只是对manager模式适配一下可以使用FileMixedMetaManager本地文件存储meta.dat位点数据，避免目前manager方式时生产环境必须使用zookeeper，和file-instance.xml默认配置类似； 参数对应于CanalParameter新增参数的dataDir和metaFileFlushPeriod； tks
806,支持GTID模式下，默认获取当前gtid值作为起始值 tks
805,关于position设置 请问我设置日志读取位置，是不是只需要设置instance.proerties中的jouralName和position就行，那meda.dat里的呢，为什么它与instance.proerties中的不一样呢，到底哪个生效？ 我是用default-instance.xml zk集群 1. 使用default-instance.xml后，默认是通过zk管理位点信息的.所以 meda.dat在这里无效的 2. meda.dat针对的模式是file-instance.xml 3. instance.properties 中的位点与zk 位点 关系(以 default-instance.xml为例)  * 当zk中位点中不存在时，canal启动的位点以 instance.properties中为准，若没有，则通过 `show master status` 取最新的位点信息  *  当 zk 中存在位点信息时，以zk中的记录为准.  所以在正常数据同步的情况下，zk的位点和instance.properties中配置的位点是不同的。canal 也是通过其来实现断点续传的能力 @wingerx 你好，我尝试删除了zk上的位点信息，让其重新生成，按照你的说法应该同步Instance.properties中的位点信息（我设置过了），但是却没有，这个原因可能是出在哪里呢 1. Instance.properties中配置的只和启动时找位点有关 2. zk中记录的是client端消费成功后ack 时的位点 并不是说再instance.properties中配置了某位点信息，zk就会同步更新成这个位点. 
804,canal如何处理mysql中的blob数据类型？ 求大神请教canal中是如何处理mysql中的blob数据类型？ 在网上查找到的资料如下（不知道是否正确）： Canal 将 binlog 中的值序列化成了 String 格式给下游程序，因此在 Blob 格式的数据序列化成 String 时为了节省空间，强制使用了 IOS_8859_0 作为编码。因此，在如下情况下会造成中文乱码： 同步服务 JVM 使用了 UTF-8 编码 BLOB 字段中存储有中文字符 作者：haitaoyao 链接：https://www.jianshu.com/p/be3f62d4dce0 來源：简书 简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。 求大神指导！ 按照iso8859-1进行反解为bytes[]即可 谢谢指导！但是我将sqltype为longblob类型的字段使用iso8859-1反解为bytes[]再utf-8编码为string时，仍然是乱码： 这是longblob字段的内容： <img width="1128" alt="canal-blob-getvalue" src="https://user-images.githubusercontent.com/28953872/43885058-677e5658-9bea-11e8-956c-edeb50a0da5b.png"> 这是使用iso8859-1进行 反解： <img width="1123" alt="iso" src="https://user-images.githubusercontent.com/28953872/43885109-8a92545a-9bea-11e8-9102-2b5a0c10ec0d.png"> 我的代码如下： if(column.getMysqlType().toUpperCase().equals("LONGBLOB")){                     logger.info(">>>>>> is blob\n");                     logger.info("\n>>>>>> column.getValue() is: " + column.getValue());                     logger.info("\n\n\n\n\n\n>>>>>> new String(column.getValue().getBytes(\"iso8859-1\") \"UTF-8\") is: " + new String(column.getValue().getBytes("iso8859-1"))); } 请教大神如何fix？ 用 getValueBytes方法拿原始bytes 还是getBytesValue？用的爪机没办法确认。 能讲的详细点吗？大神 column没有getBytesValue方法，应该是getBytesValue ``` try {                 if (StringUtils.containsIgnoreCase(column.getMysqlType()  "BLOB")                     || StringUtils.containsIgnoreCase(column.getMysqlType()  "BINARY")) {                     // get value bytes                     builder.append(column.getName() + " : "                                    + new String(column.getValue().getBytes("ISO-8859-1")  "UTF-8"));                 } else {                     builder.append(column.getName() + " : " + column.getValue());                 }             } catch (UnsupportedEncodingException e) {             } ```             public java.lang.String getValue() {                 java.lang.Object ref = value_;                 if (!(ref instanceof java.lang.String)) {                     com.google.protobuf.ByteString bs = (com.google.protobuf.ByteString) ref;                     //用getValue()这里就已经用utf8编码了                     java.lang.String s = bs.toStringUtf8();                     if (bs.isValidUtf8()) {                         value_ = s;                     }                     return s;                 } else {                     return (java.lang.String) ref;                 }             }             public com.google.protobuf.ByteString getValueBytes() {                 java.lang.Object ref = value_;                 if (ref instanceof String) {                     com.google.protobuf.ByteString b = com.google.protobuf.ByteString.copyFromUtf8((java.lang.String) ref);                     value_ = b;                     return b;                 } else {                     //拿到原始的ByteString                     return (com.google.protobuf.ByteString) ref;                 }             } @alexlgj 先试试 @agapple 的吧。    印象在1.0.22版本里getValue （varbinary）会有信息丢失；那时候直接进去改了编码。 @agapple @lcybo 好的，谢谢两位大神的指导，我这边网速不怎么样。回复慢了，不好意思 做了个实验：         byte[] bs = {-1  -2};   //byte是负数的情况，这个就看你数据库的编码了         ByteString bstring = ByteString.copyFrom(bs);         String utf = bstring.toStringUtf8();         System.out.println(utf);    //丢数据了，变成了两个65533         String iso = new String(utf.getBytes("ISO-8859-1")  "UTF-8");         System.out.println(iso);    //没有办法还原，变成两个63 @alexlgj 编码问题蛮有意思的   按照@agapple的方法可以成功解码（blob字段内容为中文“北京”） column.getvalue得到； "image\\\":\\\"å\\\\u008C\\\\u0097äº¬\\\" \\\ new String(column.getValue().getBytes("ISO-8859-1")  "UTF-8"))得到： "image\\\":\\\"北京\ 我用的canal是1.0.24版本，您例子中的是没有访问数据库，为什么说“//byte是负数的情况，这个就看你数据库的编码了”？ latin1 128-255用一个byte是负数表示，有负数在utf-8里面代表多字节，是有一定规范的。128-255的sequence很有可能违反这些规范，就变成�了 如果有硬编码的东西（无序的128-255，非utf-8的编码），还是拿原始的byteString吧。 如果canal放进去的string，就是用的iso-8859-1编码过的，那忽略我说的吧，逃了。 @lcybo 明白了，非常感谢，大大的赞 @lcybo 之前server端传blob类型时强转了string类型，比较合理的做法是用ByteString，但现在不能改，已改老用户的blob解析处理都会乱码了 理解 两位大神，我还有些疑问： 1. 是不是canal server端会对mysql binlog的所有字段做iso-8859-1编码成string？ 2. 对编码完全不懂， 能帮忙解释下为什么这样更改编码就能正确解析出存入blob类型的字符串（"北京“）：new String(column.getValue().getBytes("ISO-8859-1")  "UTF-8")) ？该代码中的不同编码的执行顺序是怎样的？ （我的理解是canal server对blob字段的byte数组做iso-8859-1编码，但是iso-8859-1编码不能表示中文“北京”（乱码）。这时执行column.getValue()方法会进入到：if (!(ref instanceof java.lang.String)) {   对ref进行utf8-string） 大神，另外我想问下，column.getValueBytes().toByteArray()返回的是blob字段original 字节数组吗？还是总是utf-8编码或者其他编码的字节数组？为什么canal中(column.getValueBytes().toByteArray()一个汉字对应6个字节呢？ 这是我的测试结果： 插入的blob字段值： ”0123abc" java程序中打印插入的string的二进制数组内容： Arrays.toString(”0123abc".getBytes()) is： [48  49  50  51  97  98  99] canal中获得的二进制数组内容： Arrays.toString(column.getValueBytes().toByteArray() is： [48  49  50  51  97  98  99] blob字段值：”上”: Arrays.toString(”上".getBytes()) is： [-28  -72  -118]  Arrays.toString(column.getValueBytes().toByteArray() is: [-61  -92  -62  -72  -62  -118] blob字段值：”上海”: Arrays.toString(”上海".getBytes()) is： [-28  -72  -118  -26  -75  -73] Arrays.toString(column.getValueBytes().toByteArray() is： [-61  -92  -62  -72  -62  -118  -61  -90  -62  -75  -62  -73] 1.从agapple大神的描述，应该是只有blob和binary这么做了 另一个问题，如果用iso编码成string。那么string内一个char实际上放的就是单个byte，打印出来是乱码，但实际数据格式没有丢失。 您的意思是说虽然打印出来是乱码，但是二进制数据本身还是保持不变的吗？那为什么：new String(column.getValue().getBytes("UTF-8")  "UTF-8"))打印的结果是乱码呢？ getBytes("UTF-8")等同于用utf-8去编码那一堆乱码，打印出来的当然是乱码 我刚请教了下同学一些编码的知识，现在基本能理解了，谢谢大神的耐心指导，棒棒哒
803,canalConnector.subscribe()会把线程阻塞，造成stop无法停止 当以zookeeper HA模式来启动多个Canal Client时，如果某个Canal Client已经成功subsribe，则当前启动的Canal Client会阻塞在canalConnector.subscribe()方法上。 此时再调用如下的stop方法，会阻塞在join处，造成无法正常的stop     public void stop() {         if (!running) {             return;         }         //canalConnector.stopRunning();该版本的canal-client不支持         running = false;         if (thread != null) {             try {                 **thread.join();**             } catch (InterruptedException e) {                 e.printStackTrace();             }         }         KafkaProducerUtil.stop();         MDC.remove("destination");     } 我看新版的example的stop方法中增加了如下的代码，不知道是不是为了解决上面的问题的？ connector.stopRunning(); 如果是，那么之前版本该怎么解决阻塞的问题呢？ canal是通过这个机制来实现client端的HA的. 堵塞是为了避免同时有两个相同的client进行消费
802,canal example增加数据同步的样例代码 client层面的应用丰富，期望提供一个基于同步到数据库的标准实现
801,canal docker化打包支持 1.  增加Dockerfile，支持all in one的打包方式(centos + jdk + canal) 2. 参数层面，允许docker -e变量传递和修改 (java里通过System.getenv读取) 3. 交付形式 - 允许用户基于Dockerfile进行源码打包 - 上传到Docker hub，允许用户直接公网下载 对应的docker使用文档：https://github.com/alibaba/canal/wiki/Docker-QuickStart
800,canal原生支持一下RocketMQ对接 RocketMQ： https://rocketmq.apache.org/ canal原生支持一下数据写出到RocketMQ 代码已提交  参考文档: https://github.com/alibaba/canal/wiki/Canal-Kafka-RocketMQ-QuickStart 
799,关于canal接收到数据的排序问题? 当同时同一个cust_id 依次进行新增 修改 删除 新增 canal接收到两条数据的数据: 如: cust_id 类型 sql执行时间 001 add 1533002422 001 update 1533002422 001 delete 1533002423 001 add  1533002423 这样的话出现一个问题 我做数据仓库的拉链表的时候 无法知道 001 用户在1533002423毫秒情况下add和delete的顺序 我的问题是 这种情况下 还有其他的字段能提供对于同一个sql执行时间 对数据进行排序区分前后顺序 binlog里就是顺序关系 cust_id 是主键吗？如果是主键，为什么会有add两次的情况呢？？ mysql binlog机制比较复杂，内部很多优化。transaction内部只保证slave执行后的最终结果一致。
798,canal中的transaction ID获取和entry.getHeader().getLogfileOffset()偏移量的理解？ 问题1:  canal的一个message可能会对应mysql中的多个transaction，通过方法message.getEntries()可以获得List<Entry> entrys，entry又 分为transanctionBegin/ transactionEnd以及rowdata。 现在问题是，我在  > 获取transaction id<  的时候只有通过transactionEnd.getTransactionId拿到的是正常的数字，但是通过transactionbegin和rowdata获取到的则都是null。  请问canal中还有其他方法获取到transaction id吗？ 问题2:  entry.getHeader().getLogfileOffset()获取的偏移量是当前（entry）rowdata在日志文件中的序号（即第几条日志）还是字节数（即该row前面的数据所占的字节数）？ 求大神赐教！ 谢谢 自己动手通过mysqlbinlog --hexdump可以查看到commit才有xid。 offset是字节偏移。 棒，谢谢大神
797,canal如何区分一次transaction中的多条记录rowdata mysql中一次transaction可能包含多条记录（rowdata），那么canal是如何区分这些记录的呢？有没有类似oracle中的scn(system change number)来唯一标识对某个记录所做的更改？ #751 看一下这个pr是不是满足你说的场景 pr? 不好意思，方便解释的详细点吗？ 不好意思，刚没看到链接，谢谢
796,本地调试SimpleCanalClientTest抛异常java.net.ConnectException: Connection refused: connect解决办法 ![image](https://user-images.githubusercontent.com/16176283/43498536-f031ab84-9579-11e8-9bc6-09ca48546cbf.png) 将com.alibaba.otter.canal.example.SimpleCanalClientTest 类中的相应内容改成实际内容即可解决
794,kafka集成，canal event始终接收不到数据，不知道什么原因 see PR #790 
793,ByteString如何转换为CanalEntry.Entry? example 的代码里有 ``` ... CanalEntry.Entry.parseFrom(byteString) ``` OK
792,Received EOF packet from server ![image](https://user-images.githubusercontent.com/18360996/43445241-7a85f184-94d8-11e8-87f6-1bf1f0b8ddba.png) 如果slaveId 设置确实没有重复的话，最大可能请先参考 #777 
791,为什么insert会被解析成ddl？ RowChange对象信息如下。 unknownFields = {UnknownFieldSet@1470} "" bitField0_ = 30 tableId_ = 0 eventType_ = {CanalEntry$EventType@1609} "INSERT" isDdl_ = true sql_ = "insert into `test`(`school_student_id`  `school_group_id`  `like_user_id`  `school_event_id`) values('114'  '1'  '66889278'  '61')" rowDatas_ = {Collections$EmptyList@1473}  size = 0 props_ = {Collections$EmptyList@1473}  size = 0 ddlSchemaName_ = "test" memoizedIsInitialized = 1 memoizedSerializedSize = -1 memoizedSize = -1 memoizedHashCode = 0 1.  检查binlog是否为row模式 2. row模式下的RowsQueryLogEvent事件 1.0.26有解决这个问题不？ https://github.com/alibaba/canal/wiki/FAQ
790,kafka producer 适配 row data for performance #726 tks
789,启动不了 ![image](https://user-images.githubusercontent.com/18360996/43390651-4090cdb2-9421-11e8-9e18-4b1d13f8e758.png) 参照 #777 
788,canal对接性能采样初版 Hi         个人的一些实现思路如下： 1. plugable：为了方便以后对接prometheus以外的方案，并使相关代码集中，使用了service provider机制（CanalServerWithEmbedded中加载service），用runtime scope在deployer的pom里指定实现： <!-- 这里指定runtime的metrics provider--> 	<dependency> 		<groupId>com.alibaba.otter</groupId> 	        <artifactId>canal.prometheus</artifactId> 		<version>${project.version}</version> 		<scope>runtime</scope> 	</dependency> 2. 尽可能不改动canal的逻辑代码，当前的提交对原有代码改动如下： a. CanalServerWithEmbedded (ServiceProvider逻辑) b. startup.sh (加载一个javaagent，用于LTW(load time weaving))，目前注掉了，等版本稳定。 c. deployer的pom（指定metrics实现），目前同样注掉了。 因此，目前不会影响canal的逻辑。启动metrics的方式为把注掉的配置加上，再mvn install。 3. 当前实现的metrics有： - jvm 全家桶 - canal_net_inbound_bytes  从mysql接收的数据量，单位byte，用rate可计算速率。 - canal_net_outbound_bytes  向client发送的数据量，单位byte，用rate可计算速率。 - canal_instance  instance 列表 - canal_instance_traffic_delay  各instance的delay。目前还有limitation，在mysql binlog不更新时，delay会一直增加，目前的思路是利用mysql的master heart beat packet进行刷新，需要解析这个包，TODO - eventstore 的produce和ack索引：'canal_instance_store_produce_seq'与'canal_instance_store_consume_seq' 可用于计算TPS(rate(canal_instance_store_consume_seq))和ringbuffer remain evnets cound(canal_instance_store_produce_seq - canal_instance_store_consume_seq) 其他的metrics请补充。 4. 一些待完善的TODO： - HttpServer是否需要支持https。 - 本机测试用的是debug，UT之后补上。 - 参数可配置化。 - ... 思虑有不周的地方，欢迎大家补充并提出comments。 Best regards 要在IDE里面debug新feature的话，vmarg要加上 -javaagent:/pathto/aspectjweaver-${version}.jar 看了下，结构比较不错，有几个建议： 1.  可以适当改动主体server的代码，为监控做一些埋点，这也是非常有必要的 (通过aspect并不是特别建议) 2. 针对监控区分实例级和destination级，比如网络流量读取和写出，可以区分出不同的destination (因为业务上经常要找出谁是捣蛋份子，基本上就是个别的destination影响了整体) @agapple 感谢comments，我会好好想想如何进行refactor。 BTW，请教一下aspect在做埋点方面的劣势。 主要在于一些维护性，监控应该属于canal内置的能力(理想情况应该是整个binlog->parser->sink->store->client，能完整埋点做采样得到性能数据)，做外围非常灵活的可插拔并不是非常有意义 @agapple 嗯，明白了。简单来说就是利用canal server的现有机制做一些neutral的埋点。已经有一些想法了，有空整理出来。
787,用show processlist 命令看不到dump进程 [v1.0.26.alpha4]  canal启动起来后，开始dump 为什么在mysql端，用show processlist 命令看不到dump进程 确认一下是不是开始消费数据了： instance日志里有没有find start position : EntryPosition[included=false journalName=xxxx position=xxxx serverId=<null> gtid=<null> timestamp=<null>]. 然后meta.log有没有持续刷新 show slave hosts 可以看到canal的dump进程
786,请教Canal订阅阿里云RDS(MySql)的案例 [业务场景] 现有阿里云RDS（MySql)数据库，想通过自己安装canal，订阅RDS数据的binlog。 请大神提供案例。 多谢、多谢~ 参考：https://github.com/alibaba/canal/wiki/FAQ
785,no meda.dat  I accidentally deleted the file：meta.dat（Below each instance）  what should I do？ set position info in instance.properties  which you could find in meta.log.However  there might be an overlap. the latest one in meta.log @lcybo Do I need to manually create meta.dat and copy the contents of meta.log into newly meta.dat? It would be created and maintained by metamanager automatically. @lcybo Unfortunately  it doesn't automatically generate in the document. Is it because I didn't configure position and what should position fill in? @lcybo I know why I did not generate meta.dat files because I did not add the following sentence to my code: connector.subscribe (* * \ \ \ * *); @lcybo  But is this not dispensable  as long as the canal.instance.filter.regex in the Instance.properties should not be configured with connector.subscribe (". * \ \.. *"); position is selected by following priority: 1.meta.dat 2.instance.properties 3.the position via 'show master status'(probably events lost) the meta.dat would be created after successful subscription if not exist. @lcybo Now you can automatically generate meta.dat  but now I have to write connector.subscribe () in the program; connector is the CanalConnector type. If I write this  the filter attribute in Instance.properties will be rewritten into an empty string  and I can't let canal.instance.filter.regex in Instance.properties now. What do I do to make it work  rather than using connector.subscribe () in the program? Yes  filter is not necessary. Canal server work with canal client together  there is a example client in canal project. Here is the code slice:     protected void process() {         int batchSize = 5 * 1024;         while (running) {             try {                 MDC.put("destination"  destination);                 connector.connect();                 connector.subscribe();                 while (running) {                     Message message = connector.getWithoutAck(batchSize); // 获取指定数量的数据                     long batchId = message.getId();                     int size = message.getEntries().size();                     if (batchId == -1 || size == 0) {                         // try {                         // Thread.sleep(1000);                         // } catch (InterruptedException e) {                         // }                     } else {                         printSummary(message  batchId  size);                         printEntry(message.getEntries());                     }                     connector.ack(batchId); // 提交确认                     // connector.rollback(batchId); // 处理失败  回滚数据                 }             } catch (Exception e) {                 logger.error("process error!"  e);             } finally {                 connector.disconnect();                 MDC.remove("destination");             }         }     } @lcybo  If I write connector.subscribe () in the program again  the regex on the server side will be covered. How can I make regex not cover? a. If you call subscribe()  then canal.instance.filter.regex is used  ignore the empty string in meta.dat. b.Otherwise(subscribe(regex)) use the parameter 'regex' instead of canal.instance.filter.regex. @lcybo  It's like you said：you call subscribe()  then canal.instance.filter.regex is used  ignore the empty string in meta.dat.The problem has been solved，Thank you very much。 :)
784,canalserver支持一个instance实例抽取多库的么？ canalserver使用单库instance配置，不分组，然后在一个实例中抽取多个库的binlog日志。这种方式配置方法支持么？ 已找到配置方法canal.instance.defaultDatabaseName=db1 db2。这个配置适用于同一个ip、port实例下不同库。库名之间使用逗号隔开。这样配置即可。 我觉得你理解得有一点小问题，你那样是配置默认监听的多个库，但最好是在下面这个配置项中进行正则匹配就好了 所有库 canal.instance.filter.regex = .*\\..* db1 db2 canal.instance.filter.regex = db1\\..* db2\\..* LS正解
783,canal求解释下message，entries，transanctionID，batchid  event以及rowchange的关系 正在学习canal的使用，求大神梳理下canal中几个概念的关系： 目前我的理解是：一个message包含多个entry（其共同构成entries），一个entry对应一个eventtype（可以是insert/delete等），一个entry可以解析得到一个rowchange，一个rowchange可以解析得到多个rowdata，一个rowdata可以解析得到before的columns（方法getBeforeColumnsList）和after的columns。 - 问题1: 请问它们内部的关联是怎样的？ - 问题2: transactionID和batchID的区别？ - 问题3: entry.getHeader().getSchemaName())可以获取到mysql的schema，但是怎么获取mysql 的instance？ 谢谢 求大神指点 一个message就是get到的batch packet，batchId是其标识，是canal概念；transactionId是mysql事务相关的。mysql binlog 会将同表的同类型操作（insert  update等）的变更行merge为一个event以节省meta开销，entry是canal proto类型，对应mysql event。rowdata对应每一行数据。before和after对应变更前后，所以update兼具两者，insert只有after delete只有before。在特殊情况下，比如开启ndb的一些参数，update会变成insert类型。 你好，谢谢你的指点，讲得很清楚。我还有个问题：我在mysql中只插入了一个row，但是canal中显示的entrys.size() 是3，那我如何知道哪个entry才对应我刚才插入的row呢？ 应该是还有 transactionBegin 和 transactionEnd 事件，可以按 event 类型筛选一下。 On Jul 26  2018  18:20 +0800  alexlgj <notifications@github.com>  wrote: > 你好，谢谢你的指点，讲得很清楚。我还有个问题：我在mysql中只插入了一个row，但是canal中显示的entrys.size() 是3，那我如何知道哪个entry才对应我刚才插入的row呢？ > — > You are receiving this because you are subscribed to this thread. > Reply to this email directly  view it on GitHub  or mute the thread. ls正解 感谢大神的回复，现在懂了。测试了几次，结果如下： 一次insert或delete或者update操作 entrys.size() is: 3 连续执行三次insert数据操作，再启动canal client entrys.size() is: 9 当entry是transactionbegin或transactionend，对应的eventtype都是update 当entry是rowdata，对应的eventtype是对应的操作，即insert/delete/update 还遇到个问题，如何通过canal获取mysql database的instance名称，从entry的header中只找到了schema（通过entry.getHeader().getSchemaName())，但是怎么获取mysql 的instance？ instance是指port区分的mysql实例吗？ 考虑一下用serverId区分 好的，非常感谢大神
782,MysqlMultiStageCoprocessor健壮性enhance Hi     文哥的commit解决了由于EOF包带来的问题。     但MysqlMultiStageCoprocessor在其他情况下仍存在出现问题的可能。     若异常不是onEvent body里产生的，就比如#771的InterruptedException。     onShutdown里的stop与dump线程的reset，会有竞争发生。     如果MysqlMultiStageCoprocessor仅仅在dump线程内串行调用，那样比较安全。     所以改成，发生意外exception，也由publish中check，由dump线程进行reset。 Best regard [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=782) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=782) before we can accept your contribution.<br/><hr/>**Chuanyi Li L** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=782) it.</sub> [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=782) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=782) before we can accept your contribution.<br/><hr/>**Chuanyi Li L** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=782) it.</sub> tks @lcybo 你的incident定义和之前的exception重复了，直接合并为一个 ok，我当时觉得canalParseException可能有特殊处理，第二个commit特意加了incident区分了一下。看来想多了233 @lcybo 邀请你加入项目管理员，麻烦私信我邮箱联系方式：jianghang115@gmail.com @agapple 已accpet，邮箱：chuanyili0625@gmail.com  请多关照
781,最新版fastsql-preview 520 ： com.alibaba.fastsql.sql.parser.ParserException fastsql版本： 最新的520  sql语句以及Exception： case 1 :  sql_6 = "CREATE INDEX `idx_t_uid` on stu_score (`uid`) COMMENT '' ALGORITHM DEFAULT LOCK DEFAULT "; Exception in thread "main" com.alibaba.fastsql.sql.parser.ParserException: syntax error  error in :'ITHM DEFAULT LOCK DEFAULT  pos 134  line 1  column 136  token IDENTIFIER null 	at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:363) 	at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:520) 	at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:92) 	at com.test.HelloWorld.main(HelloWorld.java:266) case 2: sql = "alter table test  COLLATE utf8mb4_unicode_ci" ; Exception in thread "main" com.alibaba.fastsql.sql.parser.ParserException: syntax error  error in :'LATE utf8mb4_unicode_ci'  expect =  actual null  pos 44  line 1  column 27  token IDENTIFIER utf8mb4_unicode_ci 	at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:363) 	at com.alibaba.fastsql.sql.parser.SQLParser.accept(SQLParser.java:371) 	at com.alibaba.fastsql.sql.dialect.mysql.parser.MySqlStatementParser.parseAlterTable(MySqlStatementParser.java:4818) 	at com.alibaba.fastsql.sql.dialect.mysql.parser.MySqlStatementParser.parseAlter(MySqlStatementParser.java:3544) 	at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:264) 	at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:92) 	at com.test.HelloWorld.main(HelloWorld.java:266) case 3： sql_8 = "alter table task AUTO_INCREMENT = 20000000 COMMENT ='自增起始值'" ; Exception in thread "main" com.alibaba.fastsql.sql.parser.ParserException: syntax error  error in :'0000 COMMENT ='自增起始值''  expect ON  actual =  pos 52  line 1  column 52  token = 	at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:363) 	at com.alibaba.fastsql.sql.parser.SQLParser.accept(SQLParser.java:371) 	at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseComment(SQLStatementParser.java:3488) 	at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:354) 	at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:92) 	at com.test.HelloWorld.main(HelloWorld.java:266) 这些是有一些问题，已反馈做修改，建议先绕过 alter table test COLLATE = utf8mb4_unicode_ci alter table task AUTO_INCREMENT = 20000000    COMMENT ='自增起始值' 再补充一个ddl  CREATE TABLE `app_info` (`id` bigint(20) NOT NULL  `app_name` varchar(255) NOT NULL  PRIMARY KEY (`id`) INDEX `idx` USING BTREE (`app_name`) comment '') ;
780,fix bug #776 [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=780) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=780) before we can accept your contribution.<br/><hr/>**winger** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=780) it.</sub> [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=780) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=780) before we can accept your contribution.<br/><hr/>**winger** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=780) it.</sub> @agapple gmail邮箱(jianghang115@gmail.com)有我的留言，谢谢。 tks 
779,注释过长会引起数据错乱 注释本来是： 定向设置，32个字符，每个字符有3个值（0空1不限2确定值-指定{不分指定排除的也存为指定}3确定值-排除）第1个字符：应用渠道，第2个字符：网络环境，第3个字符：移动操作系统，第4个字符：平台版本号，第5个字符：地域定向，第6个字符：性别，第7个字符：年龄段，第8个字符：关键词【0没有；1不限；2指定；3排除；4；智能】，第9个字符：商业兴趣标签，第10个字符：AppList标签，第11个字符：设备价格标签，第12个字符：视频类型，第13个字符：运营商，第14个字符：自定义人群包 改成： 定向设置，32个字符，每个字符有3个值（0空1不限2确定值-指定{不分指定排除的也存为指定}3确定值-排除）第1个字符：应用渠道，第2个字符：网络环境，第3个字符：移动操作系统，第4个字符：平台版本号，第5个字符：地域定向，第6个字符：性别，第7个字符：年龄段，第8个字符：关键词【0没有；1不限；2指定；3排除；4；智能】，第9个字符：商业兴趣标签，第10个字符：AppList标签，第11个字符：设备价格标签，第12个字符：视频类型，第13个字符：运营商，第14个字符：自定义人群包，第15个字符：应用渠道-资讯版，第16个字符：平台版本号-资讯版 以后，有一个字段的数据出现错乱 给我一个完整的ddl  sql alter table tb_ad_group modify `target_setting` varchar(64) NOT NULL DEFAULT '00000000000000000000000000000000' COMMENT '定向设置，32个字符，每个字符有3个值（0空1不限2确定值-指定{不分指定排除的也存为指定}3确定值-排除）第1个字符：应用渠道，第2个字符：网络环境，第3个字符：移动操作系统，第4个字符：平台版本号，第5个字符：地域定向，第6个字符：性别，第7个字符：年龄段，第8个字符：关键词【0没有；1不限；2指定；3排除；4；智能】，第9个字符：商业兴趣标签，第10个字符：AppList标签，第11个字符：设备价格标签，第12个字符：视频类型，第13个字符：运营商，第14个字符：自定义人群包，第15个字符：应用渠道-资讯版，第16个字符：平台版本号-资讯版'; 最新的alpha 4解析没问题 想问您一下 如果不换版本。有什么方法吗。。。版本不是特别方便更换额
778,parse row data failed canal 1.0.26-SNAPSHOT 5.5.52-MariaDB GRANT SELECT  REPLICATION SLAVE  REPLICATION CLIENT ON *.* TO 'czbrepclient'@'%' IDENTIFIED BY PASSWORD '*A6A643DF20257C290F26693E1A59F69448428157' Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: java.lang.IllegalArgumentException: limit excceed: 148         at com.taobao.tddl.dbsync.binlog.LogBuffer.getFullString(LogBuffer.java:1123) 提供binlog文件或者测试数据复现常静  发你gmail里了，请查收
777, fix issue #771 #776 #756 [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=777) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=777) before we can accept your contribution.<br/><hr/>**winger** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=777) it.</sub> tks
776,[v1.0.26.alpha4] decode event时报limit exceed错误 ![image](https://user-images.githubusercontent.com/8179551/43183817-099b0e50-9019-11e8-8339-2d922e48b9ed.png) decode event时报limit exceed错误 应该是和这个commit有关 https://github.com/alibaba/canal/commit/89726a636530b73a6b97cecc2b5bcee4fb464f86 mysql版本是啥？ 抱歉.@gaoxiangyu  我依赖于 MySQL 5.7测试的，能把引起这个错误的MySQL 版本说下吗？方便我这边跟进，谢谢. 5.6.28 @gaoxiangyu 多谢，我跟进一下 @agapple 这个问题我来跟进. BTW，方便的时候请看下你的gmail 
775,canal支持spring4或spring5吗？ 目前在引入客户端时与项目中spring4、spring5冲突 目前默认用spring3 @agapple 有计划支持spring4或spring5吗，现在spring4用的比较普遍，spring3用的比较少 你可以提交一个PR给我，canal对spring的依赖就是一个IOC，可以升级 spring5要求java8，要注意下
774,[v1.0.26.alpha4]connect timed out错误 018-07-25 10:50:58.209 [destination = example   address = /127.0.0.1:3306   EventParser] WARN  c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - Received EOF packet from server  apparent master disconnected. It's may be duplicate slaveId   check instance config 2018-07-25 10:51:17.034 [destination = example   address = /127.0.0.1:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position  {"identity":{"slaveId":-1 "sourceAddress":{"address":"127.0.0.1" "port":3306}} "postion":{"gtid":"" "included":false "journalName":"mysql-bin.000027" "position":299492902 "serverId":2 "timestamp":1532487056000}} 2018-07-25 10:51:17.086 [destination = example   address = /127.0.0.1:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000027 position=299492902 serverId=2 gtid= timestamp=1532487056000] 2018-07-25 10:51:18.081 [destination = example   address = /127.0.0.1:3306   EventParser] WARN  c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - Received EOF packet from server  apparent master disconnected. It's may be duplicate slaveId   check instance config 2018-07-25 10:51:33.753 [destination = example   address = /127.0.0.1:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position  {"identity":{"slaveId":-1 "sourceAddress":{"address":"127.0.0.1" "port":3306}} "postion":{"gtid":"" "included":false "journalName":"mysql-bin.000027" "position":299516130 "serverId":2 "timestamp":1532487071000}} 2018-07-25 10:51:33.760 [destination = example   address = /127.0.0.1:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000027 position=299516130 serverId=2 gtid= timestamp=1532487071000] 2018-07-25 10:51:43.767 [destination = example   address = /127.0.0.1:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address 127.0.0.1/127.0.0.1:3306 has an error  retrying. caused by  java.io.IOException: connect /127.0.0.1:3306 failure 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:77) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.reconnect(MysqlConnector.java:86) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.reconnect(MysqlConnection.java:85) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:186) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Unknown Source) [na:1.8.0_152] Caused by: java.net.SocketTimeoutException: connect timed out 	at java.net.TwoStacksPlainSocketImpl.socketConnect(Native Method) ~[na:1.8.0_152] 	at java.net.AbstractPlainSocketImpl.doConnect(Unknown Source) ~[na:1.8.0_152] 	at java.net.AbstractPlainSocketImpl.connectToAddress(Unknown Source) ~[na:1.8.0_152] 	at java.net.AbstractPlainSocketImpl.connect(Unknown Source) ~[na:1.8.0_152] 	at java.net.PlainSocketImpl.connect(Unknown Source) ~[na:1.8.0_152] 	at java.net.SocksSocketImpl.connect(Unknown Source) ~[na:1.8.0_152] 	at java.net.Socket.connect(Unknown Source) ~[na:1.8.0_152] 	at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannelPool.open(BioSocketChannelPool.java:18) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannelPool.open(SocketChannelPool.java:18) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:72) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	... 4 common frames omitted 2018-07-25 10:51:43.774 [destination = example   address = /127.0.0.1:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: connect /127.0.0.1:3306 failure 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:77) 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.reconnect(MysqlConnector.java:86) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.reconnect(MysqlConnection.java:85) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:186) 	at java.lang.Thread.run(Unknown Source) Caused by: java.net.SocketTimeoutException: connect timed out 	at java.net.TwoStacksPlainSocketImpl.socketConnect(Native Method) 	at java.net.AbstractPlainSocketImpl.doConnect(Unknown Source) 	at java.net.AbstractPlainSocketImpl.connectToAddress(Unknown Source) 	at java.net.AbstractPlainSocketImpl.connect(Unknown Source) 	at java.net.PlainSocketImpl.connect(Unknown Source) 	at java.net.SocksSocketImpl.connect(Unknown Source) 	at java.net.Socket.connect(Unknown Source) 	at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannelPool.open(BioSocketChannelPool.java:18) 	at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannelPool.open(SocketChannelPool.java:18) 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:72) 	... 4 more ] 2018-07-25 10:52:13.657 [destination = example   address = /127.0.0.1:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address 127.0.0.1/127.0.0.1:3306 has an error  retrying. caused by  java.io.IOException: connect /127.0.0.1:3306 failure 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:77) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:81) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:172) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Unknown Source) [na:1.8.0_152] Caused by: java.net.SocketTimeoutException: connect timed out 	at java.net.TwoStacksPlainSocketImpl.socketConnect(Native Method) ~[na:1.8.0_152] 	at java.net.AbstractPlainSocketImpl.doConnect(Unknown Source) ~[na:1.8.0_152] 	at java.net.AbstractPlainSocketImpl.connectToAddress(Unknown Source) ~[na:1.8.0_152] 	at java.net.AbstractPlainSocketImpl.connect(Unknown Source) ~[na:1.8.0_152] 	at java.net.PlainSocketImpl.connect(Unknown Source) ~[na:1.8.0_152] 	at java.net.SocksSocketImpl.connect(Unknown Source) ~[na:1.8.0_152] 	at java.net.Socket.connect(Unknown Source) ~[na:1.8.0_152] 	at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannelPool.open(BioSocketChannelPool.java:18) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannelPool.open(SocketChannelPool.java:18) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:72) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	... 3 common frames omitted 2018-07-25 10:52:13.658 [destination = example   address = /127.0.0.1:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: connect /127.0.0.1:3306 failure 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:77) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:81) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:172) 	at java.lang.Thread.run(Unknown Source) Caused by: java.net.SocketTimeoutException: connect timed out 	at java.net.TwoStacksPlainSocketImpl.socketConnect(Native Method) 	at java.net.AbstractPlainSocketImpl.doConnect(Unknown Source) 	at java.net.AbstractPlainSocketImpl.connectToAddress(Unknown Source) 	at java.net.AbstractPlainSocketImpl.connect(Unknown Source) 	at java.net.PlainSocketImpl.connect(Unknown Source) 	at java.net.SocksSocketImpl.connect(Unknown Source) 	at java.net.Socket.connect(Unknown Source) 	at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannelPool.open(BioSocketChannelPool.java:18) 	at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannelPool.open(SocketChannelPool.java:18) 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:72) 	... 3 more ] @agapple  本机的MySQL通过 127.0.0.1:3306 连不上吧 @wingerx 这个127.0.0.1是其他具体IP，只是贴日志时替换了
773, [v1.0.26.alpha4]parse events has an error错误 ![image](https://user-images.githubusercontent.com/9798724/43177235-d82ca3c0-8ff9-11e8-87f1-e43826acb891.png) 2018-07-25 10:52:13.662 [destination = example   address = /127.0.0.1:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - parse events has an error java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.FutureTask@869cc59 rejected from java.util.concurrent.ThreadPoolExecutor@36f77020[Terminated  pool size = 0  active threads = 0  queued tasks = 0  completed tasks = 0] 	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(Unknown Source) ~[na:1.8.0_152] 	at java.util.concurrent.ThreadPoolExecutor.reject(Unknown Source) ~[na:1.8.0_152] 	at java.util.concurrent.ThreadPoolExecutor.execute(Unknown Source) ~[na:1.8.0_152] 	at java.util.concurrent.AbstractExecutorService.submit(Unknown Source) ~[na:1.8.0_152] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor.start(MysqlMultiStageCoprocessor.java:120) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor.reset(MysqlMultiStageCoprocessor.java:187) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:306) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Unknown Source) ~[na:1.8.0_152] 相同问题啊，解决没 @agapple 这不是root cause，找最前面的日志 使用v1.0.25没有问题 应该是单核机器的启动失败问题，v1.1.1已修复 @agapple v1.1.1正式版什么时候发布
772,fix issue #771 #756 [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=772) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=772) before we can accept your contribution.<br/><hr/>**winger** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=772) it.</sub> [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=772) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=772) before we can accept your contribution.<br/><hr/>**winger** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=772) it.</sub>
771,启动canal时，线程池工作异常 2018-07-24 15:44:43.247 [destination = example   address = /22.5.229.239:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position mysql-bin.000001:558240:null 2018-07-24 15:44:43.271 [destination = example   address = /22.5.229.239:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000001 position=558240 serverId=<null> gtid= timestamp=<null>] 2018-07-24 15:44:43.331 [destination = example   address = /22.5.229.239:3306   EventParser] WARN  c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - Received EOF packet from server  apparent master disconnected. It's may be duplicate slaveId   check instance config 2018-07-24 15:45:00.446 [destination = example   address = /22.5.229.239:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position {"identity":{"slaveId":-1 "sourceAddress":{"address":"cbjup04" "port":3306}} "postion":{"gtid":"" "included":false "journalName":"mysql-bin.000001" "position":559146 "serverId":16782861 "timestamp":1532417891000}} 2018-07-24 15:45:00.447 [destination = example   address = /22.5.229.239:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000001 position=559146 serverId=16782861 gtid= timestamp=1532417891000] 2018-07-24 15:45:00.453 [destination = example   address = /22.5.229.239:3306   EventParser] WARN  c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - Received EOF packet from server  apparent master disconnected. It's may be duplicate slaveId   check instance config 2018-07-24 15:45:18.422 [destination = example   address = /22.5.229.239:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position {"identity":{"slaveId":-1 "sourceAddress":{"address":"cbjup04" "port":3306}} "postion":{"gtid":"" "included":false "journalName":"mysql-bin.000001" "position":559146 "serverId":16782861 "timestamp":1532417891000}} 2018-07-24 15:45:18.422 [destination = example   address = /22.5.229.239:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000001 position=559146 serverId=16782861 gtid= timestamp=1532417891000] 2018-07-24 15:45:18.426 [destination = example   address = /22.5.229.239:3306   EventParser] WARN  c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - Received EOF packet from server  apparent master disconnected. It's may be duplicate slaveId   check instance config 2018-07-24 15:45:18.440 [destination = example   address = /22.5.229.239:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - parse events has an error java.util.concurrent.RejectedExecutionException: Task com.lmax.disruptor.WorkProcessor@58d43d50 rejected from java.util.concurrent.ThreadPoolExecutor@56057cbf[Terminated  pool size = 0  active threads = 0  queued tasks = 0  completed tasks = 0]         at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2048) ~[na:1.7.0_45]         at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:821) ~[na:1.7.0_45]         at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1372) ~[na:1.7.0_45]         at com.lmax.disruptor.WorkerPool.start(WorkerPool.java:140) ~[disruptor-3.4.2.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor.start(MysqlMultiStageCoprocessor.java:122) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor.reset(MysqlMultiStageCoprocessor.java:187) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:306) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]         at java.lang.Thread.run(Thread.java:744) ~[na:1.7.0_45] #756 相同的问题 查到问题所在了。 @agapple 在 fix #726 issue时将 binlog_flags 设置为了 BINLOG_DUMP_NON_BLOCK，master在没有可发送的binlog event之后，就会返回一个EOF package.  通常对于slave来说，一直把连接挂着可能更好，这样能更及时收到新产生的binlog event，不知@agapple 基于什么考虑，将设置改成了 BINLOG_DUMP_NON_BLOCK？ 或者说更优雅处理 EOF package 而不是打印一个误导的" Received EOF packet from server  apparent master disconnected. It's may be duplicate slaveId   check instance config" 日志信息出来? @wingerx 你好，我的问题主要是在下面，在创建parse的线程池的时候，总会报java.util.concurrent.RejectedExecutionException，而且pool size为0，我的服务器是4核的，按照parallelThreadSize的默认值为服务器总可用物理核数的60%，poolSize应该是2，而不是0。 相同问题，楼主解决没 @fanpeng1100 没有，我的问题是调用线程池报错..... @sky-mariner 线程池报错的原因请看我提交的pr #777。 对于目前最新release v1.0.26 alpha 4，先改配置 canal.instance.parser.parallel = false 用吧 @wingerx 好的，谢谢~’ @wingerx 正解
770,指定canal.instance.master.timestamp时间戳消费无效 在instance中指定了时间戳，发现client还会拿到这个时间戳以前的log，如果想只消费这个时间戳之后的log，应该配置哪些呢？单独配置canal.instance.master.timestamp是不是不可以？ 清理掉上一次的记录的位点信息，比如zookeeper节点和meta.dat文件
769,1.0.26 版本，单机未使用h2.db 再一条sql中修改多个表结构以及创建新表解析出现异常.监听的整个库 异常信息如下 Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: fetch failed by table meta:`数据库`.`_表名_new` Caused by: java.io.IOException: ErrorPacket [errorNumber=1146  fieldCount=-1  message=Table '数据库._表名_new' doesn't exist  sqlState=42S02  sqlStateMarker=#] at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.query(MysqlQueryExecutor.java:61) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:94) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMetaByDB(TableMetaCache.java:89) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:32) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:62) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:52) at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) at com.google.common.cache.LocalCache.get(LocalCache.java:3937) at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941) at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824) at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:185) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:759) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:428) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:114) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:66) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:337) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:184) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:152) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) at java.lang.Thread.run(Thread.java:748)  message=Table '数据库._表名_new' doesn't exist  sqlState=42S02  sqlStateMarker=#] 找不到表 这个表是不存在的，错误信息是在修改索引，增加表字段和创建其他表操作附近时刻发生的，是不是临时表之类的造成的解析异常 开启Table TSDB来测试吧，以前反查数据库的模式很多限制 好 我开启下h2 看后续还会发生这样的异常不
768,修改Readme.md，新增数据订阅活动公告 修改Readme.md，新增数据订阅活动公告 [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=768) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=768) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=768) it.</sub>
767,canal客户端连接问题 ZK在47.96.186.32:2181  Canal在47.96.186.32:11111 端口方面都没问题  Canal服务端日志都正常. 客户端连接的时候  ZK能成功连上  但是调用 **canalConnector.subscribe(canalClientProperties.getSubscribeFilter());** 这一段方法时抛出下面的异常  canal的autoScan已关闭. something goes wrong when subscribing from server:null 2018-07-23 14:14:30.100 [main-SendThread(47.96.186.32:2181)] DEBUG org.apache.zookeeper.ClientCnxn.readResponse(815) -                 Reading reply sessionid:0x164b770991e0058  packet:: clientPath:null serverPath:null finished:false header:: 298 4  replyHeader:: 298 12176 0  request:: '/otter/canal/destinations/example/1001/running T  response:: #7b22616374697665223a747275652c2261646472657373223a223132372e302e302e313a3536333633222c22636c69656e744964223a313030317d s{12174 12175 1532326470040 1532326470047 1 0 0 100406785942093912 59 0 12174}  2018-07-23 14:14:30.100 [taskExecutor-5] WARN  c.a.o.c.c.i.r.ClientRunningMonitor.check(168) -                 canal is running in [127.0.0.1:56363]   but not in [192.168.31.207] 2018-07-23 14:14:30.101 [taskExecutor-5] ERROR t.j.m.m.s.c.CanalIncrementSyncTask.handleException(25) -                 com.alibaba.otter.canal.protocol.exception.CanalClientException: failed to subscribe after 3 times retry. 	at com.alibaba.otter.canal.client.impl.ClusterCanalConnector.subscribe(ClusterCanalConnector.java:119) 	at tech.jiyu.micromagasin.mc.sync.canal.CanalIncrementSyncTask.sync(CanalIncrementSyncTask.java:75) 	at tech.jiyu.micromagasin.mc.sync.canal.CanalIncrementSyncTask$$FastClassBySpringCGLIB$$783fbb6f.invoke(<generated>) 	at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:204) 	at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:720) 	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157) 	at org.springframework.aop.interceptor.AsyncExecutionInterceptor$1.call(AsyncExecutionInterceptor.java:115) 	at java.util.concurrent.FutureTask.run$$$capture(FutureTask.java:266) 	at java.util.concurrent.FutureTask.run(FutureTask.java) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) 	at java.lang.Thread.run(Thread.java:748) com.alibaba.otter.canal.protocol.exception.CanalClientException: failed to subscribe after 3 times retry. 	at com.alibaba.otter.canal.client.impl.ClusterCanalConnector.subscribe(ClusterCanalConnector.java:119) 	at tech.jiyu.micromagasin.mc.sync.canal.CanalIncrementSyncTask.sync(CanalIncrementSyncTask.java:75) 	at tech.jiyu.micromagasin.mc.sync.canal.CanalIncrementSyncTask$$FastClassBySpringCGLIB$$783fbb6f.invoke(<generated>) 	at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:204) 	at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:720) 	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157) 	at org.springframework.aop.interceptor.AsyncExecutionInterceptor$1.call(AsyncExecutionInterceptor.java:115) 	at java.util.concurrent.FutureTask.run$$$capture(FutureTask.java:266) 	at java.util.concurrent.FutureTask.run(FutureTask.java) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) 	at java.lang.Thread.run(Thread.java:748) 目前找到问题的原因   ![image](https://user-images.githubusercontent.com/31718669/43062693-3e5187a2-8e8c-11e8-9c3a-beb631e3d868.png) canalServer 的canal.properties ip配置为127.0.0.1  当客户端连接至ZK时获取上面的数据连接Canal时使用的也是127.0.0.1的地址去连接Canal Server  这就变成了客户端机器在连接本地的CanalServer   一开始这只是个猜想  但是当我在本地起了一个CanalServer后就不再抛异常了 可以正常连接.  问题在于  我尝试把远端的CanalServer的ip配置成公网IP  这样应该就能正常连接了  但是改成公网IP后CanalServer 启动失败  提示出错 "**canal cannot assign requested address**"
766,cancal启动正常，java客户端可以连接上，但有数据更改后客户端获取不到。 cancal启动正常，java客户端可以连接上，但有数据更改后客户端获取不到更改。下面是记录 谁遇到过这种情况，请帮忙下。谢谢。 2018-07-23 12:29:37.519 [destination = example   address = /127.0.01:3307   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: ErrorPacket [errorNumber=1046  fieldCount=-1  message=No database selected  sqlState=3D000  sqlStateMarker=#]  with command: show create table `sbed`.`全速通库存`;show create table `sbed`.`商品备案信息表`;show create table `sbed`.`康多多库存`;show create table `sbed`.`没有GST的商品`;show create table `sbed`.`澳猫商品内部价`;show create table `sbed`.`澳猫商品申报价`;show create table `sbed`.`澳猫商品销售价`;show create table `sbed`.`网店管家商品总库存`;show create table `sbed`.`网店管家商品数据`;show create table `sbed`.`货品价格表`;show create table `sbed`.`QRTZ_BLOB_TRIGGERS`;show create table `sbed`.`QRTZ_CALENDARS`;show create table `sbed`.`QRTZ_CRON_TRIGGERS`;show create table `sbed`.`QRTZ_FIRED_TRIGGERS`;show create table `sbed`.`QRTZ_JOB_DETAILS`;show create table `sbed`.`QRTZ_LOCKS`;show create table `sbed`.`QRTZ_PAUSED_TRIGGER_GRPS`;show create table `sbed`.`QRTZ_SCHEDULER_STATE`;show create table `sbed`.`QRTZ_SIMPLE_TRIGGERS`;show create table `sbed`.`QRTZ_SIMPROP_TRIGGERS`;show create table `sbed`.`QRTZ_TRIGGERS`;show create table `sbed`.`sys_add_goods_advice`;show create table `sbed`.`sys_aomao_goods`;show create table `sbed`.`sys_aomao_goods_price`;show create table `sbed`.`sys_aomao_logistics`;show create table `sbed`.`sys_aomao_trade`;show create table `sbed`.`sys_aomao_trade_goods`;show create table `sbed`.`sys_aomao_user`;show create table `sbed`.`sys_aomao_user_group`;show create table `sbed`.`sys_aomao_user_inventory`;show create table `sbed`.`sys_attachment`;show create table `sbed`.`sys_au_data`;show create table `sbed`.`sys_auspost_base`;show create table `sbed`.`sys_auspost_log`;show create table `sbed`.`sys_auspost_order`;show create table `sbed`.`sys_common_map`;show create table `sbed`.`sys_config`;show create table `sbed`.`sys_content`;show create table `sbed`.`sys_content_taxonomy`;show create table `sbed`.`sys_currency`;show create table `sbed`.`sys_customer`;show create table `sbed`.`sys_customer_address`;show create table `sbed`.`sys_customer_category`;show create table `sbed`.`sys_customer_grade`;show create table `sbed`.`sys_dept`;show create table `sbed`.`sys_documents`;show create table `sbed`.`sys_eunionpay`;show create table `sbed`.`sys_excel_export`;show create table `sbed`.`sys_express`;show create table `sbed`.`sys_finance_log`;show create table `sbed`.`sys_finance_order`;show create table `sbed`.`sys_finance_order_goods`;show create table `sbed`.`sys_finance_refund`;show create table `sbed`.`sys_goods`;show create table `sbed`.`sys_goods_brand`;show create table `sbed`.`sys_goods_category`;show create table `sbed`.`sys_goods_price`;show create table `sbed`.`sys_goods_repertory`;show create table `sbed`.`sys_goods_return`;show create table `sbed`.`sys_goods_shop_price`;show create table `sbed`.`sys_goods_stock`;show create table `sbed`.`sys_goods_supplier`;show create table `sbed`.`sys_issue`;show create table `sbed`.`sys_issue_post`;show create table `sbed`.`sys_jingdong_refund`;show create table `sbed`.`sys_log`;show create table `sbed`.`sys_menu`;show create table `sbed`.`sys_order_detail`;show create table `sbed`.`sys_purchase`;show create table `sbed`.`sys_purchase_checkout`;show create table `sbed`.`sys_purchase_checkout_detail`;show create table `sbed`.`sys_purchase_detail`;show create table `sbed`.`sys_purchaseplan`;show create table `sbed`.`sys_purchaseplan_detail`;show create table `sbed`.`sys_quansutong_clearance`;show create table `sbed`.`sys_quansutong_express`;show create table `sbed`.`sys_quansutong_token`;show create table `sbed`.`sys_redbook_repertory`;show create table `sbed`.`sys_role`;show create table `sbed`.`sys_role_menu`;show create table `sbed`.`sys_sai_goods`;show create table `sbed`.`sys_sai_shipment_data`;show create table `sbed`.`sys_sai_stock`;show create table `sbed`.`sys_sale_num_goods`;show create table `sbed`.`sys_sales`;show create table `sbed`.`sys_sales_stock`;show create table `sbed`.`sys_schedule_job`;show create table `sbed`.`sys_schedule_job_log`;show create table `sbed`.`sys_shop`;show create table `sbed`.`sys_shop_delivery`;show create table `sbed`.`sys_shop_goods`;show create table `sbed`.`sys_shop_order`;show create table `sbed`.`sys_shop_order_goods`;show create table `sbed`.`sys_status`;show create table `sbed`.`sys_stock_all`;show create table `sbed`.`sys_stock_all_log`;show create table `sbed`.`sys_stock_all_num`;show create table `sbed`.`sys_stock_all_num_view`;show create table `sbed`.`sys_stock_allocation`;show create table `sbed`.`sys_stock_allocation_main`;show create table `sbed`.`sys_stock_goods`;show create table `sbed`.`sys_stock_store`;show create table `sbed`.`sys_stock_store_position`;show create table `sbed`.`sys_store`;show create table `sbed`.`sys_store_model`;show create table `sbed`.`sys_supplier`;show create table `sbed`.`sys_taobao_goods`;show create table `sbed`.`sys_taobao_goods_detail`;show create table `sbed`.`sys_taobao_goods_detail_item_imgs`;show create table `sbed`.`sys_taobao_goods_detail_location`;show create table `sbed`.`sys_taobao_goods_detail_prop_imgs`;show create table `sbed`.`sys_taobao_goods_detail_sku`;show create table `sbed`.`sys_taobao_goods_price`;show create table `sbed`.`sys_taobao_logistics_company`;show create table `sbed`.`sys_taobao_order`;show create table `sbed`.`sys_taobao_trade`;show create table `sbed`.`sys_taxonomy`;show create table `sbed`.`sys_third_nulaxgroup`;show create table `sbed`.`sys_tianmao_activity`;show create table `sbed`.`sys_tianmao_activity_goods`;show create table `sbed`.`sys_trade`;show create table `sbed`.`sys_trade_goods`;show create table `sbed`.`sys_trade_logistics`;show create table `sbed`.`sys_user`;show create table `sbed`.`sys_user_brand`;show create table `sbed`.`sys_user_formthead`;show create table `sbed`.`sys_user_role`;show create table `sbed`.`sys_user_store`;show create table `sbed`.`sys_user_token`;show create table `sbed`.`sys_value`;show create table `sbed`.`sys_wave_house_cost`;show create table `sbed`.`sys_yeebao_pay`;show create table `sbed`.`xhs_order`;show create table `sbed`.`xhs_order_detail`;show create table `sbed`.`xhs_stock`; Caused by: java.io.IOException: ErrorPacket [errorNumber=1046  fieldCount=-1  message=No database selected  sqlState=3D000  sqlStateMarker=#]  with command: show create table `sbed`.`全速通库存`;show create table `sbed`.`商品备案信息表`;show create table `sbed`.`康多多库存`;show create table `sbed`.`没有GST的商品`;show create table `sbed`.`澳猫商品内部价`;show create table `sbed`.`澳猫商品申报价`;show create table `sbed`.`澳猫商品销售价`;show create table `sbed`.`网店管家商品总库存`;show create table `sbed`.`网店管家商品数据`;show create table `sbed`.`货品价格表`;show create table `sbed`.`QRTZ_BLOB_TRIGGERS`;show create table `sbed`.`QRTZ_CALENDARS`;show create table `sbed`.`QRTZ_CRON_TRIGGERS`;show create table `sbed`.`QRTZ_FIRED_TRIGGERS`;show create table `sbed`.`QRTZ_JOB_DETAILS`;show create table `sbed`.`QRTZ_LOCKS`;show create table `sbed`.`QRTZ_PAUSED_TRIGGER_GRPS`;show create table `sbed`.`QRTZ_SCHEDULER_STATE`;show create table `sbed`.`QRTZ_SIMPLE_TRIGGERS`;show create table `sbed`.`QRTZ_SIMPROP_TRIGGERS`;show create table `sbed`.`QRTZ_TRIGGERS`;show create table `sbed`.`sys_add_goods_advice`;show create table `sbed`.`sys_aomao_goods`;show create table `sbed`.`sys_aomao_goods_price`;show create table `sbed`.`sys_aomao_logistics`;show create table `sbed`.`sys_aomao_trade`;show create table `sbed`.`sys_aomao_trade_goods`;show create table `sbed`.`sys_aomao_user`;show create table `sbed`.`sys_aomao_user_group`;show create table `sbed`.`sys_aomao_user_inventory`;show create table `sbed`.`sys_attachment`;show create table `sbed`.`sys_au_data`;show create table `sbed`.`sys_auspost_base`;show create table `sbed`.`sys_auspost_log`;show create table `sbed`.`sys_auspost_order`;show create table `sbed`.`sys_common_map`;show create table `sbed`.`sys_config`;show create table `sbed`.`sys_content`;show create table `sbed`.`sys_content_taxonomy`;show create table `sbed`.`sys_currency`;show create table `sbed`.`sys_customer`;show create table `sbed`.`sys_customer_address`;show create table `sbed`.`sys_customer_category`;show create table `sbed`.`sys_customer_grade`;show create table `sbed`.`sys_dept`;show create table `sbed`.`sys_documents`;show create table `sbed`.`sys_eunionpay`;show create table `sbed`.`sys_excel_export`;show create table `sbed`.`sys_express`;show create table `sbed`.`sys_finance_log`;show create table `sbed`.`sys_finance_order`;show create table `sbed`.`sys_finance_order_goods`;show create table `sbed`.`sys_finance_refund`;show create table `sbed`.`sys_goods`;show create table `sbed`.`sys_goods_brand`;show create table `sbed`.`sys_goods_category`;show create table `sbed`.`sys_goods_price`;show create table `sbed`.`sys_goods_repertory`;show create table `sbed`.`sys_goods_return`;show create table `sbed`.`sys_goods_shop_price`;show create table `sbed`.`sys_goods_stock`;show create table `sbed`.`sys_goods_supplier`;show create table `sbed`.`sys_issue`;show create table `sbed`.`sys_issue_post`;show create table `sbed`.`sys_jingdong_refund`;show create table `sbed`.`sys_log`;show create table `sbed`.`sys_menu`;show create table `sbed`.`sys_order_detail`;show create table `sbed`.`sys_purchase`;show create table `sbed`.`sys_purchase_checkout`;show create table `sbed`.`sys_purchase_checkout_detail`;show create table `sbed`.`sys_purchase_detail`;show create table `sbed`.`sys_purchaseplan`;show create table `sbed`.`sys_purchaseplan_detail`;show create table `sbed`.`sys_quansutong_clearance`;show create table `sbed`.`sys_quansutong_express`;show create table `sbed`.`sys_quansutong_token`;show create table `sbed`.`sys_redbook_repertory`;show create table `sbed`.`sys_role`;show create table `sbed`.`sys_role_menu`;show create table `sbed`.`sys_sai_goods`;show create table `sbed`.`sys_sai_shipment_data`;show create table `sbed`.`sys_sai_stock`;show create table `sbed`.`sys_sale_num_goods`;show create table `sbed`.`sys_sales`;show create table `sbed`.`sys_sales_stock`;show create table `sbed`.`sys_schedule_job`;show create table `sbed`.`sys_schedule_job_log`;show create table `sbed`.`sys_shop`;show create table `sbed`.`sys_shop_delivery`;show create table `sbed`.`sys_shop_goods`;show create table `sbed`.`sys_shop_order`;show create table `sbed`.`sys_shop_order_goods`;show create table `sbed`.`sys_status`;show create table `sbed`.`sys_stock_all`;show create table `sbed`.`sys_stock_all_log`;show create table `sbed`.`sys_stock_all_num`;show create table `sbed`.`sys_stock_all_num_view`;show create table `sbed`.`sys_stock_allocation`;show create table `sbed`.`sys_stock_allocation_main`;show create table `sbed`.`sys_stock_goods`;show create table `sbed`.`sys_stock_store`;show create table `sbed`.`sys_stock_store_position`;show create table `sbed`.`sys_store`;show create table `sbed`.`sys_store_model`;show create table `sbed`.`sys_supplier`;show create table `sbed`.`sys_taobao_goods`;show create table `sbed`.`sys_taobao_goods_detail`;show create table `sbed`.`sys_taobao_goods_detail_item_imgs`;show create table `sbed`.`sys_taobao_goods_detail_location`;show create table `sbed`.`sys_taobao_goods_detail_prop_imgs`;show create table `sbed`.`sys_taobao_goods_detail_sku`;show create table `sbed`.`sys_taobao_goods_price`;show create table `sbed`.`sys_taobao_logistics_company`;show create table `sbed`.`sys_taobao_order`;show create table `sbed`.`sys_taobao_trade`;show create table `sbed`.`sys_taxonomy`;show create table `sbed`.`sys_third_nulaxgroup`;show create table `sbed`.`sys_tianmao_activity`;show create table `sbed`.`sys_tianmao_activity_goods`;show create table `sbed`.`sys_trade`;show create table `sbed`.`sys_trade_goods`;show create table `sbed`.`sys_trade_logistics`;show create table `sbed`.`sys_user`;show create table `sbed`.`sys_user_brand`;show create table `sbed`.`sys_user_formthead`;show create table `sbed`.`sys_user_role`;show create table `sbed`.`sys_user_store`;show create table `sbed`.`sys_user_token`;show create table `sbed`.`sys_value`;show create table `sbed`.`sys_wave_house_cost`;show create table `sbed`.`sys_yeebao_pay`;show create table `sbed`.`xhs_order`;show create table `sbed`.`xhs_order_detail`;show create table `sbed`.`xhs_stock`; 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.queryMulti(MysqlQueryExecutor.java:109) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.queryMulti(MysqlConnection.java:101) 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:173) 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) 	at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:84) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) 	at java.lang.Thread.run(Thread.java:748) ] 执行那段show指令看看啥错误 同样的问题，canal1.1.1  怎么解决的。求助   @agapple  执行一下报错的那条show指令 @shuaicloud   多谢@agapple  ，执行了多个show create table XXXXX;  没报错  。 mysql5.7.18，上面有多个数据库，权限全开， 报：  ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address xxxxx/xxxxx:3306  has an error  retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: ErrorPacket [errorNumber=1046  fieldCount=-1  message=No database selected  sqlState=3D000  sqlStateMarker=#] mysql5.6.40 同一套canal服务没问题，权限全开； 5.7上执行多语句show create table，是没报错么？ > 5.7上执行多语句show create table，是没报错么？ 是的 
765,canal监控对接一下prometheus+grafana 很多人反馈过如何有效的监控canal server，初步思考是选择目前比较流行的prometheus+grafana 监控的指标： 1.  server级别的状态 (instance list / client list) + (cpu / jvm gc / net / io ...) 2.  instance级别的状态 (delay /  tps /  ringbuffer / heartbeat ... ) 如果大家已经做过类似的方案， 非常欢迎大家提交PR给我 cpu/jvm gc 这些可以直接使用是simpleclient_hotspot提供的exports @agapple 最近正好学习相关内容，我可以提pr吗，差不多周末的时候。 非常欢迎两位提交PR 根据 @agapple 的comments，结合自己使用的一些感受，说下实现canal server自身监控的一些想法， 先列出一些指标（以开启parallel模式解析为例），PS: 通过带instance tag可以将指标以instance级别收集，然后汇总： 1. 从read到store put a. 表示inbound流量的canal_inbound_bytes b. 表示dump线程pushlish events -> disruptor buffer阻塞时间片的 canal_dump_publish_blocking_time c. 表示sink线程put events -> store阻塞时间片的 canal_sink_put_blocking_time d. 通过CanalEventDownStreamHandler，可以获得代表延迟的 canal_instance_delay；各种Event type的分类统计；通过rowchange附带的rowData数量判断近期是否有批量SQL等等。 2. netty server端，使用SessionHandler调用write时，增加ChannelFutureListener回调，将client的请求和完成信息进行采样，包括destination  PacketType  write amount  batchId(仅对GET，判断是否空包)  error code  response time 等 a. 表示outbound流量的canal_inbound_bytes b. 各packetType包的分类统计 c. 空包率 d. 发生的error按error code统计 e. 响应时间 3. event store，比较天然的埋点： a. TPS，根据ackSequence计算 b. 如果是memory模式，MPS，根据ackMemSize计算 c. store remain events 数量 d. store remain memory 一些场景： - 1-b 无明显 blocking，1-d delay不断增加，1-a 流量较小 --> 可大致推断瓶颈在网络，是否带宽不够或流量被挤占。 - 1-b 有显著 blocking，1-c 无明显blocking（client消费很快）--> 可尝试增加canal.instance.parser.parallelThreadSize加快解析速度 - 1-b 和 1-c均有显著blocking  delay不断增加 --> 瓶颈在于client读写和消费 - 2-c 空包频率高 --> 可考虑调整优化get的timeout时间 - 2-b && 2-d --> client的行为是否正常 欢迎comments。 BTW  新拉了一个分支metrics_support。 @lcybo 考虑的非常全面了，点赞一个 :) <img width="910" alt="grafana" src="https://user-images.githubusercontent.com/15042781/44220795-30379600-a1b2-11e8-9f56-24dd8395f0bf.PNG"> 贴一张预览图。家里的环境，traffic随便灌的，不太容易模拟复杂场景。 太赞了  perfect! <img width="251" alt="down" src="https://user-images.githubusercontent.com/15042781/44293887-3f0d6e00-a2c2-11e8-92e7-da3ccae6c991.PNG"> 生成模板，增加datasource便利配置，instances切换 提交了模板/canal/conf/metrics/Canal_instances_tmpl.json 对dashboard重新进行了排版 使用说明文档也已告一段落。 如果有问题，我会尽快修复。 ![image](https://user-images.githubusercontent.com/834743/44321609-8ab74780-a47b-11e8-80c2-6d5baab962fa.png) 最后的效果图，具体可以参见：https://github.com/alibaba/canal/wiki/Prometheus-QuickStart
764,canal 1.0.26 偶尔断开连接然后从新连接后 部分同步停止 使用场景: 1 单台虚拟机连接20个数据库的同步任务 某一时刻突然出现大批量如下日志 但是从新连接后 部分数据库同步正常 部分数据库同步停止 zk的cursor不变化 停留在断开连接的地方.日志如下 MysqlConnector - |disConnect Mysql Connection to 10.x.x.x/10.x.x.x:3306..... MysqlConnector - |connect Mysql Connection to 10.x.x.x/10.x.x.x:3306..... MysqlConnector - |handshake initialization packet received  prepare the client authentication packet to send MysqlConnector - |client authentication packet is sent out. 目前出现多次这种情况 原因未知. 信息太少，无法定位，可以看看当时的jstack +1 不断有kill dump以及disConnect、connect 、handshake，内网测试是这样的 @kervin521 你也遇到这个问题了?还是让我提供更多的信息?现在在按照大神的意思 分析jstack. 有复现方法之后再reopen
763,canal.instance.filter.regex多个时，只能抓取到第一项，后面几项数据变更获取不到 例如： ``` canal.instance.filter.regex=vip\\.crm.* .oms.* .ser.* .sku.* .vip.* .wfe.* ``` 只能获取到“**crm**”前缀表的数据变更。**oms**及后面的获取不到。 看一下 https://github.com/alibaba/canal/wiki/FAQ
762,修改Readme.md 新增DTS说明 [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=762) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=762) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=762) it.</sub>
761,添加Canal商业化版本更新说明 添加Canal商业化版本更新说明 [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=761) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=761) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=761) it.</sub> 重复了 这个后续会增加 ，hdfs/hbase/ 等其他接口插件不？
760,canal+OTTER client端batchid=-1 canal版本：1.0.22 配置启动后，client端使用canalConnector.getWithoutAck(batchSize)getId();得到的值一直为-1。且在zk上1001node下cursor节点一直没有生成，canal端log正常。 client端log： ![image](https://user-images.githubusercontent.com/41414514/42923813-0c834b50-8b5a-11e8-83f4-bf00bdb56d34.png) 在canal端设置binlog名字以及position依旧创建不了cursor。且文件存在。 有人遇到过这样的问题吗？求教 看看canal server上的日志是否有异常
759,append gtid lastCommitted and sequenceNumber support [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=759) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=759) before we can accept your contribution.<br/><hr/>**winger** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=759) it.</sub> tks
758,client切换时延问题 部署了两个client端通过zk监听同一个destination，看了一下文档，貌似没有描述两个client切换时获取数据的时间间隔，两个client端都是通过while（true）方式不停的 subscribe、connect、getMessage。按理来说，时间间隔就应该是zk处理结点状态变化的时间，我设置的是 canal.zookeeper.flush.period=1000，从结果上来看每次间隔大约30S左右，和配置相差较大，请问，有办法缩短一下这个间隔吗，业务对实时性要求很高。 这是zookeeper的sessionTimeout，你网上找找参数调低一下
757,用了 1.0.26 p3版本，为什么 EventType.QUERY 打印的sql是 insert 或者是 create table？ > DEBUG com.pamirs.data.AbstractCanalClient - Event Type Is Query  sql --> UPDATE T_PDC_HANDOVER_INDEX_OR 代码如下 ```java  if (eventType == EventType.QUERY                                     || rowChage.getIsDdl()) {                                 if (rowChage.getIsDdl()) {                                     StringBuilder exceptionMessage = new StringBuilder();                                     exceptionMessage.append("mysql has an ddl  sql is [ ").append(rowChage.getSql()).append(" ]");                                     ExceptionHandle.handleException(new ExceptionMessageEntity(entityTask.getTableTask().getId()  ExceptionMessageEntityDesc.PAMIRS_DATA_MYSQL_PULL_DDL_MESSAGE  ExceptionMessageTypeEnum.pamirs_data  exceptionMessage.toString()));                                 } else {                                     logger.debug("Event Type Is Query  sql --> {}"  rowChage.getSql());                                 }                                return;  } ``` RowsQueryLogEvent ， 5.6之后的row模式的binlog会混杂statement和row，针对insert/update/delete也会记录对应的原始SQL，拿到的这个sql就是这货 @agapple 那应该怎么区分呢？
756, com.lmax.disruptor.FatalExceptionHandler handleEventException 刚了解canal，直接用canal.kafka-1.0.26-SNAPSHOT.tar包的时候会报一个com.alibaba.druid.pool.DruidDataSource - testWhileIdle is true  validationQuery not set，但还是可以用，但在用kafka包跑CanalLauncher.java的时候出现错误 ![image](https://user-images.githubusercontent.com/26325971/42809370-19b008e0-89e8-11e8-8ba0-bc520efcb083.png)，请问是我哪里做错了吗。 真的找不到错误在哪里，楼主help me。 ![image](https://user-images.githubusercontent.com/26325971/42810158-d57bd4ae-89e9-11e8-95a9-97a38b5d6402.png)，配置文件信息 ![image](https://user-images.githubusercontent.com/26325971/42810224-04150a7e-89ea-11e8-954b-1855e2d8fe0b.png) ![image](https://user-images.githubusercontent.com/26325971/42813766-a1392646-89f4-11e8-82ea-3279b7b1e13a.png) 请问这个怎样子解决 ![image](https://user-images.githubusercontent.com/834743/42819117-28463a7e-8a05-11e8-9058-64224e83e062.png) 出现重复的slaveId  优化了一下日志输出，去掉了无用的 我也看到了这个重复slaveId的错误，但是我只有一个在Instance.prorerties，只定义了一个 ![image](https://user-images.githubusercontent.com/26325971/42854088-b68f65c8-8a6c-11e8-94c2-abc81424ca81.png)，还有哪里要改的吗。 
755,配置的filter.regex不生效，server没有过滤掉sys库数据，有时候也没过滤掉mysql库数据 版本：1.0.26 binlog模式：ROW instance.properties配置： canal.instance.filter.regex=blacklist3\\..* canal.instance.filter.black.regex= server日志： 2018-07-17 15:45:09.388 [destination = xxxx   address = /xxxx:xxxx   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:xxxx[com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: ErrorPacket [errorNumber=1142  fieldCount=-1  message=SHOW VIEW command denied to user 'xxxx'@'xxxx for table 'host_summary'  sqlState=42000  sqlStateMarker=#]  with command: show create table `sys`.`host_summary`;show create table `sys`.`host_summary_by_file_io`;show create table `sys`.`host_summary_by_file_io_type`;show create table `sys`.`host_summary_by_stages`;show create table `sys`.`host_summary_by_statement_latency`;show create table `sys`.`host_summary_by_statement_type`;show create table `sys`.`innodb_buffer_stats_by_schema`;show create table `sys`.`innodb_buffer_stats_by_table`;show create table `sys`.`innodb_lock_waits`;show create table `sys`.`io_by_thread_by_latency`;show create table `sys`.`io_global_by_file_by_bytes`;show create table `sys`.`io_global_by_file_by_latency`;show create table `sys`.`io_global_by_wait_by_bytes`;show create table `sys`.`io_global_by_wait_by_latency`;show create table `sys`.`latest_file_io`;show create table `sys`.`memory_by_host_by_current_bytes`;show create table `sys`.`memory_by_thread_by_current_bytes`;show create table `sys`.`memory_by_user_by_current_bytes`;show create table `sys`.`memory_global_by_current_bytes`;show create table `sys`.`memory_global_total`;show create table `sys`.`metrics`;show create table `sys`.`processlist`;show create table `sys`.`ps_check_lost_instrumentation`;show create table `sys`.`schema_auto_increment_columns`;show create table `sys`.`schema_index_statistics`;show create table `sys`.`schema_object_overview`;show create table `sys`.`schema_redundant_indexes`;show create table `sys`.`schema_table_lock_waits`;show create table `sys`.`schema_table_statistics`;show create table `sys`.`schema_table_statistics_with_buffer`;show create table `sys`.`schema_tables_with_full_table_scans`;show create table `sys`.`schema_unused_indexes`;show create table `sys`.`session`;show create table `sys`.`session_ssl_status`;show create table `sys`.`statement_analysis`;show create table `sys`.`statements_with_errors_or_warnings`;show create table `sys`.`statements_with_full_table_scans`;show create table `sys`.`statements_with_runtimes_in_95th_percentile`;show create table `sys`.`statements_with_sorting`;show create table `sys`.`statements_with_temp_tables`;show create table `sys`.`sys_config`;show create table `sys`.`user_summary`;show create table `sys`.`user_summary_by_file_io`;show create table `sys`.`user_summary_by_file_io_type`;show create table `sys`.`user_summary_by_stages`;show create table `sys`.`user_summary_by_statement_latency`;show create table `sys`.`user_summary_by_statement_type`;show create table `sys`.`version`;show create table `sys`.`wait_classes_global_by_avg_latency`;show create table `sys`.`wait_classes_global_by_latency`;show create table `sys`.`waits_by_host_by_latency`;show create table `sys`.`waits_by_user_by_latency`;show create table `sys`.`waits_global_by_latency`;show create table `sys`.`x$host_summary`;show create table `sys`.`x$host_summary_by_file_io`;show create table `sys`.`x$host_summary_by_file_io_type`;show create table `sys`.`x$host_summary_by_stages`;show create table `sys`.`x$host_summary_by_statement_latency`;show create table `sys`.`x$host_summary_by_statement_type`;show create table `sys`.`x$innodb_buffer_stats_by_schema`;show create table `sys`.`x$innodb_buffer_stats_by_table`;show create table `sys`.`x$innodb_lock_waits`;show create table `sys`.`x$io_by_thread_by_latency`;show create table `sys`.`x$io_global_by_file_by_bytes`;show create table `sys`.`x$io_global_by_file_by_latency`;show create table `sys`.`x$io_global_by_wait_by_bytes`;show create table `sys`.`x$io_global_by_wait_by_latency`;show create table `sys`.`x$latest_file_io`;show create table `sys`.`x$memory_by_host_by_current_bytes`;show create table `sys`.`x$memory_by_thread_by_current_bytes`;show create table `sys`.`x$memory_by_user_by_current_bytes`;show create table `sys`.`x$memory_global_by_current_bytes`;show create table `sys`.`x$memory_global_total`;show create table `sys`.`x$processlist`;show create table `sys`.`x$ps_digest_95th_percentile_by_avg_us`;show create table `sys`.`x$ps_digest_avg_latency_distribution`;show create table `sys`.`x$ps_schema_table_statistics_io`;show create table `sys`.`x$schema_flattened_keys`;show create table `sys`.`x$schema_index_statistics`;show create table `sys`.`x$schema_table_lock_waits`;show create table `sys`.`x$schema_table_statistics`;show create table `sys`.`x$schema_table_statistics_with_buffer`;show create table `sys`.`x$schema_tables_with_full_table_scans`;show create table `sys`.`x$session`;show create table `sys`.`x$statement_analysis`;show create table `sys`.`x$statements_with_errors_or_warnings`;show create table `sys`.`x$statements_with_full_table_scans`;show create table `sys`.`x$statements_with_runtimes_in_95th_percentile`;show create table `sys`.`x$statements_with_sorting`;show create table `sys`.`x$statements_with_temp_tables`;show create table `sys`.`x$user_summary`;show create table `sys`.`x$user_summary_by_file_io`;show create table `sys`.`x$user_summary_by_file_io_type`;show create table `sys`.`x$user_summary_by_stages`;show create table `sys`.`x$user_summary_by_statement_latency`;show create table `sys`.`x$user_summary_by_statement_type`;show create table `sys`.`x$wait_classes_global_by_avg_latency`;show create table `sys`.`x$wait_classes_global_by_latency`;show create table `sys`.`x$waits_by_host_by_latency`;show create table `sys`.`x$waits_by_user_by_latency`;show create table `sys`.`x$waits_global_by_latency`; Caused by: java.io.IOException: ErrorPacket [errorNumber=1142  fieldCount=-1  message=SHOW VIEW command denied to user 'xxxx'@'xxxx' for table 'host_summary'  sqlState=42000  sqlStateMarker=#]  with command: show create table `sys`.`host_summary`;show create table `sys`.`host_summary_by_file_io`;show create table `sys`.`host_summary_by_file_io_type`;show create table `sys`.`host_summary_by_stages`;show create table `sys`.`host_summary_by_statement_latency`;show create table `sys`.`host_summary_by_statement_type`;show create table `sys`.`innodb_buffer_stats_by_schema`;show create table `sys`.`innodb_buffer_stats_by_table`;show create table `sys`.`innodb_lock_waits`;show create table `sys`.`io_by_thread_by_latency`;show create table `sys`.`io_global_by_file_by_bytes`;show create table `sys`.`io_global_by_file_by_latency`;show create table `sys`.`io_global_by_wait_by_bytes`;show create table `sys`.`io_global_by_wait_by_latency`;show create table `sys`.`latest_file_io`;show create table `sys`.`memory_by_host_by_current_bytes`;show create table `sys`.`memory_by_thread_by_current_bytes`;show create table `sys`.`memory_by_user_by_current_bytes`;show create table `sys`.`memory_global_by_current_bytes`;show create table `sys`.`memory_global_total`;show create table `sys`.`metrics`;show create table `sys`.`processlist`;show create table `sys`.`ps_check_lost_instrumentation`;show create table `sys`.`schema_auto_increment_columns`;show create table `sys`.`schema_index_statistics`;show create table `sys`.`schema_object_overview`;show create table `sys`.`schema_redundant_indexes`;show create table `sys`.`schema_table_lock_waits`;show create table `sys`.`schema_table_statistics`;show create table `sys`.`schema_table_statistics_with_buffer`;show create table `sys`.`schema_tables_with_full_table_scans`;show create table `sys`.`schema_unused_indexes`;show create table `sys`.`session`;show create table `sys`.`session_ssl_status`;show create table `sys`.`statement_analysis`;show create table `sys`.`statements_with_errors_or_warnings`;show create table `sys`.`statements_with_full_table_scans`;show create table `sys`.`statements_with_runtimes_in_95th_percentile`;show create table `sys`.`statements_with_sorting`;show create table `sys`.`statements_with_temp_tables`;show create table `sys`.`sys_config`;show create table `sys`.`user_summary`;show create table `sys`.`user_summary_by_file_io`;show create table `sys`.`user_summary_by_file_io_type`;show create table `sys`.`user_summary_by_stages`;show create table `sys`.`user_summary_by_statement_latency`;show create table `sys`.`user_summary_by_statement_type`;show create table `sys`.`version`;show create table `sys`.`wait_classes_global_by_avg_latency`;show create table `sys`.`wait_classes_global_by_latency`;show create table `sys`.`waits_by_host_by_latency`;show create table `sys`.`waits_by_user_by_latency`;show create table `sys`.`waits_global_by_latency`;show create table `sys`.`x$host_summary`;show create table `sys`.`x$host_summary_by_file_io`;show create table `sys`.`x$host_summary_by_file_io_type`;show create table `sys`.`x$host_summary_by_stages`;show create table `sys`.`x$host_summary_by_statement_latency`;show create table `sys`.`x$host_summary_by_statement_type`;show create table `sys`.`x$innodb_buffer_stats_by_schema`;show create table `sys`.`x$innodb_buffer_stats_by_table`;show create table `sys`.`x$innodb_lock_waits`;show create table `sys`.`x$io_by_thread_by_latency`;show create table `sys`.`x$io_global_by_file_by_bytes`;show create table `sys`.`x$io_global_by_file_by_latency`;show create table `sys`.`x$io_global_by_wait_by_bytes`;show create table `sys`.`x$io_global_by_wait_by_latency`;show create table `sys`.`x$latest_file_io`;show create table `sys`.`x$memory_by_host_by_current_bytes`;show create table `sys`.`x$memory_by_thread_by_current_bytes`;show create table `sys`.`x$memory_by_user_by_current_bytes`;show create table `sys`.`x$memory_global_by_current_bytes`;show create table `sys`.`x$memory_global_total`;show create table `sys`.`x$processlist`;show create table `sys`.`x$ps_digest_95th_percentile_by_avg_us`;show create table `sys`.`x$ps_digest_avg_latency_distribution`;show create table `sys`.`x$ps_schema_table_statistics_io`;show create table `sys`.`x$schema_flattened_keys`;show create table `sys`.`x$schema_index_statistics`;show create table `sys`.`x$schema_table_lock_waits`;show create table `sys`.`x$schema_table_statistics`;show create table `sys`.`x$schema_table_statistics_with_buffer`;show create table `sys`.`x$schema_tables_with_full_table_scans`;show create table `sys`.`x$session`;show create table `sys`.`x$statement_analysis`;show create table `sys`.`x$statements_with_errors_or_warnings`;show create table `sys`.`x$statements_with_full_table_scans`;show create table `sys`.`x$statements_with_runtimes_in_95th_percentile`;show create table `sys`.`x$statements_with_sorting`;show create table `sys`.`x$statements_with_temp_tables`;show create table `sys`.`x$user_summary`;show create table `sys`.`x$user_summary_by_file_io`;show create table `sys`.`x$user_summary_by_file_io_type`;show create table `sys`.`x$user_summary_by_stages`;show create table `sys`.`x$user_summary_by_statement_latency`;show create table `sys`.`x$user_summary_by_statement_type`;show create table `sys`.`x$wait_classes_global_by_avg_latency`;show create table `sys`.`x$wait_classes_global_by_latency`;show create table `sys`.`x$waits_by_host_by_latency`;show create table `sys`.`x$waits_by_user_by_latency`;show create table `sys`.`x$waits_global_by_latency`; 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.queryMulti(MysqlQueryExecutor.java:109) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.queryMulti(MysqlConnection.java:101) 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:173) 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) 	at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:84) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) 	at java.lang.Thread.run(Thread.java:745) debug看看DatabaseTableMeta.dumpTableMeta()是否filter条件没正确接收到，被client的subscribe条件给覆盖了 应该是这个问题！ 在server端双实例的切换。 client端配置如下都会影响到server端： ``` String filter = ".*"; String filter = "\\.*"; String filter = "\\.\\*"; String filter = ".*\\\\..*"; ``` 看了一下源码  如果设置成null，就不影响。 String filter = null; 具体的影响表现为，后启动的那个server会订阅server端filter之外的表，导致没权限报错，客户端getMessage就一直是空。 两个server实例配置： canal.properties 除了canal.id canal.port都一样 instance.properties 除了 canal.instance.mysql.slaveId都一样。 `canal.instance.filter.regex=blacklist3\\..*` 
754,fix issue #751 [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=754) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=754) before we can accept your contribution.<br/><hr/>**winger** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=754) it.</sub> [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=754) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=754) before we can accept your contribution.<br/><hr/>**winger** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=754) it.</sub> tks
753,com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed 运行一段时间 总是报这个错误， 版本1.0.26 com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first 	at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4832) ~[guava-18.0.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:160) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:759) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:428) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:114) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:66) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:337) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:184) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:145) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] Caused by: java.io.IOException: should execute connector.connect() first 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.<init>(MysqlQueryExecutor.java:30) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:87) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMetaByDB(TableMetaCache.java:80) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:30) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:55) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:50) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) ~[guava-18.0.jar:na] 	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) ~[guava-18.0.jar:na] 	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) ~[guava-18.0.jar:na] 	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) ~[guava-18.0.jar:na] 	at com.google.common.cache.LocalCache.get(LocalCache.java:3937) ~[guava-18.0.jar:na] 	at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941) ~[guava-18.0.jar:na] 	at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824) ~[guava-18.0.jar:na] 	at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830) ~[guava-18.0.jar:na] 	... 10 common frames omitted 2018-07-17 10:48:26.646 [destination = example   address = /172.16.203.11:3309   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address 172.16.203.11/172.16.203.11:3309 has an error  retrying. caused by  com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first 	at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4832) ~[guava-18.0.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:160) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:759) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:428) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:114) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:66) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:337) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:184) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:145) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] Caused by: java.io.IOException: should execute connector.connect() first 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.<init>(MysqlQueryExecutor.java:30) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:87) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMetaByDB(TableMetaCache.java:80) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:30) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:55) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:50) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) ~[guava-18.0.jar:na] 	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) ~[guava-18.0.jar:na] 	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) ~[guava-18.0.jar:na] 	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) ~[guava-18.0.jar:na] 	at com.google.common.cache.LocalCache.get(LocalCache.java:3937) ~[guava-18.0.jar:na] 	at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941) ~[guava-18.0.jar:na] 	at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824) ~[guava-18.0.jar:na] 	at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830) ~[guava-18.0.jar:na] 	... 10 common frames omitted 2018-07-17 10:48:26.646 [destination = example   address = /172.16.203.11:3309   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first 	at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4832) 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:160) 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:759) 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:428) 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:114) 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:66) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:337) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:184) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:145) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) 	at java.lang.Thread.run(Thread.java:748) Caused by: java.io.IOException: should execute connector.connect() first 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.<init>(MysqlQueryExecutor.java:30) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:87) 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMetaByDB(TableMetaCache.java:80) 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:30) 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:55) 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:50) 	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) 	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) 	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) 	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) 	at com.google.common.cache.LocalCache.get(LocalCache.java:3937) 	at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941) 	at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824) 	at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830) 	... 10 more ] 你下载一个最新的26版本，代码行对不上
752,GTID无法获取到 ![6b06917b225fc61a76d0b901b656ecc9](https://user-images.githubusercontent.com/5210147/42793757-40d64d60-89ae-11e8-8090-f69238c5ed5a.jpg) 我的主从都是开启的gtid，同步正常，只有在transactionend的时候会有事务id，其它时候无法获取到gtid和事务id gtid的获取方式你看过example代码了吗？在扩展属性字段里 哥，example里 entry.getHeader().getGtid() 打印出来是个空字符串 需要开启canal gtid的订阅 instance.property canal.instance.gtidon=true 把这个打开就ok！ thanks！！！
751,canal 增加标记一个事务内的 row data 计数 Hi  @agapple 针对大事务场景，由于canal在传递过程中，超过一定数量后(默认1024)会分批发送，对于一些业务场景下是需要知道一个大事务内的row data数量然后进行合并处理  
750,canal.deployer-1.0.26-SNAPSHOT版本运行后报位点错误 The details of the false information are as follows: something goes wrong with channel:[id: 0x1d0b2315  /xx.xx.xx.xx:xxxx => /xx.xx.xx.xx:xxxx]  exception=com.alibaba.otter.canal.server.exception.CanalServerException: ack error   clientId:1001 batchId:261365 is not exist   please check [New I/O server worker #1-12] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x1d0b2315  /xx.xx.xx.xx:xxxx :> /xx.xx.xx.xx:xxxx]  exception=java.nio.channels.ClosedChannelException         at org.jboss.netty.channel.socket.nio.NioWorker.cleanUpWriteBuffer(NioWorker.java:649)         at org.jboss.netty.channel.socket.nio.NioWorker.writeFromUserCode(NioWorker.java:370)         at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:137)         at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76)         at org.jboss.netty.channel.Channels.write(Channels.java:611)         at org.jboss.netty.channel.Channels.write(Channels.java:578)         at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:28)         at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:144)         at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48)         at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276)         at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302)         at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526)         at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507)         at org.jboss.netty.handler.codec.replay.ReplayingDecoder.cleanup(ReplayingDecoder.java:542)         at org.jboss.netty.handler.codec.replay.ReplayingDecoder.channelDisconnected(ReplayingDecoder.java:450)         at org.jboss.netty.channel.Channels.fireChannelDisconnected(Channels.java:360)         at org.jboss.netty.channel.socket.nio.NioWorker.close(NioWorker.java:599)         at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:119)         at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76)         at org.jboss.netty.channel.Channels.close(Channels.java:720)         at org.jboss.netty.channel.AbstractChannel.close(AbstractChannel.java:208)         at org.jboss.netty.channel.ChannelFutureListener$1.operationComplete(ChannelFutureListener.java:46)         at org.jboss.netty.channel.DefaultChannelFuture.notifyListener(DefaultChannelFuture.java:381)         at org.jboss.netty.channel.DefaultChannelFuture.addListener(DefaultChannelFuture.java:148)         at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:30)         at com.alibaba.otter.canal.server.netty.NettyUtils.error(NettyUtils.java:51)         at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:200)         at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48)         at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276)         at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302)         at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526)         at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507)         at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:444)         at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274)         at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261)         at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:350)         at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281)         at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201)         at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46)         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)         at java.lang.Thread.run(Thread.java:745)  clientId:1001 batchId:261365 is not exist   please check  服务端估计发生了instance重启 为什么会发生重启，怎么避免这个重启？
749,fastsql-2.0.0_preview_371在哪个maven下载？ ```xml <dependency>                 <groupId>com.alibaba.fastsql</groupId>                 <artifactId>fastsql</artifactId>                 <version>2.0.0_preview_371</version> </dependency> ``` 修改 pom.xml 替换 fastsql 节点内容：     <dependency>         <groupId>com.alibaba.fastsql</groupId>         <artifactId>fastsql</artifactId>         <version>2.0.0_preview_186</version>     </dependency> 371在我的二进制里有 在master分支中是不是需要把fastsql的依赖从371改成可用的版本呢？要不然导入项目都找不到这个jar的 Missing artifact com.alibaba.fastsql:fastsql:jar:2.0.0_preview_520   现在又缺这个包了，好坑呀，传到maven一下呗
748,canal的dump太慢怎么处理 MySQL的show master status mysql-bin.000093 canal的dump的zk中parse： mysql-bin.000064 每个binlog大小100M dump entry: 30~40 event/second 在MySQL中show full processlist: 显示canal的dump为 # writing to net 你这是新版本的嘛？ @WithLin v1.0.25，v1.0.26有问题有不停的断开重连数据库还有数据库连接超时问题 @agapple @kervin521 具体描述一下v1.0.26的问题  你提的太笼统
747,add restart.sh restart canal server [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=747) <br/>All committers have signed the CLA. [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=747) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=747) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=747) it.</sub> tks
746,canal server 连接数据库超时 server连接数据库超时，然后就不再重新连接了，过了大概两个小时后 ，又自动重新连接了，请问是哪个参数可以设置 超时之后过多长时间重新连接呢 你换成v1.0.25版本吧，v1.0.26有问题 v1.0.25 有这个参数设置么 我的没有问题？看一下日志？有报错吗？ 25版本已经标记不建议使用了，nio的问题 @WithLin 26版本这不停的断开重连也太大啦（下午7点之后出现此情况比较多），换成25版本就没问题，看是否是定时器哪里有问题 @WithLin  报错就是简单的连接超时 connect timeout 感觉可能是网络波动引起的，但是 它隔了2个小时才去重新获取连接，是不是太太久了。超时之后 多久去重新获取连接，没有可以自己设置的参数么 网络的问题重试就好了啊？ while(true)  catch 的时候休息多久，做重试，就好了
745,canal service因删除log下面的temp日志而服务没有重启导致磁盘空间被占用 如题，canal 服务端发现磁盘被占用，但是没找到具体占用的问题，通过lsof |grep delete发现大量如下信息： java      37924 37925       appops    2w      REG              254 1 50965216675     136005 /home/appops/canal/logs/canal/canal.log6671827459413464.tmp (deleted) 查了下，文件被删除，而进程还活着，因而造成占用空间的现象，需要kill掉进程才能恢复，但是服务总是通过kill掉来恢复肯定不方便，有什么好的解决方式吗
744,1.0.26服务端一直重复 something goes wrong with channel 服务端：1.0.26 2018-07-10 15:50:20.197 [destination = example   address = /110.110.10.27:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position  {"identity":{"slaveId":-1 "sourceAddress":{"address":"10.110.10.27" "port":3306}} "postion":{"gtid":"" "included":false "journalName":"mysql-bin.000007" "position":313931 "serverId":27 "timestamp":1531208924000}} 2018-07-10 15:50:20.203 [destination = example   address = /10.110.10.27:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000007 position=313931 serverId=27 gtid= timestamp=1531208924000] 2018-07-10 15:50:21.402 [New I/O server worker #1-1] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400   Caused by :  something goes wrong with channel:[id: 0x65065848  /192.168.37.1:60600 => /192.168.37.129:11111]  exception=com.alibaba.otter.canal.store.CanalStoreException: no match ack positionLogPosition[identity=LogIdentity[sourceAddress=10.110.10.27/10.110.10.27:3306 slaveId=-1] postion=EntryPosition[included=false journalName=mysql-bin.000007 position=313931 serverId=27 gtid= timestamp=1531208924000]] 2018-07-10 15:50:31.406 [New I/O server worker #1-1] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x65065848  /192.168.37.1:60600 :> /192.168.37.129:11111]  exception=java.nio.channels.ClosedChannelException 	at org.jboss.netty.channel.socket.nio.NioWorker.cleanUpWriteBuffer(NioWorker.java:649) 	at org.jboss.netty.channel.socket.nio.NioWorker.writeFromUserCode(NioWorker.java:370) 	at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:137) 	at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) 	at org.jboss.netty.channel.Channels.write(Channels.java:611) 	at org.jboss.netty.channel.Channels.write(Channels.java:578) 	at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:28) 	at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:144) 	at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) 	at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.cleanup(ReplayingDecoder.java:542) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.channelDisconnected(ReplayingDecoder.java:450) 	at org.jboss.netty.channel.Channels.fireChannelDisconnected(Channels.java:360) 	at org.jboss.netty.channel.socket.nio.NioWorker.close(NioWorker.java:599) 	at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:119) 	at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) 	at org.jboss.netty.channel.Channels.close(Channels.java:720) 	at org.jboss.netty.channel.AbstractChannel.close(AbstractChannel.java:208) 	at org.jboss.netty.channel.ChannelFutureListener$1.operationComplete(ChannelFutureListener.java:46) 	at org.jboss.netty.channel.DefaultChannelFuture.notifyListener(DefaultChannelFuture.java:381) 	at org.jboss.netty.channel.DefaultChannelFuture.addListener(DefaultChannelFuture.java:148) 	at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:30) 	at com.alibaba.otter.canal.server.netty.NettyUtils.error(NettyUtils.java:51) 	at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:200) 	at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) 	at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:444) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261) 	at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:350) 	at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) 	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) 	at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) 2018-07-10 15:50:32.315 [destination = example   address = /10.110.10.27:3306   EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 4052  sqlstate = HY000 errmsg = A slave with the same server_uuid/server_id as this slave has connected to the master; the first event 'mysql-bin.000007' at 313931  the last event read from 'mysql-bin.000007' at 313931  the last byte read from 'mysql-bin.000007' at 313962. 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_102] 2018-07-10 15:50:32.317 [destination = example   address = /10.110.10.27:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address 10.110.10.27/10.110.10.27:3306 has an error  retrying. caused by  java.io.IOException: Received error packet: errno = 4052  sqlstate = HY000 errmsg = A slave with the same server_uuid/server_id as this slave has connected to the master; the first event 'mysql-bin.000007' at 313931  the last event read from 'mysql-bin.000007' at 313931  the last byte read from 'mysql-bin.000007' at 313962. 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_102] 2018-07-10 15:50:32.318 [destination = example   address = /10.110.10.27:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: Received error packet: errno = 4052  sqlstate = HY000 errmsg = A slave with the same server_uuid/server_id as this slave has connected to the master; the first event 'mysql-bin.000007' at 313931  the last event read from 'mysql-bin.000007' at 313931  the last byte read from 'mysql-bin.000007' at 313962. 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) 	at java.lang.Thread.run(Thread.java:745) ] 2018-07-10 15:50:41.415 [New I/O server worker #1-2] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400   Caused by :  something goes wrong with channel:[id: 0x650a9f4a  /192.168.37.1:60616 => /192.168.37.129:11111]  exception=com.alibaba.otter.canal.store.CanalStoreException: no match ack positionLogPosition[identity=LogIdentity[sourceAddress=10.110.10.27/10.110.10.27:3306 slaveId=-1] postion=EntryPosition[included=false journalName=mysql-bin.000007 position=313931 serverId=27 gtid= timestamp=1531208924000]] 2018-07-10 15:50:46.048 [destination = example   address = /10.110.10.27:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position  {"identity":{"slaveId":-1 "sourceAddress":{"address":"10.110.10.27" "port":3306}} "postion":{"gtid":"" "included":false "journalName":"mysql-bin.000007" "position":313931 "serverId":27 "timestamp":1531208924000}} 2018-07-10 15:50:46.052 [destination = example   address = /10.110.10.27:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000007 position=313931 serverId=27 gtid= timestamp=1531208924000] 2018-07-10 15:50:51.418 [New I/O server worker #1-2] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x650a9f4a  /192.168.37.1:60616 :> /192.168.37.129:11111]  exception=java.nio.channels.ClosedChannelException 	at org.jboss.netty.channel.socket.nio.NioWorker.cleanUpWriteBuffer(NioWorker.java:649) 	at org.jboss.netty.channel.socket.nio.NioWorker.writeFromUserCode(NioWorker.java:370) 	at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:137) 	at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) 	at org.jboss.netty.channel.Channels.write(Channels.java:611) 	at org.jboss.netty.channel.Channels.write(Channels.java:578) 	at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:28) 	at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:144) 	at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) 	at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.cleanup(ReplayingDecoder.java:542) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.channelDisconnected(ReplayingDecoder.java:450) 	at org.jboss.netty.channel.Channels.fireChannelDisconnected(Channels.java:360) 	at org.jboss.netty.channel.socket.nio.NioWorker.close(NioWorker.java:599) 	at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:119) 	at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) 	at org.jboss.netty.channel.Channels.close(Channels.java:720) 	at org.jboss.netty.channel.AbstractChannel.close(AbstractChannel.java:208) 	at org.jboss.netty.channel.ChannelFutureListener$1.operationComplete(ChannelFutureListener.java:46) 	at org.jboss.netty.channel.DefaultChannelFuture.notifyListener(DefaultChannelFuture.java:381) 	at org.jboss.netty.channel.DefaultChannelFuture.addListener(DefaultChannelFuture.java:148) 	at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:30) 	at com.alibaba.otter.canal.server.netty.NettyUtils.error(NettyUtils.java:51) 	at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:200) 	at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) 	at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:444) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261) 	at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:350) 	at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) 	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) 	at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) 2018-07-10 15:51:00.260 [destination = example   address = /10.110.10.27:3306   EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 4052  sqlstate = HY000 errmsg = A slave with the same server_uuid/server_id as this slave has connected to the master; the first event 'mysql-bin.000007' at 313931  the last event read from 'mysql-bin.000007' at 313931  the last byte read from 'mysql-bin.000007' at 313962. 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_102] 2018-07-10 15:51:00.261 [destination = example   address = /10.110.10.27:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address 10.110.10.27/10.110.10.27:3306 has an error  retrying. caused by  java.io.IOException: Received error packet: errno = 4052  sqlstate = HY000 errmsg = A slave with the same server_uuid/server_id as this slave has connected to the master; the first event 'mysql-bin.000007' at 313931  the last event read from 'mysql-bin.000007' at 313931  the last byte read from 'mysql-bin.000007' at 313962. 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_102] 2018-07-10 15:51:00.261 [destination = example   address = /10.110.10.27:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: Received error packet: errno = 4052  sqlstate = HY000 errmsg = A slave with the same server_uuid/server_id as this slave has connected to the master; the first event 'mysql-bin.000007' at 313931  the last event read from 'mysql-bin.000007' at 313931  the last byte read from 'mysql-bin.000007' at 313962. 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) 	at java.lang.Thread.run(Thread.java:745) ] 2018-07-10 15:51:01.425 [New I/O server worker #1-1] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400   Caused by :  something goes wrong with channel:[id: 0x3f8a301b  /192.168.37.1:60627 => /192.168.37.129:11111]  exception=com.alibaba.otter.canal.store.CanalStoreException: no match ack positionLogPosition[identity=LogIdentity[sourceAddress=10.110.10.27/10.110.10.27:3306 slaveId=-1] postion=EntryPosition[included=false journalName=mysql-bin.000007 position=313931 serverId=27 gtid= timestamp=1531208924000]] 2018-07-10 15:51:11.430 [New I/O server worker #1-1] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x3f8a301b  /192.168.37.1:60627 :> /192.168.37.129:11111]  exception=java.nio.channels.ClosedChannelException 	at org.jboss.netty.channel.socket.nio.NioWorker.cleanUpWriteBuffer(NioWorker.java:649) 	at org.jboss.netty.channel.socket.nio.NioWorker.writeFromUserCode(NioWorker.java:370) 	at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:137) 	at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) 	at org.jboss.netty.channel.Channels.write(Channels.java:611) 	at org.jboss.netty.channel.Channels.write(Channels.java:578) 	at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:28) 	at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:144) 	at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) 	at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.cleanup(ReplayingDecoder.java:542) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.channelDisconnected(ReplayingDecoder.java:450) 	at org.jboss.netty.channel.Channels.fireChannelDisconnected(Channels.java:360) 	at org.jboss.netty.channel.socket.nio.NioWorker.close(NioWorker.java:599) 	at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:119) 	at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) 	at org.jboss.netty.channel.Channels.close(Channels.java:720) 	at org.jboss.netty.channel.AbstractChannel.close(AbstractChannel.java:208) 	at org.jboss.netty.channel.ChannelFutureListener$1.operationComplete(ChannelFutureListener.java:46) 	at org.jboss.netty.channel.DefaultChannelFuture.notifyListener(DefaultChannelFuture.java:381) 	at org.jboss.netty.channel.DefaultChannelFuture.addListener(DefaultChannelFuture.java:148) 	at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:30) 	at com.alibaba.otter.canal.server.netty.NettyUtils.error(NettyUtils.java:51) 	at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:200) 	at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) 	at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:444) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261) 	at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:350) 	at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) 	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) 	at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) 客户端：官方exampledemo ----------------  END ----> transaction id: 17715 ================> binlog[mysql-bin.00000732Ï:313931]   executeTime : 1531208924000(2018-07-10 15:48:44)   gtid : ()   delay : 317551ms ----------------  END ----> transaction id: 17715 ================> binlog[mysql-bin.000007:313931]   executeTime : 1531208924000(2018-07-10 15:48:44)   gtid : ()   delay : 317552ms process error!com.alibaba.otter.canal.protocol.exception.CanalClientException: something goes wrong with reason: something goes wrong with channel:[id: 0x092dc999  /192.168.37.1:60768 => /192.168.37.129:11111]  exception=com.alibaba.otter.canal.store.CanalStoreException: no match ack positionLogPosition[identity=LogIdentity[sourceAddress=10.110.10.27/10.110.10.27:3306 slaveId=-1] postion=EntryPosition[included=false journalName=mysql-bin.000007 position=313931 serverId=27 gtid= timestamp=1531208924000]] 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.receiveMessages(SimpleCanalConnector.java:338) ~[classes/:na] 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:315) ~[classes/:na] 	at com.alibaba.otter.canal.example.AbstractCanalClientTest.process(AbstractCanalClientTest.java:123) ~[classes/:na] 	at com.alibaba.otter.canal.example.AbstractCanalClientTest$2.run(AbstractCanalClientTest.java:87) [classes/:na] 	at java.lang.Thread.run(Thread.java:722) [na:1.7.0_17] 数据库10.2.12-MariaDB 什么问题，求大神解决  Received error packet: errno = 4052  sqlstate = HY000 errmsg = A slave with the same server_uuid/server_id as this slave has connected to the master; the first event 'mysql-bin.000007' at 313931  the last event read from 'mysql-bin.000007' at 313931  the last byte read from 'mysql-bin.000007' at 313962.
743,关于对MySQL8 的支持. 咨询咱们这个项目有对MySQL8 提供支撑的计划吗?  会支持的  你们已经开始用上mysql8了? @agapple 嗯 已经开始更换8了
742,mysql和canal部署在腾讯云上遇见的问题； 现在 在腾讯云上部署了mysql，canal服务也是在腾讯云上的。启动canal server后的报错信息如下。 也让运维家了权限和canal的账号。 2018-07-10 11:03:29.165 [Druid-ConnectionPool-Create-525230202] ERROR com.alibaba.druid.pool.DruidDataSource - create connection SQLException  url: jdbc:h2:../conf/yunjiradartrace899/h2;CACHE_SIZE=1000;MODE=MYSQL;  errorCode 28000  state 28000 org.h2.jdbc.JdbcSQLException: Wrong user name or password [28000-196] 	at org.h2.message.DbException.getJdbcSQLException(DbException.java:345) ~[h2-1.4.196.jar:1.4.196] 	at org.h2.message.DbException.get(DbException.java:179) ~[h2-1.4.196.jar:1.4.196] 	at org.h2.message.DbException.get(DbException.java:155) ~[h2-1.4.196.jar:1.4.196] 	at org.h2.message.DbException.get(DbException.java:144) ~[h2-1.4.196.jar:1.4.196] 	at org.h2.engine.Engine.validateUserAndPassword(Engine.java:336) ~[h2-1.4.196.jar:1.4.196] 	at org.h2.engine.Engine.createSessionAndValidate(Engine.java:162) ~[h2-1.4.196.jar:1.4.196] 	at org.h2.engine.Engine.createSession(Engine.java:137) ~[h2-1.4.196.jar:1.4.196] 	at org.h2.engine.Engine.createSession(Engine.java:27) ~[h2-1.4.196.jar:1.4.196] 	at org.h2.engine.SessionRemote.connectEmbeddedOrServer(SessionRemote.java:354) ~[h2-1.4.196.jar:1.4.196] 	at org.h2.jdbc.JdbcConnection.<init>(JdbcConnection.java:116) ~[h2-1.4.196.jar:1.4.196] 	at org.h2.jdbc.JdbcConnection.<init>(JdbcConnection.java:100) ~[h2-1.4.196.jar:1.4.196] 	at org.h2.Driver.connect(Driver.java:69) ~[h2-1.4.196.jar:1.4.196] 	at com.alibaba.druid.pool.DruidAbstractDataSource.createPhysicalConnection(DruidAbstractDataSource.java:1513) ~[druid-1.1.9.jar:1.1.9] 	at com.alibaba.druid.pool.DruidAbstractDataSource.createPhysicalConnection(DruidAbstractDataSource.java:1578) ~[druid-1.1.9.jar:1.1.9] 	at com.alibaba.druid.pool.DruidDataSource$CreateConnectionThread.run(DruidDataSource.java:2466) ~[druid-1.1.9.jar:1.1.9] 确定 账号密码是没有错的。 先关闭一下tablemeta tsdb的能力  这个报错主要是本地使用H2的错误  如果有对H2比较了解的可以尝试定位或者修复一下 h2作为table ddl的历史版本存储时，会基于H2的默认jdbc配置 ``` canal.instance.tsdb.dir=${canal.file.data.dir:../conf}/${canal.instance.destination:} canal.instance.tsdb.url=jdbc:h2:${canal.instance.tsdb.dir}/h2;CACHE_SIZE=1000;MODE=MYSQL; canal.instance.tsdb.dbUsername=canal canal.instance.tsdb.dbPassword=canal ``` 解读一下，就是第一次时会创建conf/$instance$/$instance.mv.db，并设置访问密码为canal/canal，如果第二次重新打开时会校验xx.mv.db是否有多进程同时使用(会出现java.lang.IllegalStateException: The file is locked)，也会校验本次的访问密码是否和第一次创建时相同 如果真遇到一些莫名其妙的问题，万能的解决办法：删除conf/对应的xx.mv.db，会重新初始化一个h2本地文件
741,canal的吞吐量锐减 2018-07-09 08:10:22.493 - clientId:1001 cursor:[mysql-bin.000274 21149132 1530956958000] address[/10.132.27.103:3306] 2018-07-09 10:11:27.493 - clientId:1001 cursor:[mysql-bin.000274 25141059 1530957528000] address[/10.132.27.103:3306] 发现canal日志报错： 2018-07-09 13:39:29.043 [destination = cube   address = /10.132.27.103:3306   EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Unexpected End Stream 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) ~[canal.parse-1.0.21.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:78) ~[canal.parse-1.0.21.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) [canal.parse-1.0.21.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) [canal.parse-1.0.21.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.7.0_55] 2018-07-09 13:39:29.044 [destination = cube   address = /10.132.27.103:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /10.132.27.103:3306 has an error  retrying. caused by  java.io.IOException: Unexpected End Stream 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) ~[canal.parse-1.0.21.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:78) ~[canal.parse-1.0.21.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) ~[canal.parse-1.0.21.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) ~[canal.parse-1.0.21.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.7.0_55] 2018-07-09 13:39:29.047 [destination = cube   address = /10.132.27.103:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:cube[java.io.IOException: Unexpected End Stream 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:78) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) 	at java.lang.Thread.run(Thread.java:745) 今天（9号）发现canal的同步超级慢，原来每小时要更新几十次。今天个同步要花1-2个小时。 canal生产者环境正常，消费者也正常。有高手指点一下查问题的思路吗？ 可以测试一下1.0.26-SNAPSHOT  针对大批量数据DML同步的 建议调大一下jvm内存.  从我的测试来看只要内存资源够大  解析速度还是非常稳定的 参考FAQ里的性能：https://github.com/alibaba/canal/wiki/FAQ
740,wiki中的一些疑问 AdminGuide  https://github.com/alibaba/canal/wiki/AdminGuide 中对 spring组装方式区别的介绍中是不是有一点模糊啊 ![image](https://user-images.githubusercontent.com/13183268/42429351-1335062e-836b-11e8-9d1e-d0566544cbf8.png) store不是只有内存模式么，这里的store的存储应该是只有memory 而不会存储到zookeeper和file当中吧。 你的理解是正确的  store这块只有memory的能力
739,canal.client-1.0.26-SNAPSHOT在哪个maven中拉呢？ ```xml                <dependency> 			<groupId>com.alibaba.otter</groupId> 			<artifactId>canal.client</artifactId> 			<version>1.0.26-SNAPSHOT</version> 		</dependency> ``` 1.0.26的客户端版本在哪个maven库呢，拉取不到啊？ mvn clean install 我的客户端程序要用1.0.26-SNAPSHOT，难道把项目打包后再拉取1.0.26-SNAPSHOT的jar包单独加入自己的项目吗，不能从maven库中直接拉取吗，1.0.25-SNAPSHOT在公网maven中都有的
738,阿里云RDS如何使用GTID 阿里云上使用的RDS版本是5.6，之前用binlog+position的方式可以正常同步数据，尝试设置 ``canal.instance.gtidon=true``后报错： ``` java.io.IOException: Received error packet: errno = 1236   sqlstate = HY000 errmsg = The slave is connecting using CHANGE MASTER TO MASTER_AUTO_POSITION = 1   but the master has purged binary logs containing GTIDs that the slave requires. ``` 请问是不是需要修改RDS的配置？ 我的canal的版本是1.0.26 这个需要mysql设置gtid的能力  你搜索一下errmsg就可以了  或者看我gtid的issue 我也遇到类似的问题。检查过gtid已经启用了。有搜索到是因为主库上GIT_PURGED有值导致从库无法同步。其他帖子里面也没有知道原因。 @agapple 
737,Parallel参数细节的完善 一些小细节： 60%processor anyway都会被 :16 覆盖掉。 其次，60%processor * 16 很可能不满足ringbuffer限制。 看的非常仔细，多谢
736,com.alibaba.fastsql.sql.parser.ParserException 场景：TSDB开启 fastsql版本：371 升级到371版本的fastsql后，目前遇见两种类型exception. 一： Student-service 2018-07-05 17:10:13.017  WARN ???? [6   EventParser] c.a.o.c.p.i.m.t.MemoryTableMeta          : [] parse faield : ALTER TABLE `platform`.`notice`  CHANGE COLUMN `content` `content` VARCHAR(3000) CHARACTER SET 'utf8mb4' COLLATE 'utf8mb4_unicode_ci' NOT NULL DEFAULT '未填写' COMMENT '默认' com.alibaba.fastsql.sql.parser.ParserException 	at com.alibaba.fastsql.sql.parser.SQLExprParser.parseCharTypeRest(SQLExprParser.java:2910) ~[Student-service.jar!/:?] 	at com.alibaba.fastsql.sql.parser.SQLExprParser.parseDataType(SQLExprParser.java:2785) ~[Student-service.jar!/:?] 	at com.alibaba.fastsql.sql.parser.SQLExprParser.parseDataType(SQLExprParser.java:2666) ~[Student-service.jar!/:?] 	at com.alibaba.fastsql.sql.dialect.mysql.parser.MySqlExprParser.parseColumn(MySqlExprParser.java:463) ~[Student-service.jar!/:?] 	at com.alibaba.fastsql.sql.dialect.mysql.parser.MySqlStatementParser.parseAlterTable(MySqlStatementParser.java:4315) ~[Student-service.jar!/:?] 	at com.alibaba.fastsql.sql.dialect.mysql.parser.MySqlStatementParser.parseAlter(MySqlStatementParser.java:3307) ~[Student-service.jar!/:?] 	at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:248) ~[Student-service.jar!/:?] 	at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[Student-service.jar!/:?] 	at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[Student-service.jar!/:?] 	 二： Student-service 2018-07-05 17:32:42.210  WARN ???? [6   EventParser] c.a.o.c.p.i.m.t.MemoryTableMeta          : [] parse faield : ALTER TABLE `student` DROP COLUMN `display_name` DROP COLUMN `system` MODIFY COLUMN `id`  bigint(20) UNSIGNED NOT NULL AUTO_INCREMENT FIRST  CHANGE COLUMN `name` `student_name`  varchar(255) CHARACTER SET utf8 COLLATE utf8_unicode_ci NOT NULL DEFAULT '' COMMENT '学生名' AFTER `id` MODIFY COLUMN `description`  varchar(255) CHARACTER SET utf8 COLLATE utf8_unicode_ci NOT NULL DEFAULT '' COMMENT '学校描述' AFTER `student_name` MODIFY COLUMN `created_at`  timestamp NOT NULL AFTER `description` MODIFY COLUMN `updated_at`  timestamp NOT NULL AFTER `created_at` CHANGE COLUMN `creator` `created_by`  varchar(50) CHARACTER SET utf8 COLLATE utf8_unicode_ci NOT NULL AFTER `updated_at` CHANGE COLUMN `updated_name` `updated_by`  varchar(50) CHARACTER SET utf8 COLLATE utf8_unicode_ci NOT NULL AFTER `created_by` ADD COLUMN `is_deleted`  tinyint(1) NOT NULL DEFAULT 0 COMMENT '删除标记，0，未删除，1，删除' AFTER `description` com.alibaba.fastsql.sql.parser.ParserException: syntax error  error in :'MODIFY COLUMN `id`  bigint(20) UNSIGN  pos 86  line 4  column 9  token COLUMN 	at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:361) ~[Student-service.jar!/:?] 	at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:498) ~[Student-service.jar!/:?] 	at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[Student-service.jar!/:?] 	at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[Student-service.jar!/:?] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:72) [Student-service.jar!/:?] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.apply(DatabaseTableMeta.java:104) [Student-service.jar!/:?] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.apply(TableMetaCache.java:228) [Student-service.jar!/:?] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseQueryEvent(LogEventConvert.java:265) [Student-service.jar!/:?] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:126) [Student-service.jar!/:?] 你这第二条SQL也够复杂的 第一条的parseCharTypeRest导致的exception比第二条常见~~~麻烦先优先处理第一条那种case吧，多谢多谢 测试发现 Alter Table中有 COLLATE 会导致解析失败。 2.0.0_preview_186版本也存在这个问题。与TSDB无关。 这是fastsql对DDL的解析支持的问题  我会记录和反馈 最新的fastsql已经修复 请问最新的sql的版本是 ？
735,启动客户端之后，cpu使用率高 启动客户端连接canal server的时候，cpu中对应的java进程使用率瞬间升高到40% - 50%之间，这个似乎不太正常吧，我看client demo里面使用了while 无限循环，这个可能是cpu飚高的原因吧，难道客户端是通过不断轮训服务端获取数据的吗？ 客户端代码如下： CanalConnector connector = CanalConnectors.newSingleConnector(new InetSocketAddress(canalServerIp                 canalServerPort)  serverDestination  canalUserName  canalPassword);         new Thread(                 () -> {                     while(true){                         try{                             connector.connect();                             connector.subscribe(monitorDatabase+"\\..*");                             while(true){                                 Message message = connector.getWithoutAck(batchSize); // 获取指定数量的数据                                 long batchId = message.getId();                                 int size = message.getEntries().size();                                 if (batchId != -1 && size != 0) {                                     try{                                         processData(message.getEntries());                                         connector.ack(batchId); // 提交确认                                         //Thread.sleep(500);                                     }catch (Exception e){                                         e.printStackTrace();                                         connector.rollback();//如果发生异常，则需要回滚上次读取到的binlog位置                                     }                                 }                             }                         }catch (Exception e){                             e.printStackTrace();                             logger.error("和canal Server交互出现异常");                         }finally {                             connector.disconnect();                         }                     }                 }         ).start(); 下面的是我机器的性能参数，请主要关注对应的java进程 Processes: 346 total  6 running  340 sleeping  2038 threads            15:09:09 Load Avg: 5.62  5.26  5.03  CPU usage: 58.75% user  19.6% sys  22.18% idle SharedLibs: 185M resident  57M data  27M linkedit. MemRegions: 78596 total  7644M resident  207M private  1706M shared. PhysMem: 16G used (2031M wired)  136M unused. VM: 1565G vsize  1110M framework vsize  0(0) swapins  0(0) swapouts. Networks: packets: 38214974/2633M in  38184606/2541M out. Disks: 284609/7890M read  251982/9950M written. PID   COMMAND             %CPU  TIME     #TH    #WQ  #PORT MEM    PURG   CMPRS  PGRP 3399  mdworker                  0.1   00:00.06 4      2    44    3252K+ 0B     0B     3399 3398  mdworker                  0.0   00:00.06 4      2    44    3272K  0B     0B     3398 3397  mdworker                  0.1   00:00.06 4      2    44    3260K+ 0B     0B     3397 3391  com.apple.au            0.0   00:00.01 2      2    16    972K   0B     0B     3391 3353  Google Chrom          0.0   00:00.10 15     1    113   13M    0B     0B     526 **3347  java                            25.9  01:28.67 144/2  1    326   1327M+ 0B     0B     665** 3344  top                            4.1   00:23.44 1/1    0    30    3668K+ 0B     0B     3344 **3333  java                           38.9  03:29.43 30     1    98    260M   0B     0B     665 3322  java                           48.9  04:08.49 33/1   1    104   606M   0B     0B     3312** 3295  tail                            0.0   00:00.14 1      0    10    360K   0B     0B     3295 另外，我的版本是  v1.0.24 client启动之后，服务端就会开始解析工作
734,add create database support 增加create database 语句支持 [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=734) <br/>All committers have signed the CLA. tks
733,v1.0.24  canal.instance.filter.regex无效 使用1.0.24版本 只需要监听两张表的binlog  在instance.properties文件中有设定: canal.instance.defaultDatabaseName = mall canal.instance.filter.regex = mall.users mall.users_extra_info 客户端代码没有写connector.subscribe()语句 但是接收到的仍然是mall数据库所有表的binlog 求解 正则应该只有硬编码是ok的，另外正则不支持复杂的～慎重
732,支持oracle的版本什么时候开源？ 如题 oracle这块不会开源了  可以参考一下https://github.com/alibaba/yugong(基于物化视图做增量) 其余的可以参考: oracle的LogMiner工具或者CDC来实现  或者
731,canal显示连接成功，但是数据库改变的时候收不到订阅消息 canal启动并没有报错，但是无论怎么操作数据库，都没有数据库改变的日志打印出来。求大神指教。 ![image](https://user-images.githubusercontent.com/8399936/42198827-071f8a10-7ebd-11e8-95ee-d0438707b334.png) 我也遇到类似的问题，用的.26版本客户端，.24版本server，zk集群连接，启动应用的时候可以消费，但是过了一段时间后，通过堆栈发现client程序被阻塞在信号量获取的地方，是不是哪里没有处理好死锁了，2个client实例消费！求解 解决了吗？ 推荐用v1.1.1新版本了
730,canal 增加 create database支持 拉了1.0.26-preview3分支测试，目前canal在处理Query事件create database，没有对建库语句进行parser 调用逻辑 LogEventConvert#parseQueryEvent -> DruidDdlParser.parse(queryString  event.getDbName()); -> fastsql_preview366#SQLCreateDatabaseStatement 目前没有fastsql的代码，大致修改如下，麻烦大神看看是否合适 ![qq 20180702202624](https://user-images.githubusercontent.com/5847660/42163743-3e061f4e-7e36-11e8-8126-24af0d1386cf.jpg) 最新的trunk master也没有处理这个问题，fastsql什么时候开源<_- 你提交一个MR给我吧 #734 提了一个PR，GitHub的MR是怎么操作的
729,update 语句为什么是ddl ？ ![image](https://user-images.githubusercontent.com/5965173/42151092-603695a8-7e0e-11e8-8386-8920b4bc4d70.png) 如图。  mysql 5.5.24   请问这个版本支持会有问题吗？ 发现会有更新数据丢失的问题，还有会解析异常。。。 ![image](https://user-images.githubusercontent.com/5965173/42151207-cd8a93f2-7e0e-11e8-8250-6ac2445ef3a3.png) DDL语法解析错误，不会影响DML数据 手动更新数据库，观察发现有的数据能够在canal client 成功收到，有一些没有，因此就得出数据有丢失的结论。 “当前的canal开源版本支持5.7及以下的版本(阿里内部mysql 5.7.13  5.6.10  mysql 5.5.18和5.1.40/48)” 因此5.5.24 官方版本，是不保证数据的可靠性，存在丢失的情况对吗？ “目前canal测试已支持mysql 5.7.13/5.6.10及以下的版本，mariadb 5.5.35和10.0.7(理论上可支持以下版本)” 但是这里描述来看。5.5.24 应该是支持的，因此不知道是什么原因。麻烦解答一下。谢谢！：D 这个报错和mariadb没关系，就是单纯的SQL语法解析出错，无法精确过滤数据而已，不打紧 用的是Mysql5.5.24 ， 出现数据更新丢失这种情况如何定位呢。 PS：交流一直加不进去，能否拉一下呢， 469673467   谢谢！ ：D
728,jar包冲突 spring-code和canal.common 引用canal.client 1.0.12后，代码报出 org.springframework.core.MethodParameter（Cannot resolve method 'hasParameterAnnotation(java.lang.Class<>） spring-code 4.3.14.RELEASE 操作 <dependency>     <groupId>com.alibaba.otter</groupId>     <artifactId>canal.client</artifactId>     <version>1.0.12</version>     <exclusions> 	...... 	<exclusion> 		<artifactId>canal.common</artifactId> 		<groupId>com.alibaba.otter</groupId> 	</exclusion>     </exclusions> </dependency>后正常， 运行代码报错java.lang.NoClassDefFoundError: com/alibaba/otter/canal/common/utils/BooleanMutex 该如何解决？ 你在canal版本也有点低了，升级一下到1.0.26
727,canal支持aliyun rds的binlog订阅 aliyun是一家伟大的公司，也是目前国内云市场占比最高的，考虑前面有太多的小伙伴和我反馈过期望能更全面的支持RDS binlog的订阅。aliyun RDS主要满足用户对于MySQL的简单方便使用，针对用户业务发展迅速，对于未来存储/计算扩展性有预期的也会使用aliyun [DRDS（ Distributed Relational Database Service）](https://www.aliyun.com/product/drds)，一个基于MySQL sharding的分布式数据库解决方案，因为存储本身是MySQL也可以比较方便的基于canal进行数据订阅。 首先明确一下针对这类云MySQL的binlog订阅，通常会面临的几个问题： 1.  账号权限问题 [已解决]  * canal的策略是模拟了MySQL Slave的行为，因此需要有SELECT  REPLICATION SLAVE  REPLICATION CLIENT的权限  * 解决思路：目前aliyun上的RDS默认创建的账号已经自带了这些权限，针对RDS 5.6/5.7的高权限实例，可以用root账号额外进行一下授权，授权操作可参考[QuickStart] 2.  binlog被删除的问题 [已解决]  * 对应的[binlog清理策略](https://help.aliyun.com/knowledge_detail/41815.html)  超过18小时之后会删除并备份到oss之上，如果canal任务停止超过18小时就会遇到xx类似错误，   * 解决思路：RDS默认提供了一段时间oss binlog的下载能力，[参考文档](https://help.aliyun.com/document_detail/26291.html)。canal可以识别位点中的时间戳，对比一下RDS中show binary logs里最早的一条binlog，如果不满足则会通过oss接口进行下载到本机进行解析，追平历史binlog之后再切换到RDS binlog中继续消费 【canal代码会支持】 ![image](https://user-images.githubusercontent.com/834743/43991319-e6de1772-9d9c-11e8-9407-fe74e48da1ce.png) 3. 主备切换导致的问题 [已解决]  * 一般云MySQL的主备方案都采用了vip模式，屏蔽了后端物理节点之间的主备切换，也就是对于canal来说只看到了单节点的MySQL ip，针对物理上进行主备切换时拿着老主库位点去订阅时会遇到xx类似错误  * 解决思路:  针对MySQL5.6+可以使用canal gtid的订阅方式(针对出现问题2时，需要进行本地binlog解析就无法很好的支持)，或者比较推荐的就是基于serverId自动识别主备切换，每次进行binlog订阅时，检查一下位点中的serverId和当前数据库节点的serverId是否一致，如果不一致说明服务端产生了主备切换，可以基于时间戳重新在新的主库中定位到对应的binlog，再继续后续的消费即可。ps.  这里定位位点时需要考虑binlog被删除的情况，参考问题2 代码提交：https://github.com/alibaba/canal/commit/ea6391d1c3231406e241ef09217ea1c78f885373 具体aliyun rds使用文档：https://github.com/alibaba/canal/wiki/aliyun-RDS-QuickStart
726,canal整体性能优化 之前有较多的小伙伴  反馈在大规模数据DML变更时消费速度有点跟不上   反馈的问题列表: 1. https://github.com/alibaba/canal/issues/672 2. https://github.com/alibaba/canal/issues/547  3. https://github.com/alibaba/canal/issues/355 4. https://github.com/alibaba/canal/issues/267 这里会做一个相对完整的测试，进行针对性的优化，同时也非常欢迎大家的参与和代码MR，一起努力解决好性能和稳定性的问题.  ps.  1.0.26会是一个里程碑式的版本。 ---- 最后的优化结果：https://github.com/alibaba/canal/wiki/Performance ![image](https://user-images.githubusercontent.com/834743/42986955-1f40b0d6-8c2a-11e8-8eb5-c5b885d8040a.png) 赞赞赞 期待ing，期待ing 赞！  我先来抛个砖，说说以前遇到的issue和一些思路： 场景是，使用load backup file的方式灌数据，同时使用canal进行同步。 每个表一个或多个文件，由于每个文件中的行都同属于一张表的同一事件（INSERT），mysql-binlog会merge一定量的行（取决于binlog-row-event-max-size）到同一event。 *单句DML影响到多行数据的情况应该也适用。 压缩的event经过解析为protobuf对象，内存占用急剧膨胀，造成fgc频繁，甚至oom。 在MySQL 5.6的版本，官方默认将binlog-row-event-max-size从1024调高到了8192。 当时我们通过将binlog-row-event-max-size设回1024，问题被绕过。 如果将server/client 的netty3升级为4，用PooledByteBuf应该可以缓解一小部分。但大头还是protobuf这个memory hogs. 补充一下，ringbuffer size被设置为4096。内存是4g.由于load backup file以后就是平稳的正常traffic。因此并不想调整内存或进一步减小ringbuffer 第一步网络优化(只做header解析，识别包大小、位点等信息，不做具体的记录级别解析) 结果：默认设置的32k的socket buffer，针对大吞吐量传输时有点过小，去掉了默认设置，让socket自我协调，默认在24c96g的物理机上测试，receiveBuffer协调结果为180k.  调整前后的性能吞吐量对比： 18MB VS  117MB，提供6倍多，socket buffer优化之后基本可以跑满网卡 第二步优化解析的能力，跑了下简单的对象解析(不做protobuf的对象构建)，刚开始速度是20MB，主要优化了时间字段的解析上，提升到了45MB.  目前遇到瓶颈了，观察系统的负载cpu最高在1.5核左右，jvm gc主要集中在young区，下一步的优化思路：多线程并行解析，最大化的使用系统负载进行优化 第三步整体并发模型设计，按照前面的优化网络和对象解析，瓶颈都在对象的深度解析上，如果要最大化性能，必须得引入并发设计，整体设计思路如下： ![image](https://user-images.githubusercontent.com/834743/42207319-3e58ac2c-7edc-11e8-966e-33f46c57e4bc.png) 基于ringbuffer的并发模型，整个过程会分为4个阶段，多个阶段之间互相隔离和依赖，把最耗瓶颈的解析这块采用多线程的模型进行加速，预计整个优化完成，整个解析可以跑满整个千兆网卡.   ps.  如果各位有万兆网卡的机型，到时候可以也帮忙跑一下效果 @lcybo 针对内存占用的问题，你有什么优化建议么？换掉protobuf？ 个人一点不成熟的想法： 因为不跨语言，可以用基于protobuf和java的protostuff。 相对来说，优点： - 比protobuf快，占用内存更少，相同的data size。 - 可以使用POJO，不用proto文件。protobuf的generated code对人太不友好了。 不足的地方： - 社区不太活跃。 - 待补充哈哈。 贴出一些细节： // Re-use (manage) this buffer to avoid allocating on every serialization     LinkedBuffer buffer = LinkedBuffer.allocate(512); 对比protobuf： byte[] result = new byte[this.getSerializedSize()]; 此外，POJO还可以参考netty的Recycler或其他的对象池，理论上，ringbuffer中event占用Entry对象的峰值就是ObjectPool需要缓存的对象个数。 ProtostuffIOUtil.mergeFrom(protostuff  **fooParsed**  schema);  //fooParsed是可以提供的，反序列化可以从ObjectPool受益。 当然这么做代码会复杂些。 byte[]的复用倒是有  基于ObjectPool的思路以前还真没考虑过  对象里的各种嵌套结构体也不太一样  不太理解复用的原理 @lcybo  并行解析如何保证binlog event的有序性？@agapple 办法挺多，举个栗子，把结果作为卫星数据，countdownlatch或其他的闭锁或parallelstream，最后还不用reorder @agapple，在外面爪机不太方便，回去再码字。 @lan1994 串行和并行的结合体，串行把基本的对象解析做好，并行主要处理一些耗时比较长的，比如DML的具体字段解析、protobuf对象构造等.    目前初步的测试结果，24个并发基本可以把一台24core的物理机cpu给跑满，吞吐量能跑到80MB/s  (从binlog接收到生成CanalEntry整体，优化前大概7MB/s，提升一个数量级)，目前瓶颈基本在cpu上了(字符串的序列化占比非常高)，下午会继续测试一下从mysql -> canal server -> canal client的整体吞吐量 关于嵌套结构： 假设Row嵌套Column的list 两个类都使用了ObjectPool，都实现了Recycable接口和recycle方法。 对Row调用recycle时，清楚数据fields，对每个Column嵌套调用recycle，再list.clear() 要注意的是内嵌的Column生命周期不应该久于Row。 关于netty的Recycler，每个对象new出来会bind ThreadLocal的回收stack，如果在同一线程进行recycle/get，那么就完全不会有竞争。如果两者分离，则会涉及到潜在线程间共享的WeakOrderQueue。 按：**串行浅解析->并发深度解析&proto对象->ringbuffer串行get** 的处理逻辑，序列化的点在SessionHandler的get，会引发深度解析线程之间的竞争（解析线程stack空的时候它们会去WeakOrderQueue偷一些对象回来），看起来至少netty的Recycler不适合这里应用。（除非提早序列化，把序列化结果放到ringbuffer） 参考： io.netty.util.Recycler<T> 对象参考： io.netty.util.internal.RecyclableArrayList BTW，sessionhandler里面的序列化： NettyUtils.write(ctx.getChannel()  packetBuilder.build().toByteArray()  null);// 输出数据 packetBuilder.build().toByteArray()每次都new byte[]，应该有优化的空间。 第三步优化已经完成，解析这块引入了ringbufer模型，分成了多个阶段：网络接收、简单解析、DML深度解析、投递到store，把中间最耗时的DML深度解析换成了并发解析的模型.   从binlog接收到生成Entry对象，测试了一下对比： 1.  未做并行化改造前，大概是5MB/s (单线程) 2. 做了并行化改造，16个并发解析，大概是80MB/S，基本是正相关的线性扩展 (基本跑满了cpu的瓶颈，如果要进一步优化，只能优化Entry对象的构造协议了) 第四步优化已经开始，主要是关注binlog解析到client收到数据的吞吐量，目前逐步压测的情况，大概是binlog下载吞吐在8MB/s，Entry对象的吞吐大概在55MB/s，大概是1:7的数据膨胀率.  目前profile看到的瓶颈主要在server在序列化Entry对象时，相比于网络传包占了50%，测试的记录大概是100字节，换算到记录的tps，目前大概是在20w record/s左右. (我测试数据是批量insert和update，binlog里会更加紧凑) ---- 如果优化掉这块，理论上Entry对象可以跑满网络带宽，预计可以整体提升150% (相比于未优化前，因为最后端的网络传输瓶颈比较大，所以吃掉了前面的几个优化带来的提升，如果要进一步优化，得改动protobuf协议设计) ---- 初步优化思路 1. 多线程提前构造好Entry的序列化结果，避免在client get操作时临时做序列化 (因为单线程，会有瓶颈) 2. 如果改动protobuf协议，可以通过设计一些字典表，压缩部分的数据存储 ![image](https://user-images.githubusercontent.com/834743/42286259-d9dede60-7fe4-11e8-9220-4ae70469bd12.png) 拜读了代码，parallel参数有些细节：[Pr#737](https://github.com/alibaba/canal/pull/737) 赞!!!!!👍 非常欢迎大家提交PR哈，最近在思考protobuf的一些优化细节，目前主要瓶颈点在于protobuf的序列化和字节放大问题。 我会尽可能去保证兼容性，但也不排除极端情况下在协议设计改动出现不兼容的情况 so.  如果有好的想法，可以尽快反馈到我这里.   enum Compression {     NONE = 1;     ZLIB = 2;     GZIP = 3;     LZF = 4; } proto里预留了压缩，如果在**并发深度解析**这一步额外进行序列化和并发压缩。 SessionHandler那里： _packetBuilder.setBody(//压缩后数据);_     然后，client那里串行接收+并行解压和反序列化。 并行压缩+网络传输 VS 无压缩网络传输，不知道表现如何。 1.  单线程压缩和网络传输，这会是一个平衡点 2. protobuf序列化之后的数据，压缩比也需要评测 第四步优化已经进行了一大半，大致的效果相比于完全未优化之前提升了35%的吞吐量，可以跑到8~9万的TPS  (本次压测数据和之前的批量insert不同，选择了业务上随机的一个库进行跑，19MB的binlog，大概网络传输在35MB左右) 优化点： 1.  CanalEntry对象的序列化提前在进入ringbuffer之前就完成，最后sessionHandler只做ByteString拷贝，有部分提升 2.  SimpleCanalConnector增加了lazyParseEntry参数，支持lazy解析CanalEntry对象，减少整个get/ack串行操作的成本，最大化提升串行的吞吐量.  目前的profile分析来看，最大的瓶颈就在于构造网络传输时有多次数组拷贝。 现在的代码：  ``` Packet.Builder packetBuilder = CanalPacket.Packet.newBuilder();                         packetBuilder.setType(PacketType.MESSAGES);                         Messages.Builder messageBuilder = CanalPacket.Messages.newBuilder();                         messageBuilder.setBatchId(message.getId());                         if (message.getId() != -1) {                             if (message.isRaw()) {                                 // for performance                                 if (!CollectionUtils.isEmpty(message.getRawEntries())) {                                     messageBuilder.addAllMessages(message.getRawEntries());                                 }                             } else {                                 if (!CollectionUtils.isEmpty(message.getEntries())) {                                     for (Entry entry : message.getEntries()) {                                         messageBuilder.addMessages(entry.toByteString());                                     }                                 }                             }                         }                                                 packetBuilder.setBody(messageBuilder.build().toByteString());                          NettyUtils.write(ctx.getChannel()  packetBuilder.build().toByteArray()  null);// 输出数据 ``` 1.  messageBuilder.build().toByteString()，会拷贝一次message.getRawEntries()的所有数据到一个byte[]里 2.  packetBuilder.build().toByteArray()，会拷贝一次message的整个数据到一个packet的byte[]里 3. NettyUtils.write 网络发送 原始的10MB的记录，会至少是原先的3倍+，针对性的优化就是直代码一次性构造protobuf的数据格式，而不是通过builder + toByteString的方式，预计还能再提升10%左右.  尝试用一次byte[]数组绕过protobuf的多次拷贝，下一步可以优化如何复用byte[]数组，避免每次请求都开辟一个新的byte[]数组，一次性开10MB的内存块，还是有一些开销的，包括client层面 测试结论： https://github.com/alibaba/canal/wiki/Performance ![image](https://user-images.githubusercontent.com/834743/42986934-0bae041a-8c2a-11e8-8f31-f3f8e016bc8d.png) 
725,EventParser] ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta EventParser] ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - pls submit github issue    show create table ddl:CREATE TABLE `batch_job_seq` (   `ID` bigint(20) NOT NULL   `UNIQUE_KEY` char(1) NOT NULL   UNIQUE KEY `UNIQUE_KEY_UN` (`UNIQUE_KEY`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8   compare failed .   db : TableMeta [schema=test  table=batch_job_seq  fileds= 	FieldMeta [columnName=ID  columnType=bigint(20)  defaultValue=null  nullable=false  key=false] 	FieldMeta [columnName=UNIQUE_KEY  columnType=char(1)  defaultValue=null  nullable=false  key=true] ]   mem : TableMeta [schema=test  table=batch_job_seq  fileds= 	FieldMeta [columnName=ID  columnType=bigint(20)  defaultValue=null  nullable=false  key=false] 	FieldMeta [columnName=UNIQUE_KEY  columnType=char(1)  defaultValue=null  nullable=false  key=false] ] 你这是老版本25的吧  升级到26  已经修复 @qingmu2017 请问你这个问题升级到26后解决了吗？我现在用的是26-SNAPSHOT，还是有这个问题
724,scheudle applySnapshotToDB faield ![image](https://user-images.githubusercontent.com/9798724/42080735-3392c10c-7bb6-11e8-8f56-23829972429c.png) 2018-06-27 22:09:34.820 [[scheduler-table-meta-snapshot]] ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - scheudle applySnapshotToDB faield com.alibaba.otter.canal.parse.exception.CanalParseException: java.net.SocketException: Broken pipe (Write failed) Caused by: java.net.SocketException: Broken pipe (Write failed) 	at java.net.SocketOutputStream.socketWrite0(Native Method) ~[na:1.8.0_144] 	at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111) ~[na:1.8.0_144] 	at java.net.SocketOutputStream.write(SocketOutputStream.java:143) ~[na:1.8.0_144] 	at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.write(BioSocketChannel.java:35) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.writeBody0(PacketManager.java:42) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.writeBody(PacketManager.java:35) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.query(MysqlQueryExecutor.java:55) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:94) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.compareTableMetaDbAndMemory(DatabaseTableMeta.java:296) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.applySnapshotToDB(DatabaseTableMeta.java:251) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.access$100(DatabaseTableMeta.java:45) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta$2.run(DatabaseTableMeta.java:84) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_144] 	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) [na:1.8.0_144] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) [na:1.8.0_144] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) [na:1.8.0_144] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_144] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_144] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144] 这是一个定时schedule的任务  明天会将内存中的table meta的数据做checkpoint  在进行的过程中会对比内存值和数据库的表结构  这个异常就是在获取数据库表结构时产生了broke pipe  后续我会增加一次重试来确保网络的有效性 我当时解决方案是注释掉 canal.properties的关于tsdb 的注释例如，canal.instance.tsdb.spring.xml，以及设置canal.instance.tsdb.enable=false，这样是不是理解彻底的关掉了table meta的时间序列版本记录功能 @agapple 此问题有什么解决方案吗 @qingmu2017 只要设置canal.instance.tsdb.enable=false，就是关闭table meta时间序列功能
723,EventParser 当遇到SQL语句执行错误的。一直parser。 2018-06-28 11:43:52.463 [main] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2018-06-28 11:43:52.470 [main] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties] 2018-06-28 11:43:52.727 [main] WARN  o.s.beans.GenericTypeAwarePropertyDescriptor - Invalid JavaBean property 'connectionCharset' being accessed! Ambiguous write methods found next to actually used [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.nio.charset.Charset)]: [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.lang.String)] 2018-06-28 11:43:52.817 [main] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2018-06-28 11:43:52.817 [main] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties] 2018-06-28 11:43:53.154 [main] ERROR com.alibaba.druid.pool.DruidDataSource - testWhileIdle is true  validationQuery not set 2018-06-28 11:43:53.598 [main] INFO  c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-example 2018-06-28 11:43:53.743 [main] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - subscribe filter change to .*\..* 2018-06-28 11:43:53.743 [main] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - start successful.... 2018-06-28 11:43:53.779 [destination = example   address = /172.16.0.2:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position  {"identity":{"slaveId":-1 "sourceAddress":{"address":"172.16.0.2" "port":3306}} "postion":{"gtid":"" "included":false "journalName":"mysql-bin.000004" "position":15872 "serverId":1 "timestamp":1530090101000}} 2018-06-28 11:43:54.357 [destination = example   address = /172.16.0.2:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : GRANT ALL PRIVILEGES ON *.* TO 'canal'@'%' IDENTIFIED WITH 'mysql_native_password' AS '*E3619321C1A937C46A0D8BD1DAC39F93B27D4458' com.alibaba.fastsql.sql.parser.ParserException: syntax error  error in :'IDENTIFIED WITH 'mysql_native_password' A'  expect BY  actual WITH  pos 58  line 1  column 55  token WITH         at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:361) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371]         at com.alibaba.fastsql.sql.parser.SQLParser.accept(SQLParser.java:369) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371]         at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseGrant(SQLStatementParser.java:1022) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371]         at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:266) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371]         at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371]         at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.applyHistoryOnMemory(DatabaseTableMeta.java:382) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:121) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:84) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171] 2018-06-28 11:43:54.357 [destination = example   address = /172.16.0.2:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : GRANT SELECT  REPLICATION SLAVE  REPLICATION CLIENT ON *.* TO 'canal'@'%' IDENTIFIED WITH 'mysql_native_password' AS '*E3619321C1A937C46A0D8BD1DAC39F93B27D4458' com.alibaba.fastsql.sql.parser.ParserException: syntax error  error in :'IDENTIFIED WITH 'mysql_native_password' A'  expect BY  actual WITH  pos 89  line 1  column 86  token WITH         at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:361) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371]         at com.alibaba.fastsql.sql.parser.SQLParser.accept(SQLParser.java:369) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371]         at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseGrant(SQLStatementParser.java:1022) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371]         at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:266) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371]         at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371]         at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.applyHistoryOnMemory(DatabaseTableMeta.java:382) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:121) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:84) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171] 2018-06-28 11:43:54.384 [destination = example   address = /172.16.0.2:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000004 position=15872 serverId=1 gtid= timestamp=1530090101000] 2018-06-28 11:43:54.543 [destination = example   address = /172.16.0.2:3306   EventParser] WARN  c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : CREATE TABLE `t_comment` (   `id` bigint NOT NULL AUTO_INCREMENT   `chapter_id` bigint not NULL   `comment_member_id` bigint not NULL   `comment_nickname` varchar(32) not NULL   `comment_headicon` varchar(256) not NULL   `is_reply` int(11) DEFAULT 0 COMMENT '回复数量'   `content` TEXT NOT NULL COMMENT '内容'   `agrees` int(11) NOT NULL DEFAULT 0 COMMENT '赞同'   `product_time` bigint(20) DEFAULT '0' COMMENT '生产时间'   `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP   PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8 COMMENT = '评论表' **2018-06-28 11:43:54.577 [destination = example   address = /172.16.0.2:3306   EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Cannot replicate anonymous transaction when @@GLOBAL.GTID_MODE = ON  at file ./mysql-bin.000004  position 16561.; the first event 'mysql-bin.000004' at 15872  the last event read from './mysql-bin.000004' at 16626  the last byte read from './mysql-bin.000004' at 16626.**         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171] 2018-06-28 11:43:54.577 [destination = example   address = /172.16.0.2:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /172.16.0.2:3306 has an error  retrying. caused by java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Cannot replicate anonymous transaction when @@GLOBAL.GTID_MODE = ON  at file ./mysql-bin.000004  position 16561.; the first event 'mysql-bin.000004' at 15872  the last event read from './mysql-bin.000004' at 16626  the last byte read from './mysql-bin.000004' at 16626.         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]         at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171] 2018-06-28 11:43:54.580 [destination = example   address = /172.16.0.2:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Cannot replicate anonymous transaction when @@GLOBAL.GTID_MODE = ON  at file ./mysql-bin.000004  position 16561.; the first event 'mysql-bin.000004' at 15872  the last event read from './mysql-bin.000004' at 16626  the last byte read from './mysql-bin.000004' at 16626.         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227)         at java.lang.Thread.run(Thread.java:748) ] = Cannot replicate anonymous transaction when @@GLOBAL.GTID_MODE = ON  at file ./mysql-bin.000004  position 16561.; the first event 'mysql-bin.000004' at 15872  the last event read from './mysql-bin.000004' at 16626  the last byte read from './mysql-bin.000004' at 16626. mysql的异常
722,canal server和mysql部署在同一台机器，解析binlog失败 canal版本1.0.25   mysql版本5.7.10   最开始我将canal server部署在跟mysql同一台机器上，启动正常，操作数据库记录，binlog日志有更新，但是canal的客户端未读取到。后来将canal server部署在其他机器上，就能正常捕获到binlog的信息了。不太清楚为什么会出现这种现象，或者是不是我漏了哪些配置？ canal 25版本不建议使用了 更换了一个版本1.0.24，确实可以了
721,canal 不能读取bin-log OpenJDK 64-Bit Server VM warning: ignoring option PermSize=96m; support was removed in 8.0 OpenJDK 64-Bit Server VM warning: ignoring option MaxPermSize=256m; support was removed in 8.0 OpenJDK 64-Bit Server VM warning: UseCMSCompactAtFullCollection is deprecated and will likely be removed in a future release. 2018-06-27 14:20:28.945 [main] INFO  com.alibaba.otter.canal.deployer.CanalLauncher - ## set default uncaught exception handler 2018-06-27 14:20:29.015 [main] INFO  com.alibaba.otter.canal.deployer.CanalLauncher - ## load canal configurations 2018-06-27 14:20:29.020 [main] INFO  com.alibaba.otter.canal.deployer.CanalLauncher - ## start the canal server. 2018-06-27 14:20:29.096 [main] INFO  com.alibaba.otter.canal.deployer.CanalController - ## start the canal server[172.16.0.2:8765] 2018-06-27 14:20:29.794 [main] WARN  o.s.beans.GenericTypeAwarePropertyDescriptor - Invalid JavaBean property 'connectionCharset' being accessed! Ambiguous write methods found next to actually used [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.nio.charset.Charset)]: [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.lang.String)] 2018-06-27 14:20:30.194 [main] ERROR com.alibaba.druid.pool.DruidDataSource - testWhileIdle is true  validationQuery not set 2018-06-27 14:20:30.634 [destination = example   address = /172.16.0.2:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /172.16.0.2:3306 has an error  retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: connect /172.16.0.2:3306 failure Caused by: java.io.IOException: connect /172.16.0.2:3306 failure         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:77) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:79) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:87) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:163) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]         at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171] Caused by: java.io.IOException: Error When doing Client Authentication:ErrorPacket [errorNumber=1045  fieldCount=-1  message=Access denied for user 'canal'@'172.16.0.2' (using password: YES)  sqlState=28000  sqlStateMarker=#]         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:199) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:74) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na]         ... 4 common frames omitted 2018-06-27 14:20:30.645 [destination = example   address = /172.16.0.2:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: connect /172.16.0.2:3306 failure Caused by: java.io.IOException: connect /172.16.0.2:3306 failure         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:77)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:79)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:87)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:163)         at java.lang.Thread.run(Thread.java:748) Caused by: java.io.IOException: Error When doing Client Authentication:ErrorPacket [errorNumber=1045  fieldCount=-1  message=Access denied for user 'canal'@'172.16.0.2' (using password: YES)  sqlState=28000  sqlStateMarker=#]         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:199)         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:74)         ... 4 more ] 2018-06-27 14:20:30.659 [main] INFO  com.alibaba.otter.canal.deployer.CanalLauncher - ## the canal server is running now ...... 你要mysql创建一个canal用启名 GRANT SELECT  REPLICATION SLAVE  REPLICATION CLIENT ON *.* TO 'canal'@'%' IDENTIFIED BY 'canal';
720,canal.instance.filter.regex 这个属性值的长度有限制吗 如题：因为需求里同一个mysql，需要取的不同数据库，表也不同，我担心这个值越来越大 请问有长度限制吗 正则表达式有长度限制，我印象中是4k
719,为什么client连不上不同的instance 2018-06-27 11:05:51 165 INFO (ZooKeeper.java:438)- Initiating client connection  connectString=10.204.52.173:2181 10.204.52.174:2181 10.204.52.175:2181 sessionTimeout=90000 watcher=com.alibaba.otter.canal.common.zookeeper.ZkClientx@2ca65ce4 2018-06-27 11:05:51 184 INFO (ClientCnxn.java:1032)- Opening socket connection to server 10.204.52.174/10.204.52.174:2181. Will not attempt to authenticate using SASL (unknown error) 2018-06-27 11:05:51 186 INFO (ClientCnxn.java:876)- Socket connection established to 10.204.52.174/10.204.52.174:2181  initiating session 2018-06-27 11:05:51 190 INFO (ClientCnxn.java:1299)- Session establishment complete on server 10.204.52.174/10.204.52.174:2181  sessionid = 0x263fdc9445e0090  negotiated timeout = 40000 2018-06-27 11:05:51 191 INFO (ZkClient.java:449)- zookeeper state changed (SyncConnected) 我两个client连接同一个server的不同instance，为什么会卡住。 zookeeper state changed (SyncConnected) 
718,v1.0.26 alpha 3 怎么使用 v1.0.26 alpha 3 怎么使用 安装example 中配置了一下 数据写入到kafka 中不完全 漏数据还是？
717,ddl新增字段异常：com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Duplicate column name 'name2' 前提：配置了多个channel，各个channel的源表都是同一张表，目标表是另一个集群的同一个库的不同表。 mysql5.7.20 canal:1.0.26，ddl新增字段报错如下： 2018-06-26 15:02:25.895 [pipelineId = 15 taskName = LoadWorker] ERROR com.alibaba.otter.node.etl.load.LoadTask - [15] loadWork executor is error! data:EtlEventData[currNid=8 nextNid=8 desc=[MemoryPipeKey[identity=Identity[channelId=15 pipelineId=15 processId=89] time=1529996545875 dataType=DB_BATCH]] processId=89 startTime=1529996545271 endTime=<null> firstTime=1529996545000 batchId=6 number=1 size=<null> exts=<null> pipelineId=15] com.alibaba.otter.node.etl.load.exception.LoadException: java.util.concurrent.ExecutionException: com.alibaba.otter.node.etl.load.exception.LoadException: com.alibaba.otter.node.etl.load.exception.LoadException: org.springframework.jdbc.BadSqlGrammarException: StatementCallback; bad SQL grammar []; nested exception is com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Duplicate column name 'name2' Caused by: java.util.concurrent.ExecutionException: com.alibaba.otter.node.etl.load.exception.LoadException: com.alibaba.otter.node.etl.load.exception.LoadException: org.springframework.jdbc.BadSqlGrammarException: StatementCallback; bad SQL grammar []; nested exception is com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Duplicate column name 'name2'         at java.util.concurrent.FutureTask.report(FutureTask.java:122) [na:1.8.0_102]         at java.util.concurrent.FutureTask.get(FutureTask.java:192) [na:1.8.0_102]         at com.alibaba.otter.node.etl.load.loader.db.DataBatchLoader.load(DataBatchLoader.java:107) ~[node.etl-4.2.16-SNAPSHOT.jar:na]         at com.alibaba.otter.node.etl.load.loader.OtterLoaderFactory.load(OtterLoaderFactory.java:50) ~[node.etl-4.2.16-SNAPSHOT.jar:na]         at com.alibaba.otter.node.etl.load.LoadTask$1.run(LoadTask.java:85) ~[node.etl-4.2.16-SNAPSHOT.jar:na]         at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_102]         at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_102]         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_102]         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_102]         at java.lang.Thread.run(Thread.java:745) [na:1.8.0_102] Caused by: com.alibaba.otter.node.etl.load.exception.LoadException: com.alibaba.otter.node.etl.load.exception.LoadException: org.springframework.jdbc.BadSqlGrammarException: StatementCallback; bad SQL grammar []; nested exception is com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Duplicate column name 'name2' Caused by: com.alibaba.otter.node.etl.load.exception.LoadException: org.springframework.jdbc.BadSqlGrammarException: StatementCallback; bad SQL grammar []; nested exception is com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Duplicate column name 'name2' 估计重复执行了DDL，忽略掉当前的DDL异常可跳过 这个要如何忽略呢？我是otter配置实现的。otter只有“跳过select异常”和“跳过Load异常”，我们这上面报错的是load异常，但如果跳过load异常，则怕会把所有的load异常都跳过，而不是只针对ddl异常跳过。那这个要如何解呢？ 有一个忽略DDL异常的选项 跳过异常，会出现：没有同步过来。 比如：同一源表，同步到同一个目标库的不同表，目标表1同步成功，目标表2被跳过异常，就没有同步了。
716,tableMetaTSDB找位点进入无限循环中 在这个类**MysqlEventParser** 下面这个方法    **### /**      * 根据给定的时间戳，在指定的binlog中找到最接近于该时间戳(必须是小于时间戳)的一个事务起始位置。      * 针对最后一个binlog会给定endPosition，避免无尽的查询      */     private EntryPosition findAsPerTimestampInSpecificLogFile(MysqlConnection mysqlConnection                                                               final Long startTimestamp                                                               final EntryPosition endPosition                                                               final String searchBinlogFile                                                               final Boolean justForPositionTimestamp)** 我的配置文件里面指定了 canal.instance.master.journal.name=mysql-bin.000344 canal.instance.master.position=106199547 开启tsdb功能后 canal服务启动的时候 程序会进入这里找起始位点 由于没有找到这个位点程序会一致找下去甚至大于106199547 了还在找导致一直出于初始化阶段，然后就会出现 客户端无法创建cursor 。https://github.com/alibaba/canal/issues/715 是位点不存在还是？  我是通过这个命令找的   show binlog events in'mysql-bin.000344' mysql-bin.000344	106199500	Rotate	609344520	106199547	mysql-bin.000345;pos=4 这条是最后一条记录 然后去取了106199547 作为canal.instance.master.position 然后上面的源码就进入无限循环当中 你是拿到一个位点之后，一直没有新的数据么？ 有数据的 但是已经进入下一个文件的了。    106199547 是mysql-bin.000344的最后一个位点， 但是下面是有 mysql-bin.000345文件的 最新的26版本优化了，判断退出条件为<=
715,canal1.0.25   otter客户端启动后 在zookeeper 中不创建 cursor 不更新binlog位置信息  目前我们也遇到了这个问题 https://github.com/alibaba/canal/issues/541 禁用tsdb功能即可 1.0.25版本有一些问题，换1.0.26吧
714,filter不起作用 服务端指定了canal.instance.filter.regex  客户端connector.subscribe(); 但是服务端解析binlog时  遇到已经被drop的表(不需要同步的表)的相关日志时  还是会报错  就说desc db_name.table_name失败. 其实这些表是不会出现在canal.instance.filter.regex里的  不知道该如何解决?
713,CanalParseException: parse row data failed. CanalParseException: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first 同步过程突然报错，　mysql5.7.20，canal:1.0.25，相关安装包：manager.deployer-4.2.15.tar.gz，node.deployer-4.2.15.tar.gz(ps：这种canal嵌套里面，如果单独的canal有在高版本修复该问题的话，manage和node是否也会提供相应的修复版本？或者有没有提供manager/node/canal独立部署(canal非嵌套)的安装说明？~)。 报错信息如下： com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first         at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4832) ~[guava-18.0.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:160) ~[canal.parse-1.0.25.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:759) ~[canal.parse-1.0.25.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:428) ~[canal.parse-1.0.25.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:114) ~[canal.parse-1.0.25.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:66) ~[canal.parse-1.0.25.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:337) ~[canal.parse-1.0.25.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:184) ~[canal.parse-1.0.25.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:145) [canal.parse-1.0.25.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) [canal.parse-1.0.25.jar:na]         at java.lang.Thread.run(Thread.java:745) [na:1.8.0_102] Caused by: java.io.IOException: should execute connector.connect() first         at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.<init>(MysqlQueryExecutor.java:30) ~[canal.parse.driver-1.0.25.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:87) [canal.parse-1.0.25.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMetaByDB(TableMetaCache.java:80) ~[canal.parse-1.0.25.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:30) ~[canal.parse-1.0.25.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:55) ~[canal.parse-1.0.25.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:50) ~[canal.parse-1.0.25.jar:na]         at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) ~[guava-18.0.jar:na]         at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) ~[guava-18.0.jar:na]         at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) ~[guava-18.0.jar:na]         at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) ~[guava-18.0.jar:na]         at com.google.common.cache.LocalCache.get(LocalCache.java:3937) ~[guava-18.0.jar:na]         at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941) ~[guava-18.0.jar:na]         at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824) ~[guava-18.0.jar:na]         at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830) ~[guava-18.0.jar:na]         ... 10 common frames omitted 看下[Issue604](https://github.com/alibaba/canal/issues/640)。升级到26版本 好的，我试试先，谢谢
712,org.h2.message.DbException: General error: "java.lang.IllegalStateException: The file is locked: nio:/usr/local/canal/conf/business/h2.mv.db 版本1.0.25，canal服务启动的是2个instance，出现以下2个问题 1.canal server启动后会产生unauthenticated user 非认证用户连接 2.锁文件 2018-06-22 14:08:32 database: flush org.h2.message.DbException: General error: "java.lang.IllegalStateException: The file is locked: nio:/usr/local/canal/conf/business/h2.mv.db [1.4.196/7]" [50000-196] 	at org.h2.message.DbException.get(DbException.java:168) 	at org.h2.message.DbException.convert(DbException.java:295) 	at org.h2.mvstore.db.MVTableEngine$1.uncaughtException(MVTableEngine.java:95) 	at org.h2.mvstore.MVStore.panic(MVStore.java:378) 	at org.h2.mvstore.MVStore.<init>(MVStore.java:361) 	at org.h2.mvstore.MVStore$Builder.open(MVStore.java:2930) 	at org.h2.mvstore.db.MVTableEngine$Store.open(MVTableEngine.java:155) 	at org.h2.mvstore.db.MVTableEngine.init(MVTableEngine.java:100) 	at org.h2.engine.Database.getPageStore(Database.java:2476) 	at org.h2.engine.Database.open(Database.java:697) 	at org.h2.engine.Database.openDatabase(Database.java:276) 	at org.h2.engine.Database.<init>(Database.java:270) 	at org.h2.engine.Engine.openSession(Engine.java:64) 	at org.h2.engine.Engine.openSession(Engine.java:176) 	at org.h2.engine.Engine.createSessionAndValidate(Engine.java:154) 	at org.h2.engine.Engine.createSession(Engine.java:137) 	at org.h2.engine.Engine.createSession(Engine.java:27) 	at org.h2.engine.SessionRemote.connectEmbeddedOrServer(SessionRemote.java:354) 	at org.h2.jdbc.JdbcConnection.<init>(JdbcConnection.java:116) 	at org.h2.jdbc.JdbcConnection.<init>(JdbcConnection.java:100) 	at org.h2.Driver.connect(Driver.java:69) 	at com.alibaba.druid.pool.DruidAbstractDataSource.createPhysicalConnection(DruidAbstractDataSource.java:1510) 	at com.alibaba.druid.pool.DruidAbstractDataSource.createPhysicalConnection(DruidAbstractDataSource.java:1575) 	at com.alibaba.druid.pool.DruidDataSource$CreateConnectionThread.run(DruidDataSource.java:2450) Caused by: org.h2.jdbc.JdbcSQLException: General error: "java.lang.IllegalStateException: The file is locked: nio:/usr/local/canal/conf/business/h2.mv.db [1.4.196/7]" [50000-196] 	at org.h2.message.DbException.getJdbcSQLException(DbException.java:345) 	... 24 more Caused by: java.lang.IllegalStateException: The file is locked: nio:/usr/local/canal/conf/business/h2.mv.db [1.4.196/7] 	at org.h2.mvstore.DataUtils.newIllegalStateException(DataUtils.java:765) 	at org.h2.mvstore.FileStore.open(FileStore.java:173) 	at org.h2.mvstore.MVStore.<init>(MVStore.java:347) 	... 19 more @agapple  这错误还是第一次见，是否上一次的canal出现异常crash？ 这是在正式环境中第一次启动，就出现了这2种情况 @agapple  The file is locked，估计是重复启动同一个instance的，检查一下这个文件: /usr/local/canal/conf/business/h2.mv.db  对应的文件句柄被哪个进程已经打开了
711,canal连接阿里云RDS 相关rds的配置文件好像没有啊？另外canal.instance.rds.open.url=https://rds.aliyuncs.com/ 这个配置是针对外网的  是否有内网地址？ 这个是RDS binlog的openapi下载的配置，目前还未完全成型，如果需要使用可以了解一下代码 参考：https://github.com/alibaba/canal/wiki/FAQ
710,java.io.IOException: connect /xxx:3306 failure:java.io.IOException: Unexpected End 启动canal服务就报这个错，canal版本1.0.22 2018-06-19 11:26:44.358 [destination = example   address = /xxx:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser -  dump address /xxx:3306 has an error  retrying. caused by  com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: connect /xxx:3306 failure:java.io.IOException: Unexpected End Stream 	at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.readBytesAsBuffer(PacketManager.java:22) 	at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.readHeader(PacketManager.java:13) 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:199) 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:71) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:52) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:85) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:158) 	at java.lang.Thread.run(Thread.java:745) Caused by: java.io.IOException: connect /xxx:3306 failure:java.io.IOException: Unexpected End Stream 	at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.readBytesAsBuffer(PacketManager.java:22) 	at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.readHeader(PacketManager.java:13) 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:199) 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:71) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:52) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:85) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:158) 	at java.lang.Thread.run(Thread.java:745) 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:74) ~[canal.parse.driver-1.0.22.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:52) ~[canal.parse-1.0.22.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:85) ~[canal.parse-1.0.22.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:158) ~[canal.parse-1.0.22.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_11] 3306端口是通的 java.io.IOException: connect /xxx:3306 failure:java.io.IOException: Unexpected End Stream 可以试试1.0.26
709,增加canal-kafka-client的running状态监控 增加canal-kafka-client的running状态监控 kafka client运行集群的话也和canal client类似，先启动的先抢占，后启动的阻塞等待 基于 topic-groupId抢占，zookeeper中目录为： /otter/canal/topics/example/groupId tks @rewerma 针对kafka的使用，能写一个简单的说明文档给我么？群里有人咨询 @agapple   使用说明: 打包以后使用kafka/target下的canal文件夹作为服务端。conf/canal.properties 的配置修改 canal.withoutNetty = true，其他的配置项和原先一样。 新增conf/kafka.yml配置： servers: slave1:6667 slave2:6667 slave3:6667    # kafka server地址 retries: 0    # kafka 重试次数 batchSize: 16384    # kafka 批量大小 lingerMs: 1 bufferMemory: 33554432 canalBatchSize: 50    # canal的批次大小，单位 k;如果数据库写操作量大建议修改为1M filterTransactionEntry: true canalDestinations:   - canalDestination: example    # canal实例     topic: example    # kafka topic     partition:    # 分区     topics:       #  一个destination可以对应多个topic       - topic: example         partition: canal kafka client的使用参考 test 中的 com.alibaba.otter.canal.kafka.client.running.CanalKafkaClientExample 
708,数据处理失败，回滚后，会有重复处理的情况 因为是按 批次id   ack和rollback的，如果一个批次里的一条处理失败了，这个时候开始回滚，然后重新处理刚才处理失败的批次数据，如果那个批次的处理失败的之前的都是处理成功的，这次处理岂不是又重复处理了？ @agapple  重复无法避免 那只能在自己的代码中去重 我遇到过这个情况  我是是捕获异常后将当前批次的数据持久化到数据库记录异常信息. 然后正常提交该批次的数据  为了避免重复处理.  除了这个措施之外还需要避免处理了一半就异常崩溃的情况吧？
707,canal启动正常 canal客户端订阅卡住 错误日志：2018-06-20 13:43:46.730 [ZkClient-EventThread-26-vm153:2181 vm154:2181 vm155:2181] ERROR org.I0Itec.zkclient.ZkEventThread - Error handling event ZkEvent[Data of /otter/canal/destinations/yuantong_order_mysql_10.1.240.87_3307_0/1001/running changed sent to com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor$1@4e7d0621] com.alibaba.otter.canal.protocol.exception.CanalClientException: something goes wrong in initRunning method.  	at com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor.initRunning(ClientRunningMonitor.java:142) 	at com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor$1.handleDataDeleted(ClientRunningMonitor.java:71) 	at org.I0Itec.zkclient.ZkClient$6.run(ZkClient.java:549) 	at org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71) Caused by: com.alibaba.otter.canal.protocol.exception.CanalClientException: java.net.ConnectException: 拒绝连接 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.doConnect(SimpleCanalConnector.java:171) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.access$000(SimpleCanalConnector.java:48) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector$1.processActiveEnter(SimpleCanalConnector.java:396) 	at com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor.processActiveEnter(ClientRunningMonitor.java:207) 	at com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor.initRunning(ClientRunningMonitor.java:118) 	... 3 common frames omitted Caused by: java.net.ConnectException: 拒绝连接 	at sun.nio.ch.Net.connect0(Native Method) 	at sun.nio.ch.Net.connect(Net.java:454) 	at sun.nio.ch.Net.connect(Net.java:446) 	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.doConnect(SimpleCanalConnector.java:132) 	... 7 common frames omitted 有时候就会一直卡在这个错误上，需要重启才正常， 但有时候这个错误会出现，但最后能订阅成功，不需要重启，canal的版本是24的 我有时候也会出现这个问题  请问你解决了吗? 尝试一下最新的v1.1.1
706,为什么我启动两个java端的client连接canal server连不上 启动两个client连接canalserver 第二个启动时一直卡在 subcribe()那里，启动不成功
705,mysql5.7.22 canal 提示  java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Could not open log file 2018-06-20 12:02:29.240 [destination = dev   address = /192.168.255.2:3306   EventParser] INFO  c.a.otter.canal.parse.inbound.mysql.MysqlConnection - COM_BINLOG_DUMP with position:BinlogDumpCommandPacket[binlogPosition=309144234 slaveServerId=1201 binlogFileName=mysql-bin.000333 command=18] 2018-06-20 12:02:29.244 [destination = dev   address = /192.168.255.2:3306   EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Could not open log file 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) ~[canal.parse-1.0.25-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122) [canal.parse-1.0.25-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) [canal.parse-1.0.25-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171] 2018-06-20 12:02:29.244 [destination = dev   address = /192.168.255.2:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /192.168.255.2:3306 has an error  retrying. caused by java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Could not open log file 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) ~[canal.parse-1.0.25-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122) ~[canal.parse-1.0.25-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) ~[canal.parse-1.0.25-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171] 2018-06-20 12:02:29.244 [destination = dev   address = /192.168.255.2:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:dev[java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Could not open log file 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) 	at java.lang.Thread.run(Thread.java:748) ] 2018-06-20 12:02:29.244 [destination = dev   address = /192.168.255.2:3306   EventParser] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - disConnect MysqlConnection to /192.168.255.2:3306... Received error packet: errno = 1236  sqlstate = HY000 errmsg = Could not open log file 检查一下binlog文件和位点的有效性
704,当表设置的主键对字段长度进行限定时，canal不识别该列 CREATE TABLE `job_execution_script` (   `app_id` INT(11) NOT NULL   `job_id` INT(11) NOT NULL   `script_name` VARCHAR(512) NOT NULL DEFAULT ''   `committer_name` VARCHAR(128) NOT NULL DEFAULT ''   PRIMARY KEY (`app_id` `job_id` `script_name`(100) `committer_name`) ) ENGINE=INNODB DEFAULT CHARSET=utf8; 当表结构如上时，canal会报 “unknow column `script_name`(100)” 的ERROR~ 谢谢~ 异常栈为： 2018-06-20 10:51:32.960 [destination = ido001   address = ido001/180.137.128.151:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:ido001[java.lang.RuntimeException: unknow column : `script_name`(100) 	at com.alibaba.otter.canal.parse.inbound.TableMeta.getFieldMetaByName(TableMeta.java:73) 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.processTableElement(MemoryTableMeta.java:215) 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.parse(MemoryTableMeta.java:151) 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.find(MemoryTableMeta.java:106) 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.compareTableMetaDbAndMemory(DatabaseTableMeta.java:288) 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.applySnapshotToDB(DatabaseTableMeta.java:251) 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:129) 	at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) 	at java.lang.Thread.run(Thread.java:745) ] 你用的是最新的1.0.26版本么？这个我记得已经修复过 我用的是1.0.25版本的，这个问题还是存在~ 试试1.0.26版本
703,RDS的binlog保留周期 RDS的binlog默认保留18小时!如果我想回溯 重新从指定cursor消费 应该是找不到binlog文件或相应位置的! RDSbinlog保存在oss中，需要下载oss进行本地解析处理 参考：https://github.com/alibaba/canal/wiki/FAQ
702,指定Filter不起作用，还是能够获取到全库的监听数据。 `connector.subscribe("databaseName.tableName");` 这段代码我指定了filter，可还是能够收到所有表的binlog数据，请问是不是为哪里配置有问题？ 搜索一下历史issue filter使用的是正则表达式，你这个正则要转义一下才能用，比如你这个例子：databaseName.tableName需要转义成 > databaseName\\\\.tableName 才能用。"."在正则中代表任意字符。“\\.”含义才是点，由于单个反斜杠代表转义，因此也需要被转义，所以就变成上面那个样子了 看一下 FAQ : https://github.com/alibaba/canal/wiki/FAQ
701,canal 连续运行两天左右，就会出现 异常，不稳定 看不出来是什么影响的，canal 没有出错恢复的机制吗？ com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) [canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:145) [canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:87) [canal.parse-1.0.25.jar:na] Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first 	at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4832) ~[guava-18.0.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:759) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:160) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:428) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:114) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:66) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:337) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:184) ~[canal.parse-1.0.25.jar:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171] Caused by: java.io.IOException: should execute connector.connect() first 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.<init>(MysqlQueryExecutor.java:30) ~[canal.parse.driver-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMetaByDB(TableMetaCache.java:80) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:30) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:55) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:50) ~[canal.parse-1.0.25.jar:na] 	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) ~[guava-18.0.jar:na] 	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) ~[guava-18.0.jar:na] 	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) ~[guava-18.0.jar:na] 	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) ~[guava-18.0.jar:na] 	at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941) ~[guava-18.0.jar:na] 	at com.google.common.cache.LocalCache.get(LocalCache.java:3937) ~[guava-18.0.jar:na] 	at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824) ~[guava-18.0.jar:na] 	at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830) ~[guava-18.0.jar:na] 	... 10 common frames omitted 同时连接了3个数据库实例，只有其中一个实例down掉了，并且似乎没有办法恢复 1.0.26版本试试
700,降低kafka依赖版本，申明server的running为volatile tks
699,Could not find first log file name in binary log index file mysql:10.1.25-MariaDB canal:尝试了1.0.26alpha3、1.0.25 配置文件中，position未配置 已经翻遍了issues中类似问题解决方案，尝试删除canal meta文件，h2.mv.db文件，都不行 mysql侧发生变化后，canal example还先报这个： com.alibaba.fastsql.sql.parser.ParserException: syntax error  error in :'utf8 PAGE_CHECKSUM=0  pos 626  line 13  column 50  token IDENTIFIER PAGE_CHECKSUM 接下来报这个： Could not find first log file name in binary log index file 具体异常多贴一点 2018-06-15 11:30:11.805 [destination = example   address = /10.10.38.39:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just show master status 2018-06-15 11:32:09.338 [destination = example   address = /10.10.38.39:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TEMPORARY TABLE `ALL_PLUGINS` (   `PLUGIN_NAME` varchar(64) NOT NULL DEFAULT ''   `PLUGIN_VERSION` varchar(20) NOT NULL DEFAULT ''   `PLUGIN_STATUS` varchar(16) NOT NULL DEFAULT ''   `PLUGIN_TYPE` varchar(80) NOT NULL DEFAULT ''   `PLUGIN_TYPE_VERSION` varchar(20) NOT NULL DEFAULT ''   `PLUGIN_LIBRARY` varchar(64) DEFAULT NULL   `PLUGIN_LIBRARY_VERSION` varchar(20) DEFAULT NULL   `PLUGIN_AUTHOR` varchar(64) DEFAULT NULL   `PLUGIN_DESCRIPTION` longtext   `PLUGIN_LICENSE` varchar(80) NOT NULL DEFAULT ''   `LOAD_OPTION` varchar(64) NOT NULL DEFAULT ''   `PLUGIN_MATURITY` varchar(12) NOT NULL DEFAULT ''   `PLUGIN_AUTH_VERSION` varchar(80) DEFAULT NULL ) ENGINE=Aria DEFAULT CHARSET=utf8 PAGE_CHECKSUM=0 com.alibaba.fastsql.sql.parser.ParserException: syntax error  error in :'utf8 PAGE_CHECKSUM=0  pos 723  line 15  column 50  token IDENTIFIER PAGE_CHECKSUM 	at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:361) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] 	at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:498) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] 	at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] 	at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:84) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:32:09.340 [destination = example   address = /10.10.38.39:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TEMPORARY TABLE `COLUMNS` (   `TABLE_CATALOG` varchar(512) NOT NULL DEFAULT ''   `TABLE_SCHEMA` varchar(64) NOT NULL DEFAULT ''   `TABLE_NAME` varchar(64) NOT NULL DEFAULT ''   `COLUMN_NAME` varchar(64) NOT NULL DEFAULT ''   `ORDINAL_POSITION` bigint(21) unsigned NOT NULL DEFAULT '0'   `COLUMN_DEFAULT` longtext   `IS_NULLABLE` varchar(3) NOT NULL DEFAULT ''   `DATA_TYPE` varchar(64) NOT NULL DEFAULT ''   `CHARACTER_MAXIMUM_LENGTH` bigint(21) unsigned DEFAULT NULL   `CHARACTER_OCTET_LENGTH` bigint(21) unsigned DEFAULT NULL   `NUMERIC_PRECISION` bigint(21) unsigned DEFAULT NULL   `NUMERIC_SCALE` bigint(21) unsigned DEFAULT NULL   `DATETIME_PRECISION` bigint(21) unsigned DEFAULT NULL   `CHARACTER_SET_NAME` varchar(32) DEFAULT NULL   `COLLATION_NAME` varchar(32) DEFAULT NULL   `COLUMN_TYPE` longtext NOT NULL   `COLUMN_KEY` varchar(3) NOT NULL DEFAULT ''   `EXTRA` varchar(27) NOT NULL DEFAULT ''   `PRIVILEGES` varchar(80) NOT NULL DEFAULT ''   `COLUMN_COMMENT` varchar(1024) NOT NULL DEFAULT '' ) ENGINE=Aria DEFAULT CHARSET=utf8 PAGE_CHECKSUM=0 com.alibaba.fastsql.sql.parser.ParserException: syntax error  error in :'utf8 PAGE_CHECKSUM=0  pos 1078  line 22  column 50  token IDENTIFIER PAGE_CHECKSUM 	at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:361) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] 	at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:498) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] 	at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] 	at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:84) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:32:09.341 [destination = example   address = /10.10.38.39:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TEMPORARY TABLE `EVENTS` (   `EVENT_CATALOG` varchar(64) NOT NULL DEFAULT ''   `EVENT_SCHEMA` varchar(64) NOT NULL DEFAULT ''   `EVENT_NAME` varchar(64) NOT NULL DEFAULT ''   `DEFINER` varchar(189) NOT NULL DEFAULT ''   `TIME_ZONE` varchar(64) NOT NULL DEFAULT ''   `EVENT_BODY` varchar(8) NOT NULL DEFAULT ''   `EVENT_DEFINITION` longtext NOT NULL   `EVENT_TYPE` varchar(9) NOT NULL DEFAULT ''   `EXECUTE_AT` datetime DEFAULT NULL   `INTERVAL_VALUE` varchar(256) DEFAULT NULL   `INTERVAL_FIELD` varchar(18) DEFAULT NULL   `SQL_MODE` varchar(8192) NOT NULL DEFAULT ''   `STARTS` datetime DEFAULT NULL   `ENDS` datetime DEFAULT NULL   `STATUS` varchar(18) NOT NULL DEFAULT ''   `ON_COMPLETION` varchar(12) NOT NULL DEFAULT ''   `CREATED` datetime NOT NULL DEFAULT '0000-00-00 00:00:00'   `LAST_ALTERED` datetime NOT NULL DEFAULT '0000-00-00 00:00:00'   `LAST_EXECUTED` datetime DEFAULT NULL   `EVENT_COMMENT` varchar(64) NOT NULL DEFAULT ''   `ORIGINATOR` bigint(10) NOT NULL DEFAULT '0'   `CHARACTER_SET_CLIENT` varchar(32) NOT NULL DEFAULT ''   `COLLATION_CONNECTION` varchar(32) NOT NULL DEFAULT ''   `DATABASE_COLLATION` varchar(32) NOT NULL DEFAULT '' ) ENGINE=Aria DEFAULT CHARSET=utf8 PAGE_CHECKSUM=0 com.alibaba.fastsql.sql.parser.ParserException: syntax error  error in :'utf8 PAGE_CHECKSUM=0  pos 1234  line 26  column 50  token IDENTIFIER PAGE_CHECKSUM 	at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:361) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] 	at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:498) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] 	at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] 	at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:84) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:32:09.342 [destination = example   address = /10.10.38.39:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TEMPORARY TABLE `PARAMETERS` (   `SPECIFIC_CATALOG` varchar(512) NOT NULL DEFAULT ''   `SPECIFIC_SCHEMA` varchar(64) NOT NULL DEFAULT ''   `SPECIFIC_NAME` varchar(64) NOT NULL DEFAULT ''   `ORDINAL_POSITION` int(21) NOT NULL DEFAULT '0'   `PARAMETER_MODE` varchar(5) DEFAULT NULL   `PARAMETER_NAME` varchar(64) DEFAULT NULL   `DATA_TYPE` varchar(64) NOT NULL DEFAULT ''   `CHARACTER_MAXIMUM_LENGTH` int(21) DEFAULT NULL   `CHARACTER_OCTET_LENGTH` int(21) DEFAULT NULL   `NUMERIC_PRECISION` int(21) DEFAULT NULL   `NUMERIC_SCALE` int(21) DEFAULT NULL   `DATETIME_PRECISION` bigint(21) unsigned DEFAULT NULL   `CHARACTER_SET_NAME` varchar(64) DEFAULT NULL   `COLLATION_NAME` varchar(64) DEFAULT NULL   `DTD_IDENTIFIER` longtext NOT NULL   `ROUTINE_TYPE` varchar(9) NOT NULL DEFAULT '' ) ENGINE=Aria DEFAULT CHARSET=utf8 PAGE_CHECKSUM=0 com.alibaba.fastsql.sql.parser.ParserException: syntax error  error in :'utf8 PAGE_CHECKSUM=0  pos 853  line 18  column 50  token IDENTIFIER PAGE_CHECKSUM 	at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:361) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] 	at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:498) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] 	at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] 	at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:84) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:32:09.342 [destination = example   address = /10.10.38.39:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TEMPORARY TABLE `PARTITIONS` (   `TABLE_CATALOG` varchar(512) NOT NULL DEFAULT ''   `TABLE_SCHEMA` varchar(64) NOT NULL DEFAULT ''   `TABLE_NAME` varchar(64) NOT NULL DEFAULT ''   `PARTITION_NAME` varchar(64) DEFAULT NULL   `SUBPARTITION_NAME` varchar(64) DEFAULT NULL   `PARTITION_ORDINAL_POSITION` bigint(21) unsigned DEFAULT NULL   `SUBPARTITION_ORDINAL_POSITION` bigint(21) unsigned DEFAULT NULL   `PARTITION_METHOD` varchar(18) DEFAULT NULL   `SUBPARTITION_METHOD` varchar(12) DEFAULT NULL   `PARTITION_EXPRESSION` longtext   `SUBPARTITION_EXPRESSION` longtext   `PARTITION_DESCRIPTION` longtext   `TABLE_ROWS` bigint(21) unsigned NOT NULL DEFAULT '0'   `AVG_ROW_LENGTH` bigint(21) unsigned NOT NULL DEFAULT '0'   `DATA_LENGTH` bigint(21) unsigned NOT NULL DEFAULT '0'   `MAX_DATA_LENGTH` bigint(21) unsigned DEFAULT NULL   `INDEX_LENGTH` bigint(21) unsigned NOT NULL DEFAULT '0'   `DATA_FREE` bigint(21) unsigned NOT NULL DEFAULT '0'   `CREATE_TIME` datetime DEFAULT NULL   `UPDATE_TIME` datetime DEFAULT NULL   `CHECK_TIME` datetime DEFAULT NULL   `CHECKSUM` bigint(21) unsigned DEFAULT NULL   `PARTITION_COMMENT` varchar(80) NOT NULL DEFAULT ''   `NODEGROUP` varchar(12) NOT NULL DEFAULT ''   `TABLESPACE_NAME` varchar(64) DEFAULT NULL ) ENGINE=Aria DEFAULT CHARSET=utf8 PAGE_CHECKSUM=0 com.alibaba.fastsql.sql.parser.ParserException: syntax error  error in :'utf8 PAGE_CHECKSUM=0  pos 1323  line 27  column 50  token IDENTIFIER PAGE_CHECKSUM 	at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:361) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] 	at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:498) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] 	at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] 	at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:84) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:32:09.343 [destination = example   address = /10.10.38.39:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TEMPORARY TABLE `PLUGINS` (   `PLUGIN_NAME` varchar(64) NOT NULL DEFAULT ''   `PLUGIN_VERSION` varchar(20) NOT NULL DEFAULT ''   `PLUGIN_STATUS` varchar(16) NOT NULL DEFAULT ''   `PLUGIN_TYPE` varchar(80) NOT NULL DEFAULT ''   `PLUGIN_TYPE_VERSION` varchar(20) NOT NULL DEFAULT ''   `PLUGIN_LIBRARY` varchar(64) DEFAULT NULL   `PLUGIN_LIBRARY_VERSION` varchar(20) DEFAULT NULL   `PLUGIN_AUTHOR` varchar(64) DEFAULT NULL   `PLUGIN_DESCRIPTION` longtext   `PLUGIN_LICENSE` varchar(80) NOT NULL DEFAULT ''   `LOAD_OPTION` varchar(64) NOT NULL DEFAULT ''   `PLUGIN_MATURITY` varchar(12) NOT NULL DEFAULT ''   `PLUGIN_AUTH_VERSION` varchar(80) DEFAULT NULL ) ENGINE=Aria DEFAULT CHARSET=utf8 PAGE_CHECKSUM=0 com.alibaba.fastsql.sql.parser.ParserException: syntax error  error in :'utf8 PAGE_CHECKSUM=0  pos 719  line 15  column 50  token IDENTIFIER PAGE_CHECKSUM 	at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:361) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] 	at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:498) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] 	at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] 	at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:84) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:32:09.344 [destination = example   address = /10.10.38.39:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TEMPORARY TABLE `PROCESSLIST` (   `ID` bigint(4) NOT NULL DEFAULT '0'   `USER` varchar(128) NOT NULL DEFAULT ''   `HOST` varchar(64) NOT NULL DEFAULT ''   `DB` varchar(64) DEFAULT NULL   `COMMAND` varchar(16) NOT NULL DEFAULT ''   `TIME` int(7) NOT NULL DEFAULT '0'   `STATE` varchar(64) DEFAULT NULL   `INFO` longtext   `TIME_MS` decimal(22 3) NOT NULL DEFAULT '0.000'   `STAGE` tinyint(2) NOT NULL DEFAULT '0'   `MAX_STAGE` tinyint(2) NOT NULL DEFAULT '0'   `PROGRESS` decimal(7 3) NOT NULL DEFAULT '0.000'   `MEMORY_USED` int(7) NOT NULL DEFAULT '0'   `EXAMINED_ROWS` int(7) NOT NULL DEFAULT '0'   `QUERY_ID` bigint(4) NOT NULL DEFAULT '0'   `INFO_BINARY` blob   `TID` bigint(4) NOT NULL DEFAULT '0' ) ENGINE=Aria DEFAULT CHARSET=utf8 PAGE_CHECKSUM=0 com.alibaba.fastsql.sql.parser.ParserException: syntax error  error in :'utf8 PAGE_CHECKSUM=0  pos 774  line 19  column 50  token IDENTIFIER PAGE_CHECKSUM 	at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:361) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] 	at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:498) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] 	at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] 	at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:84) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:32:09.344 [destination = example   address = /10.10.38.39:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TEMPORARY TABLE `ROUTINES` (   `SPECIFIC_NAME` varchar(64) NOT NULL DEFAULT ''   `ROUTINE_CATALOG` varchar(512) NOT NULL DEFAULT ''   `ROUTINE_SCHEMA` varchar(64) NOT NULL DEFAULT ''   `ROUTINE_NAME` varchar(64) NOT NULL DEFAULT ''   `ROUTINE_TYPE` varchar(9) NOT NULL DEFAULT ''   `DATA_TYPE` varchar(64) NOT NULL DEFAULT ''   `CHARACTER_MAXIMUM_LENGTH` int(21) DEFAULT NULL   `CHARACTER_OCTET_LENGTH` int(21) DEFAULT NULL   `NUMERIC_PRECISION` int(21) DEFAULT NULL   `NUMERIC_SCALE` int(21) DEFAULT NULL   `DATETIME_PRECISION` bigint(21) unsigned DEFAULT NULL   `CHARACTER_SET_NAME` varchar(64) DEFAULT NULL   `COLLATION_NAME` varchar(64) DEFAULT NULL   `DTD_IDENTIFIER` longtext   `ROUTINE_BODY` varchar(8) NOT NULL DEFAULT ''   `ROUTINE_DEFINITION` longtext   `EXTERNAL_NAME` varchar(64) DEFAULT NULL   `EXTERNAL_LANGUAGE` varchar(64) DEFAULT NULL   `PARAMETER_STYLE` varchar(8) NOT NULL DEFAULT ''   `IS_DETERMINISTIC` varchar(3) NOT NULL DEFAULT ''   `SQL_DATA_ACCESS` varchar(64) NOT NULL DEFAULT ''   `SQL_PATH` varchar(64) DEFAULT NULL   `SECURITY_TYPE` varchar(7) NOT NULL DEFAULT ''   `CREATED` datetime NOT NULL DEFAULT '0000-00-00 00:00:00'   `LAST_ALTERED` datetime NOT NULL DEFAULT '0000-00-00 00:00:00'   `SQL_MODE` varchar(8192) NOT NULL DEFAULT ''   `ROUTINE_COMMENT` longtext NOT NULL   `DEFINER` varchar(189) NOT NULL DEFAULT ''   `CHARACTER_SET_CLIENT` varchar(32) NOT NULL DEFAULT ''   `COLLATION_CONNECTION` varchar(32) NOT NULL DEFAULT ''   `DATABASE_COLLATION` varchar(32) NOT NULL DEFAULT '' ) ENGINE=Aria DEFAULT CHARSET=utf8 PAGE_CHECKSUM=0 com.alibaba.fastsql.sql.parser.ParserException: syntax error  error in :'utf8 PAGE_CHECKSUM=0  pos 1603  line 33  column 50  token IDENTIFIER PAGE_CHECKSUM 	at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:361) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] 	at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:498) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] 	at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] 	at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:84) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:32:09.345 [destination = example   address = /10.10.38.39:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TEMPORARY TABLE `SYSTEM_VARIABLES` (   `VARIABLE_NAME` varchar(64) NOT NULL DEFAULT ''   `SESSION_VALUE` varchar(2048) DEFAULT NULL   `GLOBAL_VALUE` varchar(2048) DEFAULT NULL   `GLOBAL_VALUE_ORIGIN` varchar(64) NOT NULL DEFAULT ''   `DEFAULT_VALUE` varchar(2048) DEFAULT NULL   `VARIABLE_SCOPE` varchar(64) NOT NULL DEFAULT ''   `VARIABLE_TYPE` varchar(64) NOT NULL DEFAULT ''   `VARIABLE_COMMENT` varchar(2048) NOT NULL DEFAULT ''   `NUMERIC_MIN_VALUE` varchar(21) DEFAULT NULL   `NUMERIC_MAX_VALUE` varchar(21) DEFAULT NULL   `NUMERIC_BLOCK_SIZE` varchar(21) DEFAULT NULL   `ENUM_VALUE_LIST` longtext   `READ_ONLY` varchar(3) NOT NULL DEFAULT ''   `COMMAND_LINE_ARGUMENT` varchar(64) DEFAULT NULL ) ENGINE=Aria DEFAULT CHARSET=utf8 PAGE_CHECKSUM=0 com.alibaba.fastsql.sql.parser.ParserException: syntax error  error in :'utf8 PAGE_CHECKSUM=0  pos 768  line 16  column 50  token IDENTIFIER PAGE_CHECKSUM 	at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:361) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] 	at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:498) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] 	at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] 	at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:84) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:32:09.346 [destination = example   address = /10.10.38.39:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TEMPORARY TABLE `TRIGGERS` (   `TRIGGER_CATALOG` varchar(512) NOT NULL DEFAULT ''   `TRIGGER_SCHEMA` varchar(64) NOT NULL DEFAULT ''   `TRIGGER_NAME` varchar(64) NOT NULL DEFAULT ''   `EVENT_MANIPULATION` varchar(6) NOT NULL DEFAULT ''   `EVENT_OBJECT_CATALOG` varchar(512) NOT NULL DEFAULT ''   `EVENT_OBJECT_SCHEMA` varchar(64) NOT NULL DEFAULT ''   `EVENT_OBJECT_TABLE` varchar(64) NOT NULL DEFAULT ''   `ACTION_ORDER` bigint(4) NOT NULL DEFAULT '0'   `ACTION_CONDITION` longtext   `ACTION_STATEMENT` longtext NOT NULL   `ACTION_ORIENTATION` varchar(9) NOT NULL DEFAULT ''   `ACTION_TIMING` varchar(6) NOT NULL DEFAULT ''   `ACTION_REFERENCE_OLD_TABLE` varchar(64) DEFAULT NULL   `ACTION_REFERENCE_NEW_TABLE` varchar(64) DEFAULT NULL   `ACTION_REFERENCE_OLD_ROW` varchar(3) NOT NULL DEFAULT ''   `ACTION_REFERENCE_NEW_ROW` varchar(3) NOT NULL DEFAULT ''   `CREATED` datetime DEFAULT NULL   `SQL_MODE` varchar(8192) NOT NULL DEFAULT ''   `DEFINER` varchar(189) NOT NULL DEFAULT ''   `CHARACTER_SET_CLIENT` varchar(32) NOT NULL DEFAULT ''   `COLLATION_CONNECTION` varchar(32) NOT NULL DEFAULT ''   `DATABASE_COLLATION` varchar(32) NOT NULL DEFAULT '' ) ENGINE=Aria DEFAULT CHARSET=utf8 PAGE_CHECKSUM=0 com.alibaba.fastsql.sql.parser.ParserException: syntax error  error in :'utf8 PAGE_CHECKSUM=0  pos 1228  line 24  column 50  token IDENTIFIER PAGE_CHECKSUM 	at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:361) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] 	at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:498) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] 	at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] 	at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:84) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:32:09.347 [destination = example   address = /10.10.38.39:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TEMPORARY TABLE `VIEWS` (   `TABLE_CATALOG` varchar(512) NOT NULL DEFAULT ''   `TABLE_SCHEMA` varchar(64) NOT NULL DEFAULT ''   `TABLE_NAME` varchar(64) NOT NULL DEFAULT ''   `VIEW_DEFINITION` longtext NOT NULL   `CHECK_OPTION` varchar(8) NOT NULL DEFAULT ''   `IS_UPDATABLE` varchar(3) NOT NULL DEFAULT ''   `DEFINER` varchar(189) NOT NULL DEFAULT ''   `SECURITY_TYPE` varchar(7) NOT NULL DEFAULT ''   `CHARACTER_SET_CLIENT` varchar(32) NOT NULL DEFAULT ''   `COLLATION_CONNECTION` varchar(32) NOT NULL DEFAULT ''   `ALGORITHM` varchar(10) NOT NULL DEFAULT '' ) ENGINE=Aria DEFAULT CHARSET=utf8 PAGE_CHECKSUM=0 com.alibaba.fastsql.sql.parser.ParserException: syntax error  error in :'utf8 PAGE_CHECKSUM=0  pos 626  line 13  column 50  token IDENTIFIER PAGE_CHECKSUM 	at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:361) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] 	at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:498) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] 	at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] 	at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:84) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:32:17.341 [destination = example   address = /10.10.38.39:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000001×{G? position=4239821 serverId=3839 gtid=<null> timestamp=1529033212000] 2018-06-15 11:32:18.653 [destination = example   address = /10.10.38.39:3306   EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Could not find first log file name in binary log index file 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:32:18.655 [destination = example   address = /10.10.38.39:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /10.10.38.39:3306 has an error  retrying. caused by  java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Could not find first log file name in binary log index file 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:32:18.658 [destination = example   address = /10.10.38.39:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Could not find first log file name in binary log index file 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) 	at java.lang.Thread.run(Thread.java:748) ] 2018-06-15 11:32:30.620 [destination = example   address = /10.10.38.39:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just show master status 2018-06-15 11:32:45.110 [destination = example   address = /10.10.38.39:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000001×{G? position=4240327 serverId=3839 gtid=<null> timestamp=1529033528000] 2018-06-15 11:32:45.287 [destination = example   address = /10.10.38.39:3306   EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Could not find first log file name in binary log index file 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:32:45.288 [destination = example   address = /10.10.38.39:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /10.10.38.39:3306 has an error  retrying. caused by  java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Could not find first log file name in binary log index file 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:32:45.289 [destination = example   address = /10.10.38.39:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Could not find first log file name in binary log index file 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) 	at java.lang.Thread.run(Thread.java:748) ] 2018-06-15 11:32:56.095 [destination = example   address = /10.10.38.39:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just show master status 2018-06-15 11:35:34.307 [destination = example   address = /10.10.38.39:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000001×{G? position=4240471 serverId=3839 gtid=<null> timestamp=1529033565000] 2018-06-15 11:35:34.724 [destination = example   address = /10.10.38.39:3306   EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Could not find first log file name in binary log index file 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:35:34.725 [destination = example   address = /10.10.38.39:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /10.10.38.39:3306 has an error  retrying. caused by  java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Could not find first log file name in binary log index file 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:35:34.725 [destination = example   address = /10.10.38.39:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Could not find first log file name in binary log index file 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) 	at java.lang.Thread.run(Thread.java:748) ] 2018-06-15 11:35:52.169 [destination = example   address = /10.10.38.39:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just show master status 2018-06-15 11:50:06.738 [destination = example   address = /10.10.38.39:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000001×{G? position=4240821 serverId=3839 gtid=<null> timestamp=1529033734000] 2018-06-15 11:50:07.002 [destination = example   address = /10.10.38.39:3306   EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Could not find first log file name in binary log index file 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:50:07.003 [destination = example   address = /10.10.38.39:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /10.10.38.39:3306 has an error  retrying. caused by  java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Could not find first log file name in binary log index file 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:50:07.006 [destination = example   address = /10.10.38.39:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Could not find first log file name in binary log index file 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) 	at java.lang.Thread.run(Thread.java:748) ] 2018-06-15 11:50:24.984 [destination = example   address = /10.10.38.39:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just show master status 2018-06-15 11:51:40.902 [destination = example   address = /10.10.38.39:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000001×{G? position=4241171 serverId=3839 gtid=<null> timestamp=1529034606000] 2018-06-15 11:51:41.216 [destination = example   address = /10.10.38.39:3306   EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Could not find first log file name in binary log index file 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:51:41.217 [destination = example   address = /10.10.38.39:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /10.10.38.39:3306 has an error  retrying. caused by  java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Could not find first log file name in binary log index file 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] PAGE_CHECKSUM 这是mariadb的关键字吧？ 是mariadb的关键字 看这个报错，是创建本地内存表的时候，不能识别这个关键字。。。 如果用mysql替换掉内存数据库，需要怎么搞 我把tsdb关了，目前正常了。。。 在instance.properties中，配置了canal.instance.defaultDatabaseName=aaa，还是会收到当前实例上，所有数据库的变更事件，那配置这个default的意义是？ 这个default没啥意义 I have seen the same problem.There is my log: environment:mysql5.7.16-log Source distribution  canal:canal.deployer-1.0.24 2018-07-11 16:44:26.054 [destination = example   address = /***.***.**.**:3307   EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Could not find first log file name in binary log index file 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) ~[canal.parse-1.0.22.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) [canal.parse-1.0.22.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210) [canal.parse-1.0.22.jar:na] 	at java.lang.Thread.run(Unknown Source) [na:1.8.0_161] 2018-07-11 16:44:26.054 [destination = example   address = /***.***.**.**:3307   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /***.***.**.**:3307 has an error  retrying. caused by  java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Could not find first log file name in binary log index file 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) ~[canal.parse-1.0.22.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) ~[canal.parse-1.0.22.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210) ~[canal.parse-1.0.22.jar:na] 	at java.lang.Thread.run(Unknown Source) [na:1.8.0_161] 2018-07-11 16:44:26.055 [destination = example   address = /***.***.**.**:3307   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Could not find first log file name in binary log index file 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210) 	at java.lang.Thread.run(Unknown Source) ] sqlstate = HY000 errmsg = Could not find first log file name in binary log index file    binlog被删除 but I can execute the sql to find out that file show binary logs 
698,ClientRunningMonitor shutdown delayExector threadpool #697 #697 tks
697,ClientRunningMonitor shutdown delayExector threadpool Version：1.0.26.alpha2 当client在cluster模式下发生ha切换时，com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor 会submit job到delayExector，必须在stop方法里执行delayExector.shutdown()，否则不能优雅shutdown应用 一会儿提交pr，please assign the issue to me. Thanx.
696,使用canal1.0.24和1.0.25的问题 spring场景： default-instance.xml 1.0.24版本的connector.subscribe（）方法有时会卡住，而且得不到binlog的订阅消息。zookeeper上的信息一切正常，但就是消费不了binlog消息。 1.0.25版本的则不会有这个问题。 现在生产环境上使用的是1.0.24，想问下，这个是1.0.24已有的bug吗 杀个thread dump看一下卡在哪里。一般是没有获得zk上的running锁。是几个客户端节点？ @nbqyqx 就一个客户端，现在不用zk，直连也是收不到binlog消息，换1.025就能收到了。1.0.24现在什么都做不了 2018-06-15 10:32:57.010 [destination = szctest   address = /47.97.185.74:3306   EventParser] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - connect MysqlConnection to /47.97.185.74:3306... 2018-06-15 10:32:57.072 [destination = szctest   address = /47.97.185.74:3306   EventParser] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - handshake initialization packet received  prepare the client authentication packet to send 2018-06-15 10:32:57.073 [destination = szctest   address = /47.97.185.74:3306   EventParser] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - client authentication packet is sent out. 2018-06-15 10:32:57.105 [destination = szctest   address = /47.97.185.74:3306   EventParser] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - disConnect MysqlConnection to /47.97.185.74:3306... 2018-06-15 10:32:57.105 [destination = szctest   address = /47.97.185.74:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /47.97.185.74:3306 has an error  retrying. caused by  com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: connect /47.97.185.74:3306 failure:java.io.IOException: Error When doing Client Authentication:ErrorPacket [errorNumber=1045  fieldCount=-1  message=Access denied for user 'retl'@'113.116.141.191' (using password: YES)  sqlState=28000  sqlStateMarker=#] 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:208) 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:71) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:56) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:86) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:157) 	at java.lang.Thread.run(Unknown Source) Caused by: java.io.IOException: connect /47.97.185.74:3306 failure:java.io.IOException: Error When doing Client Authentication:ErrorPacket [errorNumber=1045  fieldCount=-1  message=Access denied for user 'retl'@'113.116.141.191' (using password: YES)  sqlState=28000  sqlStateMarker=#] 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:208) 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:71) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:56) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:86) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:157) 	at java.lang.Thread.run(Unknown Source) 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:74) ~[canal.parse.driver-1.0.24.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:56) ~[canal.parse-1.0.24.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:86) ~[canal.parse-1.0.24.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:157) ~[canal.parse-1.0.24.jar:na] 	at java.lang.Thread.run(Unknown Source) [na:1.8.0_172] 无语啊/47.97.185.74:3306  这个是我的mysql   但这个是啥  'retl'@'113.116.141.191'  找到原因了，是我的instance里的配置和default-instance.xml里的配置不一样， 我的instance里是canal.instance.master.dbUsername， 而xml里用的是${canal.instance.dbUsername} 之前 改group时 没弄回来
695,增加kafka对canal binlog信息的生成消费支持 1.在canal server端直接订阅destination获取message发送到kafka 2.在canal.properties中增加canal.withoutNetty属性，可以关闭server端的netty服务 3增加kafka的client来直接消费message数据 原来的 canal server -- netty -- message --> nio -- canal client 架构 通过kafka支持的架构： canal server -- kafka prducer -- message --> KAFKA --> kafka consumer -- canal kafka client  Great support! Looking forward to accept the patch. https://github.com/alibaba/canal/issues/669 非常nice的PR，我会仔细看一下 已经合并 效率真高 ：）
694,canal解析速度问题 一个库里面今天40分钟内我们做了2kw数据插入操作，导致大量binglog生成，但是canal解析速度非常慢，话了将近6个小时才追上实际位点。不知道这个如何调优？ 参考FAQ里的性能：https://github.com/alibaba/canal/wiki/FAQ
693,监听不到binlog 使用1.0.23监听的时候，mysql的binlog配置log-bin=mysql186-bin这种结构，监听不到。请问是否和binlog文件名称有关系？ 还没人反馈过类似问题，请描述重现步骤 应该是我来回切换集群模式造成的，清理zk下生成的文件就好了
692,指定位点进行同步的的时候使用时间戳指定的问题 如果只使用时间戳指定，不使用binlog文件名辅助，代码会去获取头尾位置的binlog文件，这个时候如果dba粗暴管理或者其他情况导致binlog文件被物理删除，会获取不到binlog起始文件导致报错。 关键代码： ``` class MysqlEventParser 。。。 // 根据时间查找binlog位置     private EntryPosition findByStartTimeStamp(MysqlConnection mysqlConnection  Long startTimestamp) {         EntryPosition endPosition = findEndPosition(mysqlConnection);         EntryPosition startPosition = findStartPosition(mysqlConnection); 。。。   /* 查询当前的binlog位置      */     private EntryPosition findStartPosition(MysqlConnection mysqlConnection) {         try {             ResultSetPacket packet = mysqlConnection.query("show binlog events limit 1");             List<String> fields = packet.getFieldValues();             if (CollectionUtils.isEmpty(fields)) {                 throw new CanalParseException("command : 'show binlog events limit 1' has an error! pls check. you need (at least one of) the SUPER REPLICATION CLIENT privilege(s) for this operation");             }             EntryPosition endPosition = new EntryPosition(fields.get(0)  Long.valueOf(fields.get(1)));             return endPosition;         } catch (IOException e) {             throw new CanalParseException("command : 'show binlog events limit 1' has an error!"  e);         }     } ```   show binlog events limit 1 的时候会报错： > MySQL [(none)]> show binlog events limit 1; > ERROR 29 (HY000): File './mysqlmaster-bin.000001' not found (Errcode: 2 - No such file or directory) 是不是使用 show binary logs; 命令获取 size 大于 0 的文件，然后再使用 show binlog events 来获取起始位点信息会好一点？ 可以考虑提交一个PR给我
691,java集群链接的时候报错了 com.alibaba.otter.canal.protocol.exception.CanalClientException: java.io.IOException: end of stream when reading header 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:277) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:248) 	at com.alibaba.otter.canal.client.impl.ClusterCanalConnector.getWithoutAck(ClusterCanalConnector.java:174) 	at com.dangdang.canal.DDPimJob.run(DDPimJob.java:55) 	at java.lang.Thread.run(Unknown Source) Caused by: java.io.IOException: end of stream when reading header 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.read(SimpleCanalConnector.java:374) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.readNextPacket(SimpleCanalConnector.java:365) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.receiveMessages(SimpleCanalConnector.java:282) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:275) 你还是报一下版本号吧，我觉得应该加个模板的。。。。不然各式各样的问题。。。。 看你的样子使用的是25版本的
690,canal.instance.filter.regex支持匹配多个schema吗 比如我有3个数据库，分库了。分别是pay_n_1  pay_n_2  pay_n_3，其它库不是我关心的，我只关心这三个。 canal.instance.filter.regex 怎么写，我写成canal.instance.filter.regex=pay_n_\*\\..\*  不起作用 pay_n_\\\w+\\..\*  这样就行了
689,canal实例要手动创建吗？ 如题，实例要每次手动创建吗？如果自动配置 应该怎么配置。每次好像只能自动生成一个文件夹，但是客户端连接时会报错
688,canal.instance.defaultDatabaseName设置成指定的库后，为啥其它库的增删改也能收到消息 如题，我只指定了一个instance，instance里只指定了一个库，但是消费时，所有库的改变都能监听到 查一下历史的issue @agapple 谢谢
687,windows上的canal server会不时的报错：远程主机强迫关闭了一个现有的连接。 报错的日志是instance的日志 canal日志没有报错；stack日志如下： 2018-06-11 19:21:30.110 [New I/O server worker #1-1] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x0c80035e  /10.204.241.135:58426 => /10.204.241.135:11111]  exception=java.io.IOException: 远程主机强迫关闭了一个现有的连接。 	at sun.nio.ch.SocketDispatcher.write0(Native Method) 	at sun.nio.ch.SocketDispatcher.write(Unknown Source) 	at sun.nio.ch.IOUtil.writeFromNativeBuffer(Unknown Source) 	at sun.nio.ch.IOUtil.write(Unknown Source) 	at sun.nio.ch.SocketChannelImpl.write(Unknown Source) 	at org.jboss.netty.channel.socket.nio.SocketSendBufferPool$PooledSendBuffer.transferTo(SocketSendBufferPool.java:243) 	at org.jboss.netty.channel.socket.nio.NioWorker.write0(NioWorker.java:470) 	at org.jboss.netty.channel.socket.nio.NioWorker.writeFromUserCode(NioWorker.java:388) 	at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:137) 	at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) 	at org.jboss.netty.channel.Channels.write(Channels.java:611) 	at org.jboss.netty.channel.Channels.write(Channels.java:578) 	at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:28) 	at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:144) 	at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) 	at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:444) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261) 	at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:350) 	at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) 	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) 	at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) 	at java.lang.Thread.run(Unknown Source) 主要还是超时机制导致，26版本会做一下优化 https://github.com/alibaba/canal/issues/640，关注一下这个
686,canal server swap被占满，导致磁盘空间100%使用 如题，v1.0.26 alpha 2版本 canal server swap被占满，导致磁盘空间100%使用 swap内存？ %Cpu(s):  0.2 us   0.2 sy   0.0 ni  99.6 id   0.0 wa   0.0 hi   0.0 si   0.0 st KiB Mem:   8193776 total   8050240 used    143536 free     87860 buffers KiB Swap:        0 total         0 used         0 free   6344652 cached canal server的这个swap cached 只增不减，导致最终磁盘空间使用率为100% 当前：rootfs           60G   12G   46G  20% / 重启canal server以后，swap cached为0，磁盘空间恢复正常，然后就开始下一轮只增不减的过程了
685,数据库连接问题 @agapple  你好!我现在基于V25 取消了客户端 改为eventStore2Kafka;现在数据库连接出了问题!你能帮忙看一下吗? ![gf vm9d xhrh z s5 f ps](https://user-images.githubusercontent.com/24663485/41216549-63ed2b40-6d87-11e8-99d9-d1c750d0d7d9.png) 
684,开启detecting，经常报Connection reset by peer 看到其他issue说建议开启心跳，所以把它开启 canal.instance.detecting.enable =true canal.instance.detecting.heartbeatHaEnable = true 结果很容易出错，报错时不影响数据捕获，第二天再看就无法捕获数据。关闭后正常。 1.0.25版本，我的这两个参数都是false，还是不稳定，我这是客户端间隔一段时间连，服务端就会出Connection reset by peer，但也不是必出现，在我们的α环境出，在我本地测试了一下午，就不出这个错 建议更新到26版本的 https://github.com/alibaba/canal/issues/640，关注一下这个
683,求问：v1.0.26 alpha 2版本出现消费堆积，并且server端磁盘被占满 如题，求问，在v1.0.26 alpha 2版本，昨天有大概四个小时的时间，发现canal消费出现延迟(即当前消费的数据主要是update数据，出现延迟，是两小时前已经更新的数据库，然后canal client才消费到。（insert数据没有问题）)另外canal server的磁盘不知道是被那个文件给占满了。重启server后，磁盘空间恢复正常，但当时服务并未恢复，一段时间后自动恢复了。也没有发现明显的报错，所以想请教下大家有没有遇到这样的问题，以及出现问题的原因是什么呢，多谢
682,Fix #631 Boolean java type support tks
681,fix issue #680 [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=681) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=681) before we can accept your contribution.<br/><hr/>**winger** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=681) it.</sub> [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=681) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=681) before we can accept your contribution.<br/><hr/>**winger** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=681) it.</sub> 这块主干已经修复了，非这样的改动模式
680,当table 的unique key 是"非标准"字段名时，tsdb解析出错 如下图所示表结构: ![image](https://user-images.githubusercontent.com/33280738/41092395-58152322-6a7b-11e8-9d54-3208c4863e27.png) canal 通过fastsql 解析出来的结果如下： ![image](https://user-images.githubusercontent.com/33280738/41092506-9d2a60bc-6a7b-11e8-918b-407415af0e6d.png) 而此时 canal 通过 `name(200)`比对内存与实际表字段时就会出现字段名对应不上的问题。 下载最新的1.0.26发布包
679,Merge pull request #1 from alibaba/master merge from master [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=679) <br/>All committers have signed the CLA. [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=679) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=679) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=679) it.</sub>
678,com.alibaba.otter.canal.parse.exception.CanalParseException: can't find start position for example ![image](https://user-images.githubusercontent.com/9798724/41083138-0893606a-6a63-11e8-89e7-9fad84702144.png) ![image](https://user-images.githubusercontent.com/9798724/41083182-1f12c8f8-6a63-11e8-81f8-66bb0c8e52d3.png) 找不到合适的位点
677,canal.instance.gtidon 该有缺省值，不能强制要求配置保证向前兼容 版本: v1.0.26 alpha 3 2018-06-07 12:25:29.861 [main] ERROR c.a.o.c.common.zookeeper.running.ServerRunningMonitor - start failed com.google.common.collect.ComputationException: com.alibaba.otter.canal.common.CanalException: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'instance' defined in class path resource [spring/default-instance.xml]: Cannot resolve reference to bean 'eventParser' while setting bean property 'eventParser'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'eventParser' defined in class path resource [spring/default-instance.xml]: Initialization of bean failed; nested exception is org.springframework.beans.TypeMismatchException: Failed to convert property value of type 'java.lang.String' to required type 'boolean' for property 'isGTIDMode'; nested exception is java.lang.IllegalArgumentException: Invalid boolean value []         at com.google.common.collect.MapMaker$ComputingMapAdapter.get(MapMaker.java:889) ~[guava-18.0.jar:na]         at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.start(CanalServerWithEmbedded.java:98) ~[canal.server-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.deployer.CanalController$2$1.processActiveEnter(CanalController.java:128) ~[canal.deployer-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.common.zookeeper.running.ServerRunningMonitor.processActiveEnter(ServerRunningMonitor.java:245) ~[canal.common-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.common.zookeeper.running.ServerRunningMonitor.initRunning(ServerRunningMonitor.java:150) ~[canal.common-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.common.zookeeper.running.ServerRunningMonitor.start(ServerRunningMonitor.java:104) ~[canal.common-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.deployer.CanalController.start(CanalController.java:415) [canal.deployer-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.deployer.CanalLauncher.main(CanalLauncher.java:38) [canal.deployer-1.0.26-SNAPSHOT.jar:na] Caused by: com.alibaba.otter.canal.common.CanalException: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'instance' defined in class path resource [spring/default-instance.xml]: Cannot resolve reference to bean 'eventParser' while setting bean property 'eventParser'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'eventParser' defined in class path resource [spring/default-instance.xml]: Initialization of bean failed; nested exception is org.springframework.beans.TypeMismatchException: Failed to convert property value of type 'java.lang.String' to required type 'boolean' for property 'isGTIDMode'; nested exception is java.lang.IllegalArgumentException: Invalid boolean value [] Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'instance' defined in class path resource [spring/default-instance.xml]: Cannot resolve reference to bean 'eventParser' while setting bean property 'eventParser'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'eventParser' defined in class path resource [spring/default-instance.xml]: Initialization of bean failed; nested exception is org.springframework.beans.TypeMismatchException: Failed to convert property value of type 'java.lang.String' to required type 'boolean' for property 'isGTIDMode'; nested exception is java.lang.IllegalArgumentException: Invalid boolean value []         at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:334) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE]         at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:108) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE]         at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyPropertyValues(AbstractAutowireCapableBeanFactory.java:1417) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE]         at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1158) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE]         at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:519) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE]         at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:458) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE]         at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:296) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE]         at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:223) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE]         at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:293) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE]         at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:194) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE]         at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:633) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE]         at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:932) ~[spring-context-3.2.9.RELEASE.jar:3.2.9.RELEASE]
676,这个也是404呀manager.deployer-x.y.z.tar.gz https://github.com/alibaba/otter/releases/download/otter-x.y.z/manager.deployer-x.y.z.tar.gz xyz替换成具体版本
675,server多个instance同时启动，每次只能最多5个工作 一台canal server 部署了15个instance，每次起来只有5个instance能获取到位点信息。而且重启这五个不固定。 有其中一个报SHOW VIEW command没权限，需要给账号开这个权限？ 文档里只提到select，replication slave，replication client权限 2018-06-06 19:18:22.458 [destination = mysql.05.upanb.cn   address = mysql.05.upanb.cn/192.168.250.214:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address mysql.05.upanb.cn/192.168.250.214:3306 has an error  retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: ErrorPacket [errorNumber=1142  fieldCount=-1  message=SHOW VIEW command denied to user 'canal'@'192.168.249.54' for table 'taglist'  sqlState=42000  sqlStateMarker=#] with command: show create table `pointstore`.`admin`;show create table `pointstore`.`blacklist`;show create table `pointstore`.`clearuk`;show create table `pointstore`.`duibaorder`;show create table `pointstore`.`point_detail_description`;show create table `pointstore`.`returnpoint`;show create table `pointstore`.`s_admin`;show create table `pointstore`.`t_adminlog`;show create table `pointstore`.`t_broker`;show create table `pointstore`.`t_broker_copy`;show create table `pointstore`.`t_brokercompany`;show create table `pointstore`.`t_exchangerule`;show create table `pointstore`.`t_luckproduct`;show create table `pointstore`.`t_luckrecord`;show create table `pointstore`.`t_luckrecord2`;show create table `pointstore`.`t_luckrecord3`;show create table `pointstore`.`t_luckrecord4`;show create table `pointstore`.`t_oauthinfo`;show create table `pointstore`.`t_oauthinfo_bak1`;show create table `pointstore`.`t_oauthinfo_copy`;show create table `pointstore`.`t_pointactivity`;show create table `pointstore`.`t_pointdetail_new`;show create table `pointstore`.`t_pointmain_new`;show create table `pointstore`.`t_pointmain_new_20180212`;show create table `pointstore`.`t_pointmain_new_bak`;show create table `pointstore`.`t_pointmain_new_copy`;show create table `pointstore`.`t_pointproduct`;show create table `pointstore`.`t_pointrule`;show create table `pointstore`.`t_pointsetting`;show create table `pointstore`.`t_pointstoreadminauthority`;show create table `pointstore`.`t_pointstoreinvoice`;show create table `pointstore`.`t_pointstorevisitlog`;show create table `pointstore`.`t_productinventory`;show create table `pointstore`.`t_productpic`;show create table `pointstore`.`t_productscategory`;show create table `pointstore`.`t_productsexchange_new`;show create table `pointstore`.`t_productsexpand`;show create table `pointstore`.`t_productshow`;show create table `pointstore`.`t_provincecityarea`;show create table `pointstore`.`t_ruletocity`;show create table `pointstore`.`taglist`;show create table `pointstore`.`test`;show create table `pointstore`.`timelist`;show create table `pointstore`.`v_brokerpoint`;show create table `pointstore`.`v_luckrecord`;show create table `pointstore`.`v_pointstoreinvoicetotal`;show create table `pointstore`.`v_pointstoreinvoicetotal_a`;show create table `pointstore`.`v_productshow`;show create table `pointstore`.`v_productshowlist`;show create table `pointstore`.`v_productwarehouse`;show create table `pointstore`.`yearlist`; Caused by: java.io.IOException: ErrorPacket [errorNumber=1142  fieldCount=-1  message=SHOW VIEW command denied to user 'canal'@'192.168.249.54' for table 'taglist'  sqlState=42000  sqlStateMarker=#] 2018-06-06 19:18:22.459 [destination = mysql.05.upanb.cn   address = mysql.05.upanb.cn/192.168.250.214:3306   EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - disConnect MysqlConnection to mysql.05.upanb.cn/192.168.250.214:3306... 2018-06-06 19:18:22.459 [destination = mysql.05.upanb.cn   address = mysql.05.upanb.cn/192.168.250.214:3306   EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - disConnect MysqlConnection to mysql.05.upanb.cn/192.168.250.214:3306.. tsdb经常报错，可以不用吗？ 2018-06-07 00:44:16.671 [destination = mysql.13.upanb.cn   address = mysql.13.upanb.cn/192.168.250.226:3306   EventParser] ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - pls submit github issue  show create table ddl:CREATE TABLE `group_order_history_detail_new` (   `result_id` int(11) NOT NULL DEFAULT '0' COMMENT '订单号'   `year` int(11) NOT NULL DEFAULT '2018' COMMENT '年'   `month` int(11) NOT NULL DEFAULT '2' COMMENT '月'   `order_id` int(11) NOT NULL DEFAULT '0' COMMENT '订单中心单号'   `product_id` int(11) NOT NULL DEFAULT '0' COMMENT '项目ID'   `product_name` varchar(50) NOT NULL DEFAULT '' COMMENT '项目名称'   `broker_id` int(11) NOT NULL DEFAULT '0' COMMENT '经纪人ID'   `broker_name` varchar(50) NOT NULL DEFAULT '' COMMENT '经纪人名字'   `broker_phone` varchar(50) NOT NULL DEFAULT '' COMMENT '经纪人电话号码'   `building_id` int(11) NOT NULL DEFAULT '0' COMMENT '楼盘ID'   `sale_id` int(11) NOT NULL DEFAULT '0' COMMENT '销售ID'   `sale_name` varchar(50) NOT NULL DEFAULT '' COMMENT '销售人员名称'   `sale_group_id` int(11) NOT NULL DEFAULT '0' COMMENT '销售小组ID'   `sale_group_name` varchar(45) NOT NULL DEFAULT '' COMMENT '销售小组名字'   `expand_id` int(11) NOT NULL DEFAULT '0' COMMENT '拓展ID'   `expand_name` varchar(50) NOT NULL DEFAULT '' COMMENT '拓展人员名称'   `expand_group_id` int(11) NOT NULL DEFAULT '0' COMMENT '拓展小组ID'   `expand_group_name` varchar(45) NOT NULL DEFAULT '' COMMENT '拓展小组名称'   `accounts_amount` bigint(20) NOT NULL DEFAULT '0' COMMENT '应收款'   `out_accounts_amount` bigint(20) NOT NULL DEFAULT '0' COMMENT '应付款'   `actual_ascription_amount` bigint(20) NOT NULL DEFAULT '0' COMMENT '归属收入（应收-应付）'   `sale_actual_ascription_amount` bigint(20) NOT NULL DEFAULT '0' COMMENT '销售归属收入'   `expand_actual_ascription_amount` bigint(20) NOT NULL DEFAULT '0' COMMENT '拓展归属收入'   `public_amount` bigint(20) NOT NULL DEFAULT '0' COMMENT '公共池收入(归属收入)'   `cav_accounts_amount` bigint(20) NOT NULL DEFAULT '0' COMMENT '低于180天的实际已收款'   `out_cav_accounts_amount` bigint(20) NOT NULL DEFAULT '0' COMMENT '超过180天的实际已收款'   `sale_divide_rate` double(5 4) NOT NULL DEFAULT '0.0000' COMMENT '销售拓展分成比例'   `subscription_time` timestamp NOT NULL DEFAULT '0000-00-00 00:00:00' ON UPDATE CURRENT_TIMESTAMP COMMENT '认购时间'   `handle_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '客户成交日期'   `confirm_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '上传回执单时间'   `confirmation_cycle` int(11) NOT NULL DEFAULT '0' COMMENT '确认周期'   `sale_actual_accounts_amount` bigint(20) NOT NULL DEFAULT '0' COMMENT '销售小组应收金额'   `sale_actual_cav_accounts_amount` bigint(20) NOT NULL DEFAULT '0' COMMENT '销售小组已收金额'   `expand_actual_accounts_amount` bigint(20) NOT NULL DEFAULT '0' COMMENT '拓展小组应收金额'   `expand_actual_cav_accounts_amount` bigint(20) NOT NULL DEFAULT '0' COMMENT '拓展小组已收金额'   `is_baoxiao` tinyint(4) NOT NULL DEFAULT '0' COMMENT '订单是否包销 非包销 0 包销 1 类包销'   `actual_cost` bigint(20) NOT NULL DEFAULT '0' COMMENT '实际销售费用（实际易快报中提报申请支付的业绩提成金额）'   `fixed_amount` bigint(20) NOT NULL DEFAULT '0' COMMENT '固定收益'   `cav_fixed_amount` bigint(20) NOT NULL DEFAULT '0' COMMENT '已回固定收益'   `receivable_commission` bigint(20) NOT NULL DEFAULT '0' COMMENT '应收佣金'   `cav_commission` bigint(20) NOT NULL DEFAULT '0' COMMENT '已回佣金'   `pay_commission` bigint(20) NOT NULL DEFAULT '0' COMMENT '应付佣金'   `cav_commission` bigint(20) NOT NULL DEFAULT '0' COMMENT '已回佣金'   `pay_commission` bigint(20) NOT NULL DEFAULT '0' COMMENT '应付佣金'   `receivable_deal` bigint(20) NOT NULL DEFAULT '0' COMMENT '应收成交奖'   `cav_deal` bigint(20) NOT NULL DEFAULT '0' COMMENT '已回成交奖'   `pay_deal` bigint(20) NOT NULL DEFAULT '0' COMMENT '应付成交奖'   `receivable_jump` bigint(20) NOT NULL DEFAULT '0' COMMENT '应收跳点'   `cav_jump` bigint(20) NOT NULL DEFAULT '0' COMMENT '已回跳点'   `pay_jump` bigint(20) NOT NULL DEFAULT '0' COMMENT '应付跳点'   `receivable_premium` bigint(20) NOT NULL DEFAULT '0' COMMENT '应收溢价'   `cav_premium` bigint(20) NOT NULL DEFAULT '0' COMMENT '已回溢价'   `pay_premium` bigint(20) NOT NULL DEFAULT '0' COMMENT '应付溢价'   `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间'   `close_time` timestamp NULL DEFAULT NULL ON UPDATE CURRENT_TIMESTAMP COMMENT '订单关闭时间'   `is_close` tinyint(4) NOT NULL DEFAULT '0' COMMENT '0：订单未关闭，1：订单关闭'   `sys_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '系统时间'   `is_delete` tinyint(4) NOT NULL DEFAULT '0' COMMENT '是否删除 0:未删除 1：删除'   UNIQUE KEY `index` (`result_id` `year` `month`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8   compare failed .  db : TableMeta [schema=tops_data_gdm  table=group_order_history_detail_new  fileds=         FieldMeta [columnName=result_id  columnType=int(11)  defaultValue=0  nullable=false  key=true]         FieldMeta [columnName=year  columnType=int(11)  defaultValue=2018  nullable=false  key=true]         FieldMeta [columnName=month  columnType=int(11)  defaultValue=2  nullable=false  key=true]         FieldMeta [columnName=order_id  columnType=int(11)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=product_id  columnType=int(11)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=product_name  columnType=varchar(50)  defaultValue=  nullable=false  key=false]         FieldMeta [columnName=broker_id  columnType=int(11)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=broker_name  columnType=varchar(50)  defaultValue=  nullable=false  key=false]         FieldMeta [columnName=broker_phone  columnType=varchar(50)  defaultValue=  nullable=false  key=false]         FieldMeta [columnName=building_id  columnType=int(11)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=sale_id  columnType=int(11)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=sale_name  columnType=varchar(50)  defaultValue=  nullable=false  key=false]         FieldMeta [columnName=sale_group_id  columnType=int(11)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=sale_group_name  columnType=varchar(45)  defaultValue=  nullable=false  key=false]         FieldMeta [columnName=expand_id  columnType=int(11)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=expand_name  columnType=varchar(50)  defaultValue=  nullable=false  key=false]         FieldMeta [columnName=expand_group_id  columnType=int(11)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=expand_group_name  columnType=varchar(45)  defaultValue=  nullable=false  key=false]         FieldMeta [columnName=accounts_amount  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=out_accounts_amount  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=actual_ascription_amount  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=sale_actual_ascription_amount  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=expand_actual_ascription_amount  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=public_amount  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=cav_accounts_amount  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=out_cav_accounts_amount  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]   FieldMeta [columnName=cav_accounts_amount  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=out_cav_accounts_amount  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=sale_divide_rate  columnType=double(5 4)  defaultValue=0.0000  nullable=false  key=false]         FieldMeta [columnName=subscription_time  columnType=timestamp  defaultValue=0000-00-00 00:00:00  nullable=false  key=false]         FieldMeta [columnName=handle_time  columnType=timestamp  defaultValue=CURRENT_TIMESTAMP  nullable=false  key=false]         FieldMeta [columnName=confirm_time  columnType=timestamp  defaultValue=CURRENT_TIMESTAMP  nullable=false  key=false]         FieldMeta [columnName=confirmation_cycle  columnType=int(11)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=sale_actual_accounts_amount  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=sale_actual_cav_accounts_amount  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=expand_actual_accounts_amount  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=expand_actual_cav_accounts_amount  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=is_baoxiao  columnType=tinyint(4)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=actual_cost  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=fixed_amount  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=cav_fixed_amount  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=receivable_commission  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=cav_commission  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=pay_commission  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=receivable_deal  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=cav_deal  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=pay_deal  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=receivable_jump  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=cav_jump  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=pay_jump  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=receivable_premium  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=cav_premium  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=pay_premium  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=create_time  columnType=timestamp  defaultValue=CURRENT_TIMESTAMP  nullable=false  key=false]         FieldMeta [columnName=close_time  columnType=timestamp  defaultValue=null  nullable=true  key=false]         FieldMeta [columnName=is_close  columnType=tinyint(4)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=sys_time  columnType=timestamp  defaultValue=CURRENT_TIMESTAMP  nullable=false  key=false]         FieldMeta [columnName=is_delete  columnType=tinyint(4)  defaultValue=0  nullable=false  key=false] ]  mem : TableMeta [schema=tops_data_gdm  table=group_order_history_detail_new  fileds=         FieldMeta [columnName=result_id  columnType=int(11)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=year  columnType=int(11)  defaultValue=2018  nullable=false  key=false]         FieldMeta [columnName=month  columnType=int(11)  defaultValue=2  nullable=false  key=false]         FieldMeta [columnName=order_id  columnType=int(11)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=product_id  columnType=int(11)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=product_name  columnType=varchar(50)  defaultValue=  nullable=false  key=false]         FieldMeta [columnName=broker_id  columnType=int(11)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=broker_name  columnType=varchar(50)  defaultValue=  nullable=false  key=false]         FieldMeta [columnName=broker_phone  columnType=varchar(50)  defaultValue=  nullable=false  key=false]         FieldMeta [columnName=building_id  columnType=int(11)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=sale_id  columnType=int(11)  defaultValue=0  nullable=false  key=false]   FieldMeta [columnName=building_id  columnType=int(11)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=sale_id  columnType=int(11)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=sale_name  columnType=varchar(50)  defaultValue=  nullable=false  key=false]         FieldMeta [columnName=sale_group_id  columnType=int(11)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=sale_group_name  columnType=varchar(45)  defaultValue=  nullable=false  key=false]         FieldMeta [columnName=expand_id  columnType=int(11)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=expand_name  columnType=varchar(50)  defaultValue=  nullable=false  key=false]         FieldMeta [columnName=expand_group_id  columnType=int(11)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=expand_group_name  columnType=varchar(45)  defaultValue=  nullable=false  key=false]         FieldMeta [columnName=accounts_amount  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=out_accounts_amount  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=actual_ascription_amount  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=sale_actual_ascription_amount  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=expand_actual_ascription_amount  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=public_amount  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=cav_accounts_amount  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=out_cav_accounts_amount  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=sale_divide_rate  columnType=double(5 4)  defaultValue=0.0000  nullable=false  key=false]         FieldMeta [columnName=subscription_time  columnType=timestamp  defaultValue=0000-00-00 00:00:00  nullable=false  key=false]         FieldMeta [columnName=handle_time  columnType=timestamp  defaultValue=CURRENT_TIMESTAMP  nullable=false  key=false]         FieldMeta [columnName=confirm_time  columnType=timestamp  defaultValue=CURRENT_TIMESTAMP  nullable=false  key=false]         FieldMeta [columnName=confirmation_cycle  columnType=int(11)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=sale_actual_accounts_amount  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=sale_actual_cav_accounts_amount  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=expand_actual_accounts_amount  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=expand_actual_cav_accounts_amount  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=is_baoxiao  columnType=tinyint(4)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=actual_cost  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=fixed_amount  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=cav_fixed_amount  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=receivable_commission  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=cav_commission  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=pay_commission  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=receivable_deal  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=cav_deal  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=pay_deal  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=receivable_jump  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=cav_jump  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=pay_jump  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=receivable_premium  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=cav_premium  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=pay_premium  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=create_time  columnType=timestamp  defaultValue=CURRENT_TIMESTAMP  nullable=false  key=false]         FieldMeta [columnName=close_time  columnType=timestamp  defaultValue=null  nullable=true  key=false]         FieldMeta [columnName=is_close  columnType=tinyint(4)  defaultValue=0  nullable=false  key=false]   FieldMeta [columnName=close_time  columnType=timestamp  defaultValue=null  nullable=true  key=false]         FieldMeta [columnName=is_close  columnType=tinyint(4)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=sys_time  columnType=timestamp  defaultValue=CURRENT_TIMESTAMP  nullable=false  key=false]         FieldMeta [columnName=is_delete  columnType=tinyint(4)  defaultValue=0  nullable=false  key=false] ] 2018-06-07 00:59:47.133 [destination = mysql.04.upanb.cn   address = mysql.04.upanb.cn/192.168.250.213:3306   EventParser] ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - pls submit github issue  show create table ddl:CREATE TABLE `tk_broker_tag` (   `broker_id` bigint(20) NOT NULL COMMENT '经纪人id'   `tag_id` bigint(20) NOT NULL COMMENT '标签id 表tk_tag的主键'   `tag_count` bigint(20) NOT NULL DEFAULT '0' COMMENT '经纪人被评价该标签的次数'   `sys_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP   UNIQUE KEY `IX_broker_tag` (`broker_id` `tag_id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4   compare failed .  db : TableMeta [schema=tops_kber  table=tk_broker_tag  fileds=         FieldMeta [columnName=broker_id  columnType=bigint(20)  defaultValue=null  nullable=false  key=true]         FieldMeta [columnName=tag_id  columnType=bigint(20)  defaultValue=null  nullable=false  key=true]         FieldMeta [columnName=tag_count  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=sys_time  columnType=timestamp  defaultValue=CURRENT_TIMESTAMP  nullable=false  key=false] ]  mem : TableMeta [schema=tops_kber  table=tk_broker_tag  fileds=         FieldMeta [columnName=broker_id  columnType=bigint(20)  defaultValue=null  nullable=false  key=false]         FieldMeta [columnName=tag_id  columnType=bigint(20)  defaultValue=null  nullable=false  key=false]         FieldMeta [columnName=tag_count  columnType=bigint(20)  defaultValue=0  nullable=false  key=false]         FieldMeta [columnName=sys_time  columnType=timestamp  defaultValue=CURRENT_TIMESTAMP  nullable=false  key=false] ] 你这是啥版本？新的版本已经修复过这个问题 1.0.25 Neal Hu nealhu@apache.org > 在 2018年6月7日，09:51，agapple <notifications@github.com> 写道： >  > 你这是啥版本？新的版本已经修复过这个问题 >  > — > You are receiving this because you authored the thread. > Reply to this email directly  view it on GitHub  or mute the thread. 使用最新的1.0.26
674,v1.0.26 alpha 2版本client有对应maven库的jar包？在哪个maven库中呢？ ![image](https://user-images.githubusercontent.com/9798724/41032611-83024bf2-69b6-11e8-89fd-542e572f7ed1.png) 下载代码进行本地clean install吧 
673,本地测试连接远程canal似乎没有获得消息 我在本地使用canal服务时，发现可以收到消息。 在一台远程服务器上使用canal时，本地似乎一直没有收到消息。 `CanalConnectors.newSingleConnector(new InetSocketAddress("xxx.xxx.xxx.xxx"  11111)  "example"  ""  "");` 在本地每次连接远程，在example/example.log里可以看到这样的日志 `2018-06-06 17:57:15.309 [New I/O server worker #1-1] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - subscribe filter change to .*\..*` 这样看起来应该已经连接到远程的cannal服务。但是本地一直没有收到bin-log消息。 canal log看了吗？有错误吗？ 今天早上重新换了1.0.26-alpha3，在一台MySQL5.6.40上已经没有问题。本地测试看起来已经没有什么问题了。在另外一台服务器上重新下载相同版本的canal，发现MySQL5.7.21似乎不太稳定，有时候本地可以收到消息，但大部分时间是不行的。不知道与5.7.21是否对于canal有兼容性问题？ 看一下canal server的日志，是否有一些链接或者解析异常 **example.log** ``` 2018-06-07 11:03:51.958 [Thread-4] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - stop successful.... 2018-06-07 11:04:30.064 [main] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2018-06-07 11:04:30.068 [main] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties] 2018-06-07 11:04:30.218 [main] WARN  o.s.beans.GenericTypeAwarePropertyDescriptor - Invalid JavaBean property 'connectionCharset' being accessed! Ambiguous write methods found next to actually used [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.nio.charset.Charset)]: [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.lang.String)] 2018-06-07 11:04:30.268 [main] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2018-06-07 11:04:30.268 [main] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties] 2018-06-07 11:04:30.493 [main] ERROR com.alibaba.druid.pool.DruidDataSource - testWhileIdle is true  validationQuery not set 2018-06-07 11:04:30.769 [main] INFO  c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-example  2018-06-07 11:04:30.882 [main] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - subscribe filter change to .*\..* 2018-06-07 11:04:30.882 [main] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - start successful.... 2018-06-07 11:04:30.908 [destination = example   address = /127.0.0.1:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position  {"identity":{"slaveId":-1 "sourceAddress":{"address":"localhost" "port":3306}} "postion":{"gtid":"" "included":false "journalName":"mysql-bin.000002" "position":5818 "serverId":1 "timestamp":1528340336000}} 2018-06-07 11:04:31.332 [destination = example   address = /127.0.0.1:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000002 position=5818 serverId=1 gtid= timestamp=1528340336000] 2018-06-07 11:05:17.176 [New I/O server worker #1-2] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - subscribe filter change to .*\..* 2018-06-07 11:07:50.895 [New I/O server worker #1-3] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - subscribe filter change to .*\..* ``` **canal.log** ``` 2018-06-07 11:03:51.829 [Thread-4] INFO  com.alibaba.otter.canal.deployer.CanalLauncher - ## stop the canal server 2018-06-07 11:03:51.962 [Thread-4] INFO  com.alibaba.otter.canal.deployer.CanalController - ## stop the canal server[IP保密处理:11111] 2018-06-07 11:03:51.962 [Thread-4] INFO  com.alibaba.otter.canal.deployer.CanalLauncher - ## canal server is down. Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=96m; support was removed in 8.0 Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=256m; support was removed in 8.0 Java HotSpot(TM) 64-Bit Server VM warning: UseCMSCompactAtFullCollection is deprecated and will likely be removed in a future release. 2018-06-07 11:04:29.664 [main] INFO  com.alibaba.otter.canal.deployer.CanalLauncher - ## set default uncaught exception handler 2018-06-07 11:04:29.707 [main] INFO  com.alibaba.otter.canal.deployer.CanalLauncher - ## load canal configurations 2018-06-07 11:04:29.707 [main] INFO  com.alibaba.otter.canal.deployer.CanalLauncher - ## start the canal server. 2018-06-07 11:04:29.749 [main] INFO  com.alibaba.otter.canal.deployer.CanalController - ## start the canal server[IP保密处理:11111] 2018-06-07 11:04:30.218 [main] WARN  o.s.beans.GenericTypeAwarePropertyDescriptor - Invalid JavaBean property 'connectionCharset' being accessed! Ambiguous write methods found next to actually used [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.nio.charset.Charset)]: [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.lang.String)] 2018-06-07 11:04:30.493 [main] ERROR com.alibaba.druid.pool.DruidDataSource - testWhileIdle is true  validationQuery not set 2018-06-07 11:04:30.908 [destination = example   address = /127.0.0.1:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position  {"identity":{"slaveId":-1 "sourceAddress":{"address":"localhost" "port":3306}} "postion":{"gtid":"" "included":false "journalName":"mysql-bin.000002" "position":5818 "serverId":1 "timestamp":1528340336000}} 2018-06-07 11:04:30.933 [main] INFO  com.alibaba.otter.canal.deployer.CanalLauncher - ## the canal server is running now ...... 2018-06-07 11:04:31.332 [destination = example   address = /127.0.0.1:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000002 position=5818 serverId=1 gtid= timestamp=1528340336000] ``` meta.log看看消费的情况 问题解决了。因为MySQL5.7.21这台服务器上某个Java程序已经连接11111端口消费，我看了日志发现数据被该程序消费了。导致我本地消费不到。 总结给其他人的建议： 1） 因为本地无法消费远程canal信息，请升级到1.0.26版本。升级后解决了我本地无法消费远程一台5.6.40的bin-log数据 2）如果升级后还是无法消费数据，请检查是否其他应用正在消费bin-log数据 Thanks everyone.
672,Mysql产生的binlog，大量数据的时候，同步到Canal慢，这个怎么解决？ 做了一些大量的数据的测试，比如更新一个100W的表，同步到Canal，会7-8秒才能到Canal 我Get数据的时候，batchid为-1，然后我就猜想服务端的RingBuffer  doPut应该还没有put到数据，我想应该是Mysql到Canal Server会有阻塞的过程，这种有没有办法降阻塞的时间？比如mysql5.7的多线程并行的操作。 帮忙分析几个过程，写入到binlog -> canal接收到 -> canal解析存储到memory -> client接收，看一下具体瓶颈点 1. 写binlog（涉及小IO，应该问题不大） 2.  canal  （去fetch的时候网络IO，IO/Thread） 3. 当client  设置取值比较大的时候 以上三点设计到的IO操作。 我们最近通过测试发现    while (fetcher.fetch())  拉取数据qps在1000的样子  由于canal读数据是单线程的，如果瞬间插入百万数据对canal客户端读取的延长性还是很大的，不知道楼主这个问题解决了吗 你现在延迟 多久 我100万数据大概在25分左右 50W条数据 5分钟左右 50W条数据 5分钟左右 采用的单线程方式吗？  将数据同步到es 还是redis  加我wx aleenjava 探讨一下 我是这样做的，开了2个线程，一个线程不断的取数据，然后放到阻塞队列，然后第二个线程就从阻塞队列消费，然后按顺序的发送到MQ上。 @DevWithLin 你这边是基于canal的解析，完成了对接kafka吗？ 我这边用的是 基于配置界面的目前可对接rabbitmq和activemq，kafka没有用呢  后续会完成kafka。 @agapple  我还打算 让canalClient支持下Sky-Walking. 我是这样做的，开了2个线程，一个线程不断的取数据，然后放到阻塞队列，然后第二个线程就从阻塞队列消费，然后按顺序的发送到MQ上。 这和单线程消费顺序发送到MQ 上有什么区别？ 当MQ发送耗时大于从Canal Server Get数据的时候，这就很有意义了。 代码在哪里 方面分享一下吗？ 好简单的，都不用代码的啊。开两个线程一个阻塞队列，模拟生产消费。 我就是用这种方式 实现的 没达到你说得效果 100万数据大概 20分钟 我这边很快啊~ 你发到哪里的？ 写入到es 中 写入kafka的话 跟maxwell差不多？  @chenglinjava68  估计是 ES的瓶颈吧，我写rabbitmq的，平均8000多个消息。 每秒8000? 有点夸张了吧
671,canal v1.0.25版本报错java.nio.channels.ClosedByInterruptException: null 2018-06-05 00:50:56.419 [destination = example   address = testrds1.amazonaws.com/10.8.3.18:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address testrds1.corlbeqxlnts.us-west-2.rds.amazonaws.com/10.8.3.180:3306 has an error  retrying. caused by  java.nio.channels.ClosedByInterruptException: null 	at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannel.read(SocketChannel.java:49) ~[canal.parse.driver-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:151) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:77) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:137) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) ~[canal.parse-1.0.25.jar:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144] 2018-06-05 00:50:56.425 [destination = example   address = testrds1.amazonaws.com/10.8.3.18:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.nio.channels.ClosedByInterruptException 	at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannel.read(SocketChannel.java:49) 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:151) 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:77) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:137) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) 	at java.lang.Thread.run(Thread.java:748) ] 试试 26版本的 看下[Issue604](https://github.com/alibaba/canal/issues/640)。
670,支持mongo->es同步 [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=670) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=670) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=670) it.</sub> [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=670) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=670) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=670) it.</sub> 1. 这个改动有点大，包含了mongo的解析和同步功能，部分应该是拆分到client代码 2. 代码有一些冲突
669,Canal 设计基于Server Client模式的用意是什么？ Canal 设计基于Server Client模式的用意是什么？ 在Server端直接解析binlog并消费是不是更加可靠与高效？ 楼主这个问题可以这样问，分布式系统的意义是什么 除了让可编程的客户端更轻量外，还有什么考虑？server同样可以做成分布式，目前也是支持server ha的 为了做方便运维管理，也可以直接嵌入式带入server，直连mysql 对，这样的效率和可靠性会更高。我们打算封装下canal client的代码，支持配置成嵌入式server。另外还抽象了可扩展的outconnector spi机制，目前做了hbase，es，log4j的connector。到时候考虑贡献给社区 非常欢迎提交PR 已经合并了对应的kafka相关代码 欢迎下载：https://github.com/alibaba/canal/releases/tag/canal-1.0.26-preview-3
668,Canal Server HA切换时会同步binlog的offset信息吗？ 会的，可以把位点记录到zookeeper中 切换到default-instance.xml 用zk记录位点信息咯 切换到 StandBy DB 的时候，binlog 会完全对不上，这种情况是需要人工处理？一直以为这里的HA是自动的，但是想起来不会那么简单。 原谅我在这个closed issue继续发问，因为没有一个可以直接高效沟通的地方。。。
667,canal Broken pipe  看代码好像是去更新MySQL中的meta_snapshot， 这张表有什么用呢？ ## canal 日志 ```bash 2018-06-03 21:24:30.826 [[scheduler-table-meta-snapshot]] ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - scheudle applySnapshotToDB faield com.alibaba.otter.canal.parse.exception.CanalParseException: java.net.SocketException: Broken pipe (Write failed) Caused by: java.net.SocketException: Broken pipe (Write failed)  at java.net.SocketOutputStream.socketWrite0(Native Method) ~[na:1.8.0_171]  at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111) ~[na:1.8.0_171]  at java.net.SocketOutputStream.write(SocketOutputStream.java:143) ~[na:1.8.0_171]  at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.write(BioSocketChannel.java:35) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na]  at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.writeBody0(PacketManager.java:42) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na]  at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.writeBody(PacketManager.java:35) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na]  at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.query(MysqlQueryExecutor.java:55) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na]  at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:94) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]  at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.compareTableMetaDbAndMemory(DatabaseTableMeta.java:296) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]  at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.applySnapshotToDB(DatabaseTableMeta.java:251) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]  at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.access$100(DatabaseTableMeta.java:45) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]  at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta$2.run(DatabaseTableMeta.java:84) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_171]  at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) [na:1.8.0_171]  at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) [na:1.8.0_171]  at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) [na:1.8.0_171]  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_171]  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_171]  at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171] ``` ##canal-client ```bash mory=0.024801254  InstallDirSize=11.233233  LastUpdatetime=2018-05-31 17:01:28.0}]  old=[\{LastUpdatetime=2018-05-31 17:01:23.0}]} 10:11:45.401 [pool-6-thread-1] INFO com.tops001.foundation.canal.connector.CanalLog4JConnector - \{"data":[\{"Id":1 "NodeName":"新增节点" "NodeCreateTime":1484977299000 "NodeIP":"WIN-8RG5KGC1JNM" "NodeLastUpdatetime":1527757293000 "IfCheckState":false}] "database":"tops_bops_task" "old":[\{"NodeLastUpdatetime":1527757288000}] "sql":"" "table":"task_node" "ts":1528078305401 "type":"UPDATE"} 10:11:45.401 [pool-6-thread-1] DEBUG com.tops001.foundation.canal.connector.es.syn.ElasticsearchSynService - Dml\{database='tops_bops_task'  table='task_node'  type='UPDATE'  ts=1528078305401  sql=''  data=[\{Id=1  NodeName=新增节点  NodeCreateTime=2017-01-21 13:41:39.0  NodeIP=WIN-8RG5KGC1JNM  NodeLastUpdatetime=2018-05-31 17:01:33.0  IfCheckState=false}]  old=[\{NodeLastUpdatetime=2018-05-31 17:01:28.0}]} 10:11:45.402 [Thread-12] WARN com.alibaba.otter.canal.client.impl.ClusterCanalConnector - something goes wrong when getWithoutAck data from server:null com.alibaba.otter.canal.protocol.exception.CanalClientException: java.io.IOException: end of stream when reading header  at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:291) ~[canal.client-1.0.26.4.jar!/:?]  at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:264) ~[canal.client-1.0.26.4.jar!/:?]  at com.alibaba.otter.canal.client.impl.ClusterCanalConnector.getWithoutAck(ClusterCanalConnector.java:182) ~[canal.client-1.0.26.4.jar!/:?]  at com.tops001.foundation.canal.service.CanalWorker.process(CanalWorker.java:120) ~[classes!/:?]  at java.lang.Thread.run(Thread.java:748) [?:1.8.0_171] Caused by: java.io.IOException: end of stream when reading header  at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.read(SimpleCanalConnector.java:396) ~[canal.client-1.0.26.4.jar!/:?]  at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.readNextPacket(SimpleCanalConnector.java:384) ~[canal.client-1.0.26.4.jar!/:?]  at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.readNextPacket(SimpleCanalConnector.java:368) ~[canal.client-1.0.26.4.jar!/:?]  at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.receiveMessages(SimpleCanalConnector.java:296) ~[canal.client-1.0.26.4.jar!/:?]  at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:289) ~[canal.client-1.0.26.4.jar!/:?]  ... 4 more 10:11:50.417 [Thread-12] INFO com.alibaba.otter.canal.client.impl.ClusterCanalConnector - restart the connector for next round retry. ``` 你开了table的table tsdb的能力，这是一个定时任务，非主线影响client读取失败的原因
666,Mac环境下自己编译出问题 依赖这个文件，获取不到。 http://code.alibabatech.com/mvn/releases/com/alibaba/fastsql/fastsql/2.0.0_preview_228/fastsql-2.0.0_preview_228.pom 麻烦把这个依赖放到一个可靠的源上。 [ERROR] Failed to execute goal on project canal.parse: Could not resolve dependencies for project com.alibaba.otter:canal.parse:jar:1.0.26-SNAPSHOT: Failed to collect dependencies at com.alibaba.fastsql:fastsql:jar:2.0.0_preview_228: Failed to read artifact descriptor for com.alibaba.fastsql:fastsql:jar:2.0.0_preview_228: Could not transfer artifact com.alibaba.fastsql:fastsql:pom:2.0.0_preview_228 from/to alibaba (http://code.alibabatech.com/mvn/releases/): Connect to code.alibabatech.com:80 [code.alibabatech.com/119.38.217.15] failed: Operation timed out -> [Help 1] same here 同样 https://github.com/alibaba/canal/releases/tag/canal-1.0.26-preview-3，发布的tar包里有对应的jar.   fastsql后续也会开源，地表最强的sql parser工具，mysql解析兼容度程度最好
665,请问canal 是否支持级联复制? rt 即canal server从mysql 的slave节点复制binlog 应该是支持的  我先测试下 只要mysql开启log_slave_update的特性
664,关于两个canal server配同一个instance的疑问 两个canal server分别配置了两个instance：example1和example2，启动其中一个canal server，这时候这个server能够同步example1和example2这两个库的数据，这时候启动第二个canal server，这时候这个server无法消费example1和example2，这个时候集群就没有太大意义了，canal是否可以做类似rebalance之类的操作？比如rebalance之后canal server1和canal server2分别同步一个库的数据？
663,1.0.26  com.alibaba.fastsql.sql.parser.ParserException: TODO pos 545  line 9  column 83  token NULL durid 已经手动升级为：1.1.9， 启动还是报错： ` 2018-05-29 17:29:36.484 [destination = devmysql4308   address = /192.168.100.4:3308   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : /* ApplicationName=IntelliJ IDEA 2017.2.5 */ CREATE TABLE best_sign_cont_task (   id INTEGER PRIMARY KEY NOT NULL COMMENT '主键' AUTO_INCREMENT   docid VARCHAR(50) NOT NULL DEFAULT '' COMMENT '合同原件的上上签id'   pdfid VARCHAR(50) NOT NULL DEFAULT '' COMMENT '合同pdf的上上签id'   contract_id VARCHAR(50) NOT NULL DEFAULT '' COMMENT '上上签合同id'   finish TINYINT(1) NOT NULL DEFAULT 0 COMMENT '0甲方未签署 1甲方已签署 表示任务完成'   remark VARCHAR(100) NOT NULL DEFAULT '' COMMENT '备注'   sys_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE  CURRENT_TIMESTAMP  NOT NULL COMMENT '系统时间' )ENGINE=INNODB DEFAULT CHARSET=utf8 COMMENT '上上签合同创建任务记录' com.alibaba.fastsql.sql.parser.ParserException: TODO pos 545  line 9  column 83  token NULL         at com.alibaba.fastsql.sql.parser.SQLExprParser.notRationalRest(SQLExprParser.java:2557) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186]         at com.alibaba.fastsql.sql.parser.SQLExprParser.relationalRest(SQLExprParser.java:2323) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186]         at com.alibaba.fastsql.sql.parser.SQLExprParser.exprRest(SQLExprParser.java:109) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186]         at com.alibaba.fastsql.sql.parser.SQLExprParser.expr(SQLExprParser.java:97) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186]         at com.alibaba.fastsql.sql.dialect.mysql.parser.MySqlExprParser.parseColumnRest(MySqlExprParser.java:462) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186]         at com.alibaba.fastsql.sql.parser.SQLExprParser.parseColumnRest(SQLExprParser.java:2801) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186]         at com.alibaba.fastsql.sql.dialect.mysql.parser.MySqlExprParser.parseColumnRest(MySqlExprParser.java:545) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186]         at com.alibaba.fastsql.sql.dialect.mysql.parser.MySqlExprParser.parseColumn(MySqlExprParser.java:455) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186]         at com.alibaba.fastsql.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:170) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186]         at com.alibaba.fastsql.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:239) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186]         at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:165) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186]         at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:76) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186]         at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:469) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186]         at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:331) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:71) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.applyHistoryOnMemory(DatabaseTableMeta.java:382) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:121) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171] ` 我手动下载了 fastsql_2.0.0_preview_186，定位到了问题， 问题sql：  ```sql CREATE TABLE corp_best_sign_info (     sys_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP  ON UPDATE CURRENT_TIMESTAMP   not null  COMMENT '系统时间' )ENGINE=INNODB DEFAULT CHARSET=utf8 COMMENT '企业开通上上签信息表'; ``` 调整后通过的sql:  ```sql CREATE TABLE corp_best_sign_info (     sys_time TIMESTAMP  not null DEFAULT CURRENT_TIMESTAMP  ON UPDATE CURRENT_TIMESTAMP   COMMENT '系统时间' )ENGINE=INNODB DEFAULT CHARSET=utf8 COMMENT '企业开通上上签信息表'; ``` 区别在于，not null   有问题的写在了 on update current_timestamp 之后，类比了下其它的sqlparse  这种语法是OK的，麻烦看下，能否修复下。 的确是存在问题
662,当前master分支  SimpleCanalConnector#connect()方法  逻辑错误 # 运行SimpleCanalClientTest的main方法，会出异常 `process error!java.lang.NullPointerException: null 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.writeWithHeader(SimpleCanalConnector.java:392) ~[classes/:na] 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.writeWithHeader(SimpleCanalConnector.java:380) ~[classes/:na] 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:292) ~[classes/:na] 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:280) ~[classes/:na] 	at com.alibaba.otter.canal.example.AbstractCanalClientTest.process(AbstractCanalClientTest.java:125) ~[classes/:na] 	at com.alibaba.otter.canal.example.AbstractCanalClientTest$2.run(AbstractCanalClientTest.java:85) [classes/:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171]` 经排查，是SimpleCanalConnector的connect方法出错 ``` public void connect() throws CanalClientException {         if (connected) {             return;         }         if (runningMonitor != null) {             if (!runningMonitor.isStart()) {                 runningMonitor.start();             }         } else {             waitClientRunning();             if (!running) {                 return;             }             doConnect();             if (filter != null) { // 如果存在条件，说明是自动切换，基于上一次的条件订阅一次                 subscribe(filter);             }             if (rollbackOnConnect) {                 rollback();             }         }         connected = true;     } private void waitClientRunning() {         try {             if (zkClientx != null) {                 if (!connected) {// 未调用connect                     throw new CanalClientException("should connect first");                 }                 running = true;                 mutex.get();// 阻塞等待             }         } catch (InterruptedException e) {             Thread.currentThread().interrupt();             throw new CanalClientException(e);         }     } ``` running初始化为false，且仅有两个方法改变了他的值：waitClientRunning()  和  stopRunning() 。 connect()方法调用waitClientRunning()时，由于zkClienttx == null 所以running仍然为false，所以不会调用doConnect()，从而没有给writableChannel赋值，并导致异常堆栈里的空指针异常。 问题是这次提交造成的:[https://github.com/alibaba/canal/commit/4141049bb90ce6dc1ce05a58c95f2d974fdd5865](https://github.com/alibaba/canal/commit/4141049bb90ce6dc1ce05a58c95f2d974fdd5865) @rewerma  1、屏蔽两行代码就行了， 第一处： public void connect() throws CanalClientException {         if (connected) {             return;         }         if (runningMonitor != null) {             if (!runningMonitor.isStart()) {                 runningMonitor.start();             }         } else {             waitClientRunning(); //            if (!running) { //                return; //            }             doConnect();             if (filter != null) { // 如果存在条件，说明是自动切换，基于上一次的条件订阅一次                 subscribe(filter);             }             if (rollbackOnConnect) {                 rollback();             }         }         connected = true;     } 第二处：  public void subscribe(String filter) throws CanalClientException {         waitClientRunning(); //        if (!running) { //            return; //        }         try {             writeWithHeader(Packet.newBuilder()                 .setType(PacketType.SUBSCRIPTION)                 .setBody(Sub.newBuilder()                     .setDestination(clientIdentity.getDestination())                     .setClientId(String.valueOf(clientIdentity.getClientId()))                     .setFilter(filter != null ? filter : "")                     .build()                     .toByteString())                 .build()                 .toByteArray());             //             Packet p = Packet.parseFrom(readNextPacket());             Ack ack = Ack.parseFrom(p.getBody());             if (ack.getErrorCode() > 0) {                 throw new CanalClientException("failed to subscribe with reason: " + ack.getErrorMessage());             }             clientIdentity.setFilter(filter);         } catch (IOException e) {             throw new CanalClientException(e);         }     } 原因上面已经回答。zookeeper判断有问题。单机可有没有zookeeper 已修复
661,产生数据积压后，同步一段时间就会报错。异常后，又会重新从开始的位点同步。导致无法持续进行后面的同步。 版本canal-canal-1.0.26 something goes wrong when acking data from server:null com.alibaba.otter.canal.protocol.exception.CanalClientException: java.io.IOException: Broken pipe 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.ack(SimpleCanalConnector.java:339) 	at com.alibaba.otter.canal.client.impl.ClusterCanalConnector.ack(ClusterCanalConnector.java:246) 	at ins.framework.mysqltoes.client.AbstractCanalClientTest.process(AbstractCanalClientTest.java:140) 	at ins.framework.mysqltoes.client.AbstractCanalClientTest$2.run(AbstractCanalClientTest.java:93) 	at java.lang.Thread.run(Thread.java:745) Caused by: java.io.IOException: Broken pipe 	at sun.nio.ch.FileDispatcherImpl.write0(Native Method) 	at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47) 	at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93) 	at sun.nio.ch.IOUtil.write(IOUtil.java:65) 	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:471) 	at java.nio.channels.Channels.writeFullyImpl(Channels.java:78) 	at java.nio.channels.Channels.writeFully(Channels.java:98) 	at java.nio.channels.Channels.access$000(Channels.java:61) 	at java.nio.channels.Channels$1.write(Channels.java:174) 	at java.nio.channels.Channels$WritableByteChannelImpl.write(Channels.java:458) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.writeWithHeader(SimpleCanalConnector.java:382) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.writeWithHeader(SimpleCanalConnector.java:369) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.ack(SimpleCanalConnector.java:333) 	... 4 common frames omitted restart the connector for next round retry. something goes wrong when getWithoutAck data from server:null com.alibaba.otter.canal.protocol.exception.CanalClientException: something goes wrong with reason: something goes wrong with channel:[id: 0x58c3e872  /10.10.56.23:50475 => /10.10.56.23:11111]  exception=com.alibaba.otter.canal.server.exception.CanalServerException: ack error   clientId:1001 batchId:926 is not exist   please check 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.receiveMessages(SimpleCanalConnector.java:317) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:294) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:269) 	at com.alibaba.otter.canal.client.impl.ClusterCanalConnector.getWithoutAck(ClusterCanalConnector.java:177) 	at ins.framework.mysqltoes.client.AbstractCanalClientTest.process(AbstractCanalClientTest.java:126) 	at ins.framework.mysqltoes.client.AbstractCanalClientTest$2.run(AbstractCanalClientTest.java:93) 	at java.lang.Thread.run(Thread.java:745) restart the connector for next round retry. ------------------------------------------------------------------------------------------------------------- canal的server端生成了线程错误日志，hs_err_pid12248.log，记录了部分堆栈信息，但里面有下面的异常，是不是有部分代码漏掉了？ ------------------------------------------------------------------------------------------------------------- Internal exceptions (10 events): Event: 13725.231 Thread 0x00007f65bc006800 Exception <a 'java/lang/ClassNotFoundException': com/alibaba/otter/canal/protocol/position/PositionBeanInfo> (0x0000000715146718) thrown at [/HUDSON/workspace/8-2-build-linux-amd64/jdk8u51/3951/hotspot/src/share/vm/classfile/systemDictionary.cpp  line 210 Event: 13725.231 Thread 0x00007f65bc006800 Exception <a 'java/lang/ClassNotFoundException': com/alibaba/otter/canal/protocol/position/PositionCustomizer> (0x0000000715156278) thrown at [/HUDSON/workspace/8-2-build-linux-amd64/jdk8u51/3951/hotspot/src/share/vm/classfile/systemDictionary.cpp  line 2 Event: 13725.231 Thread 0x00007f65bc006800 Exception <a 'java/lang/ClassNotFoundException': com/alibaba/otter/canal/protocol/position/TimePositionCustomizer> (0x0000000715168aa0) thrown at [/HUDSON/workspace/8-2-build-linux-amd64/jdk8u51/3951/hotspot/src/share/vm/classfile/systemDictionary.cpp  li Event: 13725.232 Thread 0x00007f65bc006800 Exception <a 'java/lang/ClassNotFoundException': com/alibaba/otter/canal/protocol/position/EntryPositionCustomizer> (0x000000071517e530) thrown at [/HUDSON/workspace/8-2-build-linux-amd64/jdk8u51/3951/hotspot/src/share/vm/classfile/systemDictionary.cpp  l Event: 13725.413 Thread 0x00007f659c1da000 Exception <a 'java/lang/ClassCastException': sun.misc.Cleaner cannot be cast to java.lang.Runnable> (0x00000007168aa620) thrown at [/HUDSON/workspace/8-2-build-linux-amd64/jdk8u51/3951/hotspot/src/share/vm/interpreter/interpreterRuntime.cpp  line 381] Event: 13725.416 Thread 0x00007f659c1da000 Exception <a 'java/lang/NoClassDefFoundError': javassist/ClassPath> (0x00000007168c0840) thrown at [/HUDSON/workspace/8-2-build-linux-amd64/jdk8u51/3951/hotspot/src/share/vm/classfile/systemDictionary.cpp  line 199] Event: 13730.219 Thread 0x00007f65a000a000 Implicit null exception at 0x00007f66653cf735 to 0x00007f66653cfa61 Event: 13743.542 Thread 0x00007f659c1da000 Exception <a 'java/lang/InterruptedException'> (0x0000000709c92318) thrown at [/HUDSON/workspace/8-2-build-linux-amd64/jdk8u51/3951/hotspot/src/share/vm/runtime/objectMonitor.cpp  line 1683] Event: 13743.548 Thread 0x00007f6594003800 Implicit null exception at 0x00007f6665ecfa12 to 0x00007f6665ed0209 Event: 13743.558 Thread 0x00007f6594003800 Implicit null exception at 0x00007f6665b17426 to 0x00007f6665b19e6d 你这是openjdk ？ ack error   clientId:1001 batchId:926 is not exist   please check， 应该是canal server端有异常，重启了instance 检查了canalserver的日志，发现过一些不支持的format格式的警告。但是并不影响同步。但是一旦出现上面的异常，服务端也会报这个异常。
660,1.0.26最新版本：something goes wrong with channel:******clientId:1001 batchId:50560 is not exist   please check 今天早上发现    @@@logs/canal目录下的canal.log文件将近200M，一直报如下异常，但是查看每个instance目录下面的meta.log是正常记录消费位点的，说明是正常消费的吧。但是这个异常为什么一直存在？？ 2018-05-29 09:48:25.896 [New I/O server worker #1-1] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400   Caused by : something goes wrong with channel:[id: 0x01f1bec3  /127.0.0.1:35010 => /127.0.0.1:11111]  exception=com.alibaba.otter.canal.server.exception.CanalServerException: ack error   clientId:1001 batchId:50560 is not exist   please check 2018-05-29 09:48:25.907 [New I/O server worker #1-1] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x01f1bec3  /127.0.0.1:35010 :> /127.0.0.1:11111]  exception=java.nio.channels.ClosedChannelException 	at org.jboss.netty.channel.socket.nio.NioWorker.cleanUpWriteBuffer(NioWorker.java:649) 	at org.jboss.netty.channel.socket.nio.NioWorker.writeFromUserCode(NioWorker.java:370) 	at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:137) 	at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) 	at org.jboss.netty.channel.Channels.write(Channels.java:611) 	at org.jboss.netty.channel.Channels.write(Channels.java:578) 	at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:28) 	at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:144) 	at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) 	at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.cleanup(ReplayingDecoder.java:542) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.channelDisconnected(ReplayingDecoder.java:450) 	at org.jboss.netty.channel.Channels.fireChannelDisconnected(Channels.java:360) 	at org.jboss.netty.channel.socket.nio.NioWorker.close(NioWorker.java:599) 	at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:119) 	at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) 	at org.jboss.netty.channel.Channels.close(Channels.java:720) 	at org.jboss.netty.channel.AbstractChannel.close(AbstractChannel.java:208) 	at org.jboss.netty.channel.ChannelFutureListener$1.operationComplete(ChannelFutureListener.java:46) 	at org.jboss.netty.channel.DefaultChannelFuture.notifyListener(DefaultChannelFuture.java:381) 	at org.jboss.netty.channel.DefaultChannelFuture.addListener(DefaultChannelFuture.java:148) 	at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:30) 	at com.alibaba.otter.canal.server.netty.NettyUtils.error(NettyUtils.java:51) 	at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:200) 	at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) 	at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:444) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261) 	at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:350) 	at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) 	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) 	at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) 	at java.lang.Thread.run(Thread.java:745) 这个是bug吧，什么时候能更新啊 server端肯定产生了异常或者重启了instance，client参考demo做一下重试 @agapple doConnect() 方法是重试？ 看一下client的demo @agapple 看了AbstractCanalClientTest里面connector.disconnect()方法吧，尝试了，但是服务端还是一样的异常 protected void process() {         int batchSize = 5 * 1024;         while (running) {             try {                 MDC.put("destination"  destination);                 connector.connect();                 connector.subscribe();                 waiting = false;                 while (running) {                     Message message = connector.getWithoutAck(batchSize); // 获取指定数量的数据                     long batchId = message.getId();                     int size = message.getEntries().size();                     if (batchId == -1 || size == 0) {                         // try {                         // Thread.sleep(1000);                         // } catch (InterruptedException e) {                         // }                     } else {                         printSummary(message  batchId  size);                         printEntry(message.getEntries());                     }                     connector.ack(batchId); // 提交确认                     // connector.rollback(batchId); // 处理失败  回滚数据                 }             } catch (Exception e) {                 logger.error("process error!"  e);             } finally {                 **connector.disconnect();**                 MDC.remove("destination");             }         }     } @agapple   instance是会重启，已重启，生产线上同步的数据就重复解析了，发送到kafka的   [捂脸] 分布式系统的extractly-once是很难的，一般都需要业务考虑幂等
659,同步的时候初始化sql报错 1.0.25 版本，文档中默认配置 h2数据库 ` 2018-05-28 14:36:38.957 [destination = myexample   address = /172.16.9.11:3307   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `columns_priv` (   `Host` char(60) COLLATE utf8_bin NOT NULL DEFAULT ''   `Db` char(64) COLLATE utf8_bin NOT NULL DEFAULT ''   `User` char(16) COLLATE utf8_bin NOT NULL DEFAULT ''   `Table_name` char(64) COLLATE utf8_bin NOT NULL DEFAULT ''   `Column_name` char(64) COLLATE utf8_bin NOT NULL DEFAULT ''   `Timestamp` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP   `Column_priv` set('Select' 'Insert' 'Update' 'References') CHARACTER SET utf8 NOT NULL DEFAULT ''   PRIMARY KEY (`Host` `Db` `User` `Table_name` `Column_name`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='Column privileges' com.alibaba.druid.sql.parser.ParserException: error pos 428  line 8  column 17  token SET 	at com.alibaba.druid.sql.parser.SQLExprParser.name(SQLExprParser.java:1334) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLExprParser.parseDataType(SQLExprParser.java:2310) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLExprParser.parseDataType(SQLExprParser.java:2301) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.dialect.mysql.parser.MySqlExprParser.parseColumn(MySqlExprParser.java:493) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:115) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.25.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101] 2018-05-28 14:36:38.960 [destination = myexample   address = /172.16.9.11:3307   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `db` (   `Host` char(60) COLLATE utf8_bin NOT NULL DEFAULT ''   `Db` char(64) COLLATE utf8_bin NOT NULL DEFAULT ''   `User` char(16) COLLATE utf8_bin NOT NULL DEFAULT ''   `Select_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Insert_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Update_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Delete_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Create_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Drop_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Grant_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `References_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Index_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Alter_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Create_tmp_table_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Lock_tables_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Create_view_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Show_view_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Create_routine_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Alter_routine_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Execute_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Event_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Trigger_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   PRIMARY KEY (`Host` `Db` `User`)   KEY `User` (`User`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='Database privileges' com.alibaba.druid.sql.parser.ParserException: syntax error  error in :''Y') CHARACTER SET utf8 NOT NULL DE'  expect RPAREN  actual IDENTIFIER pos 225  line 5  column 31  token IDENTIFIER CHARACTER 	at com.alibaba.druid.sql.parser.SQLParser.printError(SQLParser.java:284) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLParser.accept(SQLParser.java:292) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:191) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.25.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101] 2018-05-28 14:36:38.961 [destination = myexample   address = /172.16.9.11:3307   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `event` (   `db` char(64) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL DEFAULT ''   `name` char(64) NOT NULL DEFAULT ''   `body` longblob NOT NULL   `definer` char(77) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL DEFAULT ''   `execute_at` datetime DEFAULT NULL   `interval_value` int(11) DEFAULT NULL   `interval_field` enum('YEAR' 'QUARTER' 'MONTH' 'DAY' 'HOUR' 'MINUTE' 'WEEK' 'SECOND' 'MICROSECOND' 'YEAR_MONTH' 'DAY_HOUR' 'DAY_MINUTE' 'DAY_SECOND' 'HOUR_MINUTE' 'HOUR_SECOND' 'MINUTE_SECOND' 'DAY_MICROSECOND' 'HOUR_MICROSECOND' 'MINUTE_MICROSECOND' 'SECOND_MICROSECOND') DEFAULT NULL   `created` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP   `modified` timestamp NOT NULL DEFAULT '0000-00-00 00:00:00'   `last_executed` datetime DEFAULT NULL   `starts` datetime DEFAULT NULL   `ends` datetime DEFAULT NULL   `status` enum('ENABLED' 'DISABLED' 'SLAVESIDE_DISABLED') NOT NULL DEFAULT 'ENABLED'   `on_completion` enum('DROP' 'PRESERVE') NOT NULL DEFAULT 'DROP'   `sql_mode` set('REAL_AS_FLOAT' 'PIPES_AS_CONCAT' 'ANSI_QUOTES' 'IGNORE_SPACE' 'NOT_USED' 'ONLY_FULL_GROUP_BY' 'NO_UNSIGNED_SUBTRACTION' 'NO_DIR_IN_CREATE' 'POSTGRESQL' 'ORACLE' 'MSSQL' 'DB2' 'MAXDB' 'NO_KEY_OPTIONS' 'NO_TABLE_OPTIONS' 'NO_FIELD_OPTIONS' 'MYSQL323' 'MYSQL40' 'ANSI' 'NO_AUTO_VALUE_ON_ZERO' 'NO_BACKSLASH_ESCAPES' 'STRICT_TRANS_TABLES' 'STRICT_ALL_TABLES' 'NO_ZERO_IN_DATE' 'NO_ZERO_DATE' 'INVALID_DATES' 'ERROR_FOR_DIVISION_BY_ZERO' 'TRADITIONAL' 'NO_AUTO_CREATE_USER' 'HIGH_NOT_PRECEDENCE' 'NO_ENGINE_SUBSTITUTION' 'PAD_CHAR_TO_FULL_LENGTH') NOT NULL DEFAULT ''   `comment` char(64) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL DEFAULT ''   `originator` int(10) unsigned NOT NULL   `time_zone` char(64) CHARACTER SET latin1 NOT NULL DEFAULT 'SYSTEM'   `character_set_client` char(32) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL   `collation_connection` char(32) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL   `db_collation` char(32) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL   `body_utf8` longblob   PRIMARY KEY (`db` `name`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COMMENT='Events' com.alibaba.druid.sql.parser.ParserException: error pos 1035  line 16  column 14  token SET 	at com.alibaba.druid.sql.parser.SQLExprParser.name(SQLExprParser.java:1334) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLExprParser.parseDataType(SQLExprParser.java:2310) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLExprParser.parseDataType(SQLExprParser.java:2301) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.dialect.mysql.parser.MySqlExprParser.parseColumn(MySqlExprParser.java:493) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:115) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.25.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101] 2018-05-28 14:36:38.964 [destination = myexample   address = /172.16.9.11:3307   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `func` (   `name` char(64) COLLATE utf8_bin NOT NULL DEFAULT ''   `ret` tinyint(1) NOT NULL DEFAULT '0'   `dl` char(128) COLLATE utf8_bin NOT NULL DEFAULT ''   `type` enum('function' 'aggregate') CHARACTER SET utf8 NOT NULL   PRIMARY KEY (`name`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='User defined functions' com.alibaba.druid.sql.parser.ParserException: syntax error  error in :'te') CHARACTER SET utf8 NOT NULL  '  expect RPAREN  actual IDENTIFIER pos 221  line 5  column 39  token IDENTIFIER CHARACTER 	at com.alibaba.druid.sql.parser.SQLParser.printError(SQLParser.java:284) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLParser.accept(SQLParser.java:292) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:191) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.25.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101] 2018-05-28 14:36:38.967 [destination = myexample   address = /172.16.9.11:3307   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `proc` (   `db` char(64) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL DEFAULT ''   `name` char(64) NOT NULL DEFAULT ''   `type` enum('FUNCTION' 'PROCEDURE') NOT NULL   `specific_name` char(64) NOT NULL DEFAULT ''   `language` enum('SQL') NOT NULL DEFAULT 'SQL'   `sql_data_access` enum('CONTAINS_SQL' 'NO_SQL' 'READS_SQL_DATA' 'MODIFIES_SQL_DATA') NOT NULL DEFAULT 'CONTAINS_SQL'   `is_deterministic` enum('YES' 'NO') NOT NULL DEFAULT 'NO'   `security_type` enum('INVOKER' 'DEFINER') NOT NULL DEFAULT 'DEFINER'   `param_list` blob NOT NULL   `returns` longblob NOT NULL   `body` longblob NOT NULL   `definer` char(77) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL DEFAULT ''   `created` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP   `modified` timestamp NOT NULL DEFAULT '0000-00-00 00:00:00'   `sql_mode` set('REAL_AS_FLOAT' 'PIPES_AS_CONCAT' 'ANSI_QUOTES' 'IGNORE_SPACE' 'NOT_USED' 'ONLY_FULL_GROUP_BY' 'NO_UNSIGNED_SUBTRACTION' 'NO_DIR_IN_CREATE' 'POSTGRESQL' 'ORACLE' 'MSSQL' 'DB2' 'MAXDB' 'NO_KEY_OPTIONS' 'NO_TABLE_OPTIONS' 'NO_FIELD_OPTIONS' 'MYSQL323' 'MYSQL40' 'ANSI' 'NO_AUTO_VALUE_ON_ZERO' 'NO_BACKSLASH_ESCAPES' 'STRICT_TRANS_TABLES' 'STRICT_ALL_TABLES' 'NO_ZERO_IN_DATE' 'NO_ZERO_DATE' 'INVALID_DATES' 'ERROR_FOR_DIVISION_BY_ZERO' 'TRADITIONAL' 'NO_AUTO_CREATE_USER' 'HIGH_NOT_PRECEDENCE' 'NO_ENGINE_SUBSTITUTION' 'PAD_CHAR_TO_FULL_LENGTH') NOT NULL DEFAULT ''   `comment` text CHARACTER SET utf8 COLLATE utf8_bin NOT NULL   `character_set_client` char(32) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL   `collation_connection` char(32) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL   `db_collation` char(32) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL   `body_utf8` longblob   PRIMARY KEY (`db` `name` `type`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COMMENT='Stored Procedures' com.alibaba.druid.sql.parser.ParserException: error pos 864  line 16  column 14  token SET 	at com.alibaba.druid.sql.parser.SQLExprParser.name(SQLExprParser.java:1334) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLExprParser.parseDataType(SQLExprParser.java:2310) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLExprParser.parseDataType(SQLExprParser.java:2301) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.dialect.mysql.parser.MySqlExprParser.parseColumn(MySqlExprParser.java:493) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:115) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.25.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101] 2018-05-28 14:36:38.968 [destination = myexample   address = /172.16.9.11:3307   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `procs_priv` (   `Host` char(60) COLLATE utf8_bin NOT NULL DEFAULT ''   `Db` char(64) COLLATE utf8_bin NOT NULL DEFAULT ''   `User` char(16) COLLATE utf8_bin NOT NULL DEFAULT ''   `Routine_name` char(64) CHARACTER SET utf8 NOT NULL DEFAULT ''   `Routine_type` enum('FUNCTION' 'PROCEDURE') COLLATE utf8_bin NOT NULL   `Grantor` char(77) COLLATE utf8_bin NOT NULL DEFAULT ''   `Proc_priv` set('Execute' 'Alter Routine' 'Grant') CHARACTER SET utf8 NOT NULL DEFAULT ''   `Timestamp` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP   PRIMARY KEY (`Host` `Db` `User` `Routine_name` `Routine_type`)   KEY `Grantor` (`Grantor`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='Procedure privileges' com.alibaba.druid.sql.parser.ParserException: syntax error  error in :'RE') COLLATE utf8_bin NOT NULL   `'  expect RPAREN  actual IDENTIFIER pos 313  line 6  column 47  token IDENTIFIER COLLATE 	at com.alibaba.druid.sql.parser.SQLParser.printError(SQLParser.java:284) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLParser.accept(SQLParser.java:292) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:191) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.25.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101] 2018-05-28 14:36:38.972 [destination = myexample   address = /172.16.9.11:3307   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `tables_priv` (   `Host` char(60) COLLATE utf8_bin NOT NULL DEFAULT ''   `Db` char(64) COLLATE utf8_bin NOT NULL DEFAULT ''   `User` char(16) COLLATE utf8_bin NOT NULL DEFAULT ''   `Table_name` char(64) COLLATE utf8_bin NOT NULL DEFAULT ''   `Grantor` char(77) COLLATE utf8_bin NOT NULL DEFAULT ''   `Timestamp` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP   `Table_priv` set('Select' 'Insert' 'Update' 'Delete' 'Create' 'Drop' 'Grant' 'References' 'Index' 'Alter' 'Create View' 'Show view' 'Trigger') CHARACTER SET utf8 NOT NULL DEFAULT ''   `Column_priv` set('Select' 'Insert' 'Update' 'References') CHARACTER SET utf8 NOT NULL DEFAULT ''   PRIMARY KEY (`Host` `Db` `User` `Table_name`)   KEY `Grantor` (`Grantor`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='Table privileges' com.alibaba.druid.sql.parser.ParserException: error pos 422  line 8  column 16  token SET 	at com.alibaba.druid.sql.parser.SQLExprParser.name(SQLExprParser.java:1334) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLExprParser.parseDataType(SQLExprParser.java:2310) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLExprParser.parseDataType(SQLExprParser.java:2301) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.dialect.mysql.parser.MySqlExprParser.parseColumn(MySqlExprParser.java:493) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:115) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.25.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101] 2018-05-28 14:36:38.974 [destination = myexample   address = /172.16.9.11:3307   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `user` (   `Host` char(60) COLLATE utf8_bin NOT NULL DEFAULT ''   `User` char(16) COLLATE utf8_bin NOT NULL DEFAULT ''   `Password` char(41) CHARACTER SET latin1 COLLATE latin1_bin NOT NULL DEFAULT ''   `Select_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Insert_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Update_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Delete_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Create_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Drop_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Reload_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Shutdown_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Process_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `File_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Grant_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `References_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Index_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Alter_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Show_db_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Super_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Create_tmp_table_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Lock_tables_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Execute_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Repl_slave_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Repl_client_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Create_view_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Show_view_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Create_routine_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Alter_routine_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Create_user_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Event_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Trigger_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Create_tablespace_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `ssl_type` enum('' 'ANY' 'X509' 'SPECIFIED') CHARACTER SET utf8 NOT NULL DEFAULT ''   `ssl_cipher` blob NOT NULL   `x509_issuer` blob NOT NULL   `x509_subject` blob NOT NULL   `max_questions` int(11) unsigned NOT NULL DEFAULT '0'   `max_updates` int(11) unsigned NOT NULL DEFAULT '0'   `max_connections` int(11) unsigned NOT NULL DEFAULT '0'   `max_user_connections` int(11) unsigned NOT NULL DEFAULT '0'   `plugin` char(64) COLLATE utf8_bin DEFAULT ''   `authentication_string` text COLLATE utf8_bin   `password_expired` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   PRIMARY KEY (`Host` `User`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='Users and global privileges' com.alibaba.druid.sql.parser.ParserException: syntax error  error in :''Y') CHARACTER SET utf8 NOT NULL DE'  expect RPAREN  actual IDENTIFIER pos 256  line 5  column 31  token IDENTIFIER CHARACTER 	at com.alibaba.druid.sql.parser.SQLParser.printError(SQLParser.java:284) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLParser.accept(SQLParser.java:292) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:191) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.25.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101] 2018-05-28 14:38:19.380 [destination = myexample   address = /172.16.9.11:3307   EventParser] ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - pls submit github issue  show create table ddl:CREATE TABLE `hierquerysellist` (   `fieldtype_id` bigint(20) NOT NULL   `whereclause` longtext NOT NULL   `filtervariants` char(1) NOT NULL   UNIQUE KEY `fieldtype_id` (`fieldtype_id`) USING BTREE ) ENGINE=MyISAM DEFAULT CHARSET=utf8   compare failed .   db : TableMeta [schema=bboss  table=hierquerysellist  fileds= 	FieldMeta [columnName=fieldtype_id  columnType=bigint(20)  defaultValue=null  nullable=false  key=true] 	FieldMeta [columnName=whereclause  columnType=longtext  defaultValue=null  nullable=false  key=false] 	FieldMeta [columnName=filtervariants  columnType=char(1)  defaultValue=null  nullable=false  key=false] ]   mem : TableMeta [schema=bboss  table=hierquerysellist  fileds= 	FieldMeta [columnName=fieldtype_id  columnType=bigint(20)  defaultValue=null  nullable=false  key=false] 	FieldMeta [columnName=whereclause  columnType=longtext  defaultValue=null  nullable=false  key=false] 	FieldMeta [columnName=filtervariants  columnType=char(1)  defaultValue=null  nullable=false  key=false] ] 2018-05-28 14:38:19.590 [destination = myexample   address = /172.16.9.11:3307   EventParser] ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - pls submit github issue  show create table ddl:CREATE TABLE `hierquerysellist_fields` (   `fieldtype_id` bigint(20) NOT NULL   `sequencenr` smallint(6) NOT NULL   `fieldname` varchar(50) NOT NULL   UNIQUE KEY `fieldtype_id` (`fieldtype_id` `sequencenr`) USING BTREE ) ENGINE=MyISAM DEFAULT CHARSET=utf8   compare failed .   db : TableMeta [schema=bboss  table=hierquerysellist_fields  fileds= 	FieldMeta [columnName=fieldtype_id  columnType=bigint(20)  defaultValue=null  nullable=false  key=true] 	FieldMeta [columnName=sequencenr  columnType=smallint(6)  defaultValue=null  nullable=false  key=true] 	FieldMeta [columnName=fieldname  columnType=varchar(50)  defaultValue=null  nullable=false  key=false] ]   mem : TableMeta [schema=bboss  table=hierquerysellist_fields  fileds= 	FieldMeta [columnName=fieldtype_id  columnType=bigint(20)  defaultValue=null  nullable=false  key=false] 	FieldMeta [columnName=sequencenr  columnType=smallint(6)  defaultValue=null  nullable=false  key=false] 	FieldMeta [columnName=fieldname  columnType=varchar(50)  defaultValue=null  nullable=false  key=false] ] 2018-05-28 14:38:20.005 [destination = myexample   address = /172.16.9.11:3307   EventParser] ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - pls submit github issue  show create table ddl:CREATE TABLE `linkquerysellist` (   `fieldtype_id` bigint(20) NOT NULL   `whereclause` longtext NOT NULL   `filtervariants` char(1) NOT NULL   UNIQUE KEY `fieldtype_id` (`fieldtype_id`) USING BTREE ) ENGINE=MyISAM DEFAULT CHARSET=utf8   compare failed .   db : TableMeta [schema=bboss  table=linkquerysellist  fileds= 	FieldMeta [columnName=fieldtype_id  columnType=bigint(20)  defaultValue=null  nullable=false  key=true] 	FieldMeta [columnName=whereclause  columnType=longtext  defaultValue=null  nullable=false  key=false] 	FieldMeta [columnName=filtervariants  columnType=char(1)  defaultValue=null  nullable=false  key=false] ]   mem : TableMeta [schema=bboss  table=linkquerysellist  fileds= 	FieldMeta [columnName=fieldtype_id  columnType=bigint(20)  defaultValue=null  nullable=false  key=false] 	FieldMeta [columnName=whereclause  columnType=longtext  defaultValue=null  nullable=false  key=false] 	FieldMeta [columnName=filtervariants  columnType=char(1)  defaultValue=null  nullable=false  key=false] ] 2018-05-28 14:38:20.837 [destination = myexample   address = /172.16.9.11:3307   EventParser] ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - pls submit github issue  show create table ddl:CREATE TABLE `parentlinkedsellist` (   `fieldtype_id` bigint(20) NOT NULL   `whereclause` longtext NOT NULL   `filtervariants` char(1) NOT NULL   `linkfield` varchar(50) NOT NULL   UNIQUE KEY `fieldtype_id` (`fieldtype_id`) USING BTREE ) ENGINE=MyISAM DEFAULT CHARSET=utf8   compare failed .   db : TableMeta [schema=bboss  table=parentlinkedsellist  fileds= 	FieldMeta [columnName=fieldtype_id  columnType=bigint(20)  defaultValue=null  nullable=false  key=true] 	FieldMeta [columnName=whereclause  columnType=longtext  defaultValue=null  nullable=false  key=false] 	FieldMeta [columnName=filtervariants  columnType=char(1)  defaultValue=null  nullable=false  key=false] 	FieldMeta [columnName=linkfield  columnType=varchar(50)  defaultValue=null  nullable=false  key=false] ]   mem : TableMeta [schema=bboss  table=parentlinkedsellist  fileds= 	FieldMeta [columnName=fieldtype_id  columnType=bigint(20)  defaultValue=null  nullable=false  key=false] 	FieldMeta [columnName=whereclause  columnType=longtext  defaultValue=null  nullable=false  key=false] 	FieldMeta [columnName=filtervariants  columnType=char(1)  defaultValue=null  nullable=false  key=false] 	FieldMeta [columnName=linkfield  columnType=varchar(50)  defaultValue=null  nullable=false  key=false] ] 2018-05-28 14:38:21.359 [destination = myexample   address = /172.16.9.11:3307   EventParser] ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - pls submit github issue  show create table ddl:CREATE TABLE `querysellist` (   `fieldtype_id` bigint(20) NOT NULL   `query` longtext NOT NULL   `filtervariants` char(1) NOT NULL   `sort_order` varchar(1) NOT NULL   UNIQUE KEY `fieldtype_id` (`fieldtype_id`) USING BTREE ) ENGINE=MyISAM DEFAULT CHARSET=utf8   compare failed .   db : TableMeta [schema=bboss  table=querysellist  fileds= 	FieldMeta [columnName=fieldtype_id  columnType=bigint(20)  defaultValue=null  nullable=false  key=true] 	FieldMeta [columnName=query  columnType=longtext  defaultValue=null  nullable=false  key=false] 	FieldMeta [columnName=filtervariants  columnType=char(1)  defaultValue=null  nullable=false  key=false] 	FieldMeta [columnName=sort_order  columnType=varchar(1)  defaultValue=null  nullable=false  key=false] ]   mem : TableMeta [schema=bboss  table=querysellist  fileds= 	FieldMeta [columnName=fieldtype_id  columnType=bigint(20)  defaultValue=null  nullable=false  key=false] 	FieldMeta [columnName=query  columnType=longtext  defaultValue=null  nullable=false  key=false] 	FieldMeta [columnName=filtervariants  columnType=char(1)  defaultValue=null  nullable=false  key=false] 	FieldMeta [columnName=sort_order  columnType=varchar(1)  defaultValue=null  nullable=false  key=false] ] 2018-05-28 14:38:28.334 [destination = myexample   address = /172.16.9.11:3307   EventParser] ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - compare failed   check log 2018-05-28 14:38:28.356 [destination = myexample   address = /172.16.9.11:3307   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /172.16.9.11:3307 has an error  retrying. caused by  com.alibaba.otter.canal.parse.exception.CanalParseException: can't find init table meta for myexample with position : EntryPosition[included=false journalName=mysql-bin.000001 position=1314 serverId=1 timestamp=1527489302000] 2018-05-28 14:38:28.359 [destination = myexample   address = /172.16.9.11:3307   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:myexample[com.alibaba.otter.canal.parse.exception.CanalParseException: can't find init table meta for myexample with position : EntryPosition[included=false journalName=mysql-bin.000001 position=1314 serverId=1 timestamp=1527489302000] ] 2018-05-28 14:38:40.151 [destination = myexample   address = /172.16.9.11:3307   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just show master status 2018-05-28 14:40:01.773 [destination = myexample   address = /172.16.9.11:3307   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `columns_priv` (   `Host` char(60) COLLATE utf8_bin NOT NULL DEFAULT ''   `Db` char(64) COLLATE utf8_bin NOT NULL DEFAULT ''   `User` char(16) COLLATE utf8_bin NOT NULL DEFAULT ''   `Table_name` char(64) COLLATE utf8_bin NOT NULL DEFAULT ''   `Column_name` char(64) COLLATE utf8_bin NOT NULL DEFAULT ''   `Timestamp` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP   `Column_priv` set('Select' 'Insert' 'Update' 'References') CHARACTER SET utf8 NOT NULL DEFAULT ''   PRIMARY KEY (`Host` `Db` `User` `Table_name` `Column_name`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='Column privileges' com.alibaba.druid.sql.parser.ParserException: error pos 428  line 8  column 17  token SET 	at com.alibaba.druid.sql.parser.SQLExprParser.name(SQLExprParser.java:1334) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLExprParser.parseDataType(SQLExprParser.java:2310) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLExprParser.parseDataType(SQLExprParser.java:2301) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.dialect.mysql.parser.MySqlExprParser.parseColumn(MySqlExprParser.java:493) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:115) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.25.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101] 2018-05-28 14:40:01.775 [destination = myexample   address = /172.16.9.11:3307   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `db` (   `Host` char(60) COLLATE utf8_bin NOT NULL DEFAULT ''   `Db` char(64) COLLATE utf8_bin NOT NULL DEFAULT ''   `User` char(16) COLLATE utf8_bin NOT NULL DEFAULT ''   `Select_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Insert_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Update_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Delete_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Create_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Drop_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Grant_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `References_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Index_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Alter_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Create_tmp_table_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Lock_tables_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Create_view_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Show_view_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Create_routine_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Alter_routine_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Execute_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Event_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Trigger_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   PRIMARY KEY (`Host` `Db` `User`)   KEY `User` (`User`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='Database privileges' com.alibaba.druid.sql.parser.ParserException: syntax error  error in :''Y') CHARACTER SET utf8 NOT NULL DE'  expect RPAREN  actual IDENTIFIER pos 225  line 5  column 31  token IDENTIFIER CHARACTER 	at com.alibaba.druid.sql.parser.SQLParser.printError(SQLParser.java:284) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLParser.accept(SQLParser.java:292) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:191) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.25.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101] 2018-05-28 14:40:01.776 [destination = myexample   address = /172.16.9.11:3307   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `event` (   `db` char(64) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL DEFAULT ''   `name` char(64) NOT NULL DEFAULT ''   `body` longblob NOT NULL   `definer` char(77) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL DEFAULT ''   `execute_at` datetime DEFAULT NULL   `interval_value` int(11) DEFAULT NULL   `interval_field` enum('YEAR' 'QUARTER' 'MONTH' 'DAY' 'HOUR' 'MINUTE' 'WEEK' 'SECOND' 'MICROSECOND' 'YEAR_MONTH' 'DAY_HOUR' 'DAY_MINUTE' 'DAY_SECOND' 'HOUR_MINUTE' 'HOUR_SECOND' 'MINUTE_SECOND' 'DAY_MICROSECOND' 'HOUR_MICROSECOND' 'MINUTE_MICROSECOND' 'SECOND_MICROSECOND') DEFAULT NULL   `created` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP   `modified` timestamp NOT NULL DEFAULT '0000-00-00 00:00:00'   `last_executed` datetime DEFAULT NULL   `starts` datetime DEFAULT NULL   `ends` datetime DEFAULT NULL   `status` enum('ENABLED' 'DISABLED' 'SLAVESIDE_DISABLED') NOT NULL DEFAULT 'ENABLED'   `on_completion` enum('DROP' 'PRESERVE') NOT NULL DEFAULT 'DROP'   `sql_mode` set('REAL_AS_FLOAT' 'PIPES_AS_CONCAT' 'ANSI_QUOTES' 'IGNORE_SPACE' 'NOT_USED' 'ONLY_FULL_GROUP_BY' 'NO_UNSIGNED_SUBTRACTION' 'NO_DIR_IN_CREATE' 'POSTGRESQL' 'ORACLE' 'MSSQL' 'DB2' 'MAXDB' 'NO_KEY_OPTIONS' 'NO_TABLE_OPTIONS' 'NO_FIELD_OPTIONS' 'MYSQL323' 'MYSQL40' 'ANSI' 'NO_AUTO_VALUE_ON_ZERO' 'NO_BACKSLASH_ESCAPES' 'STRICT_TRANS_TABLES' 'STRICT_ALL_TABLES' 'NO_ZERO_IN_DATE' 'NO_ZERO_DATE' 'INVALID_DATES' 'ERROR_FOR_DIVISION_BY_ZERO' 'TRADITIONAL' 'NO_AUTO_CREATE_USER' 'HIGH_NOT_PRECEDENCE' 'NO_ENGINE_SUBSTITUTION' 'PAD_CHAR_TO_FULL_LENGTH') NOT NULL DEFAULT ''   `comment` char(64) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL DEFAULT ''   `originator` int(10) unsigned NOT NULL   `time_zone` char(64) CHARACTER SET latin1 NOT NULL DEFAULT 'SYSTEM'   `character_set_client` char(32) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL   `collation_connection` char(32) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL   `db_collation` char(32) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL   `body_utf8` longblob   PRIMARY KEY (`db` `name`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COMMENT='Events' com.alibaba.druid.sql.parser.ParserException: error pos 1035  line 16  column 14  token SET 	at com.alibaba.druid.sql.parser.SQLExprParser.name(SQLExprParser.java:1334) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLExprParser.parseDataType(SQLExprParser.java:2310) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLExprParser.parseDataType(SQLExprParser.java:2301) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.dialect.mysql.parser.MySqlExprParser.parseColumn(MySqlExprParser.java:493) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:115) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.25.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101] 2018-05-28 14:40:01.778 [destination = myexample   address = /172.16.9.11:3307   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `func` (   `name` char(64) COLLATE utf8_bin NOT NULL DEFAULT ''   `ret` tinyint(1) NOT NULL DEFAULT '0'   `dl` char(128) COLLATE utf8_bin NOT NULL DEFAULT ''   `type` enum('function' 'aggregate') CHARACTER SET utf8 NOT NULL   PRIMARY KEY (`name`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='User defined functions' com.alibaba.druid.sql.parser.ParserException: syntax error  error in :'te') CHARACTER SET utf8 NOT NULL  '  expect RPAREN  actual IDENTIFIER pos 221  line 5  column 39  token IDENTIFIER CHARACTER 	at com.alibaba.druid.sql.parser.SQLParser.printError(SQLParser.java:284) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLParser.accept(SQLParser.java:292) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:191) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.25.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101] 2018-05-28 14:40:01.780 [destination = myexample   address = /172.16.9.11:3307   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `proc` (   `db` char(64) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL DEFAULT ''   `name` char(64) NOT NULL DEFAULT ''   `type` enum('FUNCTION' 'PROCEDURE') NOT NULL   `specific_name` char(64) NOT NULL DEFAULT ''   `language` enum('SQL') NOT NULL DEFAULT 'SQL'   `sql_data_access` enum('CONTAINS_SQL' 'NO_SQL' 'READS_SQL_DATA' 'MODIFIES_SQL_DATA') NOT NULL DEFAULT 'CONTAINS_SQL'   `is_deterministic` enum('YES' 'NO') NOT NULL DEFAULT 'NO'   `security_type` enum('INVOKER' 'DEFINER') NOT NULL DEFAULT 'DEFINER'   `param_list` blob NOT NULL   `returns` longblob NOT NULL   `body` longblob NOT NULL   `definer` char(77) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL DEFAULT ''   `created` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP   `modified` timestamp NOT NULL DEFAULT '0000-00-00 00:00:00'   `sql_mode` set('REAL_AS_FLOAT' 'PIPES_AS_CONCAT' 'ANSI_QUOTES' 'IGNORE_SPACE' 'NOT_USED' 'ONLY_FULL_GROUP_BY' 'NO_UNSIGNED_SUBTRACTION' 'NO_DIR_IN_CREATE' 'POSTGRESQL' 'ORACLE' 'MSSQL' 'DB2' 'MAXDB' 'NO_KEY_OPTIONS' 'NO_TABLE_OPTIONS' 'NO_FIELD_OPTIONS' 'MYSQL323' 'MYSQL40' 'ANSI' 'NO_AUTO_VALUE_ON_ZERO' 'NO_BACKSLASH_ESCAPES' 'STRICT_TRANS_TABLES' 'STRICT_ALL_TABLES' 'NO_ZERO_IN_DATE' 'NO_ZERO_DATE' 'INVALID_DATES' 'ERROR_FOR_DIVISION_BY_ZERO' 'TRADITIONAL' 'NO_AUTO_CREATE_USER' 'HIGH_NOT_PRECEDENCE' 'NO_ENGINE_SUBSTITUTION' 'PAD_CHAR_TO_FULL_LENGTH') NOT NULL DEFAULT ''   `comment` text CHARACTER SET utf8 COLLATE utf8_bin NOT NULL   `character_set_client` char(32) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL   `collation_connection` char(32) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL   `db_collation` char(32) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL   `body_utf8` longblob   PRIMARY KEY (`db` `name` `type`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COMMENT='Stored Procedures' com.alibaba.druid.sql.parser.ParserException: error pos 864  line 16  column 14  token SET 	at com.alibaba.druid.sql.parser.SQLExprParser.name(SQLExprParser.java:1334) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLExprParser.parseDataType(SQLExprParser.java:2310) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLExprParser.parseDataType(SQLExprParser.java:2301) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.dialect.mysql.parser.MySqlExprParser.parseColumn(MySqlExprParser.java:493) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:115) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.25.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101] 2018-05-28 14:40:01.783 [destination = myexample   address = /172.16.9.11:3307   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `procs_priv` (   `Host` char(60) COLLATE utf8_bin NOT NULL DEFAULT ''   `Db` char(64) COLLATE utf8_bin NOT NULL DEFAULT ''   `User` char(16) COLLATE utf8_bin NOT NULL DEFAULT ''   `Routine_name` char(64) CHARACTER SET utf8 NOT NULL DEFAULT ''   `Routine_type` enum('FUNCTION' 'PROCEDURE') COLLATE utf8_bin NOT NULL   `Grantor` char(77) COLLATE utf8_bin NOT NULL DEFAULT ''   `Proc_priv` set('Execute' 'Alter Routine' 'Grant') CHARACTER SET utf8 NOT NULL DEFAULT ''   `Timestamp` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP   PRIMARY KEY (`Host` `Db` `User` `Routine_name` `Routine_type`)   KEY `Grantor` (`Grantor`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='Procedure privileges' com.alibaba.druid.sql.parser.ParserException: syntax error  error in :'RE') COLLATE utf8_bin NOT NULL   `'  expect RPAREN  actual IDENTIFIER pos 313  line 6  column 47  token IDENTIFIER COLLATE 	at com.alibaba.druid.sql.parser.SQLParser.printError(SQLParser.java:284) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLParser.accept(SQLParser.java:292) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:191) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.25.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101] 2018-05-28 14:40:01.789 [destination = myexample   address = /172.16.9.11:3307   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTab` 使用 mysql 配置后 又有下面的错误 `2018-05-28 15:06:27.531 [destination = myexample   address = /172.16.9.11:3307   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `user` (   `Host` char(60) COLLATE utf8_bin NOT NULL DEFAULT ''   `User` char(16) COLLATE utf8_bin NOT NULL DEFAULT ''   `Password` char(41) CHARACTER SET latin1 COLLATE latin1_bin NOT NULL DEFAULT ''   `Select_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Insert_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Update_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Delete_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Create_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Drop_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Reload_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Shutdown_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Process_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `File_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Grant_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `References_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Index_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Alter_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Show_db_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Super_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Create_tmp_table_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Lock_tables_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Execute_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Repl_slave_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Repl_client_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Create_view_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Show_view_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Create_routine_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Alter_routine_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Create_user_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Event_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Trigger_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Create_tablespace_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `ssl_type` enum('' 'ANY' 'X509' 'SPECIFIED') CHARACTER SET utf8 NOT NULL DEFAULT ''   `ssl_cipher` blob NOT NULL   `x509_issuer` blob NOT NULL   `x509_subject` blob NOT NULL   `max_questions` int(11) unsigned NOT NULL DEFAULT '0'   `max_updates` int(11) unsigned NOT NULL DEFAULT '0'   `max_connections` int(11) unsigned NOT NULL DEFAULT '0'   `max_user_connections` int(11) unsigned NOT NULL DEFAULT '0'   `plugin` char(64) COLLATE utf8_bin DEFAULT ''   `authentication_string` text COLLATE utf8_bin   `password_expired` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   PRIMARY KEY (`Host` `User`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='Users and global privileges'` 换成26版本试试，这是25版本的bug。 我升级到26也是这个问题。不过，我感觉这像是个权限问题，我给mysql中的canal用户赋了全部权限，就没问题了。 GRANT ALL PRIVILEGES ON *.* TO 'canal'@'%' ; FLUSH PRIVILEGES; LS正解 版本v1.0.25 开启 tsdb之后， 出现这种错误。删除之后就好了
658,canal启动报错（v1.0.25） 2018-05-28 01:42:57.009 [main] INFO  com.alibaba.otter.canal.deployer.CanalLauncher - ## the canal server is running now ...... 2018-05-28 01:42:57.876 [destination = example   address = /127.0.0.1:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just show master status 2018-05-28 01:44:10.494 [destination = example   address = /127.0.0.1:3306   EventParser] WARN  com.taobao.tddl.dbsync.binlog.CharsetConversion - Unexpect mysql charset: 246 2018-05-28 01:44:10.495 [destination = example   address = /127.0.0.1:3306   EventParser] WARN  com.taobao.tddl.dbsync.binlog.CharsetConversion - Unexpect mysql charset: 246 2018-05-28 01:44:10.495 [destination = example   address = /127.0.0.1:3306   EventParser] WARN  com.taobao.tddl.dbsync.binlog.CharsetConversion - Unexpect mysql charset: 246 2018-05-28 01:44:10.495 [destination = example   address = /127.0.0.1:3306   EventParser] WARN  com.taobao.tddl.dbsync.binlog.LogEvent - unsupported character set in query log:  这是警告 不是错误 @agapple @DevWithLin 两个canal实现主备模式其slaveId是否要相同（仅一个工作）？ 主备模式，应该不相同才是。1.0.26会默认随机生成一个
657,提出一个关于MemoryMetaManager优化的建议 在项目运行启动订阅时有遇到一个异常(项目采用的是CanalServerWithEmbedded内嵌，关键的配置MetaMode.MIXED & IndexMode.MEMORY_META_FAILBACK)，栈输出如下： ``` dump address /xxx.xx.xx.xxx:3306 has an error  retrying. caused by  java.util.ConcurrentModificationException 	at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:901)  	at java.util.ArrayList$Itr.next(ArrayList.java:851)  	at com.alibaba.otter.canal.parse.index.MetaLogPositionManager.getLatestIndexBy(MetaLogPositionManager.java:56)  	at com.alibaba.otter.canal.parse.index.FailbackLogPositionManager.getLatestIndexBy(FailbackLogPositionManager.java:68)  	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPositionInternal(MysqlEventParser.java:567)  	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPosition(MysqlEventParser.java:509) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:167)  	at java.lang.Thread.run(Thread.java:745)  ``` 看代码发现CanalServerWithEmbedded中的subscribe方法中有调用MetaManager的**subscribe**方法，最终会调用到MemoryMetaManager的subscribe方法，在得到destinations中key对应的list后，修改list（向list里add元素）。代码: ```     public synchronized void subscribe(ClientIdentity clientIdentity) throws CanalMetaManagerException {         List<ClientIdentity> clientIdentitys = destinations.get(clientIdentity.getDestination());         if (clientIdentitys.contains(clientIdentity)) {             clientIdentitys.remove(clientIdentity);         }         clientIdentitys.add(clientIdentity);     } ``` 而MemoryMetaManager类中还有一个方法**listAllSubscribeInfo**会直接返回destinations中key对应的list。代码： ```     public synchronized List<ClientIdentity> listAllSubscribeInfo(String destination) throws CanalMetaManagerException {         return destinations.get(destination);     } ``` 我遇到的问题是在MetaLogPositionManager的getLatestIndexBy方法遍历metaManager.listAllSubscribeInfo调用结果时，业务代码线程调用了subscribe方法修改了遍历的list集合，导致了ConcurrentModificationException。 ```     public LogPosition getLatestIndexBy(String destination) {         List<ClientIdentity> clientIdentities = metaManager.listAllSubscribeInfo(destination);         LogPosition result = null;         if (!CollectionUtils.isEmpty(clientIdentities)) {             // 尝试找到一个最小的logPosition             for (ClientIdentity clientIdentity : clientIdentities) {  //在此处遍历时链表被其它线程修改                 LogPosition position = (LogPosition) metaManager.getCursor(clientIdentity);                 if (position == null) {                     continue;                 }                 if (result == null) {                     result = position;                 } else {                     result = CanalEventUtils.min(result  position);                 }             }         }         return result;     } ``` 这里感觉在MemoryMetaManager中的listAllSubscribeInfo方法有比较大的隐患，外部拿到引用以后可能在不同的线程中做遍历或者修改。 如果这里每次都拷贝一个新list返回的话不知道对效率上的损失是否太大，如果不希望过多拷贝的话可以返回unmodifiableList，阻止其他类对这个集合的修改，但是这样在其它类遍历这个集合时还是可能出现我这种ConcurrentModificationException。 想问一下你对这个问题是怎么考虑的呢 @agapple 我也遇到这个问题了，我是在多个client订阅一个destination的场景 ` 2018-05-29 15:02:37.929 ERROR [test-firehose-20] c.a.o.c.c.a.LogAlarmHandler:19 - destination:test-firehose-20[java.util.ConcurrentModificationException 	at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:901) 	at java.util.ArrayList$Itr.next(ArrayList.java:851) 	at com.alibaba.otter.canal.parse.index.MetaLogPositionManager.getLatestIndexBy(MetaLogPositionManager.java:56) 	at com.alibaba.otter.canal.parse.index.FailbackLogPositionManager.getLatestIndexBy(FailbackLogPositionManager.java:68) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPositionInternal(MysqlEventParser.java:405) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPosition(MysqlEventParser.java:347) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:164) 	at java.lang.Thread.run(Thread.java:748) ` listAllSubscribeInfo修改为返回一个拷贝对象
656,最新release版本中 regex 过滤不起作用 canal-server 配置了regex如下 canal.instance.dbUsername=canal canal.instance.dbPassword=canal canal.instance.defaultDatabaseName=EDU_DISCOUNT_PRICE_TEST canal.instance.connectionCharset=UTF-8 # table regex #canal.instance.filter.regex=.*\\..* canal.instance.filter.regex=EDU_DISCOUNT_PRICE_TEST\\..* # table black regex #canal.instance.filter.black.regex=ONLINE_EDU_TEST\\..* ################################################# Client 代码 connector.subscribe("EDU_DISCOUNT_PRICE_TEST.t_online_discount"); 同样配置了过滤，但是仍然有不相关的库的表更新信息。数据库确认binlog格式为 ROW canal 定位问题真的很不方便。。。QQ群也加不进去，求解答。 `connector.subscribe("EDU_DISCOUNT_PRICE_TEST.t_online_discount"); ` 改成 `connector.subscribe("EDU_DISCOUNT_PRICE_TEST\\.t_online_discount");` LS正解
655,SimpleCanalClientTest 连Canal 一直报NullPointerException 错误 按照QuickStart 和 ClientSample 搭建了Canal 环境，但跑SimpleCanalClientTest 一直报一个NullPointerException 错误，反复对照检查了配置端口等，的确没发现什么问题，望能给点建议排查错误，具体错误信息如下： 2018-05-26 19:53:29.679 [Thread-1] ERROR com.alibaba.otter.canal.example.AbstractCanalClientTest - process error! java.lang.NullPointerException: null 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.writeWithHeader(SimpleCanalConnector.java:392) ~[classes/:na] 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.writeWithHeader(SimpleCanalConnector.java:380) ~[classes/:na] 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:292) ~[classes/:na] 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:280) ~[classes/:na] 	at com.alibaba.otter.canal.example.AbstractCanalClientTest.process(AbstractCanalClientTest.java:125) ~[classes/:na] 问题找到了，是本地1.26 snapshot 版本的源代码没有build成功导致的，换成1.25 成功build 后，问题消失
654,服务端总是意外关闭 版本1.0.22 ![image](https://user-images.githubusercontent.com/15358736/40471583-a9d97202-5f69-11e8-8c22-ae02a30da03a.png) 进程意外终止 是因为sql报错过多吗 建议使用1.0.26版本
653,ROW模式下rowChange：eventType = 'query' show binlog events 结果如下： ![aaaaaaaaaa](https://user-images.githubusercontent.com/21019407/40354822-61a69268-5de7-11e8-9809-49405492b60f.jpg) 客户端消费时Entry数量为3，分别是TRANSACTIONBEGIN，ROWDATA和TRANSACTIONEND，其中ROWDATA解析后rowChange打印如下： ![bbbbbbbbbb](https://user-images.githubusercontent.com/21019407/40354993-e068b05e-5de7-11e8-9cd9-985081534f54.jpg) 对应binlog配置： ![ccccccccc](https://user-images.githubusercontent.com/21019407/40355107-26765a9c-5de8-11e8-9907-80a065b961f9.jpg) 客户端配置错误，导致canal服务端忽略了对应表的binlog
652,parser  information_schema中的表出错 2018-05-22 15:12:39.374 [destination = example   address = zhenghongchen.cn/60.205.220.19:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TEMPORARY TABLE `SYSTEM_VARIABLES` (   `VARIABLE_NAME` varchar(64) NOT NULL DEFAULT ''   `SESSION_VALUE` varchar(2048) DEFAULT NULL   `GLOBAL_VALUE` varchar(2048) DEFAULT NULL   `GLOBAL_VALUE_ORIGIN` varchar(64) NOT NULL DEFAULT ''   `DEFAULT_VALUE` varchar(2048) DEFAULT NULL   `VARIABLE_SCOPE` varchar(64) NOT NULL DEFAULT ''   `VARIABLE_TYPE` varchar(64) NOT NULL DEFAULT ''   `VARIABLE_COMMENT` varchar(2048) NOT NULL DEFAULT ''   `NUMERIC_MIN_VALUE` varchar(21) DEFAULT NULL   `NUMERIC_MAX_VALUE` varchar(21) DEFAULT NULL   `NUMERIC_BLOCK_SIZE` varchar(21) DEFAULT NULL   `ENUM_VALUE_LIST` longtext DEFAULT NULL   `READ_ONLY` varchar(3) NOT NULL DEFAULT ''   `COMMAND_LINE_ARGUMENT` varchar(64) DEFAULT NULL ) ENGINE=Aria DEFAULT CHARSET=utf8 PAGE_CHECKSUM=0 com.alibaba.fastsql.sql.parser.ParserException: syntax error  error in :'utf8 PAGE_CHECKSUM=0'  expect IDENTIFIER  actual IDENTIFIER pos 781  line 16  column 50  token IDENTIFIER PAGE_CHECKSUM 	at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:305) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] 	at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:427) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] 	at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:76) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] 	at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:469) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] 	at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:331) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:71) ~[classes/:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [classes/:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [classes/:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:84) [classes/:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) [classes/:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_51] 找到原因是，fastsql-2.0.0_preview_186.jar对MariaDB的aria引擎的表支持的不好 版本1.0.26-SNAPSHOT
651,blob字段如何同步？ blob字段用column.getValue()的话是个string无法再转为byte[]，其中有个一个column.getValueBytes()方法好像也无法获取他原先的byte[]值
650,mysql5.7新建表会导致canal服务器挂起 ![qq 20180522085329](https://user-images.githubusercontent.com/30070755/40336666-a225b214-5d9d-11e8-9257-389d5d78cd0c.png) 建议使用1.0.26版本
649,fastsql的preview包无法下载 拉取最新的master代码后，本地编译失败，无法下载fastsql的preview包。 maven地址为： ```  <dependency>                 <groupId>com.alibaba.fastsql</groupId>                 <artifactId>fastsql</artifactId>                 <version>2.0.0_preview_228</version> </dependency> ```
648,TSDB show create table时没过滤View 开启TSDB之后，出现show create table找不到表的情况，仔细一看是因为我们的库中有一些view。 其实我有点不明白即是因为要用 show create table呢？像之前一样用 describe tablename 的方式不是很好吗？ show create table失败之后会回退到desc table name.  主要为解决unique key的获取，show create table针对多个唯一键时可以正确提取
647,rds_instance.properties配置文件中的几个参数 canal.instance.rds.open.accesskey= canal.instance.rds.open.secretkey= canal.instance.rds.instanceId= canal.instance.rds.startTime= canal.instance.rds.endTime= 这几个参数好像在介绍中没有说明意思。 麻烦给提供下。 要设置从固定binlog文件的位点读数据，还是用 canal.instance.master.journal.name= canal.instance.master.position= canal.instance.master.timestamp= 的配置吗？但是配置完后，删除 meta.dat 后没有生效，看到meta.dat里还是之前的位点而不是自己设置的位点。 
646,a little improve about the safety of canal high available 因为停顿导致的zookeeper会话超时(例如gc)  再次initRunning失败(`即使别的canal server延迟initRunning`)  需要停止对binglog的订阅  @agapple 关于这个前面我们有过简短的讨论 见 #600   我做了一些优化   请review一下 @agapple 麻烦看一下  3Q ^-^ @spccold 为啥关闭了？
645,a little improve about the safety of canal high available  因为停顿导致的zookeeper会话超时(例如gc)  再次initRunning失败(`即使别的canal server延迟initRunning`)  需要停止对binglog的订阅  @agapple 关于这个前面我们有过简短的讨论 见 #600   我做了一些优化   请review一下 [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=645) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=645) before we can accept your contribution.<br/><hr/>**wuwo** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=645) it.</sub> [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=645) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=645) before we can accept your contribution.<br/><hr/>**wuwo** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=645) it.</sub> see #646 
644,A slave with the same server_uuid/server_id as this slave has connected to the master MANAGER/NODE4.2.14在运行过程中不断出现相同的server_uuid/server_id，不知该如何消除这个错误？ pid:31 nid:11 exception:canal:MiddleEast:java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = A slave with the same server_uuid/server_id as this slave has connected to the master; the first event 'mysql-bin.000011' at 77235  the last event read from './mysql-bin.000012' at 150  the last byte read from './mysql-bin.000012' at 150.     at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95)     at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:121)     at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209)     at java.lang.Thread.run(Thread.java:748) 这个问题通过删除原存储着server_uuid的auto.cnf文件，并通过重启Mysql数据库重新生成新的server_uuid后，目前不再报以上错误信息了，观察一段时间再追加评论。 几天后，继续出现同样的故障报错。 经过试验，暂时判断为如果pipline开启了超时或者延时的报警监控，并且这些监控开启了自动重启选项，那么当监控认为超时后，会开启新的同步进程，而事实上远端的node只是因为网速过慢影响了相应时间，并没有真正的挂掉，因此开启新的进程后就导致了重复的slave在请求master。出现了“A slave with the same server_uuid/server_id as this slave has connected to the master”的提示，目前在监控上，只保留了“异常”的监控，删掉了其余的监控，运行还算正常，正在观察。 解决问题就好，新的canal 1.0.26会采用随机uuid的做法来规避
643,canal server log 显示parse error mysql为公司内部定制版本，对应mysql官方可能是5.5，请问如果要修改这里的解析规则，可行性高吗？如果可以的话，能提供一个学习路径，方便去定制这里的parser吗？感谢！ .mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `procs_priv` (   `Host` char(60) COLLATE utf8_bin NOT NULL DEFAULT ''   `Db` char(64) COLLATE utf8_bin NOT NULL DEFAULT ''   `User` char(16) COLLATE utf8_bin NOT NULL DEFAULT ''   `Routine_name` char(64) CHARACTER SET utf8 NOT NULL DEFAULT ''   `Routine_type` enum('FUNCTION' 'PROCEDURE') COLLATE utf8_bin NOT NULL   `Grantor` char(77) COLLATE utf8_bin NOT NULL DEFAULT ''   `Proc_priv` set('Execute' 'Alter Routine' 'Grant') CHARACTER SET utf8 NOT NULL DEFAULT ''   `Timestamp` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP   PRIMARY KEY (`Host` `Db` `User` `Routine_name` `Routine_type`)   KEY `Grantor` (`Grantor`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='Procedure privileges' com.alibaba.druid.sql.parser.ParserException: syntax error  error in :'RE') COLLATE utf8_bin NOT NULL   `'  expect RPAREN  actual IDENTIFIER pos 313  line 6  column 47  token IDENTIFIER COLLATE 	at com.alibaba.druid.sql.parser.SQLParser.printError(SQLParser.java:284) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLParser.accept(SQLParser.java:292) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:191) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.6.jar:1.1.6] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.25.jar:na] 使用当前最新版本canal，已经修复
642,WritableByteChannel channel 为null 使用版本为1.0.26  CanalConnector connector = CanalConnectors.newSingleConnector(new InetSocketAddress(ip  11111)             destination "" ""); 1.在创建CanalConnector 对象之后发现channel 为null不知道是什么问题 2.mysql的账号，也是根据教程的语句直接生成的 3.server也启动了 start cmd :  java   -Xms128m -Xmx512m -XX:PermSize=128m  -Djava.awt.headless=true -Djava.net.preferIPv4Stack=true -Dapplication.codeset=UTF-8 -Dfile.encoding=UTF-8  -server -Xdebug -Xnoagent -Djava.compiler=NONE -Xrunjdwp:transport=dt_socket address=9099 server=y suspend=n  -DappName=otter-canal -Dlogback.configurationFile="E:\canal-dep\bin\\..\conf\logback.xml" -Dcanal.conf="E:\canal-dep\bin\\..\conf\canal.properties" -classpath "E:\canal-dep\bin\\..\conf\..\lib\*;E:\canal-dep\bin\\..\conf" java   -Xms128m -Xmx512m -XX:PermSize=128m  -Djava.awt.headless=true -Djava.net.preferIPv4Stack=true -Dapplication.codeset=UTF-8 -Dfile.encoding=UTF-8  -server -Xdebug -Xnoagent -Djava.compiler=NONE -Xrunjdwp:transport=dt_socket address=9099 server=y suspend=n  -DappName=otter-canal -Dlogback.configurationFile="E:\canal-dep\bin\\..\conf\logback.xml" -Dcanal.conf="E:\canal-dep\bin\\..\conf\canal.properties" -classpath "E:\canal-dep\bin\\..\conf\..\lib\*;E:\canal-dep\bin\\..\conf" com.alibaba.otter.canal.deployer.CanalLauncher Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0 Listening for transport dt_socket at address: 9099 2018-05-17 15:17:09.947 [main] WARN  o.s.beans.GenericTypeAwarePropertyDescriptor - Invalid JavaBean property 'connectionCharset' being accessed! Ambiguous write methods found next to actually used [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.nio.charset.Charset)]: [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.lang.String)] 2018-05-17 15:17:10.143 [main] ERROR com.alibaba.druid.pool.DruidDataSource - testWhileIdle is true  validationQuery not set 2018-05-17 15:17:10.427 [destination = example   address = /192.168.2.165:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just show master status 4. 运行canal.example-1.0.26-SNAPSHOT 闪退 自己编译的包？ 已经修复，可以重新下载1.0.26-alpha 3
641,表结构解析失败，unknow column tag: canal-1.0.26-preview-2 表中有一列名：conf_key 2018-05-17 13:28:36.412 [destination = xxxxx   address = xxxxx   EventParser] ERROR c.a.otter.canal.p arse.inbound.mysql.MysqlEventParser - dump address /10.4.217.125:5002 has an error  retrying. caused by java.lang.RuntimeException: unknow column : `conf_key`(8)         at com.alibaba.otter.canal.parse.inbound.TableMeta.getFieldMetaByName(TableMeta.java:74) ~[canal.parse-1.0.26-SN APSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.processTableElement(MemoryTableMeta.java:227 ) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.parse(MemoryTableMeta.java:153) ~[canal.pars e-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.find(MemoryTableMeta.java:108) ~[canal.parse -1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.compareTableMetaDbAndMemory(DatabaseTableM eta.java:289) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.applySnapshotToDB(DatabaseTableMeta.java:2 51) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:129) ~[can al.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParse r.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) ~[canal.parse-1 .0.26-SNAPSHOT.jar:na]         at java.lang.Thread.run(Thread.java:745) [na:1.8.0_45] ![image](https://user-images.githubusercontent.com/1378499/40158776-dc4bd130-59d8-11e8-9f1c-7c82d3d4774a.png) 和1.0.25 比较，MemoryTableMeta 删掉这几行，就不会出错了
640,channal经常关闭 你好，我这边运行一段时间后，几小时吧，confi下面配置了9个 instance 然后有些instance下面就报下面的日志了，binlog文件变更也拿不到了，经常遇到这种问题，代码里面，是一直获取数据的状态，没哟关闭channal的操作。 2018-05-16 16:07:37.638 [New I/O server worker #1-7] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x58f255d8  /192.168.47.232:59420 :> /192.16 8.47.243:11111]  exception=java.nio.channels.ClosedChannelException         at org.jboss.netty.channel.socket.nio.NioWorker.cleanUpWriteBuffer(NioWorker.java:643)         at org.jboss.netty.channel.socket.nio.NioWorker.writeFromUserCode(NioWorker.java:370)         at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:137)         at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76)         at org.jboss.netty.channel.Channels.write(Channels.java:611)         at org.jboss.netty.channel.Channels.write(Channels.java:578)         at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:28)         at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:144)         at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48)         at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:275)         at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302)         at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndFireMessageReceived(ReplayingDecoder.java:525)         at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:506)         at org.jboss.netty.handler.codec.replay.ReplayingDecoder.cleanup(ReplayingDecoder.java:541)         at org.jboss.netty.handler.codec.replay.ReplayingDecoder.channelDisconnected(ReplayingDecoder.java:449)         at org.jboss.netty.channel.Channels.fireChannelDisconnected(Channels.java:360)         at org.jboss.netty.channel.socket.nio.NioWorker.close(NioWorker.java:593)         at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:119)         at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76)         at org.jboss.netty.channel.Channels.close(Channels.java:720)         at org.jboss.netty.channel.AbstractChannel.close(AbstractChannel.java:200)         at org.jboss.netty.channel.ChannelFutureListener$1.operationComplete(ChannelFutureListener.java:46)         at org.jboss.netty.channel.DefaultChannelFuture.notifyListener(DefaultChannelFuture.java:381)         at org.jboss.netty.channel.DefaultChannelFuture.addListener(DefaultChannelFuture.java:148)         at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:30)         at com.alibaba.otter.canal.server.netty.NettyUtils.error(NettyUtils.java:51)         at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:200)         at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48)         at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:275)         at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302)         at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndFireMessageReceived(ReplayingDecoder.java:525)         at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:506)         at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:443)         at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274)         at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261)         at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:349)         at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:280)         at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:200)         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)         at java.lang.Thread.run(Thread.java:722) 我这也出现了类型你这种 something goes wrong with channel错误，我的是只要客户端一直连着服务器就没事，我测试过客户端跑了一天一夜是没有断过的，但是客户端一旦不连接canal服务端，超过一定时间(具体多久不清楚)，再次连接服务端时，就会出现下面的这种错误。 2018-05-15 15:13:52.219 [New I/O server worker #1-2] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x24e1e7e8  /192.168.2.128:55307   => /192.168.1.38:11111  ]  exception=java.io .IOException: Connection reset by peer at sun.nio.ch  .FileDispatcherImpl.read0(Native Method) at sun.nio.ch .SocketDispatcher.read(SocketDispatcher.java:39) at sun.nio.ch .IOUtil.readIntoNativeBuffer(IOUtil.java:223) at sun.nio.ch .IOUtil.read(IOUtil.java:192) at sun.nio.ch .SocketChannelImpl.read(SocketChannelImpl.java:379)         at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:322)         at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281)         at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201)         at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46)         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)         at java.lang.Thread.run(Thread.java:745) 请说明下canal的版本。 谢谢回答！！我用的是1.0.25. @DevWithLin  你换成1.0.26试试。 能说明下是什么原因导致的嘛？？ @DevWithLin  25版本使用了nio做binlog读取，不太稳定，不建议使用了 Netty有idle超时close机制，客户端在创建connector时可以设置，如果不设置服务端默认5分钟主动断开channel，再次获取数据就会报错，需要重连connector。 com.alibaba.otter.canal.server.netty.handler.ClientAuthenticationHandler IdleStateHandler idleStateHandler = new IdleStateHandler(NettyUtils.hashedWheelTimer     readTimeout     writeTimeout     0     TimeUnit.MILLISECONDS); ctx.getPipeline().addBefore(SessionHandler.class.getName()     IdleStateHandler.class.getName()     idleStateHandler); IdleStateAwareChannelHandler idleStateAwareChannelHandler = new IdleStateAwareChannelHandler() {     public void channelIdle(ChannelHandlerContext ctx  IdleStateEvent e) throws Exception {         logger.warn("channel:{} idle timeout exceeds  close channel to save server resources..."             ctx.getChannel());         ctx.getChannel().close();     } }; ctx.getPipeline().addBefore(SessionHandler.class.getName()     IdleStateAwareChannelHandler.class.getName()     idleStateAwareChannelHandler); @agapple  26版本。会不定时重启instance，我一个canal服务大概20个instance。这个有限制吗 没有instance的数量限制啊，是否开启了scan=true，找到变更的文件 25版本使用了nio做binlog读取，不太稳定，不建议使用了。。。 @agapple 那您建议使用哪个版本？可以稳定的读取binlog日志 1.0.26 1.0.26也会时不时出现 Connection reset by peer，此时binlog变更可被捕捉。使用的客户端是官方example fa34eb0e59f257574996cb03da58cd2e8353e94d，调整了下默认的空闲链接超时时间，之前只有1分钟空闲超时，调整为1小时 ![image](https://user-images.githubusercontent.com/834743/41271502-9e424160-6e43-11e8-984b-29f03d187889.png) 大伙对超时的管理不够精细，只能调大进行粗放型管理 我用的1.0.25版本，channel也老被关闭，一旦被关闭了 位点都不会再移动了，但是清掉meta.dat 重启就又ok了。如果是因为超时原因导致channel关闭了，eventParser会重新和mysql建立连接，按道理讲位点应该继续移动才对。 @fangchunsheng 你的位点可能是另一个问题，1.0.25和mysql交互使用了netty nio的模式，在26版本换成了bio，可以试试1.0.26
639,异常：fetch failed by table meta，版本1.0.26，tsdb.enable=true 原sql是对表card_record 进行了两次alter操作 alter table card_record modify column customization_id bigint unsigned NOT NULL COMMENT '定制id' | 查看详情 alter table card_record modify column upgraded_customization_id bigint unsigned NOT NULL COMMENT '升级后定制id' | 查看详情 线上一次alter操作步骤是： 1. DROP TABLE IF EXISTS `_card_record_gho` 2. DROP TABLE IF EXISTS `_card_record_del` 3. create table `yushitai_test`.`_card_record_gho` like `yushitai_test`.`card_record` 4. alter table `yushitai_test`.`_card_record_gho`  modify column customization_id bigint unsigned NOT NULL COMMENT 'ŚģöŚą∂id' 5. insert 数据到_card_record_gho 6. rename table `yushitai_test`.`card_record` to `yushitai_test`.`_card_record_del` `yushitai_test`.`_card_record_gho` to `yushitai_test`.`card_record` 手动设置位点到两条alter操作时间之前，重新消费时，当执行第二次alter 操作时，到步骤5时，会出现exception： fetch failed by table meta:`yushitai_test`.`_card_record_gho` 2018-05-14 15:32:44.078 [destination = yushitai_test   address = /10.32.200.228:5002   EventParser] WARN  c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : DROP TABLE IF EXISTS `_card_record_gho` /* generated by server */ 2018-05-14 15:32:44.078 [destination = yushitai_test   address = /10.32.200.228:5002   EventParser] WARN  c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : DROP TABLE IF EXISTS `_card_record_del` /* generated by server */ 2018-05-14 15:32:44.078 [destination = yushitai_test   address = /10.32.200.228:5002   EventParser] WARN  c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : DROP TABLE IF EXISTS `_card_record_ghc` /* generated by server */ 2018-05-14 15:32:44.079 [destination = yushitai_test   address = /10.32.200.228:5002   EventParser] WARN  c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : create /* gh-ost */ table `yushitai_test`.`_card_record_ghc` ( id bigint auto_increment last_update timestamp not null DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP hint varchar(64) charset ascii not null value varchar(255) charset ascii not null primary key(id) unique key hint_uidx(hint) ) auto_increment=256 2018-05-14 15:32:44.079 [destination = yushitai_test   address = /10.32.200.228:5002   EventParser] WARN  c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : create /* gh-ost */ table `yushitai_test`.`_card_record_gho` like `yushitai_test`.`card_record` 2018-05-14 15:32:44.080 [destination = yushitai_test   address = /10.32.200.228:5002   EventParser] WARN  c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : alter /* gh-ost */ table `yushitai_test`.`_card_record_gho`  modify column customization_id bigint unsigned NOT NULL COMMENT 'ŚģöŚą∂id' 2018-05-14 15:32:44.085 [destination = yushitai_test   address = /10.32.200.228:5002   EventParser] WARN  c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : create /* gh-ost */ table `yushitai_test`.`_card_record_del` ( id int auto_increment primary key ) engine=InnoDB comment='ghost-cut-over-sentry' 2018-05-14 15:32:44.090 [destination = yushitai_test   address = /10.32.200.228:5002   EventParser] WARN  c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : DROP TABLE IF EXISTS `_card_record_del` /* generated by server */ 2018-05-14 15:32:44.090 [destination = yushitai_test   address = /10.32.200.228:5002   EventParser] WARN  c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : rename /* gh-ost */ table `yushitai_test`.`card_record` to `yushitai_test`.`_card_record_del`  `yushitai_test`.`_card_record_gho` to `yushitai_test`.`card_record` 2018-05-14 15:32:44.091 [destination = yushitai_test   address = /10.32.200.228:5002   EventParser] WARN  c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : DROP TABLE IF EXISTS `_card_record_ghc` /* generated by server */ 2018-05-14 15:32:44.104 [destination = yushitai_test   address = /10.32.200.228:5002   EventParser] WARN  c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : rename table `_card_record_del` to _card_record_del_bak20180508125310 2018-05-14 15:32:45.093 [destination = yushitai_test   address = /10.32.200.228:5002   EventParser] WARN  c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : DROP TABLE IF EXISTS `_card_record_gho` /* generated by server */ 2018-05-14 15:32:45.094 [destination = yushitai_test   address = /10.32.200.228:5002   EventParser] WARN  c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : DROP TABLE IF EXISTS `_card_record_del` /* generated by server */ 2018-05-14 15:32:45.094 [destination = yushitai_test   address = /10.32.200.228:5002   EventParser] WARN  c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : DROP TABLE IF EXISTS `_card_record_ghc` /* generated by server */ 2018-05-14 15:32:45.096 [destination = yushitai_test   address = /10.32.200.228:5002   EventParser] WARN  c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : create /* gh-ost */ table `yushitai_test`.`_card_record_ghc` ( id bigint auto_increment last_update timestamp not null DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP hint varchar(64) charset ascii not null value varchar(255) charset ascii not null primary key(id) unique key hint_uidx(hint) ) auto_increment=256 2018-05-14 15:32:45.096 [destination = yushitai_test   address = /10.32.200.228:5002   EventParser] WARN  c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : create /* gh-ost */ table `yushitai_test`.`_card_record_gho` like `yushitai_test`.`card_record` 2018-05-14 15:32:45.097 [destination = yushitai_test   address = /10.32.200.228:5002   EventParser] WARN  c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : alter /* gh-ost */ table `yushitai_test`.`_card_record_gho`  modify column upgraded_customization_id bigint unsigned NOT NULL COMMENT 'ŚćáÁļßŚźéŚģöŚą∂id' 2018-05-14 15:32:45.103 [destination = yushitai_test   address = /10.32.200.228:5002   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - ERROR ## parse this event has an error   last position : [EntryPosition[included=false journalName=mysql-bin.000056 position=286998741 serverId=32114196 timestamp=1525755246000]] com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: fetch failed by table meta:`yushitai_test`.`_card_record_gho` Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: fetch failed by table meta:`yushitai_test`.`_card_record_gho` Caused by: java.io.IOException: ErrorPacket [errorNumber=1146  fieldCount=-1  message=Table 'yushitai_test._card_record_gho' doesn't exist  sqlState=42S02  sqlStateMarker=#]  with command: show create table `yushitai_test`.`_card_record_gho` at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.query(MysqlQueryExecutor.java:61) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:94) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:167) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:759) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:428) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:114) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:66) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:337) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:184) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:152) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_45] 定位为一个ddl解析bug
638,为什么address存储的是hostname而不是 ip  {   "@type": "com.alibaba.otter.canal.protocol.position.LogPosition"   "identity": {     "slaveId": -1     "sourceAddress": {       "address": "DESKTOP-1RFA2LT"       "port": 3306     }   }   "postion": {     "included": false     "journalName": "mysql-bin.000011"     "position": 3382     "serverId": 1     "timestamp": 1526277481000   } } 默认序列化之后的结果，没有特殊处理
637,canal1.0.22 启动问题 2018-05-11 00:00:24.243 [destination = yuntu-management   address = rm-wz91b2j3ypnw1fbr6o.mysql.rds.aliyuncs.com/112.74.151.78:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - disconnect address rm-wz91b2j3ypnw1fbr6o.mysql.rds.aliyuncs.com/112.74.151.78:3306 has an error  retrying.  caused by  java.io.IOException: KILL DUMP 470730777 failure:java.io.IOException: ErrorPacket [errorNumber=1094  fieldCount=-1  message=Unknown thread id: 470730777  sqlState=HY000  sqlStateMarker=#]  with command: KILL CONNECTION 470730777 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlUpdateExecutor.update(MysqlUpdateExecutor.java:49) 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.disconnect(MysqlConnector.java:104) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.disconnect(MysqlConnection.java:60) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:242) 	at java.lang.Thread.run(Thread.java:745) 尝试用一下最新的1.0.26版本
636,canal 的file store 或mix store 有计划什么时候开发？ 如题。 暂时没有，可以对接一下数据写入到kafka中 @agapple   这几天也在犹豫写到kafka还是rocketmq，有什么建议吗？ 我觉得都可以把，我们公司用的是rocketmq @agapple 你好，想请教下。我们现在自研数据管道服务，其中一点不是很清楚，binlog解析之后写到mq是不是好一些？这样就可以通过mq提供订阅服务了，这是比较简单的做法，下游的client也可和管道服务进行解耦。 但是canal自己实现类似mq的一套复杂的订阅api，虽然有提供java客户端sdk，但是go、rust这些都没有，不方便使用。 所以，有几个问题，想请教下： 1. canal自己实现订阅api是基于什么考虑呢？是考虑mq的一些潜在的坑，比如消息大小限制、atleast once等因素吗？ 2. canal的订阅api虽然提供让client ack的机制。这应该是想尽量让client不收到重复消息（假设client每处理一个消息之后就ack）。但是ack位点有做持久化和容灾吗？也即是canal leader切换后，client 不会重复收到某条消息 OR 仍然有可能收到重复的消息。 3. 如果client有可能收到重复消息，建议客户端是如何做幂等处理？ 4. 对于大事务（比如好几G大小），canal的api是怎么处理呢？ hello，anybody here？ canal的server/client结构主要是基于运维管理的角度考虑，之前有想法是内置一套kafka的能力，但考虑成熟度，建议是直接用client接到数据后存储到MQ一份
634,Canal是如何在高并发下保证事务一致的 Hi， 想问一下下述情况Canal是如何实现的： 事务A 先发起 更新 某个字段为A 事务B 后发起 更新 上述字段为B 但是A事务在B事务后提交 Canal是如何保证值更新正确的呢 canal只是负责解析binlog而已，你先理解一下何谓binlog
633,binlog位点前移，ddl操作丢失 canal 版本：1.0.25 手动设置cursor的timestamp至 t1，当前时间为t2，t2 > t1 如果t1~t2这段时间之内产生了ddl 操作，binlog 位点前移至t1，重新消费这段时间的binlog，ddl操作丢失 canal server log 如下： canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : CREATE TABLE     canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : alter table screen_fee add  这不是ddl丢失，而是回到了T1时间之后，在重新解析发现在t1 -> t2中有ddl，重新加入到tsdb中发现已经加过了
631,MySQL驱动包支持Boolean类型，canal不支持boolean类型转换 MySQL的驱动包中对tinyint的长度为1，对应sqlType=-7 转化为Java布尔类型，参考MySQL驱动源码 ![image](https://user-images.githubusercontent.com/9798724/39748475-bff67406-52e2-11e8-9233-21df121524b9.png) 而Canal对对tinyint的长度为1，对应sqlType=-6，无法处理为Java的布尔类型 没有专门针对tinyint(1)做过特殊处理，目前统一按照了tinyint类型-6 有计划支持吗？ 可以考虑提交一个PR给我 https://github.com/alibaba/canal/pull/682 @agapple please assign the issue to me :)
630,同一个server 无法开启两个客户端订阅 如题 启动一个server 然后在同一个客户端 用两个线程订阅  会报 batchid 不存在 是因为订阅端必须唯一 还是因为某个订阅ID一致的原因? 一个instance同时只能一个订阅成功，其他client会阻塞standby，等待工作中的client释放订阅后才能订阅成功 LS正解
629,如何配置只订阅指定库 而不是所有DB 如题 可以配置canal.instance.filter.regex 参考[AdminGuide](https://github.com/alibaba/canal/wiki/AdminGuide) 已解决 被客户端覆盖了
628,DML 的 rowChage.getSql() 获取不到sql信息 DML 的 rowChage.getSql() 获取不到sql信息 @rewerma getSql()方法是获取DDL语句的 LS正解 那么DML就无法获取SQL了？maxwell是能够获取到sql，希望加入此功能 你要理解它获取的是什么event事件的sql，5.6之后有一个rowsQueryLogEvent，有对应DML的sql文本
627,canal server经常会挂，而且进程不退出 canal server经常会挂，而且进程不退出，根本监测不到server是不是挂了，本来写了个监控进程脚本挂了就重启，但是没什么用，每次重启都要删掉meta.dat ，否则会报 线程ID 不存在。。 希望能做的人性化点。 补充一下，在亚马逊rds上用这个，域名访问，经常域名对应的IP会自动变动，变化时canal server就是解析不到ip，log里一直报错。只有重启canal server才能解决。维护很麻烦，是否可以加上自动重连功能 一直都有自动重试的机制，怀疑是jvm的dns cache，导致ip解析到的一直是同一个ip，可以尝试关闭jvm dns cache
626,支持MySQL5.7的XA binlog事件 支持一下MySQL 5.7之后的XA能力：https://mysqlserverteam.com/improvements-to-xa-support-in-mysql-5-7/ ` TRANSACTION_CONTEXT_EVENT= 36   VIEW_CHANGE_EVENT= 37   /* Prepared XA transaction terminal event similar to Xid */   XA_PREPARE_LOG_EVENT= 38 ` > | mysql-bin.000011 | 13688 | Gtid           |         1 |       13753 | SET @@SESSION.GTID_NEXT= 'f1ceb61a-a5d5-11e7-bdee-107c3dbcf8a7:26' | | mysql-bin.000011 | 13753 | Query          |         1 |       13846 | XA START X'74657374' X'' 1                                         | | mysql-bin.000011 | 13846 | Rows_query     |         1 |       13897 | # update test set id = id + 5                                      | | mysql-bin.000011 | 13897 | Table_map      |         1 |       13985 | table_id: 256 (test.test)                                          | | mysql-bin.000011 | 13985 | Update_rows_v1 |         1 |       14167 | table_id: 256 flags: STMT_END_F                                    | | mysql-bin.000011 | 14167 | Query          |         1 |       14258 | XA END X'74657374' X'' 1                                           | | mysql-bin.000011 | 14258 | XA_prepare     |         1 |       14298 | XA PREPARE X'74657374' X'' 1                                       | | mysql-bin.000011 | 14298 | Gtid           |         1 |       14363 | SET @@SESSION.GTID_NEXT= 'f1ceb61a-a5d5-11e7-bdee-107c3dbcf8a7:27' | | mysql-bin.000011 | 14363 | Query          |         1 |       14459 | XA ROLLBACK X'74657374' X'' 1                                      | | mysql-bin.000011 | 14459 | Gtid           |         1 |       14524 | SET @@SESSION.GTID_NEXT= 'f1ceb61a-a5d5-11e7-bdee-107c3dbcf8a7:28' | | mysql-bin.000011 | 14524 | Query          |         1 |       14617 | XA START X'74657374' X'' 1                                         | | mysql-bin.000011 | 14617 | Rows_query     |         1 |       14668 | # update test set id = id + 5                                      | | mysql-bin.000011 | 14668 | Table_map      |         1 |       14756 | table_id: 256 (test.test)                                          | | mysql-bin.000011 | 14756 | Update_rows_v1 |         1 |       14938 | table_id: 256 flags: STMT_END_F                                    | | mysql-bin.000011 | 14938 | Query          |         1 |       15029 | XA END X'74657374' X'' 1                                           | | mysql-bin.000011 | 15029 | XA_prepare     |         1 |       15069 | XA PREPARE X'74657374' X'' 1                                       | | mysql-bin.000011 | 15069 | Gtid           |         1 |       15134 | SET @@SESSION.GTID_NEXT= 'f1ceb61a-a5d5-11e7-bdee-107c3dbcf8a7:29' | | mysql-bin.000011 | 15134 | Query          |         1 |       15228 | XA COMMIT X'74657374' X'' 1      mysql  xa的binlog提交格式，会在prepare时就记录到binlog并且对外可见，而后续会对xa继续进行commit或者rollback，如果执行rollback时还需要执行回滚对应的SQL(xa rollback没有记录对应的逆向binlog，需要在内存里暂缓存xa prepare的变更) http://mysql.taobao.org/monthly/2017/11/06/， 一片比较不错介绍MySQL XA 上面地址404... 上面地址去掉后面的逗号就能看了  http://mysql.taobao.org/monthly/2017/11/06/ 输出格式类似： ``` ================> binlog[mysql-bin.000012:35614]   BEGIN ----> Thread id: 3 ------> XA START  X'63' X'' 1  END ----> transaction id: 0 ------> XA END  X'63' X'' 1 ================> binlog[mysql-bin.000012:36022] ----------------> binlog[mysql-bin.000012:35459]  ------> XA ROLLBACK  X'62' X'' 1 ----------------> binlog[mysql-bin.000012:36209]  ------> XA COMMIT  X'63' X'' 1 ``` 打印xa信息： ``` protected void printXAInfo(List<Pair> pairs) {         if (pairs == null) {             return;         }         String xaType = null;         String xaXid = null;         for (Pair pair : pairs) {             String key = pair.getKey();             if (StringUtils.endsWithIgnoreCase(key  "XA_TYPE")) {                 xaType = pair.getValue();             } else if (StringUtils.endsWithIgnoreCase(key  "XA_XID")) {                 xaXid = pair.getValue();             }         }         if (xaType != null && xaXid != null) {             logger.info(" ------> " + xaType + " " + xaXid);         }     } ```
625, batchId:73 is not the firstly:72 2018-05-06 10:26:53.805 [destination = example   address = /127.0.0.1:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position  {"identity":{"slaveId":-1 "sourceAddress":{"address":"localhost" "port":3306}} "postion":{"included":false "journalName":"mysql-bin.000002" "position":26359074 "serverId":1 "timestamp":1525570849000}} 2018-05-06 10:26:53.845 [destination = example   address = /127.0.0.1:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000002 position=26359074 serverId=1 timestamp= 1525570849000] 2018-05-06 10:29:45.154 [New I/O server worker #1-1] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400   Caused by :  something goes wrong with channel:[id: 0x61b78711  /192.168.0.111:60228 => /192.168.0.111:11111]  exception=com.alibaba.otter.canal.meta.exception.CanalMetaManagerException: batchId:72 is not the firstly:71 2018-05-06 10:29:45.171 [New I/O server worker #1-1] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x61b78711  /192.168.0.111:60228 :> /192.168.0.111:11111]  exception=java.nio.channels.ClosedChannelException 	at org.jboss.netty.channel.socket.nio.NioWorker.cleanUpWriteBuffer(NioWorker.java:649) 	at org.jboss.netty.channel.socket.nio.NioWorker.writeFromUserCode(NioWorker.java:370) 	at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:137) 	at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) 	at org.jboss.netty.channel.Channels.write(Channels.java:611) 	at org.jboss.netty.channel.Channels.write(Channels.java:578) 	at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:28) 	at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:144) 	at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) 	at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.cleanup(ReplayingDecoder.java:542) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.channelDisconnected(ReplayingDecoder.java:450) 	at org.jboss.netty.channel.Channels.fireChannelDisconnected(Channels.java:360) 	at org.jboss.netty.channel.socket.nio.NioWorker.close(NioWorker.java:599) 	at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:119) 	at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) 	at org.jboss.netty.channel.Channels.close(Channels.java:720) 	at org.jboss.netty.channel.AbstractChannel.close(AbstractChannel.java:208) 	at org.jboss.netty.channel.ChannelFutureListener$1.operationComplete(ChannelFutureListener.java:46) 	at org.jboss.netty.channel.DefaultChannelFuture.notifyListener(DefaultChannelFuture.java:381) 	at org.jboss.netty.channel.DefaultChannelFuture.addListener(DefaultChannelFuture.java:148) 	at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:30) 	at com.alibaba.otter.canal.server.netty.NettyUtils.error(NettyUtils.java:51) 	at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:200) 	at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) 	at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:444) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261) 	at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:350) 	at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) 	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) 	at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) 	at java.lang.Thread.run(Thread.java:748) 没有顺序ack 没有顺序ack？什么意思？能不能说得更详细点，谢谢 @mennyzfy 调用getWithoutAck获取Message的时候会生成一个递增的batchId并赋值到Message的id字段。在ack的时候需要客户端保证按照获取Message的顺序来ack确认消息。 @zwangbo 我是按照这个demo来运行的，下面是代码： while (running) {             try {                 MDC.put("destination"  destination);                 connector.connect();                 connector.subscribe("");                 while (running) {                     Message message = connector.getWithoutAck(batchSize); // 获取指定数量的数据                     long batchId = message.getId();                     int size = message.getEntries().size();                     if (batchId == -1 || size == 0) {                         try {                             Thread.sleep(1000);                         } catch (InterruptedException e) {                         }                     } else {                             resolveEntry(message.getEntries());                     }                     connector.ack(batchId); // 提交确认                 }             } catch (Exception e) {                 logger.error("process error!"  e);             } finally {                 connector.disconnect();                 MDC.remove("destination");             }         } 也是消息被消费后才确认的，而且是单线程，为什么顺序还会不一样？ @agapple  虽然，确实是由于没有顺序确认ack引起的，但是，这无形中就让我的吞吐量和tps下降了不少，我这边是通过开多线程处理的，现在只能单线程处理 @cjj137783 这一块可以考虑自己走异步处理然后做一个buffer来控制ack顺序以及ack不丢。虽然我觉得这个功能实现在canal这一边也是不错的。 otter的做法就是做了一个异步buffer，来顺序ack batchId @mennyzfy 怎么解决的问题啊 为啥会漏掉一个branchID 参考FAQ :  https://github.com/alibaba/canal/wiki/FAQ
624,Caused by: java.net.ConnectException: Connection refused: connect canal服务端已经成功运行，新建maven项目测试，出现拒绝连接请求。 ![](https://ww1.sinaimg.cn/large/005zWjpngy1fr2wcvwun6j310q0eemyo.jpg) 自己环境或者网络问题
623,在CanalConnector接口加入stopRunning方法，用于优雅停止client [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=623) <br/>All committers have signed the CLA. [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=623) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=623) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=623) it.</sub> tks 
622,block waiting interrupted  by stop request resolve issue https://github.com/alibaba/canal/issues/619 [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=622) <br/>All committers have signed the CLA. [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=622) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=622) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=622) it.</sub> tks
621,close zkclientx if necessary when stop canal and fix CanalInstanceWithManager#doInitEventParser bug tks，的确是代码copy问题
620,close zkclientx when stop canal [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=620) <br/>All committers have signed the CLA. 怎么关闭了？ 新提了一个 #621  嗯，已经合并
619,在cluster client模式下无法获得排他锁时不能优雅停机 com.alibaba.otter.canal.client.impl.SimpleCanalConnector:448                 mutex.get();// 阻塞等待 无法退出 建议在CanalConnector接口中加一个stopSubscribe方法停止阻塞等待 你这个只能解决subscribe时阻塞的问题吧，如果是connect或者其他操作被mutex.get()锁定 都可以在stop时用thread.interrupt退出，捕获并淹没异常只处理了subscribe，因为这个情况在cluster情况下很常见 Neal Hu nealhu@apache.org > 在 2018年5月7日，10:18，agapple <notifications@github.com> 写道： >  > 你这个只能解决subscribe时阻塞的问题吧，如果是connect或者其他操作被mutex.get()锁定 >  > — > You are receiving this because you authored the thread. > Reply to this email directly  view it on GitHub  or mute the thread. >  有PR提交提供了stopRunning的方法，可以退出
618,支持通过GTID同步binlog 1. 增加instance级配置`gtidon`，指示该instance是否启用GTID模式。 2. driver包增加GTID相关类：`GTIDSet`，`UUIDSet`是GTID表示，`BinlogDumpGTIDCommandPacket`是`COM_BINLOG_DUMP_GTID`命令。 3. 补全dbsync包下的`GtidLogEvent`事件的解析。 4. protocol包下，增加了`EntryType`枚举项`GTIDLOG`，是binlog流中的GTIDLOG事件；`Header`增加了字段`gtid`，这里后面单独段落详细说明。 5. parse包下，`ErosaConnection`增加根据GTID同步的dump方法。 6. parse包下，`AbstractEventParser.start内`，如果instance使用GTID模式，则发送`COM_BINLOG_DUMP_GTID`命令给mysql。 [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=618) <br/>All committers have signed the CLA. 非常赞的Pull Request，合并之后我再稍微调整一下细节 :smile: 很高兴能有贡献～ good job 
617,something goes wrong when acking data from server:null 客户端报错如下： 2018-05-03 17:26:50.735 [pool-2-thread-1]  WARN com.alibaba.otter.canal.client.impl.ClusterCanalConnector.?:? - something goes wrong when acking data from server:null com.alibaba.otter.canal.protocol.exception.CanalClientException: java.io.IOException: Broken pipe 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.ack(SimpleCanalConnector.java:324) 	at com.alibaba.otter.canal.client.impl.ClusterCanalConnector.ack(ClusterCanalConnector.java:253) 	at com.lyj.order.es.sync.service.impl.AbstractCanalListenerServiceImpl.lambda$startListener$2(AbstractCanalListenerServiceImpl.java:137) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) Caused by: java.io.IOException: Broken pipe 	at sun.nio.ch.FileDispatcherImpl.write0(Native Method) 	at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47) 	at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93) 	at sun.nio.ch.IOUtil.write(IOUtil.java:65) 	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:471) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.writeWithHeader(SimpleCanalConnector.java:358) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.ack(SimpleCanalConnector.java:318) 	... 5 more 服务端报错如下： ERROR com.alibaba.otter.canal.server.netty.NettyUtils 50 - ErrotCode:400   Caused by: something goes wrong with  channel:[id:0x0f4b9efb  /10.0.39.56:50004 => /10.0.11.161:11111]  exception=com.alibaba.otter.canal.server.exception.CanalServerException:  ack error   clientId:1001 batchld:718 is not exist   pleasecheck 请各位大牛帮忙看下是什么问题？ @agapple     欢迎大神指点~ 服务端instance重启或者发生了rollback，客户端做重试即可
616,canal.instance.filter.regex Perl正则表达式问题 使用 canal\\.canal.* 和canal.test1两种形式无法获取到binlog 可以多看下adminGuide或者FAQ
615,manage管理有没有example看看呢 可以看看你们内部manager管理是如何做的吗 可以参考下otter的manager
614,cursor停止更新 您好，我在使用canal期间遇到了一个问题，描述如下： 我们目前使用canal是单机部署，一台canal server和一台canal client，canal server端运行一段时间之后meta.dat和cursor停止更新，client端无法接受到数据变更推送，meta.log如下所示： 2018-04-23 15:06:39.109 - clientId:1001 cursor:[mysql-bin.000673 73407748 1524467197000] address[rm-uf6u3c8i3cejqaia6.mysql.rds.aliyuncs.com/172.19.153.0:3306] 2018-04-23 15:06:45.109 - clientId:1001 cursor:[mysql-bin.000673 73410335 1524467203000] address[rm-uf6u3c8i3cejqaia6.mysql.rds.aliyuncs.com/172.19.153.0:3306] 2018-04-23 15:06:51.109 - clientId:1001 cursor:[mysql-bin.000673 73412633 1524467209000] address[rm-uf6u3c8i3cejqaia6.mysql.rds.aliyuncs.com/172.19.153.0:3306] 2018-04-23 15:06:57.109 - clientId:1001 cursor:[mysql-bin.000673 73438765 1524467215000] address[rm-uf6u3c8i3cejqaia6.mysql.rds.aliyuncs.com/172.19.153.0:3306] 2018-04-23 15:07:02.109 - clientId:1001 cursor:[mysql-bin.000673 73456385 1524467221000] address[rm-uf6u3c8i3cejqaia6.mysql.rds.aliyuncs.com/172.19.153.0:3306] 在04.23日就不再更新，删除meta.dat重启服务端之后也没用 并且在meta.dat中也没有cursor信息： ``` {"clientDatas":[{"clientIdentity":{"clientId":1001 "destination":"rm-uf6u3c8i3cejqaia6" "filter":""}}] "destination":"rm-uf6u3c8i3cejqaia6"} ``` 如果看到了烦请回复一下，如果有觉得描述不清的地方，烦请指出，谢谢。 版本是1.0.25 这是aliyun的rds? 得关注一下解析过程是否有异常了 版本是1.0.25的时候有报这个错： ``` 2018-04-27 18:29:02.113 [destination = financial   address = /10.44.88.50:13306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address EulerOS-BaseTemplate/10.44.88.50:13306 has an error  retrying. caused by java.nio.channels.ClosedByInterruptException: null at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannel.read(SocketChannel.java:49) ~[canal.parse.driver-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:151) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:77) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:137) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) ~[canal.parse-1.0.25.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131] 2018-04-27 18:29:02.118 [destination = financial   address = /10.44.88.50:13306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:financial[java.nio.channels.ClosedByInterruptException at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannel.read(SocketChannel.java:49) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:151) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:77) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:137) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) at java.lang.Thread.run(Thread.java:748) ] ``` 升级为1.0.26-SNAPSHOT的时候，刚刚报了这个错，然后解析停止： ``` 2018-05-09 10:10:56.214 [destination = mysql-qry-1   address = mysql-qry-1.db.prod.alsh.xingbianli.com/10.0.96.8:3306   EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: EOF encountered. 	at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.read(BioSocketChannel.java:80) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:174) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:80) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:144) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131] 2018-05-09 10:10:56.215 [destination = mysql-qry-1   address = mysql-qry-1.db.prod.alsh.xingbianli.com/10.0.96.8:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address mysql-qry-1.db.prod.alsh.xingbianli.com/10.0.96.8:3306 has an error  retrying. caused by java.io.IOException: EOF encountered. 	at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.read(BioSocketChannel.java:80) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:174) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:80) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:144) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131] 2018-05-09 10:10:56.217 [destination = mysql-qry-1   address = mysql-qry-1.db.prod.alsh.xingbianli.com/10.0.96.8:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:mysql-qry-1[java.io.IOException: EOF encountered. 	at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.read(BioSocketChannel.java:80) 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:174) 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:80) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:144) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) 	at java.lang.Thread.run(Thread.java:748) ] 2018-05-09 10:11:11.241 [destination = mysql-qry-1   address = mysql-qry-1.db.prod.alsh.xingbianli.com/10.0.96.8:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address mysql-qry-1.db.prod.alsh.xingbianli.com/10.0.96.8:3306 has an error  retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: connect mysql-qry-1.db.prod.alsh.xingbianli.com/10.0.96.8:3306 failure Caused by: java.io.IOException: connect mysql-qry-1.db.prod.alsh.xingbianli.com/10.0.96.8:3306 failure 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:77) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:77) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:87) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:160) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131] Caused by: java.net.ConnectException: Connection refused (Connection refused) 	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[na:1.8.0_131] 	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[na:1.8.0_131] 	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[na:1.8.0_131] 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[na:1.8.0_131] 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[na:1.8.0_131] 	at java.net.Socket.connect(Socket.java:589) ~[na:1.8.0_131] 	at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannelPool.open(BioSocketChannelPool.java:20) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannelPool.open(SocketChannelPool.java:18) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:72) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	... 4 common frames omitted 2018-05-09 10:11:11.242 [destination = mysql-qry-1   address = mysql-qry-1.db.prod.alsh.xingbianli.com/10.0.96.8:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:mysql-qry-1[com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: connect mysql-qry-1.db.prod.alsh.xingbianli.com/10.0.96.8:3306 failure Caused by: java.io.IOException: connect mysql-qry-1.db.prod.alsh.xingbianli.com/10.0.96.8:3306 failure 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:77) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:77) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:87) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:160) 	at java.lang.Thread.run(Thread.java:748) Caused by: java.net.ConnectException: Connection refused (Connection refused) 	at java.net.PlainSocketImpl.socketConnect(Native Method) 	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) 	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) 	at java.net.Socket.connect(Socket.java:589) 	at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannelPool.open(BioSocketChannelPool.java:20) 	at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannelPool.open(SocketChannelPool.java:18) 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:72) 	... 4 more ] 2018-05-09 10:11:24.537 [destination = mysql-qry-1   address = mysql-qry-1.db.prod.alsh.xingbianli.com/10.0.96.8:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address mysql-qry-1.db.prod.alsh.xingbianli.com/10.0.96.8:3306 has an error  retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: connect mysql-qry-1.db.prod.alsh.xingbianli.com/10.0.96.8:3306 failure Caused by: java.io.IOException: connect mysql-qry-1.db.prod.alsh.xingbianli.com/10.0.96.8:3306 failure 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:77) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:77) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:87) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:160) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131] Caused by: java.net.ConnectException: Connection refused (Connection refused) 	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[na:1.8.0_131] 	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[na:1.8.0_131] 	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[na:1.8.0_131] 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[na:1.8.0_131] 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[na:1.8.0_131] 	at java.net.Socket.connect(Socket.java:589) ~[na:1.8.0_131] 	at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannelPool.open(BioSocketChannelPool.java:20) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannelPool.open(SocketChannelPool.java:18) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:72) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	... 4 common frames omitted 2018-05-09 10:11:24.538 [destination = mysql-qry-1   address = mysql-qry-1.db.prod.alsh.xingbianli.com/10.0.96.8:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:mysql-qry-1[com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: connect mysql-qry-1.db.prod.alsh.xingbianli.com/10.0.96.8:3306 failure Caused by: java.io.IOException: connect mysql-qry-1.db.prod.alsh.xingbianli.com/10.0.96.8:3306 failure 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:77) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:77) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:87) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:160) 	at java.lang.Thread.run(Thread.java:748) Caused by: java.net.ConnectException: Connection refused (Connection refused) 	at java.net.PlainSocketImpl.socketConnect(Native Method) 	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) 	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) 	at java.net.Socket.connect(Socket.java:589) 	at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannelPool.open(BioSocketChannelPool.java:20) 	at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannelPool.open(SocketChannelPool.java:18) 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:72) 	... 4 more ] 2018-05-09 10:11:35.676 [destination = mysql-qry-1   address = mysql-qry-1.db.prod.alsh.xingbianli.com/10.0.96.8:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address mysql-qry-1.db.prod.alsh.xingbianli.com/10.0.96.8:3306 has an error  retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: connect mysql-qry-1.db.prod.alsh.xingbianli.com/10.0.96.8:3306 failure Caused by: java.io.IOException: connect mysql-qry-1.db.prod.alsh.xingbianli.com/10.0.96.8:3306 failure 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:77) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:77) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:87) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:160) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131] Caused by: java.net.ConnectException: Connection refused (Connection refused) 	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[na:1.8.0_131] 	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[na:1.8.0_131] 	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[na:1.8.0_131] 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[na:1.8.0_131] 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[na:1.8.0_131] 	at java.net.Socket.connect(Socket.java:589) ~[na:1.8.0_131] 	at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannelPool.open(BioSocketChannelPool.java:20) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannelPool.open(SocketChannelPool.java:18) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:72) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	... 4 common frames omitted 2018-05-09 10:11:35.676 [destination = mysql-qry-1   address = mysql-qry-1.db.prod.alsh.xingbianli.com/10.0.96.8:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:mysql-qry-1[com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: connect mysql-qry-1.db.prod.alsh.xingbianli.com/10.0.96.8:3306 failure Caused by: java.io.IOException: connect mysql-qry-1.db.prod.alsh.xingbianli.com/10.0.96.8:3306 failure 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:77) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:77) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:87) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:160) 	at java.lang.Thread.run(Thread.java:748) Caused by: java.net.ConnectException: Connection refused (Connection refused) 	at java.net.PlainSocketImpl.socketConnect(Native Method) 	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) 	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) 	at java.net.Socket.connect(Socket.java:589) 	at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannelPool.open(BioSocketChannelPool.java:20) 	at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannelPool.open(SocketChannelPool.java:18) 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:72) 	... 4 more ] ``` 找到代码是这里： ![image](https://user-images.githubusercontent.com/20578246/39792999-ffc6327c-5375-11e8-9fca-b1db368d953f.png) 该账户与数据库的链接也中断了 请问一下这什么原因？谢谢了 过了一段时间，大概半小时之后又重连上了 Caused by: java.net.ConnectException: Connection refused (Connection refused)
613,java.nio.channels.ClosedByInterruptException 2018-04-27 18:29:02.113 [destination = financial   address = /10.44.88.50:13306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address EulerOS-BaseTemplate/10.44.88.50:13306 has an error  retrying. caused by java.nio.channels.ClosedByInterruptException: null         at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannel.read(SocketChannel.java:49) ~[canal.parse.driver-1.0.25.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:151) ~[canal.parse-1.0.25.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:77) ~[canal.parse-1.0.25.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:137) ~[canal.parse-1.0.25.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) ~[canal.parse-1.0.25.jar:na]         at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131] 2018-04-27 18:29:02.118 [destination = financial   address = /10.44.88.50:13306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:financial[java.nio.channels.ClosedByInterruptException         at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannel.read(SocketChannel.java:49)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:151)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:77)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:137)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220)         at java.lang.Thread.run(Thread.java:748) ] canal：1.0.25版本 destination：financial，deduct 场景：deduct一直能够监测mysql的binlog，financial监测一段时间之后抛了一个这个异常，然后canal client那边一直消费canal server的financal，但是financial不能监测binlog，重启之后又正常了，但是过一段一时间有这样重复了 大约过了2.5hours financal destination就挂了 试试1.0.26版本 我们用的是1.0.26，也有这个问题。在DUMP时报 java.nio.channels.ClosedByInterruptException，据网上反馈1.0.24没有这个问题，正在做降级验证。
612,c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta 1.0.25版本 canal.instance.tsdb.enable=false 2018-04-27 17:11:17.990 [destination = billingondemandfee01db   address = /10.179.171.159:40002   EventParser] ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - pls submit github issue  show create table ddl:CREATE TABLE `t_billing_accumulator_conf` (   `serviceTypeCode` varchar(64) NOT NULL   `resourceTypeCode` varchar(64) NOT NULL   `accumulateFactorName` varchar(64) NOT NULL   `amount` int(11) NOT NULL   `measureId` int(11) NOT NULL   `logTime` datetime DEFAULT NULL   `memo` varchar(200) DEFAULT NULL   `extendParams` varchar(1024) DEFAULT NULL   UNIQUE KEY `uniqueRecord` (`serviceTypeCode` `resourceTypeCode` `accumulateFactorName`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8   compare failed .  db : TableMeta [schema=billingondemandfee01db  table=t_billing_accumulator_conf  fileds=         FieldMeta [columnName=serviceTypeCode  columnType=varchar(64)  defaultValue=null  nullable=false  key=true]         FieldMeta [columnName=resourceTypeCode  columnType=varchar(64)  defaultValue=null  nullable=false  key=true]         FieldMeta [columnName=accumulateFactorName  columnType=varchar(64)  defaultValue=null  nullable=false  key=true]         FieldMeta [columnName=amount  columnType=int(11)  defaultValue=null  nullable=false  key=false]         FieldMeta [columnName=measureId  columnType=int(11)  defaultValue=null  nullable=false  key=false]         FieldMeta [columnName=logTime  columnType=datetime  defaultValue=null  nullable=true  key=false]         FieldMeta [columnName=memo  columnType=varchar(200)  defaultValue=null  nullable=true  key=false]         FieldMeta [columnName=extendParams  columnType=varchar(1024)  defaultValue=null  nullable=true  key=false] ]  mem : TableMeta [schema=billingondemandfee01db  table=t_billing_accumulator_conf  fileds=         FieldMeta [columnName=serviceTypeCode  columnType=varchar(64)  defaultValue=null  nullable=false  key=false]         FieldMeta [columnName=resourceTypeCode  columnType=varchar(64)  defaultValue=null  nullable=false  key=false]         FieldMeta [columnName=accumulateFactorName  columnType=varchar(64)  defaultValue=null  nullable=false  key=false]         FieldMeta [columnName=amount  columnType=int(11)  defaultValue=null  nullable=false  key=false]         FieldMeta [columnName=measureId  columnType=int(11)  defaultValue=null  nullable=false  key=false]         FieldMeta [columnName=logTime  columnType=datetime  defaultValue=null  nullable=true  key=false]         FieldMeta [columnName=memo  columnType=varchar(200)  defaultValue=null  nullable=true  key=false]         FieldMeta [columnName=extendParams  columnType=varchar(1024)  defaultValue=null  nullable=true  key=false] ] 2018-04-27 17:11:20.911 [destination = billingondemandfee01db   address = /10.179.171.159:40002   EventParser] ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - compare failed   check log 已知问题，可以使用1.0.26版本测试一下
611,canal启动正常，抓不到binlog数据 canal启动正常，抓不到binlog数据。请教各位大神。 多看下FAQ
610,DDL语句的comment中文出现乱码 MySQL中，charset为 utf8mb4，collation为 utf8mb4_general_ci 测试过程中建表语句如下： ```sql CREATE TABLE `xxx` (   `id` int(11) NOT NULL AUTO_INCREMENT COMMENT '主键id，自增'   `run_time` int(13) NOT NULL DEFAULT '0' COMMENT '运行总时长-秒'   `init_time` int(13) NOT NULL DEFAULT '0' COMMENT '初始化时长-秒'   `simulation_time` int(13) NOT NULL DEFAULT '0' COMMENT '模拟计算耗时-秒'   `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间'   `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间'   PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='hadoop处理能力标准监控程序运行时长'; ``` Canal server接收后，得到的SQL语句如下： ```sql create table `xxx` ( `id` int(11) NOT NULL auto_increment COMMENT 'šłĽťĒģidÔľĆŤá™ŚĘě' `run_time` int(13) NOT NULL DEFAULT 0 COMMENT 'ŤŅźŤ°ĆśÄĽśó∂ťēŅ-Áßí' `init_time` int(13) NOT NULL DEFAULT 0 COMMENT 'ŚąĚŚßčŚĆĖśó∂ťēŅ-Áßí'   `simulation_time` int(13) NOT NULL DEFAULT 0 COMMENT 'ś®°śčüŤģ°ÁģóŤÄóśó∂-Áßí'   `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT 'ŚąõŚĽļśó∂ťóī' `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT 'śõīśĖįśó∂ťóī' PRIMARY KEY(`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT 'hadoopŚ§ĄÁźÜŤÉĹŚäõś†áŚáÜÁõĎśéßÁ®čŚļŹŤŅźŤ°Ćśó∂ťēŅ'; ``` 问题可能的原因，在canal server接收到binlog日志时，会从binlog日志中得到 clientCharset = 45，从而得到 charsetName = MacCentralEurope，再解码得出上述的带中文乱码的SQL https://github.com/alibaba/canal/blob/f46133d1168071741e8e3e4235aa635c5870a976/dbsync/src/main/java/com/taobao/tddl/dbsync/binlog/event/QueryLogEvent.java#L482 https://github.com/alibaba/canal/blob/f46133d1168071741e8e3e4235aa635c5870a976/dbsync/src/main/java/com/taobao/tddl/dbsync/binlog/CharsetConversion.java#L106 ```java         // 这两项定义是否正确？         putEntry(45  "utf8mb4"  "utf8mb4_general_ci"  "MacCentralEurope");         putEntry(46  "utf8mb4"  "utf8mb4_bin"  "MacCentralEurope"); ``` 你最好debug一下event收到的编码是啥 1、MySQL中，charset为 utf8mb4，collation为 utf8mb4_general_ci，在mysql官网对应的 ID= 45，在生产库中执行这个sql ```SQL SHOW COLLATION WHERE Charset = 'utf8mb4'; ``` 查出来也是一样的 ![image](https://user-images.githubusercontent.com/4264237/39353745-2b1011a8-4a3b-11e8-973d-ceda62a823c7.png) 2、并且在debug时，在下面代码执行过程，从binlog日志中得到 clientCharset = 45，从而得到 charsetName = MacCentralEurope  ```java         /* A 2nd variable part; this is common to all versions */         final int queryLen = dataLen - dbLen - 1;         dbname = buffer.getFixString(dbLen + 1);         if (clientCharset >= 0) {             charsetName = CharsetConversion.getJavaCharset(clientCharset);             if ((charsetName != null) && (Charset.isSupported(charsetName))) {                 query = buffer.getFixString(queryLen  charsetName);             } else {                 logger.warn("unsupported character set in query log: " + "\n    ID = " + clientCharset + "  Charset = "                             + CharsetConversion.getCharset(clientCharset) + "  Collation = "                             + CharsetConversion.getCollation(clientCharset));                 query = buffer.getFixString(queryLen);             }         } else {             query = buffer.getFixString(queryLen);         } ``` debug截图 ![image](https://user-images.githubusercontent.com/4264237/39354991-8745483c-4a3e-11e8-89b4-7d3d54a943de.png) 3、我尝试把 ```java putEntry(45  "utf8mb4"  "utf8mb4_general_ci"  "MacCentralEurope"); ``` 改成了 ```java putEntry(45  "utf8mb4"  "utf8mb4_general_ci"  "UTF-8"); ``` 然后，乱码就解决了 debug截图 ![image](https://user-images.githubusercontent.com/4264237/39354985-80b28a34-4a3e-11e8-8765-3238ef3d0b02.png) 4、在代码中，与 utf8mb4 对应的 javaCharset 只有 45，46 两项所为 ```MacCentralEurope```，除此之外，其他都对应到了 ```UTF-8``` ![image](https://user-images.githubusercontent.com/4264237/39353950-bc57543c-4a3b-11e8-9ba1-0a066b8f9573.png) https://github.com/alibaba/canal/blob/f46133d1168071741e8e3e4235aa635c5870a976/dbsync/src/main/java/com/taobao/tddl/dbsync/binlog/CharsetConversion.java#L106 应该是当时的笔误，已修复
609,数据解析类型错误 insert->query header {   version: 1   logfileName: "mysql-bin.001545\302\243\302\235E\004"   logfileOffset: 86861287   serverId: 5243   serverenCode: "UTF-8"   executeTime: 1524571682000   sourceType: MYSQL   schemaName: ""   tableName: ""   eventLength: 112   eventType: QUERY } entryType: ROWDATA storeValue: "\020\aZYinsert into data_compare.lzw_receive_json_log(min_id max_id) values (116521634 116521636)" 是因为解析的乱码问题吗 这是mysql5.6的rows_query_log_event，就是一条query
608,no match ack positionLogPosition message[batchId=1 size=259]  15:00:18.080 [kafka-producer-network-thread | producer-1] DEBUG o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Using older server API v2 to send PRODUCE {acks=-1 timeout=30000 partitionSizes=[case_flow_topic-1=3032]} with correlation id 9 to node 1003 . . 15:00:18.623 [kafka-producer-network-thread | producer-1] DEBUG o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Using older server API v2 to send PRODUCE {acks=-1 timeout=30000 partitionSizes=[case_flow_topic-1=4021]} with correlation id 56 to node 1003 com.alibaba.otter.canal.protocol.exception.CanalClientException: something goes wrong with reason: something goes wrong with channel:[id: 0x6c969f24  /192.168.113.5:38172 => /192.168.113.5:11111]  exception=com.alibaba.otter.canal.store.CanalStoreException: no match ack positionLogPosition[identity=LogIdentity[sourceAddress=192.168.113.243/192.168.113.243:3306 slaveId=-1] postion=EntryPosition[included=false journalName=mysql-bin.001543 position=916315835 serverId=1803306 timestamp=1524553202000]] 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.receiveMessages(SimpleCanalConnector.java:245) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:222) 	at com.alibaba.otter.canal.client.impl.ClusterCanalConnector.getWithoutAck(ClusterCanalConnector.java:183) 	at com.zichan360.bigdata.canal.client.caseflow.SyncCaseFlowTables.main(SyncCaseFlowTables.java:32) 15:00:24.652 [main] INFO  c.a.o.c.c.impl.ClusterCanalConnector - restart the connector for next round retry. message[batchId=2 size=259]  . . . 代码 String destination = args[0];         String topic = args[1];         CanalConnector connector = CanalConnectors.newClusterConnector("zk"  destination  ""  "");         int batchSize = 1000;         int emptyCount = 0;         try {             connector.connect();             connector.rollback();             int totalEmptyCount = 120;             while (emptyCount < totalEmptyCount) {                 Message message = connector.getWithoutAck(batchSize 2000L  TimeUnit.MILLISECONDS);                 long batchId = message.getId();                 int size = message.getEntries().size();                 if (batchId == -1 || size == 0) {                     emptyCount++;                     try {                         Thread.sleep(1000);                     } catch (InterruptedException e) {                     }                 } else {                     emptyCount = 0;                     System.out.printf("message[batchId=%s size=%s] \n"  batchId  size);                     produceKafkaData(message.getEntries()  topic);                     try {                         Thread.sleep(1000);                     } catch (InterruptedException e) {                     }                 }                 try {                     connector.ack(batchId);                 } catch (Exception e) {                     e.printStackTrace();                     connector.rollback(batchId);                 }             }         } finally {             connector.disconnect();         } no match ack 估计是服务端重启了instance，client做一下重试恢复即可
607,server启动多个instance，总有instance未启动 我有两个server，分别启动3个instance server1启动成功了1个 server2启动成功了2个 启动失败的instance没有报错。 只是启动的时候没有去寻找位点，instance文件夹下的meta.dat中也没有postion的信息 跟库的关系应该不大，因为启动失败的instance并不是固定的 一般情况下，一个server推荐配置是最多启动几个instance的 没有约束instance的数量，你要开启client消费才会有meta.dat的position
606,canal服务端宕机后，为什么需要重启2次canal 服务端才能重新获取到binlog日志数据     向各位请教个问题，问题如下所述。     当canal（1.0.25）服务端宕机后，为什么需要重启2次canal 服务端，才能重新获取到数据库binlog日志数据。重启1次无法获取到数据库binlog日志数据。 今天试了几次又没问题了。回头再找找原因。
605,事务较大的情况下binlog数据丢失 在测试canal同步数据到es的时候，从mysql dump出一个表，导出的时候每250行切分为一个事务，然后把表truncate掉，再import导出的sql，发现同步完成后es中会比mysql少三百条左右的数据（总数据量4W+）。经调试发现是canal在同步binlog过程中发生了数据丢失，大部分事务都被完整同步了，有几个事务只同步过来不到100条数据。后来把事务每10行数据切分一个事务，就不会出现丢数据的情况了。请问一下这和网络状况有关吗？或者只是因为事务过大？ 这个问题怎么处理了？
604,找不到 com.alibaba.fastsql:fastsql:jar:2.0.0_preview_228 包，改成com.alibaba.fastsql:fastsql:jar:2.0.0_preview_186 [ERROR] Failed to execute goal on project canal.parse: Could not resolve dependencies for project com.alibaba.otter:canal.parse:jar:1.0.26-SNAPSHOT: Failure to find com.alibaba.fastsql:fastsql:jar:2.0.0_preview_228 in http://maven.aliyun.com/nexus/content/groups/public was cached in the local repository  resolution will not be reattempted until the update interval of nexus-aliyun has elapsed or updates are forced -> [Help 1] com.alibaba.fastsql:fastsql:jar:2.0.0_preview_228 这个包什么时候能放出呢？ 看一下二进制发布包里有
603,com.alibaba.otter.canal.protocol.exception.CanalClientException: java.io.IOException: 您的主机中的软件中止了一个已建立的连接。 2018-04-23 15:05:05.780-[ERROR]-[Thread-0]-com.clickplus.canal.AbstractCanalClientTest:151-process error! com.alibaba.otter.canal.protocol.exception.CanalClientException: java.io.IOException: 您的主机中的软件中止了一个已建立的连接。 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.ack(SimpleCanalConnector.java:257) ~[canal.client-1.0.4.jar:na] 	at com.clickplus.canal.AbstractCanalClientTest.process(AbstractCanalClientTest.java:143) ~[classes/:na] 	at com.clickplus.canal.AbstractCanalClientTest$2.run(AbstractCanalClientTest.java:86) [classes/:na] 	at java.lang.Thread.run(Thread.java:722) [na:1.7.0_17] Caused by: java.io.IOException: 您的主机中的软件中止了一个已建立的连接。 	at sun.nio.ch.SocketDispatcher.write0(Native Method) ~[na:1.7.0_17] 	at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:51) ~[na:1.7.0_17] 	at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:94) ~[na:1.7.0_17] 	at sun.nio.ch.IOUtil.write(IOUtil.java:65) ~[na:1.7.0_17] 	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:450) ~[na:1.7.0_17] 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.writeWithHeader(SimpleCanalConnector.java:289) ~[canal.client-1.0.4.jar:na] 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.ack(SimpleCanalConnector.java:253) ~[canal.client-1.0.4.jar:na] 	... 3 common frames omitted 问题描述最好详细点，否则不好分析 @jnliao 谢谢回答，小数据量时，canal运行是没有问题的，当有大事务执行时（千级万级的insert），就会报这个错误，调大了buffersize和客户端的batchsize都没什么用 我之前处理数据时也遇到过这种问题。 1、使用场景 我使用的是canal 1.0.25，客户端采用定时任务实时处理canal服务端获取的日志数据，并且每个定时任务都是使用同一个客户端链接对象。数据库瞬间更新1w条数据时，canal客户端处理数据时，运行一会就会一直报你说的这种错误——“Caused by: java.io.IOException: 您的主机中的软件中止了一个已建立的连接。” 2、解决办法 每个定时任务都使用新创建的客户端链接对象。并在任务结束时断开链接。 https://github.com/alibaba/canal/issues/640，关注一下这个
602,迁出代码后，mvn 构建canal.common:jar:1.0.26-SNAPSHOT 包找不到 Could not find artifact com.alibaba.otter:canal.common:jar:1.0.26-SNAPSHOT in nexus-aliyun (http://maven.aliyun.com/nexus/content/groups/public) 查了下，只有1.0.25 以及 9.0.0 我对common 包重新 install 后成功了；  没有aliyun 的 deploy 权限
601,canal运行38个小时左右，客户端报错Broken pipe，服务端没报错，什么原因 com.alibaba.otter.canal.protocol.exception.CanalClientException: java.io.IOException: Broken pipe 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.ack(SimpleCanalConnector.java:343) Caused by: java.io.IOException: Broken pipe 	at sun.nio.ch.FileDispatcherImpl.write0(Native Method) 	at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47) 	at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93) 	at sun.nio.ch.IOUtil.write(IOUtil.java:65) 	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:471) 	at java.nio.channels.Channels.writeFullyImpl(Channels.java:78) 	at java.nio.channels.Channels.writeFully(Channels.java:98) 	at java.nio.channels.Channels.access$000(Channels.java:61) 	at java.nio.channels.Channels$1.write(Channels.java:174) 	at java.nio.channels.Channels$WritableByteChannelImpl.write(Channels.java:458) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.writeWithHeader(SimpleCanalConnector.java:400) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.writeWithHeader(SimpleCanalConnector.java:387) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.ack(SimpleCanalConnector.java:337) 	... 6 more 这是被服务端主动断开连接了 服务端为什么主动断开连接了，这是不是canal的问题？或者请问要怎么处理？ 检查server的日志 server端没报错，就客户端报错了 我也有这个问题，请问这是咋回事？我用的是25版本的。 https://github.com/alibaba/canal/issues/640，关注一下这个
599,大数据量时 canal服务端报c.a.o.c.server.netty.handler.ClientAuthenticationHandler - idle timeout exceeds  close channel to save server resources 问题场景： 万级的数据进行insert操作时，canal服务端会报 c.a.o.c.server.netty.handler.ClientAuthenticationHandler - channel:[id: 0x3c24ea0e  /192.168.1.111:64402 => /192.168.1.111:11111] idle timeout exceeds  close channel to save server resources 错误``。 客户端的日志显示，订阅的binlog会一直重复出现(0-10000再循环0-10000) 导致delay时间变得很长，别的表的同步就进不去了。 这种情况是要扩大canal的ringbuffer和客户端的batchsize吗，本地测试了一下都没有解决问题 你这个异常时链接idle timeout超时，最好你需要观察一下运行堆栈 我也遇到这个问题了，怎么解决的 server端 batchMode设置为itemSize  然后，client端的batchSize设置小一点，默认超时时间 30s ack掉就可以。 https://github.com/alibaba/canal/issues/640，关注一下这个
598,一个rds   mysql库配置三个instance，只有两个能正常读到binlog？ rds上一个库的一个表分了一千张，所以配置了三个instance，然后每个过滤不同的表。但是只有两个instance能正常消费到binlog日志。另外一个instance即使 表有数据，也消费不到    batchId 返回的是-1 我也碰到类似的问题，总是有instance没有去获取位点，instance文件下的meta.dat中没有postion信息 升级到1.0.26版本，正常了。
597,com.alibaba.otter.canal.protocol.exception.CanalClientException: java.io.IOException: Broken pipe； 当数据量小的时候没有问题，当有万级的数据同步插入时，canal就会报com.alibaba.otter.canal.protocol.exception.CanalClientException: java.io.IOException: Broken pipe 错误。 现象是canal一直读取插入万级的日志，循环调用。导致在mysql中先插入又删除，再插入删除，一直重复。请问是什么原因 https://github.com/alibaba/canal/issues/640，关注一下这个
596,canal集群模式下，客户端调用connector.subscribe("schema1\\..")无效 hi ，dear   在使用canal进行开发的时候碰到了如下的问题： 1. 问题描述：    canal集群环境下，客户端调用connector.subscribe("schema1\\..")无效，压根监听不到schema1数据库下的表数据变化的日志。并且修改回去connector.subscribe()再次重启客户端，之前能正常监听所有表的功能也立马失效。重启canal服务端也没用。但是修改canal服务端的zookeeper指向，再重启canal服务端，又能正常恢复监听所有的表。如果再次在客户端处调用带有filter参数的subscribe方法，则会重复上述问题。 2. 环境    机器A canal server 1 (版本1.0.24)     机器B  canal server 2 (版本1.0.24)    机器C  zookeeper  额外在问下：connector.subscribe中的filter写法有没有什么特殊的要求，只是一个正则表达式吗还是有什么其他的规则？？？？ QQ群加不进去，所以只有邮件问询，希望能受到你的回复，谢谢 如果要订阅的数据库是test，表是user和log 则调用 connector.subscribe("test.user test.log"); LS正解
595, 'show master status' has an error! 2018-04-02 04:12:46.303 [destination = example   address = /192.168.1.111:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just show master status 2018-04-02 04:12:46.307 [destination = example   address = /192.168.1.111:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /192.168.1.111:3306 has an error  retrying. caused by  com.alibaba.otter.canal.parse.exception.CanalParseException: command : 'show master status' has an error! pls check. you need (at least one of) the SUPER REPLICATION CLIENT privilege(s) for this operation 可以设置数据库用户权限为 GRANT SELECT  SUPER  REPLICATION SLAVE  REPLICATION CLIENT  SHOW VIEW ON *.*  TO 'xxx'@'%' 或GRANT ALL PRIVILEGES ON *.*  TO 'xxx'@'%' 试试 这个设置了权限也不好用啊。请问是因为版本的问题吗? 请问解决了没有?@sunsetyan
594,canal 一直在prepare to find start position just show master status 2018-04-19 14:43:12.055 [Thread-4] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - stop successful.... 2018-04-19 14:43:17.670 [main] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2018-04-19 14:43:17.677 [main] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [walleter/instance.properties] 2018-04-19 14:43:17.755 [main] WARN  org.springframework.beans.TypeConverterDelegate - PropertyEditor [com.sun.beans.editors.EnumEditor] found through deprecated global PropertyEditorManager fallback - consider using a more isolated form of registration  e.g. on the BeanWrapper/BeanFactory! 2018-04-19 14:43:17.886 [main] INFO  c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-walleter 2018-04-19 14:43:17.921 [main] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - start successful.... 2018-04-19 14:43:17.952 [destination = walleter   address = /10.45.14.188:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just show master status 2018-04-19 14:46:55.129 [Thread-4] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - stop CannalInstance for null-walleter 2018-04-19 14:46:55.134 [Thread-4] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - stop successful.... prepare to find start position just show master  status 这个不是异常状态 但是数据库postion有变化 canal没有变化 一直是这个动静 发自网易邮箱大师 在2018年04月26日 19:36，agapple<notifications@github.com> 写道： prepare to find start position just show master status 这个不是异常状态 — You are receiving this because you authored the thread. Reply to this email directly  view it on GitHub  or mute the thread. @mersap  可能是你的position配置的太老了 mysql那边已经删除了对应的binlog。 可能是例子里给的默认batchSize太大了   改小点就好了
593,1.0.26 这个版本bug造成的吗? 请问canal的版本兼容mysql5.x所有版本吗?我的是5.1.73 2018-04-01 14:11:50.712 [main] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2018-04-01 14:11:51.147 [main] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties] 2018-04-01 14:11:52.319 [main] WARN  o.s.beans.GenericTypeAwarePropertyDescriptor - Invalid JavaBean property 'connectionCharset' being accessed! Ambiguous write methods found next to actually used [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.lang.String)]: [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.nio.charset.Charset)] 2018-04-01 14:11:52.700 [main] INFO  c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-example  2018-04-01 14:11:52.774 [main] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - start successful.... 2018-04-01 14:11:52.815 [destination = example   address = /192.168.1.110:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /192.168.1.110:3306 has an error  retrying. caused by  java.lang.NoSuchMethodError: method java.net.InetSocketAddress.getHostString with signature ()Ljava.lang.String; was not found.         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.<init>(MysqlConnector.java:53) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.<init>(MysqlConnector.java:63) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.<init>(MysqlConnection.java:70) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.buildMysqlConnection(MysqlEventParser.java:308) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.buildErosaConnection(MysqlEventParser.java:76) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:154) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]         at java.lang.Thread.run(libgcj.so.10) [na:na] 2018-04-01 14:11:52.960 [destination = example   address = /192.168.1.110:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.lang.NoSuchMethodError: method java.net.InetSocketAddress.getHostString with signature ()Ljava.lang.String; was not found.    at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.<init>(MysqlConnector.java:53)    at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.<init>(MysqlConnector.java:63)    at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.<init>(MysqlConnection.java:70)    at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.buildMysqlConnection(MysqlEventParser.java:308)    at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.buildErosaConnection(MysqlEventParser.java:76)    at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:154)    at java.lang.Thread.run(libgcj.so.10) ] 2018-04-01 14:11:52.965 [destination = example   address = /192.168.1.110:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - parse events has an error java.lang.NullPointerException: null         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.afterDump(MysqlEventParser.java:137) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:249) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]         at java.lang.Thread.run(libgcj.so.10) ~[na:na] java.lang.NoSuchMethodError: method java.net.InetSocketAddress.getHostString with signature ()Ljava.lang.String; was not found. 你这是啥jdk版本啊 25版本也有这个问题。jdk1.8 对应的 配置i canal.instance.dbUsername=canal canal.instance.dbPassword=canal canal.instance.defaultDatabaseName=test canal.instance.connectionCharset=UTF-8 这个我只能怀疑是jdk的问题，代码层面oracle的jdk7和jdk8我本地都跑过 有具体的复现方法，再reopen吧
592,开启druid.ddl=true以及使用tsdb，会出现改表结构一直报column size is not match的问题 最近碰上一个问题，因为druid在解析我们的表结构的时候总是报错，无法通过。所以关掉了druid的ddl以及canal query ddl的识别。如果此时更新了业务表的列，例如增加列。在新的event来的时候，会发生，event里的fields数量多，而tableMeta里的列数少，而抛出异常。 ` public TableMeta getTableMeta(String schema  String table  boolean useCache  EntryPosition position) {         TableMeta tableMeta = null;         if (tableMetaTSDB != null) {             //TODO 觉得此处应该加上useCache判断，如果使用了cache，从缓存走。如果不使用cache，就走数据库直连查询             tableMeta = tableMetaTSDB.find(schema  table);             if (tableMeta == null) {                 // 因为条件变化，可能第一次的tableMeta没取到，需要从db获取一次，并记录到snapshot中                 String fullName = getFullName(schema  table);                 try {                     ResultSetPacket packet = connection.query("show create table " + fullName);                     String createDDL = null;                     if (packet.getFieldValues().size() > 0) {                         createDDL = packet.getFieldValues().get(1);                     }                     // 强制覆盖掉内存值                     tableMetaTSDB.apply(position  schema  createDDL  "first");                     tableMeta = tableMetaTSDB.find(schema  table);                 } catch (IOException e) {                     throw new CanalParseException("fetch failed by table meta:" + fullName  e);                 }             }             return tableMeta;         } else {             if (!useCache) {                 tableMetaDB.invalidate(getFullName(schema  table));             }             return tableMetaDB.getUnchecked(getFullName(schema  table));         }     }` 你关掉了tsdb，就没法解决这个列不匹配的问题。如果针对DDL无法解析，建议反馈对应的DDL，推动TSDB的完善 DDL的错误是因为底层的Druid包的问题。因为一直线上使用，所以暂时没法等待Druid更新。然后TSDB这个问题个人认为是Canal包里tsdb分支的逻辑问题。暂时使用上我们停止了tsdb的使用。这段时间没有发现新的问题。 用的是fastsql，和druid不同的package
591,关于sqltype 的一些疑问 hi， 看CanalEntry.java 中 SqlType是字段在java中的类型 getSqlType()出来的是数值，抱歉，我没有找到数值对应的java类型，可以在那里查到吗？ thx 找到了 > 找到了 在哪？
590,Fix issue #586 event size不准确的问题 calculateSize由binlog事件大小改为返回serializedSize，内存大小更准确可控 [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=590) <br/>All committers have signed the CLA. 这个getSerializedSize的方法实现原理是遍历所有的数据进行大小计算，是有一些成本 已经有新的方案了  获取了序列化之后的长度
589,关于EventTransactionBuffer与MemoryEventStoreWithBuffer的一些疑问 # 我现在的理解 ## EventTransactionBuffer sink将event分发给EventTransactionBuffer，如果收集到一个完整的transaction，或者达到了buffer的size上限就会触发flush，将缓存数据通过callback经过filter等操作以后发送到MemoryEventStoreWithBuffer. ## MemoryEventStoreWithBuffer 接收到一个List<Event>（应该就是刚刚发过来的list事务），检查如果空位空间足够，就遍历list中的数据，逐条加入entries中。 每一次get数据的时候，是根据传入的batchSize来拿的，拿完以后会计算消息的PositionRange（这次get的头尾LogPosition），会set可被ack的点为这段消息中的最后一个TRANSACTIONBEGIN或者TRANSACTIONEND。这个ack会在元数据管理updateCursor时传进去，为了让cursor中总是记录的事务的开始或者结束。 # 我的疑问 现在我们在开发的东西，是希望以Transaction来作为单位来将消息发给下游的（如果过大的transaction会分割成多个transaction包，里面记录一个布尔值表示是否是事务结束包），现在主要矛盾是MemoryEventStoreWithBuffer里面存储的粒度是entry，所以根据batchSIze来拿的话拿出去可能同个事务中数据是会被分2次拿到的。 如果是从MemoryEventStoreWithBuffer拿出数据之后，再嵌套一个类似EventTransactionBuffer的缓存， ack的时候也还要保留原来的batchId来ack。 目前觉得可以去实现一个自己的MemoryTransactionStoreWithBuffer，里面的缓存的粒度用transaction包，这样每次get出来都是以transaction包为单位的形式，之后可以继承CanalInstanceWithManager覆盖里面的initEventStore方法，将MemoryEventStoreWithBuffer替换为自己实现的MemoryTransactionStoreWithBuffer。 想请教一下当时canal设计的考虑，以后有没有考虑提供按transaction包为单位的put，get，ack。然后我目前的想法是不是合理，不知道我上面的表述是不是够清楚，望回复。 要考虑大事务的情况，一般我们内部不建议这么玩。很多工作你可以在client层面来支持，比如client层面本身支持异步ack之后，你可以自己搞一个buffer，凑足一个事务后回调业务 感谢建议哈，client层面是可以做这个。但是我还有些考虑： 目前我们其实用了内嵌canal+一个消息中间件来提供下游数据订阅。现在是想要往消息中间件中发事务粒度的消息（业务中几乎都是小事务）。如果在canal client层getWithoutAck方法拿出batchSize数据，然后放入一个buffer，这样感觉代码结构上又多了一层，在batchSize个数据中包含多个事务的情况下，要协调根据事务发送消息给mq，以及消费完一个batchSize数据以后根据MessageId去ack canal。（因为这种情况下的发送给mq的Trasaction与从canal拉出来的MessageId没有什么联系。如果是server里面保证了get出来的一个Message就是一个事物包，那就直接发送完走ack逻辑就可以了） 如果考虑server层面按下面的做法是不是能避免大事务的影响呢： 考虑flush出一个事物包，包里加一个标记位。如果小事务就单独的事物包，标记位表示没有后续包；大事务就会因为buffer满以后切成多个，除了最后一个都标记为有后续包。这样子做的话应该也能处理大事务的问题吧。 我这样的需求，是不是实现在server端比较好呢，写完以后的感觉是getWithoutAck拿出来的数据是以事物包为单元的（大事务会是多个事物包（少数情况））。 如果是打标的方式，是建议在server上操作 好的，感谢回复，辛苦！ 如果针对一个大表的一个大事物包含了太多数据，很容易引发OOM问题，之前项目里错误设置了batchSize（5*1024），结果client经常异常中止。这不是少见的情况，糟糕的历史系统，经常一个事物修改成千上万条数据，甚至来个清空表，表本身数据量还不小 @shang7053 嗯嗯，会有设置上限，针对大的事务会截断多次flush的 看了你的描述，又结合源码看了下 1、服务器端的canal.instance.transactionn.size这个参数的配置和client端的batchSize的设置是不是应该设置的一样会比较合理些？ 2、如果一个完整事务数据的长度超过了客户端设置的这个batchSize，比如一个事务的实际长度是1.5倍个batchSize，第一次get获取一个batchSize的数据，再次get时，是不是只会给我返剩下的0.5倍的数据？会不会把下一个事务的数据也返回给我？ @zwangbo  @jingshenbusi6530 目前Canal中实现的消息获取是以Entry（Event）为粒度的（在MemoryEventStoreWithBuffer中），所以batchSize只是针对的拿多少个Entry，不会针对事务做截断返回。 所以感觉： 1. 这两个配置没有必然的联系，batchSize主要看自己这边的消费效率调整就好了。 2. 只要buff中储存了足够多的数据，每次一get都能拿到batchSize个Event，不会因为事务截断。（但是事务头尾进入MemoryEventStoreWithBuffer时，会被设置为可被ack的位点，也就是说ack一个MessageId的时候总是会选择一个BatchSize组中事务头尾的position） LS理解正确
588, Could not find artifact com.alibaba.fastsql:fastsql:jar:2.0.0_preview_159 初始化工程的时候，出现: Caused by: org.eclipse.aether.resolution.DependencyResolutionException: Could not find artifact com.alibaba.fastsql:fastsql:jar:2.0.0_preview_159 in central  目前二进制发布包里有包含 @agapple 二进制文件在哪里。最新的228，386没有看到 https://github.com/alibaba/canal/releases/tag/canal-1.0.26-preview-3，这里有二进制的包
587,这个项目能打包成一个jar吗 这个项目能打包成一个jar吗，而不是一个tar的文件，项目里面很多main文件，到底用的是哪一个文件启动的 看一下wiki里的quickStart
586,MemoryEventStoreWithBuffer event size不准确 MemoryEventStoreWithBuffer `    private long calculateSize(Event event) {         // 直接返回binlog中的事件大小         return event.getEntry().getHeader().getEventLength();     }` binlog事件经过Canal解析，实际内存占用是event_length的十倍以上，这样bufferSize * bufferMemUnit算出的实例内存占用就不准确了，是不是考虑改成event.getEntry().getSerializedSize(); 测试结果： calculateSize 179，SerializedSize 1790 calculateSize 8291， ，SerializedSize 155000 是有这么一个问题，拿的是binlog里的大小，展开之后会补充字段，类型等信息，会比之前要膨胀很多 新的1.0.26版本已经按照序列化大小进行计算了
585,RowsQueryEvent不支持对表名的黑白名单filter.regex和filter.black.regex过滤呢 是怎么考虑的，看源码LogEventConvert里面对ROWS_QUERY_LOG_EVENT、ANNOTATE_ROWS_EVENT都没过滤表名。  另外 另外对于文档中的canal.instance.filter.query.dml属性的描述： > 是否忽略DML的query语句，比如insert/update/delete table.(mysql5.6的ROW模式可以包含statement模式的query记录) 有点看不懂，设置为true之后DML里排除select，还是排除insert/update/delete table？如果我使用canal是想同步数据到另一个存储介质，是不是可以设置成true？ RowsQueryLogEvent，只能获取到tableName，无法获取库名，表达式过滤的时候无法精确匹配
584,1.0.19版本，canal.instance.filter.query.dcl\dml 过滤失效 1.0.19版本，canal.instance.filter.query.dcl\dml 过滤失效，仍然会打印update\delete\insert等日志 你设置成true还是false？
583,关于数据库版本，5.7.18的咨询？ 目前我们的数据库是5.7.18，但是在你们的文档中说是支持5.7，括号中又特别注明了是5.7.13，那么是不是意味着仅测试了5.7.13？对于18版本有什么影响吗？ 18版本没有做过测试，你可以做一下评测，一般小版本不太会有binlog变化 好的，我们在正式环境中还没有遇到小版本问题，看了看change log，好像也没有特别指明binlog的变化。
582,canal和mycat canal可以搭配mycat使用吗 只能直连一个物理MySQL mycat只是一个sql路由，他无法产生binlog日志，canal也巧妇难为无米之炊
581,HA模式，正常关闭一个server节点，zookeeper切换节点之后，client消费不到数据 我先启动了两个server注册至zookeeper， client连接zookeeper，选择了一个节点进行订阅。 当我关闭正在被订阅的server节点之后， zookeeper进行了节点切换，切换至了另一个节点。 但是client报错 16:13:46.186 [main] WARN  c.a.o.c.c.impl.ClusterCanalConnector - something goes wrong when getWithoutAck data from server:/192.168.1.158:11111 com.alibaba.otter.canal.protocol.exception.CanalClientException: java.io.IOException: end of stream when reading header 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:224) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:207) 	at com.alibaba.otter.canal.client.impl.ClusterCanalConnector.getWithoutAck(ClusterCanalConnector.java:165) 	at ClientSample.start(ClientSample.java:43) 	at ClientSample.main(ClientSample.java:116) Caused by: java.io.IOException: end of stream when reading header 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.read(SimpleCanalConnector.java:307) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.readNextPacket(SimpleCanalConnector.java:295) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.receiveMessages(SimpleCanalConnector.java:229) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:222) 	... 4 more 然后订阅不到数据，只是不停的打印 16:19:29.846 [main-SendThread(192.168.0.81:2181)] DEBUG org.apache.zookeeper.ClientCnxn - Got ping response for sessionid: 0x161835c99bb1978 after 3ms 切换至的新server日志 2018-04-10 15:40:09.114 [pool-2-thread-1] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2018-04-10 15:40:09.118 [pool-2-thread-1] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [gjj/instance.properties] 2018-04-10 15:40:09.284 [pool-2-thread-1] WARN  o.s.beans.GenericTypeAwarePropertyDescriptor - Invalid JavaBean property 'connectionCharset' being accessed! Ambiguous write methods found next to actually used [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.nio.charset.Charset)]: [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.lang.String)] 2018-04-10 15:40:09.381 [pool-2-thread-1] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2018-04-10 15:40:09.387 [pool-2-thread-1] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [gjj/instance.properties] 2018-04-10 15:40:09.496 [New I/O server worker #1-1] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400   Caused by :  something goes wrong with channel:[id: 0x6cf56c87  /192.168.43.133:54479 => /192.168.1.158:11111]  exception=com.alibaba.otter.canal.server.exception.CanalServerException: destination:gjj should start first 2018-04-10 15:40:09.501 [New I/O server worker #1-1] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400   Caused by :  something goes wrong with channel:[id: 0x6cf56c87  /192.168.43.133:54479 :> /192.168.1.158:11111]  exception=com.alibaba.otter.canal.server.exception.CanalServerException: destination:gjj should start first 2018-04-10 15:40:09.509 [New I/O server worker #1-1] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x6cf56c87  /192.168.43.133:54479 :> /192.168.1.158:11111]  exception=java.nio.channels.ClosedChannelException 	at org.jboss.netty.channel.socket.nio.NioWorker.cleanUpWriteBuffer(NioWorker.java:649) 	at org.jboss.netty.channel.socket.nio.NioWorker.writeFromUserCode(NioWorker.java:370) 	at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:137) 	at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) 	at org.jboss.netty.channel.Channels.write(Channels.java:611) 	at org.jboss.netty.channel.Channels.write(Channels.java:578) 	at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:30) 	at com.alibaba.otter.canal.server.netty.NettyUtils.error(NettyUtils.java:51) 	at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:200) 	at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) 	at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.cleanup(ReplayingDecoder.java:542) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.channelDisconnected(ReplayingDecoder.java:450) 	at org.jboss.netty.channel.Channels.fireChannelDisconnected(Channels.java:360) 	at org.jboss.netty.channel.socket.nio.NioWorker.close(NioWorker.java:599) 	at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:119) 	at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) 	at org.jboss.netty.channel.Channels.close(Channels.java:720) 	at org.jboss.netty.channel.AbstractChannel.close(AbstractChannel.java:208) 	at org.jboss.netty.channel.ChannelFutureListener$1.operationComplete(ChannelFutureListener.java:46) 	at org.jboss.netty.channel.DefaultChannelFuture.notifyListener(DefaultChannelFuture.java:381) 	at org.jboss.netty.channel.DefaultChannelFuture.addListener(DefaultChannelFuture.java:148) 	at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:30) 	at com.alibaba.otter.canal.server.netty.NettyUtils.error(NettyUtils.java:51) 	at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:200) 	at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) 	at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:444) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261) 	at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:350) 	at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) 	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) 	at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) 2018-04-10 15:40:09.649 [pool-2-thread-1] ERROR com.alibaba.druid.pool.DruidDataSource - testWhileIdle is true  validationQuery not set 2018-04-10 15:40:10.337 [pool-2-thread-1] INFO  c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-gjj  2018-04-10 15:40:10.392 [pool-2-thread-1] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - subscribe filter change to bshousingfund.bsh_data_log 2018-04-10 15:40:10.392 [pool-2-thread-1] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - start successful.... 2018-04-10 15:40:11.537 [destination = gjj   address = /192.168.0.72:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position  {"identity":{"slaveId":-1 "sourceAddress":{"address":"192.168.0.72" "port":3306}} "postion":{"included":false "journalName":"mysql-bin.000017" "position":864668422 "serverId":2821031644 "timestamp":1523346001000}} 2018-04-10 15:41:13.152 [New I/O server worker #1-3] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - subscribe filter change to bshousingfund.bsh_data_log 2018-04-10 16:07:49.172 [New I/O server worker #1-4] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - subscribe filter change to bshousingfund.bsh_data_log 我看最后是起来了啊，切换到了新server上 server是切换过去了，但是client消费不能正常消费了 @CXninesuns 遇到同样的问题。zk上看running的instance已经切换地址，但zk上的节点还是旧的地址。这样client确实不能继续消费数据。我使用的是canal_1.0.20出现了这个问题，不知道你是否已经解决了该问题。 建议都用26-alpha版本试试，印象中高可用切换修复过几个问题
580,无法连接canal服务端 使用下面方法创建canal连接 `CanalConnector canalConnector = CanalConnectors.newSingleConnector(new InetSocketAddress(canalHost Integer.parseInt(canalPort)) canalDestination "" ""); canalConnector.connect();` 但是调用connect()方法时报错。报错如下： ` Caused by: com.alibaba.otter.canal.protocol.exception.CanalClientException: java.net.ConnectException: Connection timed out: connect 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.doConnect(SimpleCanalConnector.java:178) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.connect(SimpleCanalConnector.java:102)` 请问是什么原因呢？之前一直还好好的。从今天开始突然报这个错了。使用的canal版本为canal.deployer-1.0.25 找到原因了，是canal服务端所在服务器的防火墙拦截了11111端口。开放此端口就好了。
579,canal server 取不到RDS  binlog? 情况是这样的，我们mysql 是阿里云的rds，canal server启动后，canal client也启动，然后往mysql数据库并发写数据，刚开始是可以看到client有接到数据并解析到hbase的并且数据没有丢失。然后过段时间再并发写数据，client就接不到数据了。。。。当我重启canal server后，就看到client可以接到数据并解析了。 部署的otter，也出现了运行一段时间(几天或者一个月)后，不能从canal接到数据，导致otter上的同步通道停止？ @songpengpeng 你配置了多个destination吗?destination 每个destination中的serveid是一样的吗？ @mjjian0 你用的是啥版本？建议是用1.0.24+以上的 @agapple  我用的是 1.0.25的版本 @agapple 这种情况下的  meta.dat  里面内容没有cursor和postion的信息。 几个destination目录中 meta.dat   clientId是一样的这个有关系吗？ @agapple 多个instance 配置的时候  canal.instance.rds.instanceId参数需要配置吗，保证唯一？？ 你看到的是rds localbinlog解析的配置文件吧，不是直连binlog @agapple   是 rds_instance.properties 这个不是 解析rds的库的配置吗 rds_instance只是拉取rds上的oss binlog，不作为解析用 @agapple 嗯，我们mysql是阿里云 rds，然后配置文件我就使用的这个。 参考FAQ： https://github.com/alibaba/canal/wiki/FAQ
578,求问1.0.26的稳定版什么时候发布？谢谢~ 如图，求问，谢谢 目前还有1，2个已知bug，需要进行修复 谢谢回复，另外想问会有一个大概时间吗，比如会在6月份又或者十一前或者年底这样稍微准确的时间点吗，谢谢 预计在近1，2个月之内吧 一年过去了 非常感谢你的回复 最新的版本已发布
577,把canal服务端的日志级别改成DEBUG 然后无限快速打印下面这一条日志，然后获取不到变更数据，这是什么原因？ 2018-04-09 15:12:46.395 [destination = xxx_2   address = /xxx.xxx.xxx.xxx:3306   EventParser] DEBUG c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - set EntryPosition[included=false journalName=mysql-bin.000789 position=8764275 serverId=10294 timestamp=1523224556000] to be pending start position before finding another proper one... 2018-04-09 15:39:33.727 [destination = xxx   address = /192.168.xxx.94:3306   EventParser] DEBUG c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - set EntryPosition[included=false journalName=mysql-bin.000789 position=12934350 serverId=xxx94 timestamp=1523224841000] to be pending start position before finding another proper one... 2018-04-09 15:39:33.804 [destination = xxx   address = /192.168.xxx.94:3306   EventParser] DEBUG c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - compare exit condition:mysql-bin.000789 12934977 1523224841000  startTimestamp=1523259171081... 2018-04-09 15:39:33.805 [destination = xxx   address = /192.168.xxx.94:3306   EventParser] DEBUG c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - set EntryPosition[included=false journalName=mysql-bin.000789 position=12934977 serverId=xxx94 timestamp=1523224841000] to be pending start position before finding another proper one... 2018-04-09 15:39:33.886 [destination = xxx   address = /192.168.xxx.94:3306   EventParser] DEBUG c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - compare exit condition:mysql-bin.000789 12935073 1523224841000  startTimestamp=1523259171081... 2018-04-09 15:39:33.886 [destination = xxx   address = /192.168.xxx.94:3306   EventParser] DEBUG c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - set EntryPosition[included=false journalName=mysql-bin.000789 position=12935073 serverId=xxx94 timestamp=1523224841000] to be pending start position before finding another proper one... 2018-04-09 15:39:33.972 [destination = xxx   address = /192.168.xxx.94:3306   EventParser] DEBUG c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - compare exit condition:mysql-bin.000789 12935687 1523224841000  startTimestamp=1523259171081... 2018-04-09 15:39:33.972 [destination = xxx   address = /192.168.xxx.94:3306   EventParser] DEBUG c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - set EntryPosition[included=false journalName=mysql-bin.000789 position=12935687 serverId=xxx94 timestamp=1523224841000] to be pending start position before finding another proper one... 2018-04-09 15:39:34.052 [destination = xxx   address = /192.168.xxx.94:3306   EventParser] DEBUG c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - compare exit condition:mysql-bin.000789 12935783 1523224841000  startTimestamp=1523259171081... 2018-04-09 15:39:34.052 [destination = xxx   address = /192.168.xxx.94:3306   EventParser] DEBUG c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - set EntryPosition[included=false journalName=mysql-bin.000789 position=12935783 serverId=xxx94 timestamp=1523224841000] to be pending start position before finding another proper one... 2018-04-09 15:39:34.128 [destination = xxx   address = /192.168.xxx.94:3306   EventParser] DEBUG c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - compare exit condition:mysql-bin.000789 12938728 1523224841000  startTimestamp=1523259171081... 2018-04-09 15:39:34.128 [destination = xxx   address = /192.168.xxx.94:3306   EventParser] DEBUG c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - set EntryPosition[included=false journalName=mysql-bin.000789 position=12938728 serverId=xxx94 timestamp=1523224841000] to be pending start position before finding another proper one... 2018-04-09 15:39:34.207 [destination = xxx   address = /192.168.xxx.94:3306   EventParser] DEBUG c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - compare exit condition:mysql-bin.000789 12938824 1523224841000  startTimestamp=1523259171081... 2018-04-09 15:39:34.207 [destination = xxx   address = /192.168.xxx.94:3306   EventParser] DEBUG c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - set EntryPosition[included=false journalName=mysql-bin.000789 position=12938824 serverId=xxx94 timestamp=1523224841000] to be pending start position before finding another proper one... 2018-04-09 15:39:34.280 [destination = xxx   address = /192.168.xxx.94:3306   EventParser] DEBUG c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - compare exit condition:mysql-bin.000789 12939336 1523224841000  startTimestamp=1523259171081... 这是在定位位点时打印的debug信息
576,canal.deployer-1.0.26-SNAPSHOT报错 有两个instance配置，分别为example和example1里面的 正常启动过后过一会儿会客户端报错： java.lang.NullPointerException  at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.writeWithHeader(SimpleCanalConnector.java:358) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.ack(SimpleCanalConnector.java:318) ack batchId的时候报的空指针 报错的代码行为：connector.ack(batchId); 代码行和我这边对不上，自己改动过？
575,MySQLSyntaxErrorException: Duplicate column name 'REQUIRED' 莫名出现了下面的问题，但表中列是没有重复的，不知道到底是因为什么，麻烦大神给个思路 pid:1 nid:1 exception:setl:com.alibaba.otter.node.etl.load.exception.LoadException: java.util.concurrent.ExecutionException: com.alibaba.otter.node.etl.load.exception.LoadException: com.alibaba.otter.node.etl.load.exception.LoadException: org.springframework.jdbc.BadSqlGrammarException: StatementCallback; bad SQL grammar []; nested exception is com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Duplicate column name 'REQUIRED' Caused by: java.util.concurrent.ExecutionException: com.alibaba.otter.node.etl.load.exception.LoadException: com.alibaba.otter.node.etl.load.exception.LoadException: org.springframework.jdbc.BadSqlGrammarException: StatementCallback; bad SQL grammar []; nested exception is com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Duplicate column name 'REQUIRED' DDL重复同步？
574,canal监控过滤问题，binlog模式为Mixed binlog模式为Mixed，如何实现只对某个固定库名进行binlog监控。 使用1.0.26版本，目前已经基于SQL完整解析来支持过滤
573,连接阿里RDS，当发生主备切换后，canal报错，并且重启canal不能正常启动 
572,binlog的ddl中出现注释导致此消息被丢弃？ 当dba操作ddl时，例如新增一个字段，如果在命令前有注释的话，canal最终解析不到这个事件。 但是如果没有前面这行注释，就能解析到这条binlog， `  /**      * <code>optional bool isDdl = 10 [default = false];</code>      *      * <pre>      ** 标识是否是ddl语句  *      * </pre>      */     public boolean getIsDdl() {       return isDdl_;     }` 也就是说，如果执行ddl时指定了注释： `# at 708574121 #180403  0:33:33 server id 248037  end_log_pos 708574490 CRC32 0x0e8bdfed       Query   thread_id=15086636      exec_time=1647  error_code=0 SET TIMESTAMP=1522686813/*!*/; -- 注释注释  ALTER TABLE on_ord_act_paid03  ADD receivables_mch_id VARCHAR(50) DEFAULT NULL COMMENT 'Foo'   MODIFY COLUMN payment_method SMALLINT NOT NULL COMMENT 'Bar' /*!*/; # at 708574490 #180403  1:01:00 server id 248037  end_log_pos 708574578 CRC32 0x51d11ba5       Query   thread_id=15086688      exec_time=0     error_code=0 SET TIMESTAMP=1522688460/*!*/;` 那么这条binlog的rowChange.getIsDdl()应该目前是返回的false。 如果去掉“-- 注释注释 ”，则返回true。 当前数据库相关信息： Server version: 5.6.21-log Source distribution innodb_version        5.6.21 当前实例的binlog配置信息： `| binlog_cache_size                       | 2097152                          | | binlog_checksum                         | CRC32                            | | binlog_direct_non_transactional_updates | OFF                              | | binlog_format                           | ROW                              | | binlog_max_flush_queue_time             | 0                                | | binlog_order_commits                    | ON                               | | binlog_row_image                        | FULL                             | | binlog_rows_query_log_events            | OFF                              | | binlog_stmt_cache_size                  | 2097152                          | | binlogging_impossible_mode              | IGNORE_ERROR                     | | innodb_api_enable_binlog                | OFF                              | | innodb_locks_unsafe_for_binlog          | OFF                              | | log_bin                                 | ON                               | | log_bin_basename                        | /data/mysql_3306/mysql-bin       | | log_bin_index                           | /data/mysql_3306/mysql-bin.index | | log_bin_trust_function_creators         | OFF                              | | log_bin_use_v1_row_events               | OFF                              | | max_binlog_cache_size                   | 18446744073709547520             | | max_binlog_size                         | 1073741824                       | | max_binlog_stmt_cache_size              | 18446744073709547520             | | simplified_binlog_gtid_recovery         | OFF                              | | sql_log_bin                             | ON                               | | sync_binlog                             | 100                              |` mysql.conf [mysqld] basedir=/usr/local/mysql datadir=/data/mysql_3306 socket=/data/mysql_3306/mysql.sock port=3306 server_id=248037 log-bin=mysql-bin binlog_format = ROW relay-log=mysqld-relay-bin sync_binlog = 100 relay_log_purge=0 log_slave_updates query_cache_type = 0 query_cache_size = 0 table_definition_cache = 2048 table_open_cache = 4096 table_open_cache_instances = 8 sql_mode=NO_ENGINE_SUBSTITUTION STRICT_TRANS_TABLES skip-name-resolve back_log = 100 max_connections = 1000 max_connect_errors = 100000 max_allowed_packet = 16M binlog_cache_size = 2M binlog_stmt_cache_size = 2M max_heap_table_size = 256M tmp_table_size = 8M sort_buffer_size = 8M thread_cache_size = 256 ft_min_word_len = 4 thread_stack = 192K long_query_time = 3 tmpdir = /tmp key_buffer_size = 8M read_buffer_size = 8M join_buffer_size = 8M read_rnd_buffer_size = 8M bulk_insert_buffer_size = 8M character_set_server = utf8 collation_server = utf8_general_ci transaction_isolation = REPEATABLE-READ default_storage_engine = InnoDB default_tmp_storage_engine = InnoDB innodb_buffer_pool_size = 30G innodb_file_per_table = 1 innodb_file_io_threads = 4 innodb_thread_concurrency = 32 innodb_flush_log_at_trx_commit = 2 innodb_log_buffer_size = 256M innodb_log_file_size = 512M innodb_log_files_in_group = 3 innodb_flush_method = O_DIRECT innodb_lock_wait_timeout = 120 innodb_open_files = 32768 innodb_online_alter_log_max_size = 32G slow_query_log = on long_query_time = 0.5 slow_query_log_file = mysql-slow [mysql] default-character-set=utf8 sock = /data/mysql_3306/mysql.sock [mysqld_safe] log-error=/data/mysql_3306/mysqld.err pid-file=/data/mysql_3306/mysqld.pid 尝试使用最新的26版本，已经基于DDL的SQL全解析的方式，可以支持注释
571,java.net.SocketTimeoutException 1、canal 的server和client 跑一段时间之后出现这个异常，重启之后依然报这个异常，重新删掉meta.dat重新启动之后又有数据了请问这是为啥？ 2、canal 不管是网络还是物理机出问题了，如何保证数据不丢失。 ![default](https://user-images.githubusercontent.com/23053967/38133429-c0a550e4-3441-11e8-8080-79e82950539e.png) 如图我想从该位点重新开始同步数据，重启不好用，该如何做？ 多看一下wiki
570,‘FULLTEXT KEY’  and ‘ADD INDEX USING BTREE’   cause parse error v1.0.25 `parse faield : ALTER TABLE `xxxxx` ADD INDEX `idx_order_id` (`order_id`) USING BTREE com.alibaba.druid.sql.parser.ParserException: syntax error  error in :'SING BTREE'  expect IDENTIFIER  actual IDENTIFIER pos 132  line 4  column 45  token IDENTIFIER BTREE         at com.alibaba.druid.sql.parser.SQLParser.printError(SQLParser.java:284) ~[druid-1.1.6.jar:1.1.6]         at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:421) ~[druid-1.1.6.jar:1.1.6] ` `2018-03-26 09:11:52.244 [destination = kd_caesar_yf   address = test.kuaihuoyun.com/118.178.142.131:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `xxxx` (   `id` int(10) unsigned NOT NULL AUTO_INCREMENT COMMENT ''   ...   FULLTEXT KEY `ft_query_oid` (`query_oid`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='' com.alibaba.druid.sql.parser.ParserException: syntax error  error in :' KEY `ft_query_oid` (`query_oid`) )'  expect RPAREN  actual IDENTIFIER pos 2255  line 37  column 16  token IDENTIFIER `ft_query_oid`         at com.alibaba.druid.sql.parser.SQLParser.printError(SQLParser.java:284) ~[druid-1.1.6.jar:1.1.6]         at com.alibaba.druid.sql.parser.SQLParser.accept(SQLParser.java:292) ~[druid-1.1.6.jar:1.1.6]         at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:191) ~[druid-1.1.6.jar:1.1.6]         at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.6.jar:1.1.6]         at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.6.jar:1.1.6]         at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.6.jar:1.1.6]         at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.6.jar:1.1.6]         at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.6.jar:1.1.6]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.25.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.apply(DatabaseTableMeta.java:104) [canal.parse-1.0.25.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.apply(TableMetaCache.java:203) [canal.parse-1.0.25.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseQueryEvent(LogEventConvert.java:194) [canal.parse-1.0.25.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:107) [canal.parse-1.0.25.jar:na] ` 能提供一下完整的SQL？或者试试1.0.26的版本？ @agapple sql 如下 ALTER TABLE xxxxx ADD INDEX idx_order_id(order_id) USING BTREE; CREATE TABLE xxxx (    id int(10) unsigned NOT NULL AUTO_INCREMENT COMMENT ''    ft_query_oid varchar(32) NOT NULL default ''    FULLTEXT KEY ft_query_oid(query_oid)  ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='' ; 我这边也出现这个问题：具体代码如下： 2018-04-03 11:39:30.726 [destination = eam   address = /172.16.XX.XX:XXXX   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : ALTER TABLE `TB_AT_CREDITCARD_WITHHOLD_INFO` ADD UNIQUE INDEX `IDX_UNIQ_IDCARD_ACCOUNT` (`CUSTOMER_ID_CARD`  `CUSTOMER_BANK_ACCOUNT`) USING BTREE com.alibaba.druid.sql.parser.ParserException: syntax error  error in :'SING BTREE'  expect IDENTIFIER  actual IDENTIFIER pos 139  line 2  column 96  token IDENTIFIER BTREE         at com.alibaba.druid.sql.parser.SQLParser.printError(SQLParser.java:284) ~[druid-1.1.6.jar:1.1.6]         at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:421) ~[druid-1.1.6.jar:1.1.6]         at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.6.jar:1.1.6]         at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.6.jar:1.1.6]         at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.6.jar:1.1.6]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.25.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.applyHistoryOnMemory(DatabaseTableMeta.java:387) [canal.parse-1.0.25.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:121) [canal.parse-1.0.25.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.25.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.25.jar:na]         at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 确认为DDL解析bug，26版本里近期会修复 升级fastsql后已修复 您好。fastsql只能找到2.0.0_preview_186和2.0.0_preview_151版本。 麻烦请问371的版本地址 http://mvnrepository.com/artifact/com.alibaba.fastsql/fastsql 可以从我的二进制包里下载一份
569,msyql 5.6.16-log canal server1.0.26 解析varchar字段 出现 > 符号   msyql  阿里云rds 高可用版 5.6.16-log ，canal server 解析varchar字段 中文字符 出现 > 符号   乱码了吧 最好能给到一个可复现的测试方式
568,canal1.0.26/1.0.25 删除meta.dat 设定canal.instance.master.timestamp为1522209915000 启动后发现读取canal消息一会后，就读取到message的real size远远少于设定的batchSize，有时候甚至result is null。排除mysql的binlog记录数不够，有什么原因吗？ real size是按照内存大小获取的，非记录数
567, canal.deployer-1.0.26-SNAPSHOT版本rds获取不到日志信息 配置在本地的mysql，然后进行操作可以获取到操作信息 配置rds上的，操作无法获取到操作信息 只配置了instance.properties，还需要配置rds_instance.properties吗？ rds上报：canal.log里面报错 2018-03-28 15:28:06.345 [New I/O server worker #1-2] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x729a1c21  /192.168.10.156:64481 => /192.168.10.156:11111]  exception=java.io.IOException: 远程主机强迫关闭了一个现有的连接。 	at sun.nio.ch.SocketDispatcher.read0(Native Method) 	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43) 	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223) 	at sun.nio.ch.IOUtil.read(IOUtil.java:192) 	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380) 	at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:322) 	at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) 	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) 	at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) exception=java.io.IOException: 远程主机强迫关闭了一个现有的连接。 估计和canal server断链了 数据库还在运行，就是感觉canal这边连不上数据库，请问那要怎么处理呢
566,canal server启动后会产生大量的unauthenticated user 非认证用户连接。canal server停止后，unauthenticated user 非认证用户连接消失了。 canal version 1.0.24，canal server是主备模式 建议换1.0.26版本
565,canal server启动后会产生大量的unauthenticated user 非认证用户连接。canal server停止后，unauthenticated user 非认证用户连接消失了，但是instance配置的数据库连接并没有断开，看链接处于sleep状态。 canal版本1.0.24 
564,CanalConnector connector=CanalConnectors.newClusterConnector(canalProperties.getZkServers() canalProperties.getDestination() "" ""); CanalConnector connector=CanalConnectors.newClusterConnector(canalProperties.getZkServers() canalProperties.getDestination() "" "")server端如何设置password https://github.com/alibaba/canal/issues/192，参考这个
563,接收端如何区分该记录是Row还是STATEMENT 接收端如何区分该记录是Row还是STATEMENT String sql=rowChage.getSql(); 可以根据sql是否为空来判断吗。 多看文档吧
562,canal.instance.filter.regex server端的canal.instance.filter.regex和client端的connector.subscribe(subscribe_pattern)有什么关系，尝试了一下发现server端的设置貌似不起作用！  connector.subscribe(subscribe_pattern)  才是最佳姿势！ LS正解
561,如何在客户端重置Canal的位置？现在的位点在远程Canal所监听的mysql binlog时间长了已经删了 message message.getId()为-1 ## 如何在客户端重置Canal的位置？   ### 问题描述：   现在的位点在远程Canal所监听的mysql binlog时间长了已经删了。    message message.getId()为永远为-1。   message.getEntries().size() 永远为 0，永远读不出数据！！ ### 疑问：   请问如何在不改动 远程Canal服务的配置或其他啊哦做的情况下，客户端自己重置位置为最新的？   就是监听最新的binlog消息，之前的binlog就忽略？？？ ## Canal 服务destination配置 ``` canal.instance.mysql.slaveId = 617 # position info canal.instance.master.address = 10.1.6.226:3307 canal.instance.master.journal.name = canal.instance.master.position = canal.instance.master.timestamp = #canal.instance.standby.address = #canal.instance.standby.journal.name = #canal.instance.standby.position = #canal.instance.standby.timestamp = # username/password canal.instance.dbUsername = mescanal canal.instance.dbPassword = mescanal canal.instance.defaultDatabaseName = canal.instance.connectionCharset = UTF-8 # table regex canal.instance.filter.regex = .*\\..* # table black regex canal.instance.filter.black.regex = ``` ## meta.data ``` cat meta.dat {"clientDatas":[{"clientIdentity":{"clientId":1001 "destination":"mysql_b7_gaotaiunyue" "filter":".*\\..*"} "cursor":{"identity":{"slaveId":-1 "sourceAddress":{"address":"10.1.6.226" "port":3307}} "postion":{"included":false "journalName":"my3307-bin.000303" "position":77593731 "serverId":3307 "timestamp":1521010166000}}}] "destination":"mysql_b7_gaotaiunyue"} ``` ## 这是日志代码 ``` 2018-03-22 10:41:54.051 [destination = mysql_b7_gaotianyue   address = /10.1.6.226:3307   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /10.1.6.226:3307 has an error  retrying. caused by java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Could not find first log file name in binary log index file 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) ~[canal.parse-1.0.22.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) ~[canal.parse-1.0.22.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210) ~[canal.parse-1.0.22.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_65] 2018-03-22 10:41:54.051 [destination = mysql_b7_gaotianyue   address = /10.1.6.226:3307   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:mysql_b7_liupengchun[java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Could not find first log file name in binary log index file 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210) 	at java.lang.Thread.run(Thread.java:745) ``` Could not find first log file name in binary log index file，位点不存在 同感，能不能在位点异常错误的情况下，自动重置到最新的位点 +1 出现次数较多 同上 如果是阿里云这类rds的情况(一个vip挂后端主备)，会自动重新定位，其余的mysql情况，因为binlog被删除，即使重新定位也没用 +1  参考FAQ :  https://github.com/alibaba/canal/wiki/FAQ +1 被搞死了
560,启动报错 PropertyAccessException 1: org.springframework.beans.MethodInvocationException: Property 'tsdbSpringXml' threw exception; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'tableMetaTSDB' defined in class path resource [spring/tsdb/h2-tsdb.xml]: Cannot resolve reference to bean 'metaHistoryDAO' while setting bean property 'metaHistoryDAO'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'metaHistoryDAO' defined in class path resource [spring/tsdb/h2-tsdb.xml]: Cannot resolve reference to bean 'sqlMapClient' while setting bean property 'sqlMapClient'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlMapClient' defined in class path resource [spring/tsdb/h2-tsdb.xml]: Cannot resolve reference to bean 'dataSource' while setting bean property 'dataSource'; nested exception is org.springframework.beans.factory.CannotLoadBeanClassException: Error loading class [com.alibaba.druid.pool.DruidDataSource] for bean with name 'dataSource' defined in class path resource [spring/tsdb/h2-tsdb.xml]: problem with class file or dependent class; nested exception is java.lang.LinkageError: loading constraint violated 	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyPropertyValues(AbstractAutowireCapableBeanFactory.java:1453) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE] 	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1158) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE] 	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:519) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE] 	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:458) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE] 	at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:296) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE] 	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:223) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE] 	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:293) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE] 	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:194) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE] 	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:328) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE] 	... 30 common frames omitted Caused by: org.springframework.beans.PropertyBatchUpdateException: Failed properties: Property 'tsdbSpringXml' threw exception; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'tableMetaTSDB' defined in class path resource [spring/tsdb/h2-tsdb.xml]: Cannot resolve reference to bean 'metaHistoryDAO' while setting bean property 'metaHistoryDAO'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'metaHistoryDAO' defined in class path resource [spring/tsdb/h2-tsdb.xml]: Cannot resolve reference to bean 'sqlMapClient' while setting bean property 'sqlMapClient'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlMapClient' defined in class path resource [spring/tsdb/h2-tsdb.xml]: Cannot resolve reference to bean 'dataSource' while setting bean property 'dataSource'; nested exception is org.springframework.beans.factory.CannotLoadBeanClassException: Error loading class [com.alibaba.druid.pool.DruidDataSource] for bean with name 'dataSource' defined in class path resource [spring/tsdb/h2-tsdb.xml]: problem with class file or dependent class; nested exception is java.lang.LinkageError: loading constraint violated 	at org.springframework.beans.AbstractPropertyAccessor.setPropertyValues(AbstractPropertyAccessor.java:101) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE] 	at org.springframework.beans.AbstractPropertyAccessor.setPropertyValues(AbstractPropertyAccessor.java:57) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE] 	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyPropertyValues(AbstractAutowireCapableBeanFactory.java:1450) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE] 	... 38 common frames omitted java.lang.LinkageError: loading constraint violated，做过二次开发？
559,canal.instance.rds.startTime 请问canal支持阿里云的RDS了，但 canal.instance.rds.startTime= canal.instance.rds.endTime= 不知道如何设置，时间格式是什么样的? 在阿里云RDS控制台如何看到这个time？ 谢谢 这是获取rds oss binlog的解析，这个时间是标准的GMT 0时区的时间
558,com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. 更多的内容描述啊 ![image](https://user-images.githubusercontent.com/18360996/37639699-6a6d3832-2c4d-11e8-9004-32a054adc0de.png) otter还经常有假死现象，没有任何日志，channel没有挂起，pipeline也是工作中，但是当源库数据变化后，没有任何操作。重启channel之后基本就能恢复，不知道这是什么原因 otter manager，我不知道是不是我们用的方式不对。假如我有上百张表需要同步数据，没有更快捷的配置办法么？只能一个一个的搞？ otter的日志部分，建议加强，一大堆日志但却没有太多有价值信息输出
557,启动报错。真佩服阿里的程序员，吹NB挺厉害，写的东西真不敢恭维。 2018-03-15 13:35:46.673 [main] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2018-03-15 13:35:46.677 [main] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties] 2018-03-15 13:35:46.782 [main] WARN  o.s.beans.GenericTypeAwarePropertyDescriptor - Invalid JavaBean property 'connectionCharset' being accessed! Ambiguous write methods found next to actually used [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.nio.charset.Charset)]: [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.lang.String)] 2018-03-15 13:35:46.786 [main] INFO  c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-example  2018-03-15 13:35:47.162 [main] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - subscribe filter change to olot\..* 2018-03-15 13:35:47.163 [main] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - start successful.... 2018-03-15 13:35:47.163 [destination = example   address = /127.0.0.1:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position by switch ::1520690568000 2018-03-15 13:35:47.445 [destination = example   address = /127.0.0.1:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - Didn't find the corresponding binlog files from mysql-bin.000001 to mysql-bin.000002 2018-03-15 13:35:47.447 [destination = example   address = /127.0.0.1:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /127.0.0.1:3306 has an error  retrying. caused by  com.alibaba.otter.canal.parse.exception.CanalParseException: can't find start position for example 2018-03-15 13:35:47.449 [destination = example   address = /127.0.0.1:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: can't find start position for example ] 2018-03-15 13:35:47.452 [nioEventLoopGroup-2-5] ERROR c.a.o.canal.parse.driver.mysql.socket.SocketChannelPool - business error. java.lang.NullPointerException: null 	at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannel.writeCache(SocketChannel.java:34) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannelPool$BusinessHandler.channelRead0(SocketChannelPool.java:96) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannelPool$BusinessHandler.channelRead0(SocketChannelPool.java:73) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) ~[netty-all-4.1.6.Final.jar:4.1.6.Final] 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:373) [netty-all-4.1.6.Final.jar:4.1.6.Final] 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:359) [netty-all-4.1.6.Final.jar:4.1.6.Final] 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:351) [netty-all-4.1.6.Final.jar:4.1.6.Final] 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1334) [netty-all-4.1.6.Final.jar:4.1.6.Final] 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:373) [netty-all-4.1.6.Final.jar:4.1.6.Final] 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:359) [netty-all-4.1.6.Final.jar:4.1.6.Final] 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:926) [netty-all-4.1.6.Final.jar:4.1.6.Final] 	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:129) [netty-all-4.1.6.Final.jar:4.1.6.Final] 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:651) [netty-all-4.1.6.Final.jar:4.1.6.Final] 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:574) [netty-all-4.1.6.Final.jar:4.1.6.Final] 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:488) [netty-all-4.1.6.Final.jar:4.1.6.Final] 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:450) [netty-all-4.1.6.Final.jar:4.1.6.Final] 	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:873) [netty-all-4.1.6.Final.jar:4.1.6.Final] 	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) [netty-all-4.1.6.Final.jar:4.1.6.Final] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131] 2018-03-15 13:35:57.859 [destination = example   address = /127.0.0.1:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position by switch ::1520690568000 2018-03-15 13:35:58.136 [destination = example   address = /127.0.0.1:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - Didn't find the corresponding binlog files from mysql-bin.000001 to mysql-bin.000002 2018-03-15 13:35:58.137 [destination = example   address = /127.0.0.1:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /127.0.0.1:3306 has an error  retrying. caused by  com.alibaba.otter.canal.parse.exception.CanalParseException: can't find start position for example 2018-03-15 13:35:58.137 [destination = example   address = /127.0.0.1:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: can't find start position for example ] 2018-03-15 13:35:58.145 [nioEventLoopGroup-2-3] ERROR c.a.o.canal.parse.driver.mysql.socket.SocketChannelPool - business error. java.lang.NullPointerException: null 	at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannel.writeCache(SocketChannel.java:34) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannelPool$BusinessHandler.channelRead0(SocketChannelPool.java:96) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannelPool$BusinessHandler.channelRead0(SocketChannelPool.java:73) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) ~[netty-all-4.1.6.Final.jar:4.1.6.Final] 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:373) [netty-all-4.1.6.Final.jar:4.1.6.Final] 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:359) [netty-all-4.1.6.Final.jar:4.1.6.Final] 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:351) [netty-all-4.1.6.Final.jar:4.1.6.Final] 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1334) [netty-all-4.1.6.Final.jar:4.1.6.Final] 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:373) [netty-all-4.1.6.Final.jar:4.1.6.Final] 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:359) [netty-all-4.1.6.Final.jar:4.1.6.Final] 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:926) [netty-all-4.1.6.Final.jar:4.1.6.Final] 	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:129) [netty-all-4.1.6.Final.jar:4.1.6.Final] 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:651) [netty-all-4.1.6.Final.jar:4.1.6.Final] 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:574) [netty-all-4.1.6.Final.jar:4.1.6.Final] 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:488) [netty-all-4.1.6.Final.jar:4.1.6.Final] 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:450) [netty-all-4.1.6.Final.jar:4.1.6.Final] 	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:873) [netty-all-4.1.6.Final.jar:4.1.6.Final] 	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) [netty-all-4.1.6.Final.jar:4.1.6.Final] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131] 有问题就反馈，没必要上纲上线，一个snapshot版本
556,Centos下编不过 [INFO] Total time: 2.336s [INFO] Finished at: Thu Mar 15 13:48:55 CST 2018 [INFO] Final Memory: 18M/248M [INFO] ------------------------------------------------------------------------ [ERROR] Failed to execute goal on project canal.parse: Could not resolve dependencies for project com.alibaba.otter:canal.parse:jar:1.0.26-SNAPSHOT: Failed to collect dependencies for [com.alibaba.otter:canal.common:jar:1.0.26-SNAPSHOT (compile)  com.alibaba.otter:canal.protocol:jar:1.0.26-SNAPSHOT (compile)  com.alibaba.otter:canal.meta:jar:1.0.26-SNAPSHOT (compile)  com.alibaba.otter:canal.sink:jar:1.0.26-SNAPSHOT (compile)  com.alibaba.otter:canal.parse.dbsync:jar:1.0.26-SNAPSHOT (compile)  com.alibaba.otter:canal.filter:jar:1.0.26-SNAPSHOT (compile)  com.alibaba.otter:canal.parse.driver:jar:1.0.26-SNAPSHOT (compile)  com.alibaba:druid:jar:1.1.9 (compile)  com.alibaba.fastsql:fastsql:jar:2.0.0_preview_135 (compile)  mysql:mysql-connector-java:jar:5.1.40 (compile)  org.apache.ibatis:ibatis-sqlmap:jar:2.3.4.726 (compile)  com.h2database:h2:jar:1.4.196 (compile)  org.apache.httpcomponents:httpclient:jar:4.5.1 (compile)  org.apache.commons:commons-compress:jar:1.9 (compile)  junit:junit:jar:4.12 (test)  org.springframework:spring-test:jar:3.2.9.RELEASE (test)]: Failed to read artifact descriptor for com.alibaba.fastsql:fastsql:jar:2.0.0_preview_135: Could not transfer artifact com.alibaba.fastsql:fastsql:pom:2.0.0_preview_135 from/to alibaba (http://code.alibabatech.com/mvn/releases/): Connection to http://code.alibabatech.com refused: Connection refused (Connection refused) -> [Help 1] [ERROR]  [ERROR] To see the full stack trace of the errors  re-run Maven with the -e switch. [ERROR] Re-run Maven using the -X switch to enable full debug logging. [ERROR]  [ERROR] For more information about the errors and possible solutions  please read the following articles: [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException [ERROR]  重新更新一下代码，fastsql代码已经上传到maven仓库
555,renew an InetSocketAddress to resolve address again resolve [issue427](https://github.com/alibaba/otter/issues/427) [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=555) <br/>All committers have signed the CLA. [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=555) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=555) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=555) it.</sub> tks
554,javax.mail.AuthenticationFailedException: 526 Authentication failure 环境：阿里云企业邮箱，SSL端口，SMTP 要求用户验证。 2018-03-12 21:51:02.457 ERROR c.a.otter.manager.biz.common.alarm.AbstractAlarmService - send alarm [AlarmMessage[message=pid:1 nid:2 exception:canal:ZjcsToDsb:java.io.IOException: socket read timeout occured ! readSize = 4  readableBytes = 0  timeout = 16000         at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannel.read(SocketChannel.java:172)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:174)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:80)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:144)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220)         at java.lang.Thread.run(Thread.java:745) receiveKey=otterteam]] to drgoon agent error! org.springframework.mail.MailAuthenticationException: Authentication failed; nested exception is javax.mail.AuthenticationFailedException: 526 Authentication failure[0]         at org.springframework.mail.javamail.JavaMailSenderImpl.doSend(JavaMailSenderImpl.java:392) ~[spring-context-support-3.1.2.RELEASE.jar:3.1.2.RELEASE]         at org.springframework.mail.javamail.JavaMailSenderImpl.send(JavaMailSenderImpl.java:306) ~[spring-context-support-3.1.2.RELEASE.jar:3.1.2.RELEASE]         at org.springframework.mail.javamail.JavaMailSenderImpl.send(JavaMailSenderImpl.java:296) ~[spring-context-support-3.1.2.RELEASE.jar:3.1.2.RELEASE]         at com.alibaba.otter.manager.biz.common.alarm.DefaultAlarmService.doSendMail(DefaultAlarmService.java:62) ~[manager.biz-4.2.16-SNAPSHOT.jar:na]         at com.alibaba.otter.manager.biz.common.alarm.DefaultAlarmService.doSend(DefaultAlarmService.java:52) ~[manager.biz-4.2.16-SNAPSHOT.jar:na]         at com.alibaba.otter.manager.biz.common.alarm.AbstractAlarmService.sendAlarmInternal(AbstractAlarmService.java:60) [manager.biz-4.2.16-SNAPSHOT.jar:na]         at com.alibaba.otter.manager.biz.common.alarm.AbstractAlarmService.access$000(AbstractAlarmService.java:38) [manager.biz-4.2.16-SNAPSHOT.jar:na]         at com.alibaba.otter.manager.biz.common.alarm.AbstractAlarmService$1.run(AbstractAlarmService.java:77) [manager.biz-4.2.16-SNAPSHOT.jar:na]         at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) [na:1.7.0_79]         at java.util.concurrent.FutureTask.run(FutureTask.java:262) [na:1.7.0_79]         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_79]         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_79]         at java.lang.Thread.run(Thread.java:745) [na:1.7.0_79] Caused by: javax.mail.AuthenticationFailedException: 526 Authentication failure[0]         at com.sun.mail.smtp.SMTPTransport$Authenticator.authenticate(SMTPTransport.java:826) ~[mail-1.4.7.jar:1.4.7]         at com.sun.mail.smtp.SMTPTransport.authenticate(SMTPTransport.java:761) ~[mail-1.4.7.jar:1.4.7]         at com.sun.mail.smtp.SMTPTransport.protocolConnect(SMTPTransport.java:685) ~[mail-1.4.7.jar:1.4.7]         at javax.mail.Service.connect(Service.java:295) ~[mail-1.4.7.jar:1.4.7]         at org.springframework.mail.javamail.JavaMailSenderImpl.doSend(JavaMailSenderImpl.java:389) ~[spring-context-support-3.1.2.RELEASE.jar:3.1.2.RELEASE]         ... 12 common frames omitted JavaMailSenderImpl只调通了163和gmail，其他的需要做一些调试
553,修改：优化在默认缓存大小不够用时自动清理或扩充内存，尽量避免频繁分配或移动内存，控制内存最大分配空间或异常自愈。所有这些检查和处理都在写缓存线程中进行。 修复：在“同步管理”中停用Channel时可能会导致相关Node进程崩溃：A fatal error has been detected by the Java Runtime Environment: EXCEPTION_ACCESS_VIOLATION (0xc0000005) 此为继 #536 的继续完善，目前已经稳定运行 4 天。配合 otter 的 PR，目前已支持大附件复制（实测超过 8 MB ）。 注意： 本次修改增加的日志确实存在 #548 中提到的错位现象，刚接触 canal 还不甚了解，待修复。 代码已经提交，-Dcanal.socketChannel=netty 可以切换 收到，感谢！
552,bug实在太多，canal.instance.master.position不起作用，也不报出一点有用的信息。 canal.instance.master.journal.name=mysql-bin.000001 canal.instance.master.position=1 canal.instance.master.timestamp=1509862400000  设置了偏移量但是不起作用，还是从总后一行开始扫描。 WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position position最小值是4
551,执行有注释的ddl语句，canal解析ddl eventType为QUERY，binlog为row模式，是否是bug？ **canal解析** 执行语句（带有注释）： #测试start #2018-02-22 #测试end alter table test_2018 add loan_amount_backstage_adjust1 varchar(64) DEFAULT NULL COMMENT '测试内容：1：无，2：有'  解析结果： eventType: QUERY isDdl：false sql: "#\305\233\304\266\304\215\305\244\304\256\304\223start\r\n#2018-02-22\r\n#\305\233\304\266\304\215\305\244\304\256\304\223end\r\nalter table test_2018 add loan_amount_backstage_adjust varchar(64) DEFAULT NULL COMMENT \'\305\233\304\266\304\215\305\244\304\256\304\223\305\232\303\234\303\226\305\232\304\243\304\273\303\224\304\276\303\2661\303\224\304\276\303\266\305\233\303\263\342\200\240\303\224\304\276\304\2062\303\224\304\276\303\266\305\233\303\272\304\214\'" ddlSchemaName: "test" —————————————————————————————————————————— 执行语句（无注释）： alter table test_2018 add loan_amount_backstage_adjust1 varchar(64) DEFAULT NULL COMMENT '测试内容：1：无，2：有'  解析结果： eventType: ALTER isDdl: true sql: "alter table test_2018 add loan_amount_backstage_adjust1 varchar(64) DEFAULT NULL COMMENT \'\305\233\304\266\304\215\305\244\304\256\304\223\305\232\303\234\303\226\305\232\304\243\304\273\303\224\304\276\303\2661\303\224\304\276\303\266\305\233\303\263\342\200\240\303\224\304\276\304\2062\303\224\304\276\303\266\305\233\303\272\304\214\'" ddlSchemaName: "test" canal解析ddl eventType为QUERY，binlog且为row模式，insert update delete 都正常，这个是canal未考虑到吗? mysql什么版本？ 是带着#号的注释么？ @agapple  是的  @fefine 5.6.29 @agapple 只要带有#号的注释 解析eventType: QUERY  升级canal到最新的版本，可以解决 @liuwenfeng554  感觉这个问题算一个坑，请问升级了解决了吗？ @renjin1984 据说新的版本  还不够稳定  暂时没升  后续如果有时间看看
550,com.alibaba.otter.canal.protocol.exception.CanalClientException: java.io.IOException: Broken pipe 这个是安装完canal后运行example示例代码出现的问题 com.alibaba.otter.canal.protocol.exception.CanalClientException: java.io.IOException: Broken pipe 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:296) ~[canal.client-1.0.25.jar:na] 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:269) ~[canal.client-1.0.25.jar:na] 	at com.iwaimai.CanalTest.AbstractCanalClientTest.process(AbstractCanalClientTest.java:116) ~[classes/:na] 	at com.iwaimai.CanalTest.AbstractCanalClientTest$2.run(AbstractCanalClientTest.java:83) [classes/:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_162] Caused by: java.io.IOException: Broken pipe 	at sun.nio.ch.FileDispatcherImpl.write0(Native Method) ~[na:1.8.0_162] 	at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47) ~[na:1.8.0_162] 	at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93) ~[na:1.8.0_162] 	at sun.nio.ch.IOUtil.write(IOUtil.java:65) ~[na:1.8.0_162] 	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:471) ~[na:1.8.0_162] 	at java.nio.channels.Channels.writeFullyImpl(Channels.java:78) ~[na:1.8.0_162] 	at java.nio.channels.Channels.writeFully(Channels.java:98) ~[na:1.8.0_162] 	at java.nio.channels.Channels.access$000(Channels.java:61) ~[na:1.8.0_162] 	at java.nio.channels.Channels$1.write(Channels.java:174) ~[na:1.8.0_162] 	at java.nio.channels.Channels$WritableByteChannelImpl.write(Channels.java:458) ~[na:1.8.0_162] 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.writeWithHeader(SimpleCanalConnector.java:382) ~[canal.client-1.0.25.jar:na] 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.writeWithHeader(SimpleCanalConnector.java:369) ~[canal.client-1.0.25.jar:na] 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:281) ~[canal.client-1.0.25.jar:na] 	... 4 common frames omitted 10:26:39.350 [Thread-0] ERROR c.i.C.AbstractCanalClientTest - process error! com.alibaba.otter.canal.protocol.exception.CanalClientException: something goes wrong with reason: something goes wrong with channel:[id: 0x57943e05  /10.11.61.139:53053 => /10.11.61.139:11111]  exception=com.alibaba.otter.canal.server.exception.CanalServerException: destination:example should start first 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.receiveMessages(SimpleCanalConnector.java:317) ~[canal.client-1.0.25.jar:na] 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:294) ~[canal.client-1.0.25.jar:na] 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:269) ~[canal.client-1.0.25.jar:na] 	at com.iwaimai.CanalTest.AbstractCanalClientTest.process(AbstractCanalClientTest.java:116) ~[classes/:na] 	at com.iwaimai.CanalTest.AbstractCanalClientTest$2.run(AbstractCanalClientTest.java:83) [classes/:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_162] 10:26:39.351 [Thread-0] ERROR c.i.C.AbstractCanalClientTest - process error! com.alibaba.otter.canal.protocol.exception.CanalClientException: something goes wrong with reason: something goes wrong with channel:[id: 0x4f01a4c1  /10.11.61.139:53054 => /10.11.61.139:11111]  exception=com.alibaba.otter.canal.server.exception.CanalServerException: destination:example should start first 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.receiveMessages(SimpleCanalConnector.java:317) ~[canal.client-1.0.25.jar:na] 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:294) ~[canal.client-1.0.25.jar:na] 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:269) ~[canal.client-1.0.25.jar:na] 	at com.iwaimai.CanalTest.AbstractCanalClientTest.process(AbstractCanalClientTest.java:116) ~[classes/:na] 	at com.iwaimai.CanalTest.AbstractCanalClientTest$2.run(AbstractCanalClientTest.java:83) [classes/:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_162] 10:26:39.352 [Thread-0] ERROR c.i.C.AbstractCanalClientTest - process error! com.alibaba.otter.canal.protocol.exception.CanalClientException: something goes wrong with reason: something goes wrong with channel:[id: 0x7e3c718f  /10.11.61.139:53055 => /10.11.61.139:11111]  exception=com.alibaba.otter.canal.server.exception.CanalServerException: destination:example should start first 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.receiveMessages(SimpleCanalConnector.java:317) ~[canal.client-1.0.25.jar:na] 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:294) ~[canal.client-1.0.25.jar:na] 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:269) ~[canal.client-1.0.25.jar:na] 	at com.iwaimai.CanalTest.AbstractCanalClientTest.process(AbstractCanalClientTest.java:116) ~[classes/:na] 	at com.iwaimai.CanalTest.AbstractCanalClientTest$2.run(AbstractCanalClientTest.java:83) [classes/:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_162] 客户端做一下重试 升级一下26版本
549,解析出错，求帮助。 2018-03-07 16:33:44.617 [destination = example   address = /127.0.0.1:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `procs_priv` (   `Host` char(60) COLLATE utf8_bin NOT NULL DEFAULT ''   `Db` char(64) COLLATE utf8_bin NOT NULL DEFAULT ''   `User` char(16) COLLATE utf8_bin NOT NULL DEFAULT ''   `Routine_name` char(64) CHARACTER SET utf8 NOT NULL DEFAULT ''   `Routine_type` enum('FUNCTION' 'PROCEDURE') COLLATE utf8_bin NOT NULL   `Grantor` char(77) COLLATE utf8_bin NOT NULL DEFAULT ''   `Proc_priv` set('Execute' 'Alter Routine' 'Grant') CHARACTER SET utf8 NOT NULL DEFAULT ''   `Timestamp` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP   PRIMARY KEY (`Host` `Db` `User` `Routine_name` `Routine_type`)   KEY `Grantor` (`Grantor`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='Procedure privileges' com.alibaba.druid.sql.parser.ParserException: syntax error  error in :'RE') COLLATE utf8_bin NOT NULL   `'  expect RPAREN  actual IDENTIFIER pos 313  line 6  column 47  token IDENTIFIER COLLATE         at com.alibaba.druid.sql.parser.SQLParser.printError(SQLParser.java:284) ~[druid-1.1.8.jar:1.1.8]         at com.alibaba.druid.sql.parser.SQLParser.accept(SQLParser.java:292) ~[druid-1.1.8.jar:1.1.8]         at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:217) ~[druid-1.1.8.jar:1.1.8]         at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:243) ~[druid-1.1.8.jar:1.1.8]         at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:161) ~[druid-1.1.8.jar:1.1.8]         at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:72) ~[druid-1.1.8.jar:1.1.8]         at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.8.jar:1.1.8]         at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:297) ~[druid-1.1.8.jar:1.1.8]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:71) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-03-07 16:33:44.801 [destination = example   address = /127.0.0.1:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /127.0.0.1:3306 has an error  retrying. caused by java.lang.NullPointerException: null         at com.alibaba.druid.sql.dialect.mysql.visitor.MySqlOutputVisitor.visit(MySqlOutputVisitor.java:611) ~[druid-1.1.8.jar:1.1.8]         at com.alibaba.druid.sql.ast.expr.SQLCharExpr.accept0(SQLCharExpr.java:50) ~[druid-1.1.8.jar:1.1.8]         at com.alibaba.druid.sql.ast.SQLObjectImpl.accept(SQLObjectImpl.java:41) ~[druid-1.1.8.jar:1.1.8]         at com.alibaba.druid.sql.dialect.mysql.visitor.MySqlOutputVisitor.visit(MySqlOutputVisitor.java:275) ~[druid-1.1.8.jar:1.1.8]         at com.alibaba.druid.sql.ast.statement.SQLColumnDefinition.accept0(SQLColumnDefinition.java:167) ~[druid-1.1.8.jar:1.1.8]         at com.alibaba.druid.sql.ast.SQLObjectImpl.accept(SQLObjectImpl.java:41) ~[druid-1.1.8.jar:1.1.8]         at com.alibaba.druid.sql.visitor.SQLASTOutputVisitor.printTableElements(SQLASTOutputVisitor.java:2567) ~[druid-1.1.8.jar:1.1.8]         at com.alibaba.druid.sql.dialect.mysql.visitor.MySqlOutputVisitor.visit(MySqlOutputVisitor.java:457) ~[druid-1.1.8.jar:1.1.8]         at com.alibaba.druid.sql.dialect.mysql.ast.statement.MySqlCreateTableStatement.accept0(MySqlCreateTableStatement.java:89) ~[druid-1.1.8.jar:1.1.8]         at com.alibaba.druid.sql.dialect.mysql.ast.statement.MySqlCreateTableStatement.accept0(MySqlCreateTableStatement.java:82) ~[druid-1.1.8.jar:1.1.8]         at com.alibaba.druid.sql.ast.SQLObjectImpl.accept(SQLObjectImpl.java:41) ~[druid-1.1.8.jar:1.1.8]         at com.alibaba.druid.sql.dialect.mysql.ast.statement.MySqlCreateTableStatement.output(MySqlCreateTableStatement.java:359) ~[druid-1.1.8.jar:1.1.8]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.snapshot(MemoryTableMeta.java:138) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.applySnapshotToDB(DatabaseTableMeta.java:241) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:129) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]         at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-03-07 16:33:44.802 [destination = example   address = /127.0.0.1:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.lang.NullPointerException         at com.alibaba.druid.sql.dialect.mysql.visitor.MySqlOutputVisitor.visit(MySqlOutputVisitor.java:611)         at com.alibaba.druid.sql.ast.expr.SQLCharExpr.accept0(SQLCharExpr.java:50)         at com.alibaba.druid.sql.ast.SQLObjectImpl.accept(SQLObjectImpl.java:41)         at com.alibaba.druid.sql.dialect.mysql.visitor.MySqlOutputVisitor.visit(MySqlOutputVisitor.java:275)         at com.alibaba.druid.sql.ast.statement.SQLColumnDefinition.accept0(SQLColumnDefinition.java:167)         at com.alibaba.druid.sql.ast.SQLObjectImpl.accept(SQLObjectImpl.java:41)         at com.alibaba.druid.sql.visitor.SQLASTOutputVisitor.printTableElements(SQLASTOutputVisitor.java:2567)         at com.alibaba.druid.sql.dialect.mysql.visitor.MySqlOutputVisitor.visit(MySqlOutputVisitor.java:457)         at com.alibaba.druid.sql.dialect.mysql.ast.statement.MySqlCreateTableStatement.accept0(MySqlCreateTableStatement.java:89)         at com.alibaba.druid.sql.dialect.mysql.ast.statement.MySqlCreateTableStatement.accept0(MySqlCreateTableStatement.java:82)         at com.alibaba.druid.sql.ast.SQLObjectImpl.accept(SQLObjectImpl.java:41)         at com.alibaba.druid.sql.dialect.mysql.ast.statement.MySqlCreateTableStatement.output(MySqlCreateTableStatement.java:359)         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.snapshot(MemoryTableMeta.java:138)         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.applySnapshotToDB(DatabaseTableMeta.java:241)         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:129)         at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170)         at java.lang.Thread.run(Thread.java:748) ] druid暂时不支持enum类型，已提交issue https://github.com/alibaba/canal/releases/tag/canal-1.0.26-preview-2，使用这个版本的包试试
548,对于binlog dump本身就是阻塞式的读写，使用更简单的BIO 这个pr会覆盖其他open的pr 536，539。 最近进行的测试，发现了SocketChannel的一些其他问题： 在构建新SocketChannel时，EventLoopGroup调next()就近分配EventLoop进行注册，经过长时间运行，某一个instance的channel如果期间中断过多次，可能round到一个其他instance的channel注册的EventLoop。由于没有在记录log前后设置MDC，日志会记到其他instance的日志文件中。多个channel共享一个EventLoop，轮流使用时间片会使读性能变差（如果消费的快）。 BIO使用较小的SO_TIMEOUT(1s)，处理闪断和interrupt。 其他改动：READ_TIMEOUT_MILLISECONDS修改为(MASTER_HEARTBEAT_PERIOD_SECONDS + 10) * 1000. +1s的情况，在一个instance无traffic放置一天的测试中，出现了几次读取header timeout，频率大致几小时一次。 @lcybo 不知 #547 的问题是不是和这个也有关系？ @lcybo 根据你之前的建议，我对 #536 的修改稍作调整。我觉得异步模式也不错的，读写可以异步进行。 @wingerx 到时候对同步和异步模式分别压测一下试试。最近因为发现还有其它问题，等等再提交。 @wingerx 谢谢你的测试。请问，当压测停的时候canal的meta里的position和show master status;之间的gap是多少，dump所使用的interface的网络流量情况是什么样的？ @Wu-Jianqiang BIO和NIO各有优势，各有适用的场景。就MySQL dump来说，在握手成功，订阅指定位置之后，canal单方面接收MySQL的data。“读写可以异步进行”  netty handler里做仅仅是将读到的data‘写入’cache，本质上还是读。对于这样的数量有限（常数）的长连接，我认为还是传统BIO更为适合。如果继续使用Netty，还有一些问题需要解决： 如果多个channel被register到同一个EventLoop（多次中断重新连接，或修改io.netty.eventLoopThreads），多个instance的‘business error’日志会记录到一个instance中，并且一个EventLoop同时处理多个channel，反而影响性能。得实现一个EventExecutorChooser保证一个EventLoop一个Channel。记录Business error时还需要用MDC dispatch. 代码已经合并了，设计上目前暂时都保留了bio和netty，方便做验证和对比
547,Canal Server 解析性能问题 不知有人压测过 Canal 的解析性能吗？目前压测 v1.0.26 alpha 1 (为屏蔽可能消费慢的原因，修改代码将event 不放入store)，开始大概每秒可以decode 万级别的event，但是持续一段时间后每秒下降到百级别的event (后续定位到是压测停的时候)。不知有人遇到过这个问题没？ 补上测试数据  （纯测试解析，因为不存store，故无消费端影响) 1. 纯insert 场景测试 ![image](https://user-images.githubusercontent.com/33280738/36956528-cf5f270e-2069-11e8-9962-18fd16789b1a.png) 2. Canal Server 接收到数据 * 使用 v1.0.26 版本的 SocketChannel 与 SocketChannelPool 测试情况 ![image](https://user-images.githubusercontent.com/33280738/36957343-b5f908fc-206e-11e8-903c-cd2a9a9e4c6f.png) 当压测停止时，性能出现陡降(此时event 并没有同步结束): 红线标注的是关掉压测的瞬间 ![image](https://user-images.githubusercontent.com/33280738/36957403-23727e86-206f-11e8-8756-808af88c7aee.png) 压测重新启动后，性能回升但抖动很大，持续一段时间后，性能回到停之前的状态 ![image](https://user-images.githubusercontent.com/33280738/36957479-a5c07a6e-206f-11e8-9e49-bf0f3287f50b.png) ![image](https://user-images.githubusercontent.com/33280738/36957515-cde4f088-206f-11e8-9132-492a18168cdf.png) * 使用 #548 @lcybo 提供的简单BIO 版本的 SocketChannel 与 SocketChannelPool 测试情况 ![image](https://user-images.githubusercontent.com/33280738/36957583-44780c9e-2070-11e8-990f-3fb942380ff2.png) 当压测停止时，性能出现提升(这个现象和MySQL slave 观察到的网络流量现象一致): ![image](https://user-images.githubusercontent.com/33280738/36957655-d170aa5c-2070-11e8-8313-3bd71242fc90.png) 是否这么一个结论，基于BIO模式的读取，性能更加稳定？ @wingerx PR了异步修复版 #553 ，有空再试试！
546,canalserver升级到1.0.25启动出现create connection SQLException 2018-03-02 16:10:49.030 [Druid-ConnectionPool-Create-124634922] ERROR com.alibaba.druid.pool.DruidDataSource - create connection SQLException  url:   errorCode 0  state null java.sql.SQLException: connect error  url   driverClass org.h2.Driver         at com.alibaba.druid.pool.DruidAbstractDataSource.createPhysicalConnection(DruidAbstractDataSource.java:1579) ~[druid-1.1.6.jar:1.1.6]         at com.alibaba.druid.pool.DruidDataSource$CreateConnectionThread.run(DruidDataSource.java:2450) ~[druid-1.1.6.jar:1.1.6] 启动时会初始化一个嵌入式的H2内存数据库，看看启动失败的具体原因吧
545,[question]请问 canal 可以订阅从库的 binlog吗？ canal 是以从库的角色对主库订阅 bin-log，现在我有两个疑问： 1.canal 是否支持订阅从库的bin-log？ 2.canal 订阅主库的 bin-log 对主库的性能损耗，是否有必要迁移到订阅从库bin-log？ 有哪位试验过订阅从库bin-log的，帮帮忙分析下，谢谢了。 canal 是否支持订阅从库的bin-log？我也想知道这个问题。 目前我们有一个需求是把很多不同区域的库同步到一个总库，也在考虑canal怎么用一个客户端监听多个服务端的问题。 目前看example里面的集群客户端是一种主备模式，也不满足这种需求。 但是如果启动多个CanalConnectors应该可以解决吧。 @yookacn 你是想一个canal-server，订阅多个master实例吗？ 如果是，可以使用default模式，配置多个destination。就可以实现了。 @smileMrLee谢谢相告，我之前还没考虑到从服务端订阅多个实例，翻了一下代码找到了。这个可以满足我的需求。
544,canal无法启动 mysql版本:5.6.39  使用canal版本1.0.25 如下 详细描述下:使用mysql版本:5.6.39  使用canal版本1.0.25 现象: ![image](https://user-images.githubusercontent.com/5006160/36886406-31d37966-1e27-11e8-8c9d-a4832afb76c5.png) canal启动的时候 通过jstack发现堵塞在这一行. 翻了下代码: ![image](https://user-images.githubusercontent.com/5006160/36886423-4a49d81e-1e27-11e8-8da3-5b3e2b0a21a0.png) 这里就连接不上了 一直阻塞 不可能是权限问题 因为已经将binlog的位置都读出来了 必现？ https://github.com/alibaba/canal/releases/tag/canal-1.0.26-preview-2，使用这个版本的包试试 必现 这个包我试试 谢谢 我也碰到这个问题，总是卡在读取位点之后的reconnect或者connect读位点。
543,变更类型为update类型，但是字段中没有一个column.getUpdated()为true，全部为false 如题 描述复现场景
542,canal启动失败[MysqlEventParser - ERROR ## findAsPerTimestampInSpecificLogFile has an error] 无论如何 每次启动canal都会失败 ## 数据库配置 ``` [mysqld] log_bin=mysql-bin binlog_format='ROW' server_id=1 datadir=/var/lib/mysql socket=/var/lib/mysql/mysql.sock symbolic-links=0 sql_mode=NO_ENGINE_SUBSTITUTION STRICT_TRANS_TABLES [mysqld_safe] log-error=/var/log/mysqld.log pid-file=/var/run/mysqld/mysqld.pid ``` ## mysql canal用户的权限配置(可以看到权限没有问题) ``` mysql> select host user password from mysql.user  where user like '%canal%'; +-----------+-------+-------------------------------------------+ | host      | user  | password                                  | +-----------+-------+-------------------------------------------+ | %         | canal | *E3619321C1A937C46A0D8BD1DAC39F93B27D4458 | | 127.0.0.1 | canal | *E3619321C1A937C46A0D8BD1DAC39F93B27D4458 | | localhost | canal | *E3619321C1A937C46A0D8BD1DAC39F93B27D4458 | +-----------+-------+-------------------------------------------+ 3 rows in set (0.05 sec) ``` ## canal 配置 ``` ## mysql serverId canal.instance.mysql.slaveId=0 # position info canal.instance.master.address=127.0.0.1:3306 canal.instance.master.journal.name= canal.instance.master.position= canal.instance.master.timestamp= # table meta tsdb info canal.instance.tsdb.enable=true canal.instance.tsdb.dir=${canal.file.data.dir:../conf}/${canal.instance.destination:} canal.instance.tsdb.url=jdbc:h2:${canal.instance.tsdb.dir}/h2;CACHE_SIZE=1000;MODE=MYSQL; #canal.instance.tsdb.url=jdbc:mysql://127.0.0.1:3306/canal_tsdb canal.instance.tsdb.dbUsername=canal canal.instance.tsdb.dbPassword=canal #canal.instance.standby.address = #canal.instance.standby.journal.name = #canal.instance.standby.position =  #canal.instance.standby.timestamp =  # username/password canal.instance.dbUsername=canal canal.instance.dbPassword=canal canal.instance.defaultDatabaseName=canal_test canal.instance.connectionCharset=UTF-8 # table regex canal.instance.filter.regex=.*\\..* # table black regex canal.instance.filter.black.regex= ``` ## 启动canal的 canal.log ``` Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=96m; support was removed in 8.0 Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=256m; support was removed in 8.0 Java HotSpot(TM) 64-Bit Server VM warning: UseCMSCompactAtFullCollection is deprecated and will likely be removed in a future release. 2018-03-01 12:04:36.678 [main] INFO  com.alibaba.otter.canal.deployer.CanalLauncher - ## set default uncaught exception handler 2018-03-01 12:04:36.781 [main] INFO  com.alibaba.otter.canal.deployer.CanalLauncher - ## load canal configurations 2018-03-01 12:04:36.783 [main] INFO  com.alibaba.otter.canal.deployer.CanalLauncher - ## start the canal server. 2018-03-01 12:04:36.932 [main] INFO  com.alibaba.otter.canal.deployer.CanalController - ## start the canal server[172.17.251.157:11111] 2018-03-01 12:04:37.993 [main] WARN  o.s.beans.GenericTypeAwarePropertyDescriptor - Invalid JavaBean property 'connectionCharset' being accessed! Ambiguous write methods found next to actually used [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.nio.charset.Charset)]: [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.lang.String)] 2018-03-01 12:04:38.661 [main] ERROR com.alibaba.druid.pool.DruidDataSource - testWhileIdle is true  validationQuery not set 2018-03-01 12:04:39.284 [main] INFO  com.alibaba.otter.canal.deployer.CanalLauncher - ## the canal server is running now ...... 2018-03-01 12:04:40.265 [destination = example   address = /127.0.0.1:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just show master status 2018-03-01 12:04:43.385 [destination = example   address = /127.0.0.1:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - ERROR ## findAsPerTimestampInSpecificLogFile has an error java.io.IOException: connect /127.0.0.1:3306 failure         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:74) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.reconnect(MysqlConnector.java:83) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.reconnect(MysqlConnection.java:76) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findAsPerTimestampInSpecificLogFile(MysqlEventParser.java:713) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findEndPositionWithMasterIdAndTimestamp(MysqlEventParser.java:373) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPositionInternal(MysqlEventParser.java:428) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPosition(MysqlEventParser.java:347) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:164) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at java.lang.Thread.run(Thread.java:745) [na:1.8.0_91] Caused by: java.io.IOException: socket read timeout occured !         at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannel.read(SocketChannel.java:78) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.readHeader(PacketManager.java:18) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:150) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:71) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na]         ... 8 common frames omitted 2018-03-01 12:04:43.390 [destination = example   address = /127.0.0.1:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /127.0.0.1:3306 has an error  retrying. caused by  com.alibaba.otter.canal.parse.exception.CanalParseException: can't find start position for example 2018-03-01 12:04:43.398 [destination = example   address = /127.0.0.1:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: can't find start position for example ] 2018-03-01 12:05:05.951 [destination = example   address = /127.0.0.1:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /127.0.0.1:3306 has an error  retrying. caused by  com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: connect /127.0.0.1:3306 failure Caused by: java.io.IOException: connect /127.0.0.1:3306 failure         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:74) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:87) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:160) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]         at java.lang.Thread.run(Thread.java:745) [na:1.8.0_91] Caused by: java.io.IOException: socket read timeout occured !         at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannel.read(SocketChannel.java:78) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.readHeader(PacketManager.java:18) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:150) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:71) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na]         ... 4 common frames omitted 2018-03-01 12:05:05.952 [destination = example   address = /127.0.0.1:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: connect /127.0.0.1:3306 failure Caused by: java.io.IOException: connect /127.0.0.1:3306 failure         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:74)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:72)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:87)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:160)         at java.lang.Thread.run(Thread.java:745) Caused by: java.io.IOException: socket read timeout occured !         at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannel.read(SocketChannel.java:78)         at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.readHeader(PacketManager.java:18)         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:150)         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:71)         ... 4 more ] ``` me too mysql 5.7.19 https://github.com/alibaba/canal/releases/tag/canal-1.0.26-preview-2，使用这个版本的包试试 @snailbing 请问问题解决了没有啊 
541,canal1.0.25 在zookeeper 中不创建 cursor  不更新binlog位置信息 开始使用的是 canal_1.0.24    zk_3.4.10中有cursor 节点，也有binlog位置信息(mysql_5.7.20)， 但是不更新zk中的binlog位置信息。后来升级到canal_1.0.25 还是不更新， 然后手动删除了 zk 下的 /otter 目录， 重新启动canal 结果都不创建 cursor 节点。  配置过程参考 https://github.com/alibaba/canal/wiki/AdminGuide  地址中写的。 1.0.25+版本之后默认启动时，会有一个定位位点的过程，https://github.com/alibaba/canal/releases/tag/canal-1.0.26-preview-2，使用这个版本的包试试
540,group模式下有时候数据库数据变动无法读取 场景:两台机器  机器一 ：部署canal server 和mysql1 机器二：部署 mysql2 instance.properties内容如下 canal.instance.master1.address=127.0.0.1:3306 canal.instance.master2.address=host2:3306 部署了canal server的那台机器数据库变动无法读取到 如果instance.properties修改为 canal.instance.master1.address=host1:3306 canal.instance.master2.address=host2:3306 则可以正确读取  不知道是我配置问题 还是代码bug 基本未环境问题
539,对于PR#536中问题的一个简单些的改动 详情请见issue[#537](https://github.com/alibaba/canal/issues/537) 支持了下netty和bio两种模式，提供canal.socketChannel=bio/netty进行切换，默认选择了bio模式.  之前最早也是使用bio的模式，针对长链接面向高吞吐的场景，bio模式会比较有优势，之前主要是无法解决链接假死的问题 和主干代码冲突比较多了，人肉合并了代码，下次提交PR之前可以合一下master代码
538,rds中对binlog保留多久？ **如题** 今天在重置消费点的时候，出现找不到消费点的情况，但我只是重置到6小时前而已。。。 默认应该是保存18个小时
537,SocketChannel中cache的一些问题 作者你好！ 最近项目也遇到了pr[#487](https://github.com/alibaba/canal/pull/487)中的问题，如果client消费不够快，canal server会将directByteBuf扩容到溢出。 pr[#487](https://github.com/alibaba/canal/pull/487)确实能解决问题，但会引发出pr[#536](https://github.com/alibaba/canal/pull/536)的问题。 pr[#536](https://github.com/alibaba/canal/pull/536) 有点问题： 1. MASTER_HEARTBEAT_PERIOD在使用change master to语句时的单位才是秒，canal不能使用change master to只能用set @MASTER_HEARTBEAT_PERIOD，单位是纳秒，可以测试一下。 2. 在读的时候动态去根据packetSize去调整ByteBuf，逻辑比较复杂。其实可以根据maxThreeBytes(略小于16M)直接设置初始capacity，结合pr[#487](https://github.com/alibaba/canal/pull/487)就可以了。 参见pr[#539](https://github.com/alibaba/canal/pull/539) BTW，dump线程使用netty+nio让这里的代码复杂化了。直接用InputStream，然后设置一个较短的SO_TIMEOUT，比如1秒 1. 对于中断的场景： try {      return in.read(bytes  index  length); } catch (SocketTimeoutException e) {      //每个1s检查线程是否被中断 } 2. 对于需要timeout的场景： try {      return in.read(bytes  index  length); } catch (SocketTimeoutException e) {      //和现在的实现类似，判断accumulatedWaitTime+SO_TIMEOUT是否超过real timeout } 这样可以应对中断场景和闪断阻塞的问题了，实现也会比较简单（感觉dump IO出的问题比较多）。 这只是我的proposal哈。 已经合并了PR代码
536,主要修复 SocketChannel 默认缓存大小（1MB）不够用时需自动扩充，否则将因缓存空间不足而造成I/O超时假象 通常复制包含 CLOB/BLOB 字段类型数据时会遭遇这个问题。1.0.24版开始表现为没有任何反应，也没有任何错误日志。升级到1.0.25版之后即报"socket read timeout occured !"错误，给人误以为网络或者数据库问题（复制是在经过公网SSH转发的两个私网之间）。后经调试跟踪发现实质是固定1MB缓存大小问题。实际运行测试发现一次性readSize达到15MB及以上。截止目前的 1.0.26-SNAPSHOT 编译发布，已稳定运行。 [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=536) <br/>All committers have signed the CLA. [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=536) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=536) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=536) it.</sub> lcybo，谢谢你的指正。 根据你提供的线索，我搜索了下 MySQL jdbc 中 com.mysql.jdbc.MysqlIO 源码，部分摘录如下： `        if (versionMeetsMinimum(4  0  8)) {             this.maxThreeBytes = (256 * 256 * 256) - 1;             this.useNewLargePackets = true;         } else {             this.maxThreeBytes = 255 * 255 * 255;             this.useNewLargePackets = false;         } ` 看起来确实可以简单的设置为： 16MB = 256 * 256 * 256 不过我还是希望代码中做必要的检查和详细日志，不然太烧脑了。 嗯，其实个人觉得dump IO这里没有用netty NIO的必要，割裂了读写线程，引入了不必要的复杂度。 用BIO通过设置较小的SO_TIMEOUT进行捕捉可以规避闪断问题和处理线程中断场景，也更加简洁。 tks
535,canal server启动时对uniquekey报错compare failed ``` 2018-02-25 10:23:39.643 [destination = example   address = rds9h7gi6v2fo2og5202.mysql.rds.aliyuncs.com/100.98.57.68:3306   EventParser] ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - pls submit github issue  show create table ddl:CREATE TABLE `center_institution_admin` (   `id` bigint(20) NOT NULL AUTO_INCREMENT   `account` varchar(20) NOT NULL COMMENT '账号'   `cinst_id` bigint(20) NOT NULL COMMENT '机构id'   `status` int(11) DEFAULT '1' COMMENT '0:关闭 1:开启'   `gmt_create` datetime DEFAULT CURRENT_TIMESTAMP   `gmt_modify` datetime DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP   `name` varchar(50) DEFAULT NULL COMMENT '账号角色名称'   `parent_id` bigint(20) NOT NULL DEFAULT '0' COMMENT '父账号ID'   `permissions` varchar(500) DEFAULT '0' COMMENT '账号权限码'   `remark` varchar(256) DEFAULT NULL   `default_account` int(11) DEFAULT '0'   PRIMARY KEY (`id`)   UNIQUE KEY `inst_account_unique_index` (`cinst_id` `account`)   KEY `parent_id_index` (`parent_id`)   KEY `center_institution_admin_account_index` (`account`) ) ENGINE=InnoDB AUTO_INCREMENT=967573674662035553 DEFAULT CHARSET=utf8 COMMENT='中心机构管理账号表'   compare failed .  db : TableMeta [schema=xiaomai  table=center_institution_admin  fileds=         FieldMeta [columnName=id  columnType=bigint(20)  nullable=false  key=true  defaultValue=null  extra=auto_increment  unique=false]         FieldMeta [columnName=account  columnType=varchar(20)  nullable=false  key=false  defaultValue=null  extra=  unique=false]         FieldMeta [columnName=cinst_id  columnType=bigint(20)  nullable=false  key=false  defaultValue=null  extra=  unique=false]         FieldMeta [columnName=status  columnType=int(11)  nullable=true  key=false  defaultValue=1  extra=  unique=false]         FieldMeta [columnName=gmt_create  columnType=datetime  nullable=true  key=false  defaultValue=CURRENT_TIMESTAMP  extra=  unique=false]         FieldMeta [columnName=gmt_modify  columnType=datetime  nullable=true  key=false  defaultValue=CURRENT_TIMESTAMP  extra=on update CURRENT_TIMESTAMP  unique=false]         FieldMeta [columnName=name  columnType=varchar(50)  nullable=true  key=false  defaultValue=null  extra=  unique=false]         FieldMeta [columnName=parent_id  columnType=bigint(20)  nullable=false  key=false  defaultValue=0  extra=  unique=false]         FieldMeta [columnName=permissions  columnType=varchar(500)  nullable=true  key=false  defaultValue=0  extra=  unique=false]         FieldMeta [columnName=remark  columnType=varchar(256)  nullable=true  key=false  defaultValue=null  extra=  unique=false]         FieldMeta [columnName=default_account  columnType=int(11)  nullable=true  key=false  defaultValue=0  extra=  unique=false] ]  mem : TableMeta [schema=xiaomai  table=center_institution_admin  fileds=         FieldMeta [columnName=id  columnType=bigint(20)  nullable=false  key=true  defaultValue=null  extra=null  unique=false]         FieldMeta [columnName=account  columnType=varchar(20)  nullable=false  key=false  defaultValue=null  extra=null  unique=true]         FieldMeta [columnName=cinst_id  columnType=bigint(20)  nullable=false  key=false  defaultValue=null  extra=null  unique=true]         FieldMeta [columnName=status  columnType=int(11)  nullable=true  key=false  defaultValue=1  extra=null  unique=false]         FieldMeta [columnName=gmt_create  columnType=datetime  nullable=true  key=false  defaultValue=CURRENT_TIMESTAMP  extra=null  unique=false]         FieldMeta [columnName=gmt_modify  columnType=datetime  nullable=true  key=false  defaultValue=CURRENT_TIMESTAMP  extra=null  unique=false]         FieldMeta [columnName=name  columnType=varchar(50)  nullable=true  key=false  defaultValue=null  extra=null  unique=false]         FieldMeta [columnName=parent_id  columnType=bigint(20)  nullable=false  key=false  defaultValue=0  extra=null  unique=false]         FieldMeta [columnName=permissions  columnType=varchar(500)  nullable=true  key=false  defaultValue=0  extra=null  unique=false]         FieldMeta [columnName=remark  columnType=varchar(256)  nullable=true  key=false  defaultValue=null  extra=null  unique=false]         FieldMeta [columnName=default_account  columnType=int(11)  nullable=true  key=false  defaultValue=0  extra=null  unique=false] ] ``` 使用的是目前master分支最新代码，https://github.com/alibaba/canal/issues/507 的问题似乎没有解决. 测试表中UK是一个联合索引（`cinst_id` `account`），因此在单独field上的unique标志应该为false，但mem中的数据对于单独field也为true 通过desc table这样的方式，获取uk时针对多列情况会出现MUL的列，导致无法精确获取对应的uk列。修复的方式使用show create table进行完整解析
534,com.alibaba.otter.canal.parse.exception.CanalParseException: column size is not match for table 运行一段时间后，后台总是报该错误，不知道是什么情况。 做了下ddl变更，导致列不匹配 使用最新的26版本，开启tsdb可以解决 26版本了，开户了tsdb，运行一段时间，还是遇到这问题了
533, findAsPerTimestampInSpecificLogFile has an error ![qq 20180223182635](https://user-images.githubusercontent.com/2507568/36590126-8ceea370-18c8-11e8-9cfb-2175cf9535f9.png) 集群方式，启动一个server成功，启动另一个server报错，如图
532,fix(dbsync): json column of zero length has no value  value parsing s… @see https://github.com/LavaCoref/canal/issues/1 [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=532) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=532) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=532) it.</sub> [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=532) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=532) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=532) it.</sub> tks
531,想问一下如果MemoryEventStoreWithBuffer中bufferMemUnit=16 *1024 会造成程序运行一段时间后卡主的情况吗？ MemoryEventStoreWithBuffer中，bufferSize    = 16 * 1024，bufferMemUnit=16 *1024，maxMemSize = batchSize * bufferMemUnit =  16 * 1024 * 16 *1024 ， 程序在正常运行2个多小时后卡住，会有mysql数据库 kill掉 socket 链接爆出，无其他明显异常 。 请问下是这个原因导致吗？ bufferMemUnit默认值大小为 =1024 server的机器内存多大？ server jvm 内存配了8G，同步的数据库 binlog文件大小一天有80个G左右。 发自网易邮箱大师 在2018年02月22日 09:45，fefine 写道: server的机器内存多大？ — You are receiving this because you authored the thread. Reply to this email directly  view it on GitHub  or mute the thread. 我这边也是出现了这样的问题  但是不确定是什么地方的。 你的bufferMemUnit 配置为多少？ 在2018年02月23日 09:27，fefine 写道: 我这边也是出现了这样的问题  但是不确定是什么地方的。 — You are receiving this because you authored the thread. Reply to this email directly  view it on GitHub  or mute the thread. 默认配置，bufferSize = 16 * 1024，bufferMemUnit= 1024，这两个乘积是16MB，不应该出现内存不够的问题。 你那里卡住 表现为 什么症状？ 你看一下这个issue[Instance假死](https://github.com/alibaba/canal/issues/527) 看着有点像socket超时卡主的问题，可以尝试一下最新的1.1.1版本
530,请问canal不支持rds的mysql5.6版本么？canalV1.0.25启动会没权限异常 2018-02-12 11:38:47.267 [destination = example   address = rm-xxxxx.mysql.rds.aliyuncs.com/47.206.15.124:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: ErrorPacket [errorNumber=1142  fieldCount=-1  message=SHOW command denied to user 'canal_monitor'@'122.177.246.186' for table 'db'  sqlState=42000  sqlStateMarker=#]  with command: show create table `mysql`.`columns_priv`;show create table `mysql`.`db`;show create table `mysql`.`db_view`;show create table `mysql`.`dml_health_check`;show create table `mysql`.`event`;show create table `mysql`.`failover_info`;show create table `mysql`.`func`;show create table `mysql`.`general_log`;show create table `mysql`.`ha_health_check`;show create table `mysql`.`help_category`;show create table `mysql`.`help_keyword`;show create table `mysql`.`help_relation`;show create table `mysql`.`help_topic`;show create table `mysql`.`innodb_index_stats`;show create table `mysql`.`innodb_table_stats`;show create table `mysql`.`ndb_binlog_index`;show create table `mysql`.`plugin`;show create table `mysql`.`proc`;show create table `mysql`.`procs_priv`;show create table `mysql`.`proxies_priv`;show create table `mysql`.`servers`;show create table `mysql`.`slave_master_info`;show create table `mysql`.`slave_relay_log_info`;show create table `mysql`.`slave_worker_info`;show create table `mysql`.`slow_log`;show create table `mysql`.`slow_log_view`;show create table `mysql`.`tables_priv`;show create table `mysql`.`time_zone`;show create table `mysql`.`time_zone_leap_second`;show create table `mysql`.`time_zone_name`;show create table `mysql`.`time_zone_transition`;show create table `mysql`.`time_zone_transition_type`;show create table `mysql`.`user`;show create table `mysql`.`user_view`; Caused by: java.io.IOException: ErrorPacket [errorNumber=1142  fieldCount=-1  message=SHOW command denied to user 'canal_monitor'@'122.177.246.186' for table 'db'  sqlState=42000  sqlStateMarker=#]  with command: show create table `mysql`.`columns_priv`;show create table `mysql`.`db`;show create table `mysql`.`db_view`;show create table `mysql`.`dml_health_check`;show create table `mysql`.`event`;show create table `mysql`.`failover_info`;show create table `mysql`.`func`;show create table `mysql`.`general_log`;show create table `mysql`.`ha_health_check`;show create table `mysql`.`help_category`;show create table `mysql`.`help_keyword`;show create table `mysql`.`help_relation`;show create table `mysql`.`help_topic`;show create table `mysql`.`innodb_index_stats`;show create table `mysql`.`innodb_table_stats`;show create table `mysql`.`ndb_binlog_index`;show create table `mysql`.`plugin`;show create table `mysql`.`proc`;show create table `mysql`.`procs_priv`;show create table `mysql`.`proxies_priv`;show create table `mysql`.`servers`;show create table `mysql`.`slave_master_info`;show create table `mysql`.`slave_relay_log_info`;show create table `mysql`.`slave_worker_info`;show create table `mysql`.`slow_log`;show create table `mysql`.`slow_log_view`;show create table `mysql`.`tables_priv`;show create table `mysql`.`time_zone`;show create table `mysql`.`time_zone_leap_second`;show create table `mysql`.`time_zone_name`;show create table `mysql`.`time_zone_transition`;show create table `mysql`.`time_zone_transition_type`;show create table `mysql`.`user`;show create table `mysql`.`user_view`; 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.queryMulti(MysqlQueryExecutor.java:109) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.queryMulti(MysqlConnection.java:93) 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:173) 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) 	at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) 	at java.lang.Thread.run(Thread.java:748) 1. 你可以指定订阅的表，而不是所有 2. 授权足够大的权限 @agapple 我已经按你说的测试了指定订阅了某个表，结果是一样的错误，阿里云rds mysql5.6版本我提工单客服说不支持SHOW CREATE TABLE `mysql`.`db_view`;这样的权限（我用高权限权账号执行确实一直报没权限），是不是这样导致的呢？ 是的，如果你没有指定表，那就是默认会订阅所有的表，订阅之前会全局拿一次表结构，所以会出现类似的权限报错
529,TimelineTransactionBarrier import CanalSinkException; Maven Compile 的一些 Warning - `TimelineTransactionBarrier.java` 中 `import com.alibaba.otter.canal.sink.exception.CanalSinkException;` - Fix Maven Wranning: The expression ${pom.version} is deprecated. Please use ${project.version} instead. - Fix MavenWranning : 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-jar-plugin is missing. [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=529) <br/>All committers have signed the CLA. [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=529) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=529) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=529) it.</sub> tks
528,Use mysql master heartbeat to detect phycial tcp connection failure. 在测试网络negative case的时候，canal dump的read会一直阻塞。 SocketChannelPool里的SO_KEEPALIVE，默认需要两小时。 可以开启master heartbeat(5.5以后)，让master在空闲时主动给canal发heartbeat event，canal当前会drop这个事件。 经测试，通过set @master_heartbeat_period 的时间单位是纳秒。 DirectLogFetcher调用带有timeout的read，timeout时间设为MASTER_HEARTBEAT_PERIOD+1s，确保超过heartbeat间隔之后再timeout，防止误杀。 tks
527,Instance假死 Server端运行正常，但是Instance全部线程无响应，日志等级开启为Info，并未发现异常日志 这是Server日志： ![http://7xo1fz.com1.z0.glb.clouddn.com/server.png](http://7xo1fz.com1.z0.glb.clouddn.com/server.png) 这是Instance日志： ![http://7xo1fz.com1.z0.glb.clouddn.com/server.png](http://7xo1fz.com1.z0.glb.clouddn.com/instance.png) 我在Instance中也开启了另外一个定时的线程，但是在卡死的时间内毫无响应。数据库在今天下午并无其他特殊操作。 问一下这有可能是什么问题？ 没数据变更？ 有，数据一直在变更 我这是canal server 启动时正常，binlog会被client消费掉。但如果几个小时，一直没有binlong产生，再产生binlog话，server就不会读到这些binlog。 @agapple  如果没有数据变更，隔一段时间instance就不会再消费binlog了吧？ 我测试的结果，为了避免这种情况，需要enable detecting，并使用 insert retl.xdual做为心跳sql. 还有其他的解决方法吗？ 你用的是自己打包的1.0.25版本？ 是的，下载的源码打包 @fefine 使用最新的26  alpha 2代码再试试，对于tcp加了so_timeout机制，可以响应长时间无binlog位点被mysql server主动断开的问题
526,is it possible that canal set with multiple mysql database source I will use canal to.. database sync.. daily  every 10minuites snapshot. but we use many mysql database servers.  I want set mltiple mysql servers as canal's input. example.  canal.instance1.master.address = 192.168.0.1:3306 <-- MYSQL A canal.instance2.master.address = 192.168.0.2:3306 <-- MYSQL B(not a slave of A) How can I set many mysql servers as canal's instance You can use group-instance.xml In the canal.properties set ***canal.instance.global.spring.xml = classpath:spring/group-instance.xml***   then update the conf/example/instance.properties  like: ``` canal.instance.master1.address=192.168.0.1:3306 canal.instance.master1.journal.name= canal.instance.master1.position=4 canal.instance.master1.timestamp= canal.instance.master2.address=192.168.0.2:3306 canal.instance.master2.journal.name=mysql-bin.000394 canal.instance.master2.position=4 canal.instance.master2.timestamp=  ``` and then update the conf/spring/group-instance.xml  like this: ``` <beans> 	<bean id="eventParser" class="com.alibaba.otter.canal.parse.inbound.group.GroupEventParser"> 		<property name="eventParsers"> 			<list> 				<ref bean="eventParser1" /> 				<ref bean="eventParser2" /> 			</list> 		</property> 	</bean> 	<bean id="eventParser1" class="com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser"> 		<!-- database1 info --> 		<property name="masterInfo"> 			<bean class="com.alibaba.otter.canal.parse.support.AuthenticationInfo"> 				<property name="address" value="${canal.instance.master1.address}" /> 				<property name="username" value="${canal.instance.dbUsername:retl}" /> 				<property name="password" value="${canal.instance.dbPassword:retl}" /> 				<property name="defaultDatabaseName" value="${canal.instance.defaultDatabaseName:retl}" /> 			</bean> 		</property> 		<property name="masterPosition"> 			<bean class="com.alibaba.otter.canal.protocol.position.EntryPosition"> 				<property name="journalName" value="${canal.instance.master1.journal.name}" /> 				<property name="position" value="${canal.instance.master1.position}" /> 				<property name="timestamp" value="${canal.instance.master1.timestamp}" /> 			</bean> 		</property> 	</bean> 	<bean id="eventParser2" class="com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser"> 		<!-- database2 info --> 		<property name="masterInfo"> 			<bean class="com.alibaba.otter.canal.parse.support.AuthenticationInfo"> 				<property name="address" value="${canal.instance.master2.address}" /> 				<property name="username" value="${canal.instance.dbUsername:retl}" /> 				<property name="password" value="${canal.instance.dbPassword:retl}" /> 				<property name="defaultDatabaseName" value="${canal.instance.defaultDatabaseName:retl}" /> 			</bean> 		</property> 		<property name="masterPosition"> 			<bean class="com.alibaba.otter.canal.protocol.position.EntryPosition"> 				<property name="journalName" value="${canal.instance.master2.journal.name}" /> 				<property name="position" value="${canal.instance.master2.position}" /> 				<property name="timestamp" value="${canal.instance.master2.timestamp}" /> 			</bean> 		</property> 	</bean> </beans> ``` That's exactly what i find. Now I will do that! Thank for your speedy answer. 
525,mysql新增字段 新增字段后，对该字段的值做修改，消费者得不到这个字段更新的消息 使用最新版的26版本，开启TSDB的能力
524,在开启binlog_rows_query_log_events开关后解析binlog全部都是query类型 现象与 #517 相同  在开启binlog_rows_query_log_events开关后，客户端抓到的dml语句为EventType.QUERY 类型，并且rowChange.getIsDdl()为false。 由于项目中需要针对ddl语句做不同的处理，因此想确认下，ddl语句的判断条件是什么？wiki中的 ![](https://camo.githubusercontent.com/73b451bd00ff40ec80ed3bd4904d4052fd34fa03/687474703a2f2f646c322e69746579652e636f6d2f75706c6f61642f6174746163686d656e742f303039302f363438332f36316366313161622d313932342d333730352d386236662d6534363431316263363036612e6a7067) 判断条件在这种状况下显然有问题。 DDL语句的判断是对create/drop/alter table等的相关操作
523,Merge pull request #1 from alibaba/master merge from master [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=523) <br/>All committers have signed the CLA. [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=523) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=523) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=523) it.</sub>
522,修复死锁bug 当通过zk-cursor启动的时候，第一个Event类型是TransactionEnd，那么txState变为2的触发条件不仅仅有ddl和dcl，clear方法中isTransactionEnd的判断应该放到txState.intValue() == 2的后面，否则将导致死锁 [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=522) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=522) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=522) it.</sub> 感谢，高质量的bug修复 @lulu2panpan 顺便问一下，你们group sink在用，源端的TPS大概多少？因为以前这种设计方式存在大量的线程通知机制，不是最优的性能设计，理想情况的设计应该是做一个merge sort的归并队列，两层store的模式性能会比较ok @agapple 我也遇到了这个问题，一段时间数据没有变更，再有数据变更，server也不会读到。请问这个是在哪个版本里解决的？ @handmail 是用的group sink ？ 性能的确是有点儿问题 有些子库长时间不产生数据，我们是通过心跳sql，强制产生binlog解决的，心跳是3s一次，这种情况下日常延迟时间在5s左右，对于高并发场景，还是有很大瓶颈的 是的，后续可以设计一个GroupXXParser来支持多库聚合，性能上会好很多 恩，这个功能还是很有必要的，这样配一个实例就ok了，大大节省运维成本 后面有时间，一起设计一下吧 @lulu2panpan 想邀请你参加canal的代码共建，可以联系我邮箱：jianghang115@gmail.com
521,修改InstanceConfig未生成或已销毁时造成ServerRunningMonitor running状态与实际不一致的问题 如果instance在server端未启动，或已经stop的情况，此时client端进行订阅，在SessionHandler中，会尝试启动这个instance的ServerRunningMonitor：                         _// 尝试启动，如果已经启动，忽略                         if (!embeddedServer.isStart(clientIdentity.getDestination())) {                             ServerRunningMonitor runningMonitor = ServerRunningMonitors.getRunningMonitor(clientIdentity.getDestination());                             if (!runningMonitor.isStart()) {                                 runningMonitor.start();                             }                         }_ 然后CanalServerWithEmbedded#start(final String destination)中第一行： _final CanalInstance canalInstance = canalInstances.get(destination);_ 会因为找不到InstanceConfig在generate instance时抛出找不到destination的异常： _if (config == null) {         throw new CanalServerException("can't find destination:{}"); }_ ServerRunningMonitor#start()第一行就把状态置为true。正常启动实例时，认为已经启动，就会忽略。 如果启动失败，应该回滚所有已改变的状态。因为start/stop会在scan和netty work线程，所以start/stop加了同步。 感谢，一个容错的考虑
520,canal ha时候 不能保证服务正常平移。 canal server ha 的时候，如果直接kill -9 方式关闭，ha不能很好平移，服务会出现当掉的情况。 如果 用命令正常stop server 则不会出现这个问题。 kill -9需要zookeeper session timeout超时才能做ha迁移
519,canal对库表的perl正则过滤，为何“库名.表名”的全匹配方式，获取不到日志？ ![image](https://user-images.githubusercontent.com/20380664/35789590-2cec3a0e-0a78-11e8-858c-54bf51ba011b.png) mysql数据解析关注的表为可以通过"库名称\\..*"方式精确到具体的库，但使用mysql.test1 mysql.test2 这种方式无法解析对应表的日志？是不是canal文档上写的有出入？？ 是否考虑下面这个点： void subscribe(String filter)                throws com.alibaba.otter.canal.protocol.exception.CanalClientException 客户端订阅，重复订阅时会更新对应的filter信息  说明：  a. 如果本次订阅中filter信息为空，则直接使用canal server服务端配置的filter信息  b. 如果本次订阅中filter信息不为空，目前会直接替换canal server服务端配置的filter信息，以本次提交的为准    TODO: 后续可以考虑，如果本次提交的filter不为空，在执行过滤时，是对canal server filter + 本次filter的交集处理，达到只取1份binlog数据，多个客户端消费不同的表   参数: clientIdentity - 抛出: com.alibaba.otter.canal.protocol.exception.CanalClientException 用mysql\\.test1，mysql\\.test2呢？ 参考FAQ: https://github.com/alibaba/canal/wiki/FAQ
518,初始启动日志要求提Issues ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - pls submit github issue  show create table ddl:CREATE TABLE `REPO_VER_REFS` (   `NODE_ID` varbinary(16) NOT NULL   `REFS_DATA` longblob NOT NULL   UNIQUE KEY `REPO_VER_REFS_IDX` (`NODE_ID`) ) ENGINE=InnoDB DEFAULT CHARSET=latin1   compare failed .   db : TableMeta [schema=urule  table=REPO_VER_REFS  fileds= 	FieldMeta [columnName=NODE_ID  columnType=varbinary(16)  defaultValue=null  nullable=false  key=true] 	FieldMeta [columnName=REFS_DATA  columnType=longblob  defaultValue=null  nullable=false  key=false] ]   mem : TableMeta [schema=urule  table=REPO_VER_REFS  fileds= 	FieldMeta [columnName=NODE_ID  columnType=varbinary(16)  defaultValue=null  nullable=false  key=false] 	FieldMeta [columnName=REFS_DATA  columnType=longblob  defaultValue=null  nullable=false  key=false] ] 类似问题，代码已经修复
517,rowChage.getEventType()获取的类型全部是是QUERY 但是rowChage.getSql()获取sql语句都是INSERT或者UPDATE或者DELETE 如题，在监控mysql一个数据库时，rowChage.getEventType()拿到的所有类型都是QUERY 但是rowChage.getSql()拿到的语句却都是INSERT或者UPDATE或者DELETE，怀疑rowChage.getEventType()没有获取到值，给了一个默认QUERY值 代码中打印的日志的语句：                   logger.info("此次操作类型:"+rowChage.getEventType().toString());                 logger.info("sql------>:"+rowChage.getSql());                 logger.info("是否是ddl----->"+rowChage.getIsDdl()); 控制台打印出来的日志：                [2018-02-01 11:29:02.614][INFO][Thread-6]c.p.mycode.canal.ClusterCanalClient-handleData:此次操作类型:QUERY                 [2018-02-01 11:29:02.615][INFO][Thread-6]c.p.mycode.canal.ClusterCanalClient-handleData:sql------>:DELETE FROM QRTZ_CREDITCARD_FIRED_TRIGGERS WHERE ENTRY_ID = 'cfc-sit-jd-web04.haomoney.local15174069672781517407028267'                 [2018-02-01 11:29:02.615][INFO][Thread-6]c.p.mycode.canal.ClusterCanalClient-handleData:是否是ddl----->false binlog非row模式
516,如何保证连接不断 我想问问，如果canal与主库的连接，死亡或者网络中断，canal有重连的机制吗 有的，看下example client例子
515,canal client的spring版本问题 canal client使用的还是3.2.9.RELEASE版本的spring，如果直接使用spring4的依赖会出现奇怪的异常。需要在项目中exclude掉spring相关的依赖，这么做总觉得会在使用过程中存在一些风险。 建议： 1. 能否跟上spring大版本升级，例：spring3 -> spring4 2. 如果canal使用spring的功能很少的话，能不能将spring的依赖设置为optional的。 reference：#371  另外，我在使用过程中，logback也同样有版本冲突的问题。也同样建议变为optional的依赖。 canal就只用了spring依赖注入 LS正解
514,对数据作出更改时，client端只解析到了事务的开始和结束 对数据作出更改时，client端只解析到了事务的开始和结束，message.getEntries()方法返回的list只有两个元素，一个对应事务开始一个对应事务结束，没有行更改对应的entry 数据被过滤了
513,Canal的Filter支持前缀匹配吗？ 比如schema是actionlog+四位数字，table是actionlog+八位数字，filter能匹配这种吗？正则需要怎么写呢？ 可以写，actionlog\\d{4}\\.actionlog\\d{8}
512,服务端启动一直失败，求助！！ canal.instance.tsdb.enable无论开启还是关闭都出现如下错误： ERROR com.alibaba.druid.pool.DruidDataSource - create connection SQLException  url: jdbc:h2:../conf/example/h2;CACHE_SIZE=1000;MODE=MYSQL;  errorCode 28000  state 28000 org.h2.jdbc.JdbcSQLException: Wrong user name or password [28000-196] 我的instance配置： `################################################# ## mysql serverId canal.instance.mysql.slaveId=1234 # position info canal.instance.master.address=192.168.3.200:3306 canal.instance.master.journal.name=mysql-bin.000001 canal.instance.master.position= canal.instance.master.timestamp= # table meta tsdb info canal.instance.tsdb.enable=false canal.instance.tsdb.dir=${canal.file.data.dir:../conf}/${canal.instance.destination:} canal.instance.tsdb.url=jdbc:h2:${canal.instance.tsdb.dir}/h2;CACHE_SIZE=1001;MODE=MYSQL; #canal.instance.tsdb.url=jdbc:mysql://127.0.0.1:3306/Test #canal.instance.tsdb.dbUsername=canal #canal.instance.tsdb.dbPassword=canal #canal.instance.standby.address = #canal.instance.standby.journal.name = #canal.instance.standby.position =  #canal.instance.standby.timestamp =  # username/password canal.instance.dbUsername=canal canal.instance.dbPassword=canal canal.instance.defaultDatabaseName= canal.instance.connectionCharset=UTF-8 # table regex canal.instance.filter.regex=Test\\..* # table black regex canal.instance.filter.black.regex= ################################################# ` >b. canal的原理是模拟自己为mysql slave，所以这里一定需要做为mysql slave的相关权限. CREATE USER canal IDENTIFIED BY 'canal';   GRANT SELECT  REPLICATION SLAVE  REPLICATION CLIENT ON *.* TO 'canal'@'%'; -- GRANT ALL PRIVILEGES ON *.* TO 'canal'@'%' ; FLUSH PRIVILEGES; https://github.com/alibaba/canal/wiki/QuickStart 是mysql没有设置canal用户名和密码吗? @HeChuanXUPT    你好，确认不是权限问题。 @hsh075623201  如果不使用tsdb的话，可以尝试使用下之前的版本 1.0.19 ， 1.0.19这个我是启动成功了的 遇到同样的问题 @WilliamGai 注释tsdb就可以了 遇到同样的问题 @hsh075623201 请问下，你是把 tsdb的全注释了吗？ 我注释了然后报异常： java.sql.SQLException: connect error  url   driverClass org.h2.Driver h2的问题  有几个反馈了   如果对H2比较了解的可以尝试优化解决 h2作为table ddl的历史版本存储时，会基于H2的默认jdbc配置 ``` canal.instance.tsdb.dir=${canal.file.data.dir:../conf}/${canal.instance.destination:} canal.instance.tsdb.url=jdbc:h2:${canal.instance.tsdb.dir}/h2;CACHE_SIZE=1000;MODE=MYSQL; canal.instance.tsdb.dbUsername=canal canal.instance.tsdb.dbPassword=canal ``` 解读一下，就是第一次时会创建conf/$instance$/$instance.mv.db，并设置访问密码为canal/canal，如果第二次重新打开时会校验xx.mv.db是否有多进程同时使用(会出现java.lang.IllegalStateException: The file is locked)，也会校验本次的访问密码是否和第一次创建时相同 如果真遇到一些莫名其妙的问题，万能的解决办法：删除conf/对应的xx.mv.db，会重新初始化一个h2本地文件
511,能结合kafka使用吗？ 你好：    能结合kafka使用吗？直接把数据写入到kafka，还是需要自己写消费者？ 目前自己写 线程的用maxwell  debezium   建议修改otter的loader 支持下其他数据源。otter管理好了canal
510,canal服务启动后连接mysql 报 java.io.IOException: socket read timeout occured ! mysql服务已启动，用户权限也分配了，启动canal后会报几分钟的连不上mysql的错误，但是过个几分钟后自己又连上了，这个问题已经困扰我几天了。 `java.io.IOException: connect /10.11.30.3:3306 failure 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:74) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:162) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131] Caused by: java.io.IOException: socket read timeout occured ! 	at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannel.read(SocketChannel.java:78) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.readHeader(PacketManager.java:18) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:150) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:71) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	... 3 common frames omitted 2018-01-26 09:35:09.559 [destination = example   address = /10.11.30.3:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: connect /10.11.30.3:3306 failure 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:74) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:72) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:162) 	at java.lang.Thread.run(Thread.java:748) Caused by: java.io.IOException: socket read timeout occured ! 	at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannel.read(SocketChannel.java:78) 	at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.readHeader(PacketManager.java:18) 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:150) 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:71) 	... 3 more ] 2018-01-26 09:35:27.296 [destination = example   address = /10.11.30.3:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /10.11.30.3:3306 has an error  retrying. caused by  com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: connect /10.11.30.3:3306 failure Caused by: java.io.IOException: connect /10.11.30.3:3306 failure 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:74) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:87) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:160) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131] Caused by: java.io.IOException: socket read timeout occured ! 	at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannel.read(SocketChannel.java:78) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.readHeader(PacketManager.java:18) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:150) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:71) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	... 4 common frames omitted 2018-01-26 09:35:27.298 [destination = example   address = /10.11.30.3:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: connect /10.11.30.3:3306 failure Caused by: java.io.IOException: connect /10.11.30.3:3306 failure 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:74) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:72) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:87) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:160) 	at java.lang.Thread.run(Thread.java:748) Caused by: java.io.IOException: socket read timeout occured ! 	at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannel.read(SocketChannel.java:78) 	at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.readHeader(PacketManager.java:18) 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:150) 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:71) 	... 4 more ] 2018-01-26 09:35:47.022 [destination = example   address = /10.11.30.3:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position  {"identity":{"slaveId":-1 "sourceAddress":{"address":"10.11.30.3" "port":3306}} "postion":{"included":false "journalName":"mysql-log-bin.000005" "position":4353 "serverId":999 "timestamp":1516859980000}}` 我也是这个问题，不知道怎么回事mysql5.7.13 看来官方的说明这个版本的mysql应该是没有问题不知道怎么出现这个问题 2018-01-31 17:04:22.134 [destination = yfmalldb   address = /127.0.0.1:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : GRANT SELECT  REPLICATION SLAVE  REPLICATION CLIENT ON *.* TO 'canal'@'127.0.0.1' IDENTIFIED WITH 'mysql_native_password' AS '*E3619321C1A937C46A0D8BD1DAC39F93B27D4458' com.alibaba.druid.sql.parser.ParserException: syntax error  error in :'FIED WITH 'mysql_native_password' A'  expect BY  actual WITH pos 97  line 1  column 93  token WITH 	at com.alibaba.druid.sql.parser.SQLParser.printError(SQLParser.java:283) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] 	at com.alibaba.druid.sql.parser.SQLParser.accept(SQLParser.java:292) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] 	at com.alibaba.druid.sql.parser.SQLStatementParser.parseGrant(SQLStatementParser.java:738) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] 	at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:207) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] 	at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] 	at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] 	at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.applyHistoryOnMemory(DatabaseTableMeta.java:388) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:121) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_111] 2018-01-31 17:04:22.134 [destination = yfmalldb   address = /127.0.0.1:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : GRANT SELECT  REPLICATION SLAVE  REPLICATION CLIENT ON *.* TO 'canal'@'localhost' IDENTIFIED WITH 'mysql_native_password' AS '*E3619321C1A937C46A0D8BD1DAC39F93B27D4458' com.alibaba.druid.sql.parser.ParserException: syntax error  error in :'FIED WITH 'mysql_native_password' A'  expect BY  actual WITH pos 97  line 1  column 93  token WITH 	at com.alibaba.druid.sql.parser.SQLParser.printError(SQLParser.java:283) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] 	at com.alibaba.druid.sql.parser.SQLParser.accept(SQLParser.java:292) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] 	at com.alibaba.druid.sql.parser.SQLStatementParser.parseGrant(SQLStatementParser.java:738) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] 	at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:207) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] 	at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] 	at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] 	at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.applyHistoryOnMemory(DatabaseTableMeta.java:388) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:121) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_111] 看着好像是授权问题，但是权限按照说明处理过了，郁闷中 Caused by: java.io.IOException: socket read timeout occured ! at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannel.read(SocketChannel.java:78) 意思是说你源库超过100秒都没有新的数据，会自动重连
509,canal client拉取binlog，server端报错java.nio.channels.ClosedByInterruptException: null ------------------------------------------------------------------------------------------- 2018-01-26 14:04:56.245  INFO 121070 --- [       Thread-4] com.gome.canalmq.service.MonitorService  : lmis speed:2461.1record/sec  can al speed:6273.7record/sec 2018-01-26 14:05:06.245  INFO 121070 --- [       Thread-4] com.gome.canalmq.service.MonitorService  : lmis speed:3666.6record/sec  can al speed:8312.6record/sec 2018-01-26 14:05:16.246  INFO 121070 --- [       Thread-4] com.gome.canalmq.service.MonitorService  : lmis speed:4077.8record/sec  can al speed:12065.4record/sec 2018-01-26 14:05:26.246  INFO 121070 --- [       Thread-4] com.gome.canalmq.service.MonitorService  : lmis speed:2859.6record/sec  can al speed:6839.5record/sec 26 2018-01-26 14:05:36.246  INFO 121070 --- [       Thread-4] com.gome.canalmq.service.MonitorService  : lmis speed:329.2record/sec  cana l speed:920.5record/sec 27 2018-01-26 14:05:46.246  INFO 121070 --- [       Thread-4] com.gome.canalmq.service.MonitorService  : lmis speed:149.3record/sec  cana l speed:419.7record/sec 2018-01-26 14:05:56.247  INFO 121070 --- [       Thread-4] com.gome.canalmq.service.MonitorService  : lmis speed:76.4record/sec  canal  speed:212.2record/sec 2018-01-26 14:06:06.247  INFO 121070 --- [       Thread-4] com.gome.canalmq.service.MonitorService  : lmis speed:23.6record/sec  canal  speed:68.9record/sec 2018-01-26 14:06:16.248  INFO 121070 --- [       Thread-4] com.gome.canalmq.service.MonitorService  : lmis speed:58.0record/sec  canal  speed:165.8record/sec 2018-01-26 14:06:26.248  INFO 121070 --- [       Thread-4] com.gome.canalmq.service.MonitorService  : lmis speed:38.7record/sec  canal  speed:108.9record/sec 2018-01-26 14:06:36.248  INFO 121070 --- [       Thread-4] com.gome.canalmq.service.MonitorService  : lmis speed:29.9record/sec  canal  speed:86.6record/sec ------------------------------------------------------------------------------------------- 2018-01-26 14:05:22.693 [destination = lmis65   address = /10.128.35.65:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address 10.128.35.65/10.128.35.65:3306 has an error  retrying. caused by  java.nio.channels.ClosedByInterruptException: null         at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannel.read(SocketChannel.java:49) ~[canal.parse.driver-1.0.25.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:151) ~[canal.parse-1.0.25.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:77) ~[canal.parse-1.0.25.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:137) ~[canal.parse-1.0.25.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) ~[canal.parse-1.0.25.jar:na]         at java.lang.Thread.run(Thread.java:745) [na:1.8.0_91] 2018-01-26 14:05:22.697 [destination = lmis65   address = /10.128.35.65:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:lmis65[java.nio.channels.ClosedByInterruptException         at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannel.read(SocketChannel.java:49)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:151)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:77)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:137)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220)         at java.lang.Thread.run(Thread.java:745) ] 2018-01-26 14:05:41.309 [destination = lmis65   address = /10.128.35.65:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position  {"identity":{"slaveId":-1 "sourceAddress":{"address":"10.128.35.65" "port":3306}} "postion":{"included":false "journalName":"mysql-bin.003759" "position":891656455 "serverId":1 "timestamp":1516929456000}} 环境 canal版本1.0.25 ，zk管理，centos7.2 ，mysql版本5.7.11 canal 1.0.25和1.0.26很慢，我换到老版本1.0.24就没问题了 我也是降到1.0.24就没问题了。在1.0.25下就会报错 ``` 2018-01-25 19:36:37.799 [destination = databus   address = /127.0.0.1:43306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - parse events has an error com.alibaba.otter.canal.parse.exception.CanalParseException: dump address /127.0.0.1:43306 has an error  retrying. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: connect /127.0.0.1:43306 failure Caused by: java.io.IOException: connect /127.0.0.1:43306 failure         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:72) ~[canal.parse.driver-1.0.25.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:71) ~[canal.parse-1.0.25.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:88) ~[canal.parse-1.0.25.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:160) ~[canal.parse-1.0.25.jar:na]         at java.lang.Thread.run(Thread.java:745) ~[na:1.7.0_79] Caused by: java.nio.channels.ClosedByInterruptException: null         at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannel.read(SocketChannel.java:55) ~[canal.parse.driver-1.0.25.jar:na]         at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.readHeader(PacketManager.java:12) ~[canal.parse.driver-1.0.25.jar:na]         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:148) ~[canal.parse.driver-1.0.25.jar:na]         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:69) ~[canal.parse.driver-1.0.25.jar:na]         ... 4 common frames omitted ``` 建议升级到26版本，解决了超时时间过短以及链接mysql切回bio的模式
508,canal client 
507,desc tableName 与 show create table tableName 对比不一致 CREATE TABLE `IM_GROUP_USER` (   `GROUPID` int(11) NOT NULL   `USERID` int(11) NOT NULL   `MANA` int(11) DEFAULT NULL   `NOTE` varchar(200) COLLATE gbk_bin DEFAULT NULL   UNIQUE KEY `IM_GROUP_USER_INDEX` (`GROUPID` `USERID`) ) ENGINE=InnoDB DEFAULT CHARSET=gbk COLLATE=gbk_bin   compare failed .   db : TableMeta [schema=imserver  table=IM_GROUP_USER  fileds= 	FieldMeta [columnName=GROUPID  columnType=int(11)  defaultValue=null  nullable=false  key=true] 	FieldMeta [columnName=USERID  columnType=int(11)  defaultValue=null  nullable=false  key=true] 	FieldMeta [columnName=MANA  columnType=int(11)  defaultValue=null  nullable=true  key=false] 	FieldMeta [columnName=NOTE  columnType=varchar(200)  defaultValue=null  nullable=true  key=false] ]   mem : TableMeta [schema=imserver  table=IM_GROUP_USER  fileds= 	FieldMeta [columnName=GROUPID  columnType=int(11)  defaultValue=null  nullable=false  key=false] 	FieldMeta [columnName=USERID  columnType=int(11)  defaultValue=null  nullable=false  key=false] 	FieldMeta [columnName=MANA  columnType=int(11)  defaultValue=null  nullable=true  key=false] 	FieldMeta [columnName=NOTE  columnType=varchar(200)  defaultValue=null  nullable=true  key=false] ] 你这mysql是啥版本来着？看上去UNIQUE KEY IM_GROUP_USER_INDEX (GROUPID USERID)，被当做了pk，发我一下desc imserver.IM_GROUP_USER 和  show create table imserver.IM_GROUP_USER 的结果 我本地重现了
506,mvn 运行 canal.example工程 jar 包丢失 在使用mvn exec:java -Dexec.mainClass="com.alibaba.otter.canal.example.SimpleCanalClientTest"运行canal.example工程时，提示俩个jar包丢失。 [WARNING] The POM for com.alibaba.otter:canal.client:jar:1.0.26-SNAPSHOT is missing  no dependency information available [WARNING] The POM for com.alibaba.otter:canal.protocol:jar:1.0.26-SNAPSHOT is missing  no dependency information available mvn clean install eclipse:eclipse -Dmaven.test.skip
505,本机运行正常，服务器报数据库连接失败 请问在本机用源码执行CanalLauncher类，group-instance.xml方式运行正常，放在centos服务器上运行就会报以下错误，配置文件完全一样。然后服务器上用instance.properties方式运行连接正常。请问怎么修复 2018-01-25 13:35:30.225 [destination = group_mysql   address = /10.138.225.198:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - parse events has an error com.alibaba.otter.canal.parse.exception.CanalParseException: dump address /10.138.225.198:3306 has an error  retrying.  Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: connect /10.138.225.198:3306 failure Caused by: java.io.IOException: connect /10.138.225.198:3306 failure 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:72) ~[canal.parse.driver-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:71) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:88) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:160) ~[canal.parse-1.0.25.jar:na] 	at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_102] Caused by: java.lang.InterruptedException: null 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1302) ~[na:1.8.0_102] 	at com.alibaba.otter.canal.common.utils.BooleanMutex$Sync.innerGet(BooleanMutex.java:123) ~[canal.common-1.0.25.jar:na] 	at com.alibaba.otter.canal.common.utils.BooleanMutex.get(BooleanMutex.java:53) ~[canal.common-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannelPool.open(SocketChannelPool.java:71) ~[canal.parse.driver-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:67) ~[canal.parse.driver-1.0.25.jar:na] 	... 4 common frames omitted 直接把源码放上去跑报的错，debug日志 2018-01-26 15:44:43.706 [destination = group_mysql   address = /10.138.225.198:3306   EventParser] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - the channel /10.138.225.198:3306 is not connected 2018-01-26 15:44:43.708 [destination = group_mysql   address = /10.138.225.198:3306   EventParser] ERROR c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - java.lang.InterruptedException: AbstractBootstrap$PendingRegistrationPromise@6d9c2400(incomplete) 	at io.netty.util.concurrent.DefaultPromise.await(DefaultPromise.java:230) 	at io.netty.channel.DefaultChannelPromise.await(DefaultChannelPromise.java:129) 	at io.netty.channel.DefaultChannelPromise.await(DefaultChannelPromise.java:28) 	at io.netty.util.concurrent.DefaultPromise.sync(DefaultPromise.java:340) 	at io.netty.channel.DefaultChannelPromise.sync(DefaultChannelPromise.java:117) 	at io.netty.channel.DefaultChannelPromise.sync(DefaultChannelPromise.java:28) 	at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannelPool.open(SocketChannelPool.java:60) 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:70) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:74) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:87) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:160) 	at java.lang.Thread.run(Thread.java:745) 2018-01-26 15:44:43.710 [destination = group_mysql   address = /10.138.225.198:3306   EventParser] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - the channel /10.138.225.198:3306 is not connected 2018-01-26 15:44:43.710 [destination = group_mysql   address = /10.138.225.198:3306   EventParser] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - the channel /10.138.225.198:3306 is not connected 2018-01-26 15:44:43.713 [destination = group_mysql   address = /10.138.225.198:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - parse events has an error com.alibaba.otter.canal.parse.exception.CanalParseException: dump address /10.138.225.198:3306 has an error  retrying.  Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: connect /10.138.225.198:3306 failure Caused by: java.io.IOException: connect /10.138.225.198:3306 failure 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:76) ~[class/:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:74) ~[class/:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:87) ~[class/:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:160) ~[class/:na] 	at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_102] Caused by: java.lang.InterruptedException: AbstractBootstrap$PendingRegistrationPromise@6d9c2400(incomplete) 	at io.netty.util.concurrent.DefaultPromise.await(DefaultPromise.java:230) ~[netty-all-4.1.6.Final.jar:4.1.6.Final] 	at io.netty.channel.DefaultChannelPromise.await(DefaultChannelPromise.java:129) ~[netty-all-4.1.6.Final.jar:4.1.6.Final] 	at io.netty.channel.DefaultChannelPromise.await(DefaultChannelPromise.java:28) ~[netty-all-4.1.6.Final.jar:4.1.6.Final] 	at io.netty.util.concurrent.DefaultPromise.sync(DefaultPromise.java:340) ~[netty-all-4.1.6.Final.jar:4.1.6.Final] 	at io.netty.channel.DefaultChannelPromise.sync(DefaultChannelPromise.java:117) ~[netty-all-4.1.6.Final.jar:4.1.6.Final] 	at io.netty.channel.DefaultChannelPromise.sync(DefaultChannelPromise.java:28) ~[netty-all-4.1.6.Final.jar:4.1.6.Final] 	at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannelPool.open(SocketChannelPool.java:60) ~[class/:na] 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:70) ~[class/:na] 	... 4 common frames omitted 2018-01-26 15:44:43.713 [canal-instance-scan-0] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - disConnect MysqlConnection to /10.138.225.197:3306... 2018-01-26 15:44:43.713 [canal-instance-scan-0] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - the channel /10.138.225.197:3306 is not connected 2018-01-26 15:44:43.713 [destination = group_mysql   address = /10.138.225.197:3306   EventParser] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - the channel /10.138.225.197:3306 is not connected 2018-01-26 15:44:43.713 [destination = group_mysql   address = /10.138.225.197:3306   EventParser] ERROR c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - java.lang.InterruptedException 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:998) 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304) 	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:231) 	at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannelPool.open(SocketChannelPool.java:63) 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:70) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:74) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:87) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:160) 	at java.lang.Thread.run(Thread.java:745) 发现用startup.sh启动就会有这个问题，自己写个脚本启动就正常启动了，继续找原因 找到了，启动参数里的-DappName=otter-canal去掉就好了。。。奇怪为啥这个参数会有影响
504, ERROR ## findAsPerTimestampInSpecificLogFile has an error ![_qyd8 tx sr 6qc8 gb74g](https://user-images.githubusercontent.com/35767127/35331901-f8093d74-0143-11e8-9e2f-89528c232bc2.png) 配置 canal.instance.master.journal.name= canal.instance.master.position= 也有相同错误， ![kqa al2uno vd1 0u_3h](https://user-images.githubusercontent.com/35767127/35332342-a2deabb6-0145-11e8-8d54-ac1735efdce7.png) 新版本已经修复
503,compile failed [ERROR] Failed to execute goal on project canal.parse: Could not resolve dependencies for project com.alibaba.otter:canal.parse:jar:1.0.26-SNAPSHOT: Failed to collect dependencies at com.alibaba:druid:jar:1.1.7-preview_0: Failed to read artifact descriptor for com.alibaba:druid:jar:1.1.7-preview_0: Could not transfer artifact com.alibaba:druid:pom:1.1.7-preview_0 from/to alibaba (http://code.alibabatech.com/mvn/releases/): Connect to code.alibabatech.com:80 [code.alibabatech.com/119.38.217.15] failed: Connection refused (Connection refused) -> [Help 1] druid切换到1.1.5好了。
502,Merge pull request #1 from alibaba/master . [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=502) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=502) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=502) it.</sub> [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=502) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=502) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=502) it.</sub>
501,group模式cursor问题 1.分库情况下总共10个库，配置了group-instance.xml。 2.有一个客户端来消费这些数据。 3.消费完了之后，服务端写cursor，在这个节点。/otter/canal/destinations/example/1001/cursor。格式化如下：{"@type":"com.alibaba.otter.canal.protocol.position.LogPosition" "identity":{"slaveId":-1 "sourceAddress":{"address":"10.20.144.15" "port":3306}} "postion":{"included":false "journalName":"mysql-bin.002253" "position":2574756 "timestamp":1363688722000}} 4.请问我总共配置了10个库，分别都消费了，这个在zk是怎么存储的呢?目前在zk中的cursor节点貌似只存储了一个吧 只存储时间戳
500,服务器断电后，重启数据库和canal服务，一直报Client requested master to start replication from position > file size 重启canalServer一直报这个错误Client requested master to start replication from position > file size，应该是数据不同步造成的吧，这种情况该怎么解决 当instance.xml中的canal.instance.master.position或者meta.dat中的position大于实际binlog的size的时候会报这个错。 LS正解
499,运行一段时间，server线程挂起，报错如下，请问如何解决  ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - scheudle applySnapshotToDB faield com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: should execute connector.connect() first Caused by: java.io.IOException: should execute connector.connect() first         at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.<init>(MysqlQueryExecutor.java:30) ~[canal.parse.driver-1.0.25.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:87) ~[canal.parse-1.0.25.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.compareTableMetaDbAndMemory(DatabaseTableMeta.java:294) ~[canal.parse-1.0.25.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.applySnapshotToDB(DatabaseTableMeta.java:251) ~[canal.parse-1.0.25.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.access$100(DatabaseTableMeta.java:45) ~[canal.parse-1.0.25.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta$2.run(DatabaseTableMeta.java:84) ~[canal.parse-1.0.25.jar:na]         at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_144]         at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) [na:1.8.0_144]         at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) [na:1.8.0_144]         at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) [na:1.8.0_144]         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_144]         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_144] 升级一下新版本
498,ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel 2018-01-19 17:31:34.593 [New I/O server worker #1-1] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x120de79e  /127.0.0.1:51432 :> /127.0.0.1:11111]  exception=java.nio.channels.ClosedChannelException 	at org.jboss.netty.channel.socket.nio.NioWorker.cleanUpWriteBuffer(NioWorker.java:649) 	at org.jboss.netty.channel.socket.nio.NioWorker.writeFromUserCode(NioWorker.java:370) 	at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:137) 	at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) 	at org.jboss.netty.channel.Channels.write(Channels.java:611) 	at org.jboss.netty.channel.Channels.write(Channels.java:578) 	at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:28) 	at com.alibaba.otter.canal.server.netty.NettyUtils.ack(NettyUtils.java:35) 	at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:80) 	at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) 	at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.cleanup(ReplayingDecoder.java:542) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.channelDisconnected(ReplayingDecoder.java:450) 	at org.jboss.netty.channel.Channels.fireChannelDisconnected(Channels.java:360) 	at org.jboss.netty.channel.socket.nio.NioWorker.close(NioWorker.java:599) 	at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:119) 	at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) 	at org.jboss.netty.channel.Channels.close(Channels.java:720) 	at org.jboss.netty.channel.AbstractChannel.close(AbstractChannel.java:208) 	at org.jboss.netty.channel.ChannelFutureListener$1.operationComplete(ChannelFutureListener.java:46) 	at org.jboss.netty.channel.DefaultChannelFuture.notifyListener(DefaultChannelFuture.java:381) 	at org.jboss.netty.channel.DefaultChannelFuture.addListener(DefaultChannelFuture.java:148) 	at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:30) 	at com.alibaba.otter.canal.server.netty.NettyUtils.error(NettyUtils.java:51) 	at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:200) 	at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) 	at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:444) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261) 	at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:350) 	at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) 	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) 	at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) 	at java.lang.Thread.run(Thread.java:748) 这谁知道这是什么错误码？ ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400   Caused by :  something goes wrong with channel:[id: 0x27361109  /172.50.3.52:37230 => /172.50.3.52:11111]  exception=com.alibaba.otter.canal.server.exception.CanalServerException: ack error   clientId:1001 batchId:6135 is not exist   please check 2018-02-08 13:36:55.521 [New I/O server worker #1-7] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x27361109  /172.50.3.52:37230 :> /172.50.3.52:11111]  exception=java.nio.channels.ClosedChannelException 	at org.jboss.netty.channel.socket.nio.NioWorker.cleanUpWriteBuffer(NioWorker.java:649) 	at org.jboss.netty.channel.socket.nio.NioWorker.writeFromUserCode(NioWorker.java:370) 	at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:137) 	at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) 	at org.jboss.netty.channel.Channels.write(Channels.java:611) 	at org.jboss.netty.channel.Channels.write(Channels.java:578) 	at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:28) 	at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:144) 	at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) 	at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.cleanup(ReplayingDecoder.java:542) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.channelDisconnected(ReplayingDecoder.java:450) 	at org.jboss.netty.channel.Channels.fireChannelDisconnected(Channels.java:360) 	at org.jboss.netty.channel.socket.nio.NioWorker.close(NioWorker.java:599) 	at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:119) 	at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) 	at org.jboss.netty.channel.Channels.close(Channels.java:720) 	at org.jboss.netty.channel.AbstractChannel.close(AbstractChannel.java:208) 	at org.jboss.netty.channel.ChannelFutureListener$1.operationComplete(ChannelFutureListener.java:46) 	at org.jboss.netty.channel.DefaultChannelFuture.notifyListener(DefaultChannelFuture.java:381) 	at org.jboss.netty.channel.DefaultChannelFuture.addListener(DefaultChannelFuture.java:148) 	at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:30) 	at com.alibaba.otter.canal.server.netty.NettyUtils.error(NettyUtils.java:51) 	at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:200) 	at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) 	at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:444) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261) 	at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:350) 	at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) 	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) 	at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) 	at java.lang.Thread.run(Thread.java:748) ack 不能确认在往客户端写的时候，batchId不存在。batchsize=1000 @FataliBud 这个问题解决了吗？客户端并没有重复进行ack，但是确实会报这个错误。 这个问题有人解决吗？ 这个我觉得是客户端连接超时导致的，我延长的客户端的连接时间，在SIMPLECONNECTOR中。 请问这个问题有人解决了吗....... ack error   clientId:1001 batchId:6135 is not exist   please check。 服务端做了批次回滚吧 @agapple 能不能给个解决方案，或者，这个是bug吧，什么时候能更新下啊，谢谢！ 参考client的demo，做一下rollback再重新订阅一下 https://github.com/alibaba/canal/issues/640，关注一下这个
497,通信模块log小优化 以debug为例，format参数输出有以下几个重载： public void debug(String format  Object arg); public void debug(String format  Object arg1  Object arg2); public void debug(String format  Object... arguments); 有如下注释： This form avoids superfluous string concatenation when the logger is disabled for the DEBUG level. However  this variant incurs the hidden (and relatively small) cost of creating an Object[] before invoking the method  even if this logger is disabled for DEBUG. **The variants taking one and two arguments exist solely in order to avoid this hidden cost.** get和ack的调用比较频繁，即使日志级别较高也会有创建数组的开销，会带来一些性能影响。所以对于参数<=2，用重载的函数。参数>2用logger.isInfoEnabled()。 tks，精益求精
496,Merge pull request #1 from alibaba/master merge from master [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=496) <br/>All committers have signed the CLA. [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=496) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=496) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=496) it.</sub>
495,按example下的例子写canal client订阅binlog数据，无法取到数据 按example下的例子写canal client订阅binlog数据，启动后，idea的控制台会不断输出DEBUG org.apache.zookeeper.ClientCnxn - Got ping response for sessionid这样的日志，在mysql>下执行DDL或DML操作无法输出binglog相关日志数据 先好好看下wiki，应该是有地方配置不正确导致。 @olin017 我也遇到了一样的问题， 我用的版本是 1.0.25 mysql版本是 mysql  Ver 14.14 Distrib 5.7.21  for Linux (x86_64) using  EditLine wrapper 试过各种方法，始终无法获取到binlog变动。 请问你的问题解决了吗？可以说一下解决方法吗？ 换成1.0.20，一样的配置，问题解决。为了进一步确定问题，再换成1.0.25，依旧无法获取任何db change。 具体原因不知道 ，建议不要用太新的版本。 我想着有人也应该遇到了这个问题，刚好在这里找到了，1.0.25版本使用各种方法扔无法获取binlog更新，按楼上说的换成1.0.20版本解决问题，谢了小伙伴们。不清楚高版本为什么无法获取变动。 问题解决了吗？是否和binlog文件的名称格式有关？ 和canal版本有关系 | | 虫虫同学 邮箱：aiyabeetle@163.com | 签名由 网易邮箱大师 定制 在2018年06月13日 09:37，fanpeng 写道： 问题解决了吗？是否和binlog文件的名称格式有关？ — You are receiving this because you commented. Reply to this email directly  view it on GitHub  or mute the thread. 建议换成1.0.26
494,启动正常，更新数据后报错 下载v1.0.26 alpha 1版本，根据网站上的说明修改配置文件；创建数据库：test; 启动程序，在test数据库里的test表插入一条数据，报错如下，错误信息里面显示的其他库里面的表。为什么我更新test库，会报其他库的错误？ [root@localhost canal]# tail -f logs/canal/canal.log  2018-01-17 23:05:53.957 [main] INFO  com.alibaba.otter.canal.deployer.CanalLauncher - ## set default uncaught exception handler 2018-01-17 23:05:54.022 [main] INFO  com.alibaba.otter.canal.deployer.CanalLauncher - ## load canal configurations 2018-01-17 23:05:54.023 [main] INFO  com.alibaba.otter.canal.deployer.CanalLauncher - ## start the canal server. 2018-01-17 23:05:54.069 [main] INFO  com.alibaba.otter.canal.deployer.CanalController - ## start the canal server[192.168.122.1:11111] 2018-01-17 23:05:54.591 [main] WARN  o.s.beans.GenericTypeAwarePropertyDescriptor - Invalid JavaBean property 'connectionCharset' being accessed! Ambiguous write methods found next to actually used [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.lang.String)]: [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.nio.charset.Charset)] 2018-01-17 23:05:54.847 [main] ERROR com.alibaba.druid.pool.DruidDataSource - testWhileIdle is true  validationQuery not set 2018-01-17 23:05:55.127 [main] INFO  com.alibaba.otter.canal.deployer.CanalLauncher - ## the canal server is running now ...... 2018-01-17 23:05:55.968 [destination = example   address = /127.0.0.1:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just show master status 2018-01-17 23:06:09.929 [destination = example   address = /127.0.0.1:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `columns_priv` (   `Host` char(60) COLLATE utf8_bin NOT NULL DEFAULT ''   `Db` char(64) COLLATE utf8_bin NOT NULL DEFAULT ''   `User` char(16) COLLATE utf8_bin NOT NULL DEFAULT ''   `Table_name` char(64) COLLATE utf8_bin NOT NULL DEFAULT ''   `Column_name` char(64) COLLATE utf8_bin NOT NULL DEFAULT ''   `Timestamp` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP   `Column_priv` set('Select' 'Insert' 'Update' 'References') CHARACTER SET utf8 NOT NULL DEFAULT ''   PRIMARY KEY (`Host` `Db` `User` `Table_name` `Column_name`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='Column privileges' com.alibaba.druid.sql.parser.ParserException: syntax error  error in :'es') CHARACTER SET utf8 NOT NULL DE'  expect RPAREN  actual IDENTIFIER pos 479  line 8  column 62  token IDENTIFIER CHARACTER 	at com.alibaba.druid.sql.parser.SQLParser.printError(SQLParser.java:283) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] 	at com.alibaba.druid.sql.parser.SQLParser.accept(SQLParser.java:292) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] 	at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:191) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] 	at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] 	at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] 	at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] 	at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] 	at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.26-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.7.0_79] 2018-01-17 23:06:09.930 [destination = example   address = /127.0.0.1:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `db` (   `Host` char(60) COLLATE utf8_bin NOT NULL DEFAULT ''   `Db` char(64) COLLATE utf8_bin NOT NULL DEFAULT ''   `User` char(16) COLLATE utf8_bin NOT NULL DEFAULT ''   `Select_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Insert_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Update_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Delete_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Create_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Drop_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Grant_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `References_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Index_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Alter_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Create_tmp_table_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Lock_tables_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Create_view_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Show_view_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Create_routine_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Alter_routine_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Execute_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Event_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   `Trigger_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N'   PRIMARY KEY (`Host` `Db` `User`)   KEY `User` (`User`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='Database privileges' com.alibaba.druid.sql.parser.ParserException: syntax error  error in :''Y') CHARACTER SET utf8 NOT NULL DE'  expect RPAREN  actual IDENTIFIER pos 225  line 5  column 31  token IDENTIFIER CHARACTER 	at com.alibaba.druid.sql.parser.SQLParser.printError(SQLParser.java:283) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] 	at com.alibaba.druid.sql.parser.SQLParser.accept(SQLParser.java:292) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] 	at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:191) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] 	at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] 	at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] 	at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] 	at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] 一模一样的错 怎么解决? @kaikewang 这个问题的引起原因是创建表的SQL语句有“)"造成，比如枚举类型的字段，尽量避免，还有系统数据库中的有些字段是枚举的，也会报错，只要不监听这些库的变化就可以解决。 可以把实例下面的匹配所有库，修改为只匹配特定的库： ``` # table regex canal.instance.filter.regex=test\\.* ``` 依赖的druid版本没更新吧？或者将tsdb的特性关闭也可以。 druid升级到1.1.8了，可以解决set类型的问题 @wingerx 如何关闭 tsdb特性？ 遇到了同样的问题。 把conf/canal.properties这个文件里，下面一行注释掉，重启就好了。 #canal.instance.tsdb.spring.xml=classpath:spring/tsdb/h2-tsdb.xml 升级druid版本已经接近
493,1.0.25 版本设置超时报错 #492  我再这个bug中想解决读取卡主的问题，解决方案是说在1.0.25中使用新的机制参考 #297  但是我发现我设置超时会报错，不设置超时反而会正确 ![image](https://user-images.githubusercontent.com/11556152/34980458-936b5baa-fadf-11e7-8497-7a00673f6810.png) ![image](https://user-images.githubusercontent.com/11556152/34981797-95c6a5f4-fae3-11e7-851f-4ec4359c51f1.png) 你这个超时和297的超时是两回事 @agapple   谢谢百忙之中回复啊， 在第一个问题里面建议是设置超时时间的，但是在1.0.25 我只要已设置 超时时间就报socketTimoutException  但是在1.0.24 版本不会有这个错。 就是想问一下这个行为是否是一个预期结果呢。 1.0.25改动过canal server端读取mysql的超时机制，默认是3秒，不过你这个超时发送的频率也太高了，差网络条件？
492,是否要设置getWithoutAck超时时间 说真的，我被这超时时间搞得头痛，我本来是不设置超时时间的，但是好像在弱网环境会卡主  #参考： #297  后面我设置超时 以后发现，是说在规定时间内等待batchSize 填满？？？ 并且我测试的时候确实也是看到实在等待超时时间。 ![image](https://user-images.githubusercontent.com/11556152/34977258-ca958ca0-fad4-11e7-9bc6-703c495e919f.png) ![image](https://user-images.githubusercontent.com/11556152/34976865-61b85a10-fad3-11e7-80ff-adc7770a9f4c.png) 我只想避免卡主，请问是否一定要设置超时时间 在 1.0.25 版本设置超时报错 ![image](https://user-images.githubusercontent.com/11556152/34979214-9c6dc91c-fadb-11e7-9d00-32884b26ecc1.png) 建议设置timeout，主要是后端如果一直没有数据变更，会一直卡主 @agapple   嗯 好的 大概理解，所以batchSize 还不能设置很大对吧，填不满就一直卡主对吧
491,log event entry exceeded max_allowed_packet 应该是有大事务时，会出现下面的异常，canal会解析不过去。 2018-01-12 12:45:20.537 [destination =xxxx   address = /xxxxxx   EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = log event entry exceeded max_allowed_packet; Increase max_allowed_packet on master; the first event 'mysql-bin.007177' at 998888207  the last event read from './mysql-bin.007177' at 123  the last byte read from './mysql-bin.007177' at 998888226. 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) ~[canal.parse-1.0.19.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:117) [canal.parse-1.0.19.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) [canal.parse-1.0.19.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.7.0_76] 看到有一个关于max_allowed_packet的issues，https://github.com/alibaba/canal/issues/85   貌似是为了解决这个问题加了组包的逻辑，但是好像没有什么效果，大家有遇到这个问题的么？ 可以先看看binlog-row-event-max-size参数设置是多少，定义如下： https://dev.mysql.com/doc/refman/5.7/en/replication-options-binary-log.html#option_mysqld_binlog-row-event-max-size binlog-row-event-max-size默认是8k，max_allowed_packet默认是1M。binlog不会将总和>binlog-row-event-max-size的rows合到一个event中，除非单row的size已经超过了这个限制，并且>max_allowed_packet，这个时候只能改max_allowed_packet了。 补充一点， com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection#updateSettings 看上去是在这里设置max_allowed_packet(需要作者确认下) updateSettings暂时未做max packet大小的调整 怎么解决啊，我把max_allowed_packet 16mb 加到64还是会有错误 建议在mysql端修改全局的max_allowed_packet，canal本身支持多packet的重新组包
490,获取 table 的时候链接失效  destination 的 binlog dump 退出 ``` 2018-01-12 00:49:37.659 [destination = test   address = /192.168.1.0:3307   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:test[com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first         at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4832)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:160)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:759)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:428)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:120)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:66)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:337)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:184)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:145)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220)         at java.lang.Thread.run(Thread.java:745) Caused by: java.io.IOException: should execute connector.connect() first         at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.<init>(MysqlQueryExecutor.java:30)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:87)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMetaByDB(TableMetaCache.java:80)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:30)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:55)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:50)         at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527)         at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319)         at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282)         at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197)         at com.google.common.cache.LocalCache.get(LocalCache.java:3937)         at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941)         at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824)         at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830)         ... 10 more ] ```  获取 table 信息的时候会报链接失效 然后 binlog dump 就死掉了 一般是在运行数小时后发生 mysql Server version: 5.6.29 canal 版本:1.0.25 最新的主干修复过，可以尝试一下最新的包
489,canal无法获取任何数据 使用canal 1.0.24版本， mysql版本为5.7.20版本 canal启动后，有如下日志： WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just show master status 就不再有其他日志了，并且zookeeper下1001节点中没有cursor节点。 打开日志debug，一直输出日志如下： getWithoutAck successfully  clientId:1001 batchSize:1000 but result is null 另外，mysql开启了gtid_mode = ON， 在canal部署的机器上采用配置的账号连接数据库实例，执行show master status得到： +------------------+-----------+--------------+------------------+-------------------------------------------------------------------------------------------+ | File             | Position  | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set                                                                         | +------------------+-----------+--------------+------------------+-------------------------------------------------------------------------------------------+ | mysql-bin.000506 | 428663296 |              |                  | f6af278c-cf73-11e7-86ed-801844ea454c:1-6 fb35b001-cf73-11e7-9c62-801844e9f00c:1-87011966 | +------------------+-----------+--------------+------------------+-------------------------------------------------------------------------------------------+ 因此我判断账号配置没有问题，应该是能获取得到binlog position的 之前权限设置为： GRANT SELECT  REPLICATION SLAVE  REPLICATION CLIENT ON *.* TO 'canal'@'%';   现在改为： RANT ALL PRIVILEGES ON *.* TO 'canal'@'%' ;  然后重新启动canal，这时出现如下日志： 2018-01-12 11:57:30.620 [destination = 10_64_1_198_3318_instance   address = /10.64.0.101:3318   EventParser] INFO  c.a.otter.canal.parse.inbound.mysql.MysqlConnection - COM_BINLOG_DUMP with position:BinlogDumpCommandPacket[binlogPosition=523583203 slaveServerId=1234 binlogFileName=mysql-bin.000506 command=18] 2018-01-12 11:57:30.633 [destination = 10_64_1_198_3318_instance   address = /10.64.0.101:3318   EventParser] INFO  com.taobao.tddl.dbsync.binlog.LogEvent - common_header_len= 19  number_of_event_types= 38 2018-01-12 11:57:32.987 [New I/O server worker #1-2] INFO  c.a.otter.canal.server.embedded.CanalServerWithEmbedded - subscribe successfully  ClientIdentity[destination=10_64_1_198_3318_instance clientId=1001 filter=] with first position:null  2018-01-12 11:57:33.002 [New I/O server worker #1-2] DEBUG c.a.otter.canal.server.embedded.CanalServerWithEmbedded - getWithoutAck successfully  clientId:1001 batchSize:1000 but result is null 貌似获取到了position，但是最后是null：...filter=] with first position:null  tcpdump抓包有 binlog的包进来， 看来是canal处理过程中的问题了，如果是binlog解析失败，应该会抛异常吧。 好奇怪 debug代码发现 canal.instance.memory.buffer.size = 1048576 canal.instance.memory.buffer.memunit = 1024 这两个属性相乘不能大于最大int，不然会溢出导致在计算当前buffer大小的时候出问题，无法put EVENT。  这块建议文档说明下，并且能够抛出异常就更好了 这个是一个约束，目前是不能超过int的最大值
488,网络不好断线，无法再次链接 hi 你好 我们发现一个问题，就是我们服务器网络不好的时候会导致客户端断开，但是我们发现，一旦我们下线了以后，我们客户端再也无法重新链接过来， 不知道是否重连机制没有工作。 ClosedChannelException 是否就不会再重连了对么 日志如下： 2018-01-10 12:01:38.228 [New I/O server worker #1-8] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x787f32e8  /*************** => /************]  exception=java.io.IOException: Connection timed out 	at sun.nio.ch.FileDispatcherImpl.read0(Native Method) 	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39) 	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223) 	at sun.nio.ch.IOUtil.read(IOUtil.java:192) 	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380) 	at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:321) 	at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:280) 	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:200) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) 2018-01-10 12:01:38.230 [New I/O server worker #1-8] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x787f32e8  /************* :> /*****************]  exception=java.nio.channels.ClosedChannelException 	at org.jboss.netty.channel.socket.nio.NioWorker.cleanUpWriteBuffer(NioWorker.java:623) 	at org.jboss.netty.channel.socket.nio.NioWorker.close(NioWorker.java:599) 	at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:119) 	at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) 	at org.jboss.netty.channel.Channels.close(Channels.java:720) 	at org.jboss.netty.channel.AbstractChannel.close(AbstractChannel.java:200) 	at com.alibaba.otter.canal.server.netty.handler.SessionHandler.exceptionCaught(SessionHandler.java:216) 	at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.exceptionCaught(ReplayingDecoder.java:461) 	at org.jboss.netty.channel.Channels.fireExceptionCaught(Channels.java:432) 	at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:331) 	at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:280) 	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:200) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) 和 bug 重复 #297
487,dump connection is disconnected For SocketChannel.cache instance  when produced data (dump data) is faster than consume data(canal handled data)  it will caused overflow issue. should be flow control. tks
486,canal.instance.filter.regex 表过滤问题   
485,CanalParseException: parse row data failed（v1.0.25） canal server重启后可以正常运行 以下是异常信息 ------------- 2018-01-08 09:36:30.435 [pool-4-thread-1] INFO  com.alibaba.otter.canal.meta.FileMixedMetaManager - clientId:1001 cursor:[mysql-bin.000914 191343375 1515375389000] address[xxxxxx:3306] 2018/1/8 上午9:36:312018-01-08 09:36:31.435 [pool-4-thread-1] INFO  com.alibaba.otter.canal.meta.FileMixedMetaManager - clientId:1001 cursor:[mysql-bin.000914 191344652 1515375390000] address[xxxxxx:3306] 2018/1/8 上午9:36:322018-01-08 09:36:32.435 [pool-4-thread-1] INFO  com.alibaba.otter.canal.meta.FileMixedMetaManager - clientId:1001 cursor:[mysql-bin.000914 191346634 1515375391000] address[xxxxxx:3306] 2018/1/8 上午9:36:332018-01-08 09:36:33.435 [pool-4-thread-1] INFO  com.alibaba.otter.canal.meta.FileMixedMetaManager - clientId:1001 cursor:[mysql-bin.000914 191348091 1515375392000] address[xxxxxx:3306] 2018/1/8 上午9:36:342018-01-08 09:36:34.435 [pool-4-thread-1] INFO  com.alibaba.otter.canal.meta.FileMixedMetaManager - clientId:1001 cursor:[mysql-bin.000914 191349500 1515375393000] address[xxxxxx:3306] 2018/1/8 上午9:36:342018-01-08 09:36:34.475 [destination = xxxxxxx   address = xxxxxxxxxxx: 3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - ERROR ## parse this event has an error   last position : [EntryPosition[included=false journalName=mysql-bin.000914 position=191349531 serverId=200 timestamp=1515375394000]] 2018/1/8 上午9:36:34com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. 2018/1/8 上午9:36:34Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first 2018/1/8 上午9:36:34Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first 2018/1/8 上午9:36:34  at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4832) ~[guava-18.0.jar:na] 2018/1/8 上午9:36:34  at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:160) ~[canal.parse-1.0.25.jar:na] 2018/1/8 上午9:36:34  at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:759) ~[canal.parse-1.0.25.jar:na] 2018/1/8 上午9:36:34  at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:428) ~[canal.parse-1.0.25.jar:na] 2018/1/8 上午9:36:34  at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:114) ~[canal.parse-1.0.25.jar:na] 2018/1/8 上午9:36:34  at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:66) ~[canal.parse-1.0.25.jar:na] 2018/1/8 上午9:36:34  at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:337) ~[canal.parse-1.0.25.jar:na] 2018/1/8 上午9:36:34  at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:184) ~[canal.parse-1.0.25.jar:na] 2018/1/8 上午9:36:34  at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:145) [canal.parse-1.0.25.jar:na] 2018/1/8 上午9:36:34  at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) [canal.parse-1.0.25.jar:na] 2018/1/8 上午9:36:34  at java.lang.Thread.run(Thread.java:745) [na:1.8.0_111] 2018/1/8 上午9:36:34Caused by: java.io.IOException: should execute connector.connect() first 2018/1/8 上午9:36:34  at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.<init>(MysqlQueryExecutor.java:30) ~[canal.parse.driver-1.0.25.jar:na] 2018/1/8 上午9:36:34  at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:87) [canal.parse-1.0.25.jar:na] 2018/1/8 上午9:36:34  at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMetaByDB(TableMetaCache.java:80) ~[canal.parse-1.0.25.jar:na] 2018/1/8 上午9:36:34  at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:30) ~[canal.parse-1.0.25.jar:na] 2018/1/8 上午9:36:34  at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:55) ~[canal.parse-1.0.25.jar:na] 2018/1/8 上午9:36:34  at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:50) ~[canal.parse-1.0.25.jar:na] 2018/1/8 上午9:36:34  at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) ~[guava-18.0.jar:na] 2018/1/8 上午9:36:34  at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) ~[guava-18.0.jar:na] 2018/1/8 上午9:36:34  at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) ~[guava-18.0.jar:na] 2018/1/8 上午9:36:34  at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) ~[guava-18.0.jar:na] 2018/1/8 上午9:36:34  at com.google.common.cache.LocalCache.get(LocalCache.java:3937) ~[guava-18.0.jar:na] 2018/1/8 上午9:36:34  at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941) ~[guava-18.0.jar:na] 2018/1/8 上午9:36:34  at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824) ~[guava-18.0.jar:na] 2018/1/8 上午9:36:34  at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830) ~[guava-18.0.jar:na] 2018/1/8 上午9:36:34  ... 10 common frames omitted 2018/1/8 上午9:36:342018-01-08 09:36:34.479 [destination = weiming-test   address = xxxxxx:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address xxxxxx:3306 has an error  retrying. caused by 2018/1/8 上午9:36:34com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. 2018/1/8 上午9:36:34Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. 2018/1/8 上午9:36:34Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first 2018/1/8 上午9:36:34Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first 2018/1/8 上午9:36:34  at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4832) ~[guava-18.0.jar:na] 2018/1/8 上午9:36:34  at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:160) ~[canal.parse-1.0.25.jar:na] 2018/1/8 上午9:36:34  at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:759) ~[canal.parse-1.0.25.jar:na] 2018/1/8 上午9:36:34  at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:428) ~[canal.parse-1.0.25.jar:na] 2018/1/8 上午9:36:34  at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:114) ~[canal.parse-1.0.25.jar:na] 2018/1/8 上午9:36:34  at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:66) ~[canal.parse-1.0.25.jar:na] 2018/1/8 上午9:36:34  at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:337) ~[canal.parse-1.0.25.jar:na] 2018/1/8 上午9:36:34  at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:184) ~[canal.parse-1.0.25.jar:na] 2018/1/8 上午9:36:34  at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:145) ~[canal.parse-1.0.25.jar:na] 2018/1/8 上午9:36:34  at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) ~[canal.parse-1.0.25.jar:na] 2018/1/8 上午9:36:34  at java.lang.Thread.run(Thread.java:745) [na:1.8.0_111] 2018/1/8 上午9:36:34Caused by: java.io.IOException: should execute connector.connect() first 2018/1/8 上午9:36:34  at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.<init>(MysqlQueryExecutor.java:30) ~[canal.parse.driver-1.0.25.jar:na] 2018/1/8 上午9:36:34  at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:87) ~[canal.parse-1.0.25.jar:na] 2018/1/8 上午9:36:34  at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMetaByDB(TableMetaCache.java:80) ~[canal.parse-1.0.25.jar:na] 2018/1/8 上午9:36:34  at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:30) ~[canal.parse-1.0.25.jar:na] 2018/1/8 上午9:36:34  at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:55) ~[canal.parse-1.0.25.jar:na] 2018/1/8 上午9:36:34  at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:50) ~[canal.parse-1.0.25.jar:na] 2018/1/8 上午9:36:34  at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) ~[guava-18.0.jar:na] 2018/1/8 上午9:36:34  at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) ~[guava-18.0.jar:na] 2018/1/8 上午9:36:34  at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) ~[guava-18.0.jar:na] 2018/1/8 上午9:36:34  at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) ~[guava-18.0.jar:na] 2018/1/8 上午9:36:34  at com.google.common.cache.LocalCache.get(LocalCache.java:3937) ~[guava-18.0.jar:na] 2018/1/8 上午9:36:34  at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941) ~[guava-18.0.jar:na] 2018/1/8 上午9:36:34  at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824) ~[guava-18.0.jar:na] 2018/1/8 上午9:36:34  at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830) ~[guava-18.0.jar:na] 2018/1/8 上午9:36:34  ... 10 common frames omitted 2018/1/8 上午9:36:342018-01-08 09:36:34.480 [destination = weiming-test   address = xxxxxx:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:weiming-test[com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. 2018/1/8 上午9:36:34Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. 2018/1/8 上午9:36:34Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first 2018/1/8 上午9:36:34Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first 2018/1/8 上午9:36:34  at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4832) 2018/1/8 上午9:36:34  at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:160) 2018/1/8 上午9:36:34  at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:759) 2018/1/8 上午9:36:34  at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:428) 2018/1/8 上午9:36:34  at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:114) 2018/1/8 上午9:36:34  at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:66) 2018/1/8 上午9:36:34  at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:337) 2018/1/8 上午9:36:34  at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:184) 2018/1/8 上午9:36:34  at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:145) 2018/1/8 上午9:36:34  at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) 2018/1/8 上午9:36:34  at java.lang.Thread.run(Thread.java:745) 2018/1/8 上午9:36:34Caused by: java.io.IOException: should execute connector.connect() first 2018/1/8 上午9:36:34  at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.<init>(MysqlQueryExecutor.java:30) 2018/1/8 上午9:36:34  at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:87) 2018/1/8 上午9:36:34  at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMetaByDB(TableMetaCache.java:80) 2018/1/8 上午9:36:34  at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:30) 2018/1/8 上午9:36:34  at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:55) 2018/1/8 上午9:36:34  at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:50) 2018/1/8 上午9:36:34  at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) 2018/1/8 上午9:36:34  at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) 2018/1/8 上午9:36:34  at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) 2018/1/8 上午9:36:34  at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) 2018/1/8 上午9:36:34  at com.google.common.cache.LocalCache.get(LocalCache.java:3937) 2018/1/8 上午9:36:34  at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941) 2018/1/8 上午9:36:34  at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824) 2018/1/8 上午9:36:34  at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830) 2018/1/8 上午9:36:34  ... 10 more 2018/1/8 上午9:36:34] 参考 #482 
484,希望后续有对 Docker 支持的计划，环境调试特别不方便。 我本来打算自己封装镜像的，我发现需要自己写 `docker entrypoint.sh` 不说，主要是日志还有配置的处理需要在应用层做一些工作果断放弃了，希望官方后续对 Docker 进行支持。 我们自己做了一个镜像，需要的话，可以交流一下，微信gary0526 @freeme 你方便把你的 `dockerfile` 的仓库链接发一下吗？  > BTW: 你封装的镜像是改造过官方的代码吗？ 我这边期望的镜像功能有如下两点主要特性： * 日志可以统一输出并有效通过 `docker logging` 控制收集； * 可以通过 `docker environment` 环境变量控制核心配置； 修改startup.sh文件，变成前台应用就好了。 @pczhaoyun 脚本只是其中一方面，应用本身的日志和配置也要做相应适应性处理的，否则依然管理麻烦的。 @freeme 可以提交一个dockerfile PR 是啊，有这个会方便很多 docker已经支持，参考：https://github.com/alibaba/canal/wiki/Docker-QuickStart
483,建议新增ReportSlave消息 现在连上Mysql时，连接成功后就直接RequestDump了，所以在Mysql上敲show slave hosts，看不到当前这条Canal连接。虽然不影响数据同步，但对于强迫症来说，好不舒服。 解决方案是在RequestDump之前发一条ReportSlave消息上报自己，再收回一条OK消息，就可以了。 消息内容代码如下： [RegisterSlaveCommandPacket.java.txt](https://github.com/alibaba/canal/files/1610961/RegisterSlaveCommandPacket.java.txt) 然后在MysqlConnection.java中增加如下函数，在dump()中调用就好了。     private void sendRegisterSlave() throws IOException {     	RegisterSlaveCommandPacket cmd = new RegisterSlaveCommandPacket();     	cmd.setReport_host(canalHost);     	cmd.setReport_port(canalPort);     	cmd.setReport_user(connector.getUsername());     	cmd.setServerid(slaveId);     	     	byte[] cmdBody = cmd.toBytes();         logger.info("Register slave {}"  cmd);         HeaderPacket header = new HeaderPacket();         header.setPacketBodyLength(cmdBody.length);         header.setPacketSequenceNumber((byte) 0x00);         PacketManager.write(connector.getChannel()  new ByteBuffer[] { ByteBuffer.wrap(header.toBytes())                 ByteBuffer.wrap(cmdBody) });                  ByteBuffer dest = ByteBuffer.allocate(1024);         connector.getChannel().read(dest);     } 其中的变量我是通过改Spring的xml，从配置文件中拿的。 挺好的优化，方便提交一个代码PR给我？ 最新的release 没有带出这个特性吗？我下载了尝试，发现show slave hosts 仍然没有发现canal server信息。 26的测试版本应该有了 亲测 最新版1.10中没有噢。 只有在进入dump阶段才会有show slave hosts，针对位点定位阶段没有
482,first parse row data failed should execute connector.connect()  最近凌晨报错这个，导致channel挂起，mysql :5.6.35  canal:1.0.25 2018-01-07 00:00:00.085 [destination = sms_log_2   address = /127.0.0.1:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - ERROR ## parse this event has an error   last position : [EntryPosition[included=false journalName=mysql-bin.000106 position=583737799 serverId=20563 timestamp=1515254400000]] com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first 	at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4832) ~[guava-18.0.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:160) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:759) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:428) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:114) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:66) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:337) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:184) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:145) [canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) [canal.parse-1.0.25.jar:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_151] Caused by: java.io.IOException: should execute connector.connect() first 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.<init>(MysqlQueryExecutor.java:30) ~[canal.parse.driver-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:87) [canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMetaByDB(TableMetaCache.java:80) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:30) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:55) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:50) ~[canal.parse-1.0.25.jar:na] 	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) ~[guava-18.0.jar:na] 	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) ~[guava-18.0.jar:na] 	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) ~[guava-18.0.jar:na] 	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) ~[guava-18.0.jar:na] 	at com.google.common.cache.LocalCache.get(LocalCache.java:3937) ~[guava-18.0.jar:na] 	at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941) ~[guava-18.0.jar:na] 	at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824) ~[guava-18.0.jar:na] 	at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830) ~[guava-18.0.jar:na] 	... 10 common frames omitted 2018-01-07 00:00:00.087 [destination = sms_log_2   address = /127.0.0.1:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address 127.0.0.1/127.0.0.1:3306 has an error  retrying. caused by  com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first 	at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4832) ~[guava-18.0.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:160) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:759) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:428) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:114) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:66) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:337) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:184) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:145) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) ~[canal.parse-1.0.25.jar:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_151] Caused by: java.io.IOException: should execute connector.connect() first 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.<init>(MysqlQueryExecutor.java:30) ~[canal.parse.driver-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:87) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMetaByDB(TableMetaCache.java:80) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:30) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:55) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:50) ~[canal.parse-1.0.25.jar:na] 	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) ~[guava-18.0.jar:na] 	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) ~[guava-18.0.jar:na] 	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) ~[guava-18.0.jar:na] 	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) ~[guava-18.0.jar:na] 	at com.google.common.cache.LocalCache.get(LocalCache.java:3937) ~[guava-18.0.jar:na] 	at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941) ~[guava-18.0.jar:na] 	at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824) ~[guava-18.0.jar:na] 	at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830) ~[guava-18.0.jar:na] 	... 10 common frames omitted 2018-01-07 00:00:00.088 [destination = sms_log_2   address = /127.0.0.1:3306   EventParser] WARN  c.a.o.s.a.i.setl.zookeeper.termin.WarningTerminProcess - nid:3[3:canal:sms_log_2:com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first 	at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4832) 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:160) 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:759) 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:428) 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:114) 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:66) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:337) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:184) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:145) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) 	at java.lang.Thread.run(Thread.java:748) Caused by: java.io.IOException: should execute connector.connect() first 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.<init>(MysqlQueryExecutor.java:30) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:87) 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMetaByDB(TableMetaCache.java:80) 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:30) 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:55) 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:50) 	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) 	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) 	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) 	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) 	at com.google.common.cache.LocalCache.get(LocalCache.java:3937) 	at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941) 	at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824) 	at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830) 	... 10 more ]    我也遇到过这个问题 我也遇到了，导致监测不到消息 我修复以下 同样的问题，影响使用吗？ `com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector .connect() first Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first         at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4832)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:160)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:759)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:428)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:114)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:66)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:337)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:184)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:145)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220)         at java.lang.Thread.run(Unknown Source)` @agapple 
481,canal读取Integer字段的值为null，可以不是""吗 现在读取Integer字段值如果为null，读取出来就是字符段""（空字段） 可以读出来也是null吗？ 有一个isNull方法的判断
480,canal启动报错com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: ErrorPacket [errorNumber=1142  fieldCount=-1  message=SHOW command denied to user 'shencesys'@'ip' for table 'slow_log'  sqlState=42000  sqlStateMarker=#] 启动之后报错 ``` 2018-01-05 15:36:23.539 [destination = example   address = ip/ip:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just show master status 2018-01-05 15:36:30.095 [destination = example   address = ip/ip:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address ip/ip:3306 has an error  retrying. caused by  com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: ErrorPacket [errorNumber=1142  fieldCount=-1  message=SHOW command denied to user 'shencesys'@'ip' for table 'slow_log'  sqlState=42000  sqlStateMarker=#]  with command: show create table `mysql`.`event`;show create table `mysql`.`func`;show create table `mysql`.`general_log`;show create table `mysql`.`help_category`;show create table `mysql`.`help_keyword`;show create table `mysql`.`help_relation`;show create table `mysql`.`help_topic`;show create table `mysql`.`proc`;show create table `mysql`.`slow_log`;show create table `mysql`.`time_zone`;show create table `mysql`.`time_zone_leap_second`;show create table `mysql`.`time_zone_name`;show create table `mysql`.`time_zone_transition`;show create table `mysql`.`time_zone_transition_type`; Caused by: java.io.IOException: ErrorPacket [errorNumber=1142  fieldCount=-1  message=SHOW command denied to user 'shencesys'@'ip' for table 'slow_log'  sqlState=42000  sqlStateMarker=#]  with command: show create table `mysql`.`event`;show create table `mysql`.`func`;show create table `mysql`.`general_log`;show create table `mysql`.`help_category`;show create table `mysql`.`help_keyword`;show create table `mysql`.`help_relation`;show create table `mysql`.`help_topic`;show create table `mysql`.`proc`;show create table `mysql`.`slow_log`;show create table `mysql`.`time_zone`;show create table `mysql`.`time_zone_leap_second`;show create table `mysql`.`time_zone_name`;show create table `mysql`.`time_zone_transition`;show create table `mysql`.`time_zone_transition_type`; 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.queryMulti(MysqlQueryExecutor.java:109) ~[canal.parse.driver-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.queryMulti(MysqlConnection.java:93) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:173) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) ~[canal.parse-1.0.25.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) ~[canal.parse-1.0.25.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_45] 2018-01-05 15:36:30.096 [destination = example   address = ip/ip:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: ErrorPacket [errorNumber=1142  fieldCount=-1  message=SHOW command denied to user 'shencesys'@'ip' for table 'slow_log'  sqlState=42000  sqlStateMarker=#]  with command: show create table `mysql`.`event`;show create table `mysql`.`func`;show create table `mysql`.`general_log`;show create table `mysql`.`help_category`;show create table `mysql`.`help_keyword`;show create table `mysql`.`help_relation`;show create table `mysql`.`help_topic`;show create table `mysql`.`proc`;show create table `mysql`.`slow_log`;show create table `mysql`.`time_zone`;show create table `mysql`.`time_zone_leap_second`;show create table `mysql`.`time_zone_name`;show create table `mysql`.`time_zone_transition`;show create table `mysql`.`time_zone_transition_type`; Caused by: java.io.IOException: ErrorPacket [errorNumber=1142  fieldCount=-1  message=SHOW command denied to user 'shencesys'@'ip' for table 'slow_log'  sqlState=42000  sqlStateMarker=#]  with command: show create table `mysql`.`event`;show create table `mysql`.`func`;show create table `mysql`.`general_log`;show create table `mysql`.`help_category`;show create table `mysql`.`help_keyword`;show create table `mysql`.`help_relation`;show create table `mysql`.`help_topic`;show create table `mysql`.`proc`;show create table `mysql`.`slow_log`;show create table `mysql`.`time_zone`;show create table `mysql`.`time_zone_leap_second`;show create table `mysql`.`time_zone_name`;show create table `mysql`.`time_zone_transition`;show create table `mysql`.`time_zone_transition_type`; 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.queryMulti(MysqlQueryExecutor.java:109) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.queryMulti(MysqlConnection.java:93) 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:173) 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) 	at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) 	at java.lang.Thread.run(Thread.java:745) ``` 账号权限如下 ``` mysql> show grants for shencesys; +---------------------------------------------------------------------------------------------------------------------------------------------------+ | Grants for shencesys@%                                                                                                                            | +---------------------------------------------------------------------------------------------------------------------------------------------------+ | GRANT PROCESS  REPLICATION SLAVE  REPLICATION CLIENT ON *.* TO 'shencesys'@'%' IDENTIFIED BY PASSWORD '***********************************' | | GRANT SELECT  LOCK TABLES  SHOW VIEW ON `xbdchainfin`.* TO 'shencesys'@'%'                                                                        | | GRANT SELECT ON `performance_schema`.* TO 'shencesys'@'%'                                                                                         | | GRANT SELECT ON `mysql`.`func` TO 'shencesys'@'%'                                                                                                 | | GRANT SELECT ON `mysql`.`help_relation` TO 'shencesys'@'%'                                                                                        | | GRANT SELECT ON `mysql`.`time_zone_leap_second` TO 'shencesys'@'%'                                                                                | | GRANT SELECT ON `mysql`.`time_zone_transition` TO 'shencesys'@'%'                                                                                 | | GRANT SELECT ON `mysql`.`help_keyword` TO 'shencesys'@'%'                                                                                         | | GRANT SELECT ON `mysql`.`slow_log` TO 'shencesys'@'%'                                                                                             | | GRANT SELECT ON `mysql`.`event` TO 'shencesys'@'%'                                                                                                | | GRANT SELECT ON `mysql`.`proc` TO 'shencesys'@'%'                                                                                                 | | GRANT SELECT ON `mysql`.`general_log` TO 'shencesys'@'%'                                                                                          | | GRANT SELECT ON `mysql`.`help_category` TO 'shencesys'@'%'                                                                                        | | GRANT SELECT ON `mysql`.`help_topic` TO 'shencesys'@'%'                                                                                           | | GRANT SELECT ON `mysql`.`time_zone` TO 'shencesys'@'%'                                                                                            | | GRANT SELECT ON `mysql`.`time_zone_name` TO 'shencesys'@'%'                                                                                       | | GRANT SELECT ON `mysql`.`time_zone_transition_type` TO 'shencesys'@'%'                                                                            | +---------------------------------------------------------------------------------------------------------------------------------------------------+ ```  show create table `mysql`.`event`;show create table `mysql`.`func`;show create table `mysql`.`general_log`;show create table `mysql`.`help_category`;show create table `mysql`.`help_keyword`;show create table `mysql`.`help_relation`;show create table `mysql`.`help_topic`;show create table `mysql`.`proc`;show create table `mysql`.`slow_log`;show create table `mysql`.`time_zone`;show create table `mysql`.`time_zone_leap_second`;show create table `mysql`.`time_zone_name`;show create table `mysql`.`time_zone_transition`;show create table `mysql`.`time_zone_transition_type`; 执行一下对应的sql 已经解决，谢谢哈 @jinxiaoxin 是怎么解决的呀？求分享 @jinxiaoxin 可以分享下是怎么解决的问题吗？谢谢啦 @millinchen 你最后这个问题解决了吗？ @agapple 已经有权限了，为啥还要再执行一遍才行？ aliyun rds上的mysql库超级账号也不一定有权限
479,canal server同步主库时，报错com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: ErrorPacket [errorNumber=1146  fieldCount=-1  message=Table 'mysql.innodb_index_stats' doesn't exist  sqlState=42S02  sqlStateMarker=#] 我在主库insert一条数据后（insert的是一个业务表），就报错，报错的内容，涉及很多mysql的内部表，重启canan server、mysql、重置position均无效。只要主库有insert，update等，都报这个错 2018-01-04 20:51:50.268 [destination = example   address = /10.30.255.151:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /10.30.255.151:3306 has an error  retrying. caused by  com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: ErrorPacket [errorNumber=1146  fieldCount=-1  message=Table 'mysql.innodb_index_stats' doesn't exist  sqlState=42S02  sqlStateMarker=#]  with command: show create table `mysql`.`columns_priv`;show create table `mysql`.`db`;show create table `mysql`.`event`;show create table `mysql`.`func`;show create table `mysql`.`general_log`;show create table `mysql`.`help_category`;show create table `mysql`.`help_keyword`;show create table `mysql`.`help_relation`;show create table `mysql`.`help_topic`;show create table `mysql`.`innodb_index_stats`;show create table `mysql`.`innodb_table_stats`;show create table `mysql`.`ndb_binlog_index`;show create table `mysql`.`plugin`;show create table `mysql`.`proc`;show create table `mysql`.`procs_priv`;show create table `mysql`.`proxies_priv`;show create table `mysql`.`servers`;show create table `mysql`.`slave_master_info`;show create table `mysql`.`slave_relay_log_info`;show create table `mysql`.`slave_worker_info`;show create table `mysql`.`slow_log`;show create table `mysql`.`tables_priv`;show create table `mysql`.`time_zone`;show create table `mysql`.`time_zone_leap_second`;show create table `mysql`.`time_zone_name`;show create table `mysql`.`time_zone_transition`;show create table `mysql`.`time_zone_transition_type`;show create table `mysql`.`user`; Caused by: java.io.IOException: ErrorPacket [errorNumber=1146  fieldCount=-1  message=Table 'mysql.innodb_index_stats' doesn't exist  sqlState=42S02  sqlStateMarker=#]  with command: show create table `mysql`.`columns_priv`;show create table `mysql`.`db`;show create table `mysql`.`event`;show create table `mysql`.`func`;show create table `mysql`.`general_log`;show create table `mysql`.`help_category`;show create table `mysql`.`help_keyword`;show create table `mysql`.`help_relation`;show create table `mysql`.`help_topic`;show create table `mysql`.`innodb_index_stats`;show create table `mysql`.`innodb_table_stats`;show create table `mysql`.`ndb_binlog_index`;show create table `mysql`.`plugin`;show create table `mysql`.`proc`;show create table `mysql`.`procs_priv`;show create table `mysql`.`proxies_priv`;show create table `mysql`.`servers`;show create table `mysql`.`slave_master_info`;show create table `mysql`.`slave_relay_log_info`;show create table `mysql`.`slave_worker_info`;show create table `mysql`.`slow_log`;show create table `mysql`.`tables_priv`;show create table `mysql`.`time_zone`;show create table `mysql`.`time_zone_leap_second`;show create table `mysql`.`time_zone_name`;show create table `mysql`.`time_zone_transition`;show create table `mysql`.`time_zone_transition_type`;show create table `mysql`.`user`; 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.queryMulti(MysqlQueryExecutor.java:109) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.queryMulti(MysqlConnection.java:94) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:173) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_60] 没权限吧 我也是遇到这个问题，帐号权限是有的 +1
478,canal读取数据报IndexOutOfBoundsException canal读取数据报IndexOutOfBoundsException 2018-01-04 17:39:52.043 [pipelineId = 2 taskName = ProcessSelect] WARN c.a.o.s.a.i.setl.zookeeper.termin.WarningTerminProcess - nid:2[2:setl:com.alibaba.otter.node.etl.select.exceptions.SelectException: java.lang.IndexOutOfBoundsException: Index: 0 at com.alibaba.otter.node.etl.select.selector.MessageParser.parse(MessageParser.java:211) at com.alibaba.otter.node.etl.select.selector.canal.CanalEmbedSelector.selector(CanalEmbedSelector.java:258) at com.alibaba.otter.node.etl.select.SelectTask.processSelect(SelectTask.java:236) at com.alibaba.otter.node.etl.select.SelectTask.access$300(SelectTask.java:94) at com.alibaba.otter.node.etl.select.SelectTask$1.run(SelectTask.java:208) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Caused by: java.lang.IndexOutOfBoundsException: Index: 0 at java.util.Collections$EmptyList.get(Collections.java:4454) at com.alibaba.otter.canal.protocol.CanalEntry$RowChange.getRowDatas(CanalEntry.java:8107) at com.alibaba.otter.node.etl.select.selector.MessageParser.parse(MessageParser.java:109) ... 9 more otter问题，非canal
477,集成Springboot出现了问题。 @Order(value=1) @Component  public class SimpleCanalClientServer  implements CommandLineRunner{ Exception in thread "restartedMain" java.lang.reflect.InvocationTargetException 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.lang.reflect.Method.invoke(Method.java:498) 	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:49) Caused by: java.lang.ClassCastException: org.springframework.boot.context.event.ApplicationStartedEvent cannot be cast to org.springframework.boot.context.event.ApplicationEnvironmentPreparedEvent 	at org.springframework.boot.context.config.AnsiOutputApplicationListener.onApplicationEvent(AnsiOutputApplicationListener.java:34) 	at org.springframework.context.event.SimpleApplicationEventMulticaster$1.run(SimpleApplicationEventMulticaster.java:78) 	at org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:50) 	at org.springframework.context.event.SimpleApplicationEventMulticaster.multicastEvent(SimpleApplicationEventMulticaster.java:76) 	at org.springframework.boot.context.event.EventPublishingRunListener.starting(EventPublishingRunListener.java:69) 	at org.springframework.boot.SpringApplicationRunListeners.starting(SpringApplicationRunListeners.java:48) 	at org.springframework.boot.SpringApplication.run(SpringApplication.java:292) 	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1118) 	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1107) 	at com.jorden.li.ApplicationStart.main(ApplicationStart.java:23) 	... 5 more 你这个问题跟canal没关系，是使用spring boot的问题，可以加我微信（gary0526）帮你看一下
476,canal client 获取db serverip+port canal client端能获取到db 的server ip+port ? 这个暂时没有，只有对应的serverId
475,MySQL 重启后，Canal 无限循环在SocketChannel.read方法内 场景: canal 在dump的时候，直接重启MySQL 现象： debug 可以看到一直在这个地方循环出不去 ![image](https://user-images.githubusercontent.com/33280738/34552985-e07d80aa-f15f-11e7-806c-c3183b47050c.png) 控制台日志： ![image](https://user-images.githubusercontent.com/33280738/34553015-1ed73ab2-f160-11e7-9eb2-19acbf410a08.png) 无法让Canal 重新进行dump 半小时后，控制台出现如下日志： ` 2018-01-04 15:01:05.400 [Hashed wheel timer #1] WARN  c.a.o.c.server.netty.handler.ClientAuthenticationHandler - channel:[id: 0x15bf0ead  /10.10.80.108:50423 :> /10.10.80.108:11111] idle timeout exceeds  close channel to save server resources... 2018-01-04 15:01:05.403 [New I/O server worker #1-4] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x571b8f50  /10.10.80.108:58807 => /10.10.80.108:11111]  exception=java.io.IOException: Connection reset by peer 	at sun.nio.ch.FileDispatcherImpl.read0(Native Method) 	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39) 	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223) 	at sun.nio.ch.IOUtil.read(IOUtil.java:192) 	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380) 	at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:322) 	at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) 	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) 	at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) ` Canal 仍然无法进入重新dump 状态 已经有人提了 PR了 #473 
474,jvm crash 1. 部分崩溃日志 ``` A fatal error has been detected by the Java Runtime Environment: SIGSEGV (0xb) at pc=0x00007f36d5052219  pid=15219  tid=139872797697792 JRE version: Java(TM) SE Runtime Environment (8.0_74-b02) (build 1.8.0_74-b02) Java VM: Java HotSpot(TM) 64-Bit Server VM (25.74-b02 mixed mode linux-amd64 compressed oops) Problematic frame: v  ~StubRoutines::jshort_disjoint_arraycopy Stack: [0x00007f36ac67a000 0x00007f36ac6bb000]   sp=0x00007f36ac6b94a0   free space=253k Native frames: (J=compiled Java code  j=interpreted  Vv=VM code  C=native code) v  ~StubRoutines::jshort_disjoint_arraycopy J 2004 C2 io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(ILio/netty/buffer/ByteBuf;II)Lio/netty/buffer/ByteBuf; (16 bytes) @ 0x00007f36d578d132 [0x00007f36d578cfe0+0x152] J 1955 C2 io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(Lio/netty/channel/AbstractChannelHandlerContext;Ljava/lang/Object;)V (53 bytes) @ 0x00007f36d574ef70 [0x00007f36d574e800+0x770] J 3093 C2 io.netty.channel.nio.NioEventLoop.processSelectedKeys()V (33 bytes) @ 0x00007f36d59a953c [0x00007f36d59a8f00+0x63c] j  io.netty.channel.nio.NioEventLoop.run()V+126 j  io.netty.util.concurrent.SingleThreadEventExecutor$5.run()V+44 j  io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run()V+4 j  java.lang.Thread.run()V+11 v  ~StubRoutines::call_stub V  [libjvm.so+0x68c616]  JavaCalls::call_helper(JavaValue*  methodHandle*  JavaCallArguments*  Thread*)+0x1056 V  [libjvm.so+0x68cb21]  JavaCalls::call_virtual(JavaValue*  KlassHandle  Symbol*  Symbol*  JavaCallArguments*  Thread*)+0x321 V  [libjvm.so+0x68cfc7]  JavaCalls::call_virtual(JavaValue*  Handle  KlassHandle  Symbol*  Symbol*  Thread*)+0x47 V  [libjvm.so+0x723d80]  thread_entry(JavaThread*  Thread*)+0xa0 V  [libjvm.so+0xa69dcf]  JavaThread::thread_main_inner()+0xdf V  [libjvm.so+0xa69efc]  JavaThread::run()+0x11c V  [libjvm.so+0x91d9d8]  java_start(Thread*)+0x108 C  [libpthread.so.0+0x7aa1]  start_thread+0xd1 ``` 2. 使用的是1.0.25版本，启动参数配置如下： ``` jvm_args: -Xms2048m -Xmx3072m -Xmn1024m -XX:SurvivorRatio=2 -XX:PermSize=96m -XX:MaxPermSize=256m -Xss256k -XX:-UseAdaptiveSizePolicy -XX:MaxTenuringThreshold=15 -XX:+DisableExplicitGC -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:+UseCMSCompactAtFullCollection -XX:+UseFastAccessorMethods -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -Djava.awt.headless=true -Djava.net.preferIPv4Stack=true -Dfile.encoding=UTF-8 -DappName=otter-canal -Dlogback.configurationFile=/opt/canal-server/bin/../conf/logback.xml -Dcanal.conf=/opt/canal-server/bin/../conf/canal.properties ``` 3. canal.properties ``` canal.id= 1 canal.ip= canal.port= 11111 canal.zkServers=172.42.11.9:2181 172.42.11.8:2181 172.42.11.7:2181 # flush data to zk canal.zookeeper.flush.period = 1000 # flush meta cursor/parse position to file canal.file.data.dir = ${canal.conf.dir} canal.file.flush.period = 1000 ## memory store RingBuffer size  should be Math.pow(2 n) canal.instance.memory.buffer.size = 16384 ## memory store RingBuffer used memory unit size   default 1kb canal.instance.memory.buffer.memunit = 1024  ## meory store gets mode used MEMSIZE or ITEMSIZE canal.instance.memory.batch.mode = MEMSIZE ## detecing config canal.instance.detecting.enable = true #canal.instance.detecting.sql = insert into retl.xdual values(1 now()) on duplicate key update x=now() canal.instance.detecting.sql = select 1 canal.instance.detecting.interval.time = 3 canal.instance.detecting.retry.threshold = 3 canal.instance.detecting.heartbeatHaEnable = false # support maximum transaction size  more than the size of the transaction will be cut into multiple transactions delivery canal.instance.transaction.size =  1024 # mysql fallback connected to new master should fallback times canal.instance.fallbackIntervalInSeconds = 60 # network config canal.instance.network.receiveBufferSize = 16384 canal.instance.network.sendBufferSize = 16384 canal.instance.network.soTimeout = 30 # binlog filter config canal.instance.filter.query.dcl = false canal.instance.filter.query.dml = false canal.instance.filter.query.ddl = false canal.instance.filter.table.error = false canal.instance.filter.rows = false # binlog format/image check canal.instance.binlog.format = ROW STATEMENT MIXED  canal.instance.binlog.image = FULL MINIMAL NOBLOB # binlog ddl isolation canal.instance.get.ddl.isolation = false ################################################# #########               destinations            #############  ################################################# canal.destinations= example example2 # conf root dir canal.conf.dir = ../conf # auto scan instance dir add/remove and start/stop instance canal.auto.scan = true canal.auto.scan.interval = 5 canal.instance.global.mode = spring  canal.instance.global.lazy = false #canal.instance.global.manager.address = 127.0.0.1:1099 #canal.instance.global.spring.xml = classpath:spring/memory-instance.xml #canal.instance.global.spring.xml = classpath:spring/file-instance.xml canal.instance.global.spring.xml = classpath:spring/default-instance.xml ``` 4. canal运行期间，经常报这个错，但是后面都能自动重连： ``` 2018-01-03 11:03:57.331 [destination = example   address = /172.42.11.50:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first         at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4832)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:160)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:759)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:428)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:117)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:66)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:337)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:184)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:145)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220)         at java.lang.Thread.run(Thread.java:745) Caused by: java.io.IOException: should execute connector.connect() first         at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.<init>(MysqlQueryExecutor.java:30)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:87)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMetaByDB(TableMetaCache.java:80)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:30)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:55)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:50)         at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527)         at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319)         at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282)         at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197)         at com.google.common.cache.LocalCache.get(LocalCache.java:3937)         at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941)         at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824)         at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830)         ... 10 more ] ``` 2018-01-03 11:03:57.331 [destination = example   address = /172.42.11.50:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first         at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4832)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:160)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:759)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:428)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:117)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:66)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:337)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:184)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:145)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220)         at java.lang.Thread.run(Thread.java:745) Caused by: java.io.IOException: should execute connector.connect() first 我也碰到这个问题，有解决办法么 参考 #482 
473,connect mysql server failed when reconnect mysql server frequently cannot start dump biz due to mysql connection is disconnected by timeout. root cause  multi-thread issue (netty i/o thread vs. biz thread). when invoke connect()  it will return a ChannelFuture instance  and then the "addListener" method is invoked by biz thread  however  "channelRead" method in BusinessHandler is executed by netty I/O thread. ChannelFutureListener -> operationComplete() cannot guarantee that which is executed before than BusinessHandler -> channelRead() method. It will caused new connection is timeout when AbstractEventParser -> start() -> parseThread ->erosaConnection.reconnect() is invoked. ENV: canal: 1.0.25 open-jdk 1.8 OS: CentOS 6.5 -Dio.netty.eventLoopThreads: 1 (very important) another hot bug: mysql dump connection will be disconnected about 10' minutes. It's caused by using Netty4. I'm trying to figure it out.  tks
472,CanalConnector checkValid改进：检查下链接是否合法 增加了对以下规则的判断： - 链接canal server失败，一直没有一个可用的链接，返回false 这次PR感觉糅合了很多其他的改动，不太理解对于parser查找位点的改动主要是基于什么考虑？ 非常抱歉，这次的pull request 不慎将其他与主干无关的本地修改一起提交了。 我这里撤回。
471,请教：关于canal客户端报com.alibaba.otter.canal.protocol.exception.CanalClientException: java.io.IOException: end of stream when reading header这个错误 while (true) {             Message message = connector.getWithoutAck(1); // 获取指定数量的数据             long batchId = message.getId();             int size = message.getEntries().size();             if (batchId == -1 || size == 0) {                 System.out.println("empty count : " + emptyCount);             } else {                 System.out.printf("message[batchId=%s size=%s] \n"  batchId  size);             }             try {                 Thread.sleep(40000);             } catch (InterruptedException e) {             }             connector.ack(batchId); // 提交确认         } 上面我取完数据，sleep个40秒，canal连接就会断掉，报标题说明的错， 但是我如果只是Thread.sleep(5000);，sleep个5秒之类的，不会有问题。。求指导这是啥原因。 基于最新版本1.0.25。 遇到过同样问题，从日志上看是canal server主动关闭了连接，可能是避免客户端堵死造成同步中断的策略 坐等权威回答 SimpleCanalConnector里设置的soTimeout会传递到server端进行超时管理，默认60秒 这个soTimeout时间的意思是，着这个时间段内，如果client没有处理完这批数据，server会终端client吗？我程序里client每批次取1000000条数据，处理时间达到1分钟以上，然后就导致这个异常，而且这1000000条数据会重复处理，然后每次都是1分钟以上，就循环报这个异常。把soTimeout改到120秒，就ok了。 这个问题怎么解决呢 https://github.com/alibaba/canal/issues/640，关注一下这个
470,canal的客户端总是报canal报com.alibaba.otter.canal.protocol.exception.CanalClientException: java.io.IOException: end of stream when reading header这个错误 一般是需要客户端重试 我也遇到同样问题关键是handshake 的时候出来的。 后面的ClientAuth 都还没发呢。 Caused by: java.io.IOException: end of stream when reading header 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.read(SimpleCanalConnector.java:401) ~[classes/:na] 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.readNextPacket(SimpleCanalConnector.java:392) ~[classes/:na] 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.readNextPacket(SimpleCanalConnector.java:373) ~[classes/:na] 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.doConnect(SimpleCanalConnector.java:140) ~[classes/:na] 	... 4 common frames omitted 
469,canal支持ddl吗？ 支持ddl解析的，wiki文档有讲。
468,CanalServerException: destination:example1 should start first (服务端如何确认实例启动) 开启auto.scan 之后在 conf下增加了一个新实例example1 在日志可以可以看见 2017-12-29 14:24:01.056 [canal-instance-scan-0] INFO  c.a.o.canal.deployer.monitor.SpringInstanceConfigMonitor - auto notify start example1 successful. 日志说明新加的实例启动了。 但是客户端访问和服务端纪录的日志都只有 exception=com.alibaba.otter.canal.server.exception.CanalServerException: destination:example1 should start first 这种异常。很奇怪的是在服务端没有任何关于example1的日志消息产生，example是能正常产生日志的。 所以想咨询一下如何确定实例到底启动没有，既没有日志输出，也没有服务异常抛出，只有访问时才会产生一条 should start first的话，很难让人去跟踪问题所在。 你把lazy模式给设置为false 开启instance自动扫描 如果配置为true，canal.conf.dir目录下的instance配置变化会自动触发： a. instance目录新增： 触发instance配置载入，lazy为true时则自动启动 b. instance目录删除：卸载对应instance配置，如已启动则进行关闭 c. instance.properties文件变化：reload instance配置，如已启动自动进行重启操作	 我是按这个提示把lazy 设置为 true的。原本默认就是false 我印象中lazy=false是默认值，lazy=true时会在第一次客户端访问时才会去初始化 哦，谢谢
467,AbstractEventParser的buildLastPosition函数，偏移量是不是该加上事件长度更合适？ com.alibaba.otter.canal.parse.inbound.AbstractEventParser Canal Deployer 1.0.24 原代码：     protected LogPosition buildLastPosition(CanalEntry.Entry entry) { // 初始化一下         LogPosition logPosition = new LogPosition();         EntryPosition position = new EntryPosition();         position.setJournalName(entry.getHeader().getLogfileName());         position.setPosition(entry.getHeader().getLogfileOffset());         position.setTimestamp(entry.getHeader().getExecuteTime());         // add serverId at 2016-06-28         position.setServerId(entry.getHeader().getServerId());         logPosition.setPostion(position);         LogIdentity identity = new LogIdentity(runningInfo.getAddress()  -1L);         logPosition.setIdentity(identity);         return logPosition;     } 但在setPosition时，这里写入的只是这个事件的开始位置。而我们在保存入ZK等位置时，我们并不希望这个事件回头再被处理一遍啊？是不是该改成这样：         position.setPosition(entry.getHeader().getLogfileOffset()+entry.getHeader().getEventLength()); 我改过之后测过可行，最终保存的就是Event的末尾了。我看代码中Canal一直很困扰重启时的开始位置可能不是一个事务的末尾，现在这样改一下，应该处理得好多了吧？ 我在代码中还加了只是XID、DDL等事件才真的保存偏移量，别的事件不保存。这样大不了重新处理，但不该再出现开始位置不在事务开头这样的事了吧。 收到了提醒，这些在直接Ack的模式下才有用。我的场景就是相当于直接Ack的，那这个改动只是对我有用，对Canal无用了吧。
466,canal报CanalParseException: parse row data failed. 错误 我根据https://github.com/alibaba/canal/issues/161 跑了FileLogFetcherTest这个测试。 java.io.IOException: Error binlog file header: [109  121  115  113] 是什么问题也。
465,canal同步出现死锁问题 怎么处理死锁？ 2017-12-25 14:18:54 295 (main) [INFO - com.trcloud.hamal.stream.core.operator.impl.sink.GPSinkOperator.processAutoCommit(GPSinkOperator.java:138)] current sql is: INSERT INTO ods_fina_bj.account_user("account_type" "balance" "bank_frozen" "create_date" "cust_no" "del_flag" "id" "local_frozen" "parent_id" "remarks" "update_date" "user_id" "storm_time"  "etl_time") VALUES('1' '0.0' '0.0' '2017-12-25 14:18:26' '201712251418330723046283' '0' '8a32f14f815a40f5868ba7f0c8286cfe' '0.0' null 'DTS借款人开户' '2017-12-25 14:18:26' 'C33DBA5ADEB0415DBC4AE36CA3B66E9B' '2017-12-25 14:18:26'  '2017-12-25 14:18:37'); 2017-12-25 14:19:04 297 (main) [ERROR - com.trcloud.hamal.stream.core.operator.impl.sink.GPSinkOperator.processAutoCommit(GPSinkOperator.java:159)] exec sql one by one failed:  org.postgresql.util.PSQLException: ERROR: deadlock detected   Detail: Process 71574 waits for RowExclusiveLock on relation 2388238 of database 23968; blocked by process 24634. Process 24634 waits for ExclusiveLock on relation 2388316 of database 23968; blocked by process 59646. Process 59646 waits for RowExclusiveLock on relation 2388238 of database 23968; blocked by process 35132. Process 35132 waits for ExclusiveLock on relation 2388238 of database 23968; blocked by process 71574. 	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2182) 	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:1911) 	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:173) 	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:622) 	at org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:458) 	at org.postgresql.jdbc.PgStatement.executeUpdate(PgStatement.java:406) 	at com.zaxxer.hikari.pool.ProxyStatement.executeUpdate(ProxyStatement.java:120) 	at com.zaxxer.hikari.pool.HikariProxyStatement.executeUpdate(HikariProxyStatement.java) 	at com.trcloud.hamal.stream.core.operator.impl.sink.GPSinkOperator.processAutoCommit(GPSinkOperator.java:139) 	at com.trcloud.hamal.stream.core.operator.impl.sink.GPSinkOperator.process(GPSinkOperator.java:92) 	at com.trcloud.hamal.stream.core.operator.impl.canal.CanalOperatorChain.start(CanalOperatorChain.java:49) 	at com.trcloud.hamal.stream.core.StreamJob.start(StreamJob.java:121) 	at com.trcloud.hamal.stream.core.JobRunner.main(JobRunner.java:46) 这不是原生canal支持的吧？看报错应该是扩展抽取pg数据出现的问题... canal 有没有办法规避或解决这个问题？@wingerx  例如以下参数是可以？ #canal batch size canal.batch.size=100 
463,canal ddl语句重新从之前执行过的时间再次执行一遍失败  需要在哪里过滤掉已经消费过的执行情况 2017-12-22 14:12:47 365 (main) [INFO - com.trcloud.hamal.stream.core.operator.impl.canal.CanalSource.parseColumns(CanalSource.java:238)] parse ddl sql : ALTER TABLE `account_user_info_stream_test` ADD COLUMN `test001` varchar(255) NOT NULL AFTER `modifiedon` 2017-12-22 14:12:47 375 (main) [INFO - com.trcloud.hamal.stream.core.operator.impl.canal.CanalSource.parseColumns(CanalSource.java:336)] ddl result map : {"test001":{"curName":"test001" "key":false "nulled":false "oldName":"" "operPimay":false "operType":"ADD" "type":"varchar(255)"}} 2017-12-22 14:12:47 390 (main) [INFO - com.trcloud.hamal.stream.core.operator.impl.canal.Canal2SqlOperator.transform(Canal2SqlOperator.java:77)] sqlDecorators: [{"eventType":"ALTER" "sql":"ALTER TABLE test.account_user_info_stream_test ADD COLUMN test001 varchar(255)  NOT NULL DEFAULT '' ;" "storm_time":"2017-12-22 14:11:23" "tableName":"account_user_info_stream_test"}] 2017-12-22 14:12:47 432 (main) [WARN - com.trcloud.hamal.stream.core.operator.impl.sink.GPSinkOperator.process(GPSinkOperator.java:92)] batch exec error  try exec one by one 2017-12-22 14:12:47 433 (main) [INFO - com.trcloud.hamal.stream.core.operator.impl.sink.GPSinkOperator.processAutoCommit(GPSinkOperator.java:141)] current sql is: ALTER TABLE test.account_user_info_stream_test ADD COLUMN test001 varchar(255)  NOT NULL DEFAULT '' ; 2017-12-22 14:12:47 465 (main) [ERROR - com.trcloud.hamal.stream.core.operator.impl.sink.GPSinkOperator.processAutoCommit(GPSinkOperator.java:168)] exec sql one by one failed:  org.postgresql.util.PSQLException: ERROR: column "test001" of relation "account_user_info_stream_test" already exists 	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2182) 	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:1911) 	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:173) 	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:622) 	at org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:458) 	at org.postgresql.jdbc.PgStatement.executeUpdate(PgStatement.java:406) 	at com.zaxxer.hikari.pool.ProxyStatement.executeUpdate(ProxyStatement.java:120) 	at com.zaxxer.hikari.pool.HikariProxyStatement.executeUpdate(HikariProxyStatement.java) 	at com.trcloud.hamal.stream.core.operator.impl.sink.GPSinkOperator.processAutoCommit(GPSinkOperator.java:142) 	at com.trcloud.hamal.stream.core.operator.impl.sink.GPSinkOperator.process(GPSinkOperator.java:95) 	at com.trcloud.hamal.stream.core.operator.impl.canal.CanalOperatorChain.start(CanalOperatorChain.java:49) 	at com.trcloud.hamal.stream.core.StreamJob.start(StreamJob.java:121) 	at com.trcloud.hamal.stream.core.JobRunner.main(JobRunner.java:46) 2017-12-22 14:12:47 468 (main) [ERROR - com.trcloud.hamal.stream.core.operator.impl.sink.GPSinkOperator.process(GPSinkOperator.java:97)] exec sql one by one failed: 200001: sink sql failed! 2017-12-22 14:12:49 165 (main) [ERROR - com.trcloud.hamal.stream.core.operator.impl.canal.CanalOperatorChain.start(CanalOperatorChain.java:51)] 200001: sink sql failed! 重复的数据只能是客户端进行记录和过滤
462,canal 时间戳指定 看文档说： canal.instance.master.timestamp : 指定一个时间戳，canal会自动遍历mysql binlog，找到对应时间戳的binlog位点后，进行启动 那么请问格式是什么？必须要同时写上position和timestamp吗？ canal.instance.standby.timestamp = 1513844601 报错，canal.instance.standby.timestamp = ‘2017-12-22 xx:xx:xx’ 也是一样的错误 错误如下截图： ![canal_err02](https://user-images.githubusercontent.com/34462344/34283126-062ed3c2-e705-11e7-93d5-802275d5a35b.png) position info conf: canal.instance.master.address = xxxxxx:3306 canal.instance.master.journal.name = MariaDB-bin.000001 canal.instance.master.position = canal.instance.master.timestamp = 1513845143 canal.instance.standby.address = xxxxxxxxx:3306 canal.instance.standby.journal.name = MariaDB-bin.000004 canal.instance.standby.position = canal.instance.standby.timestamp = 1513844601 写着对应的timestamp即可
461,改动destination的配置文件，结果整个Canal进程都退出了，按理不是该重启吗？ 各位好， 我用的是1.0.24，canal deployer。 主要配置： canal.destinations= example canal.conf.dir = ../conf canal.auto.scan = true canal.auto.scan.interval = 5 canal.instance.global.mode = spring  canal.instance.global.lazy = false canal.instance.global.spring.xml = classpath:spring/default-instance.xml ———————————————————— 然后我试着改动example/instance.properties，在black.regex中加了点东西，看到整个Canal都退出了。简单看了一下代码也没想明白为什么没重启example。 logs/example/example.log： 2017-12-20 15:31:15.241 [canal-instance-scan-0] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - stop CannalInstance for null-example  2017-12-20 15:31:15.241 [canal-instance-scan-0] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - disConnect MysqlConnection to edc0.ecloud.com/132.122.1.162:6606... 2017-12-20 15:31:15.246 [destination = example   address = /132.122.1.162:6606   EventParser] INFO  c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O interrupted while reading from client socket java.nio.channels.ClosedByInterruptException: null         at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202) ~[na:1.8.0_66]         at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:407) ~[na:1.8.0_66]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:154) ~[canal.parse-1.0.24.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70) ~[canal.parse-1.0.24.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122) [canal.parse-1.0.24.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:222) [canal.parse-1.0.24.jar:na]         at java.lang.Thread.run(Thread.java:745) [na:1.8.0_66] 2017-12-20 15:31:15.246 [destination = example   address = /132.122.1.162:6606   EventParser] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - the channel edc0.ecloud.com/132.122.1.162:6606 is not connected 2017-12-20 15:31:15.246 [destination = example   address = /132.122.1.162:6606   EventParser] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - disConnect MysqlConnection to edc0.ecloud.com/132.122.1.162:6606... 2017-12-20 15:31:15.247 [destination = example   address = /132.122.1.162:6606   EventParser] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - connect MysqlConnection to edc0.ecloud.com/132.122.1.162:6606... 2017-12-20 15:31:15.247 [destination = example   address = /132.122.1.162:6606   EventParser] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - handshake initialization packet received  prepare the client authentication packet to send 2017-12-20 15:31:15.248 [destination = example   address = /132.122.1.162:6606   EventParser] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - client authentication packet is sent out. 2017-12-20 15:31:15.248 [destination = example   address = /132.122.1.162:6606   EventParser] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - disConnect MysqlConnection to edc0.ecloud.com/132.122.1.162:6606... 2017-12-20 15:31:15.249 [canal-instance-scan-0] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - stop successful.... 2017-12-20 15:31:15.249 [canal-instance-scan-0] INFO  c.a.otter.canal.server.embedded.CanalServerWithEmbedded - stop CanalInstances[example] successfully logs/canal/canal.log： 2017-12-20 15:14:05.117 [main] INFO  com.alibaba.otter.canal.deployer.CanalLauncher - ## the canal server is running now ..... . 2017-12-20 15:31:15.250 [Thread-4] INFO  com.alibaba.otter.canal.deployer.CanalLauncher - ## stop the canal server 2017-12-20 15:31:15.254 [Thread-4] INFO  com.alibaba.otter.canal.deployer.CanalController - ## stop the canal server[132.122.1 .162:11111] 2017-12-20 15:31:15.254 [Thread-4] INFO  com.alibaba.otter.canal.deployer.CanalLauncher - ## canal server is down. 怎么关闭example这个Destination，就导致了JVM的退出呢？求指教，谢谢！ canal.pid 文件还在 Sorry 我把canalServer的启停给注释掉了，恢复回来就好了…… 
460,ddl 出现\305\241\305\202\342\211\240\305\233\304\226\303\241，直接打印是乱码 eventType: ALTER isDdl: true sql: "ALTER TABLE `account_user_info_stream_test`\r\nADD COLUMN `test001`  varchar(255) NULL DEFAULT \'\' COMMENT \'\305\241\305\202\342\211\240\305\233\304\226\303\241\' AFTER `modifiedon`" ddlSchemaName: "test" parse ddl sql : ALTER TABLE `account_user_info_stream_test` ADD COLUMN `test001` varchar(255) NULL DEFAULT '' COMMENT 'šł≠śĖá' AFTER `modifiedon` @agapple  这个该如何转码 默认应该是iso-8859-1的编码，按照这个转码一下看看 试过 new String(sql  "ISO8859-1")...  没用  从CanalEntry.Entry entry中获取 Entry result：header {   version: 1   logfileName: "mysql-bin.006352"   logfileOffset: 1073084521   serverId: 3   serverenCode: "UTF-8"   executeTime: 1513863317000   sourceType: MYSQL   schemaName: "test"   tableName: "account_user_info_stream_test"   eventLength: 203   eventType: ALTER } entryType: ROWDATA storeValue: "\020\005P\001Z\224\001alter table test.account_user_info_stream_test add  `test00001_14` varchar(25) not null  DEFAULT \'2\' comment \'\305\233\304\266\304\215\305\244\304\256\304\223\305\232\303\244\302\243\305\232\303\244\305\205\305\232\303\251\303\234\305\232\305\271\342\211\244\'r\004test" CanalEntry.RowChange rowChange = null;                 try {                     rowChange = CanalEntry.RowChange.parseFrom(entry.getStoreValue());             ............ RowChange result：eventType: ALTER isDdl: true sql: "alter table test.account_user_info_stream_test add  `test00001_14` varchar(25) not null  DEFAULT \'2\' comment \'\305\233\304\266\304\215\305\244\304\256\304\223\305\232\303\244\302\243\305\232\303\244\305\205\305\232\303\251\303\234\305\232\305\271\342\211\244\'" ddlSchemaName: "test" 是不是这里CanalEntry.RowChange.parseFrom 有问题 @agapple  CanalEntry.Column  中文输出是正常   CanalEntry.RowChange 中文好像是直接上面类似于8进制的一串 @agapple  你用debug模式，看一下从mysql binlog里收到的数据是啥吧，QueryLogEvent这个类 **因为没办法本地调试代码 我试了采用OpenReplicator解析MySQL binlog** 2017-12-25 19:03:29 469 [binlog-parser-1] INFO  [com.taihe.cloud.binlog.InstanceListener] - cdcEvent:{ eventId:66 databaseName:test tableName:`account_user_info_stream_test` add eventType:2 timestamp:1514199796000 timestampReceipt:1514199809467 binlogName:null position:398723955 nextPostion:398724150 serverId:3 isDdl:true sql:ALTER TABLE `account_user_info_stream_test` ADD COLUMN `test002`  varchar(255) NULL COMMENT '中文' AFTER `test00001_2` before:null after:null} {   "eventId": 66   "databaseName": "test"   "tableName": "`account_user_info_stream_test`\r\nadd"   "eventType": 2   "timestamp": 1514199796000   "timestampReceipt": 1514199809467   "binlogName": null   "position": 398723955   "nextPostion": 398724150   "serverId": 3   "before": null   "after": null   "isDdl": true   "sql": "ALTER TABLE `account_user_info_stream_test`\r\nADD COLUMN `test002`  varchar(255) NULL COMMENT \u0027中文\u0027 AFTER `test00001_2`" } **同样用canal  则显示解析乱码**  Entry result：header {   version: 1   logfileName: "mysql-bin.006360"   logfileOffset: 398723955   serverId: 3   serverenCode: "UTF-8"   executeTime: 1514199796000   sourceType: MYSQL   schemaName: "test"   tableName: "account_user_info_stream_test"   eventLength: 195   eventType: ALTER } entryType: ROWDATA storeValue: "\020\005P\001Z\200\001ALTER TABLE `account_user_info_stream_test`\r\nADD COLUMN `test002`  varchar(255) NULL COMMENT \'\305\241\305\202\342\211\240\305\233\304\226\303\241\' AFTER `test00001_2`r\004test" 2017-12-25 19:02:53 902 (main) [INFO - com.trcloud.hamal.stream.core.operator.impl.canal.CanalSource.fetchEvent(CanalSource.java:142)] RowChange result：eventType: ALTER isDdl: true sql: "ALTER TABLE `account_user_info_stream_test`\r\nADD COLUMN `test002`  varchar(255) NULL COMMENT \'\305\241\305\202\342\211\240\305\233\304\226\303\241\' AFTER `test00001_2`" ddlSchemaName: "test" 2017-12-25 19:02:53 903 (main) [INFO - com.trcloud.hamal.stream.core.operator.impl.canal.CanalSource.buildEvent(CanalSource.java:179)] RowChange result：eventType: ALTER isDdl: true sql: "ALTER TABLE `account_user_info_stream_test`\r\nADD COLUMN `test002`  varchar(255) NULL COMMENT \'\305\241\305\202\342\211\240\305\233\304\226\303\241\' AFTER `test00001_2`" ddlSchemaName: "test" 2017-12-25 19:02:53 904 (main) [INFO - com.trcloud.hamal.stream.core.operator.impl.canal.CanalSource.parseColumns(CanalSource.java:239)] parse ddl sql : ALTER TABLE `account_user_info_stream_test` ADD COLUMN `test002` varchar(255) NULL COMMENT 'šł≠śĖá' AFTER `test00001_2` 2017-12-25 19:02:53 914 (main) [INFO - com.trcloud.hamal.stream.core.operator.impl.canal.CanalSource.parseColumns(CanalSource.java:337)] ddl result map : {"test002":{"comment":"šł≠śĖá" "curName":"test002" "key":false "nulled":true "oldName":"" "operPimay":false "operType":"ADD" "type":"varchar(255)"}} 2017-12-25 19:02:53 954 (main) [INFO - com.trcloud.hamal.stream.core.operator.impl.canal.Canal2SqlOperator.transform(Canal2SqlOperator.java:77)] sqlDecorators: [{"eventType":"ALTER" "sql":"ALTER TABLE test.account_user_info_stream_test ADD COLUMN test002 varchar(255)  NULL DEFAULT '' ;COMMENT ON COLUMN test.account_user_info_stream_test.test002 IS 'šł≠śĖá';" "storm_time":"2017-12-25 19:03:16" "tableName":"account_user_info_stream_test"}] ![image](https://user-images.githubusercontent.com/834743/34376917-6f694958-eb29-11e7-81b1-90119700a9dd.png) 我这里验证可以拿到正确的编码，请确保建表，binlog dump的编码保持一致，我这里全部设置为utf8 binlog dump的编码保持一致是指哪块？ 建表及都看了 好像都是UTF8 mysql> show global variables like '%character%'; +--------------------------+-------------------------------------+ | Variable_name            | Value                               | +--------------------------+-------------------------------------+ | character_set_client     | utf8                                | | character_set_connection | utf8                                | | character_set_database   | utf8                                | | character_set_filesystem | binary                              | | character_set_results    | utf8                                | | character_set_server     | utf8                                | | character_set_system     | utf8                                | | character_sets_dir       | /usr/share/percona-server/charsets/ | +--------------------------+-------------------------------------+ 8 rows in set (0.00 sec) mysql>  canal client 配置也是UTF-8 通过官方开源canal.parse.dbsync 单元测试测下binlog QueryLogEvent 输出就乱码了，dml语句是正常的  ddl 有问题  ，binlog也查看了 也是中文的 16:47:31.330 [main] INFO  c.taobao.tddl.dbsync.binlog.LogEvent - common_header_len= 19  number_of_event_types= 35 sql : ALTER TABLE `account_user_info_stream_test` DROP COLUMN `test001_1` DROP COLUMN `testt223` sql : ALTER TABLE `account_user_info_stream_test` ADD COLUMN `test001`  varchar(255) NULL COMMENT 'śąĎśėĮšł≠śĖáÔľĆšĹ†šĻĪÁ†ĀšļÜŚźóÔľü' AFTER `modifiedon` sql : ALTER TABLE `account_user_info_stream_test` ADD COLUMN `test002`  varchar(255) NULL COMMENT 'śąĎŚįĪšłćšŅ°šĹ†šĻĪÁ†Ā' AFTER `test001` sql : ALTER TABLE `account_user_info_stream_test` ADD COLUMN `test003` varchar(255) NULL COMMENT 'śÄéšĻąŚŹĮšĽ•šĻĪÁ†ĀŚĎĘ' sql : ALTER TABLE `account_user_info_stream_test` ADD COLUMN `test004` varchar(255) NULL COMMENT 'šłļšĽÄšĻąšĻĪÁ†Ā' sql : ALTER TABLE `account_user_info_stream_test` ADD COLUMN `test005` varchar(255) NULL COMMENT 'šłļšĽÄšĻąšĻĪÁ†Ā112233' 最终排查只能到这里，想确认下这块是不是有bug com.taobao.tddl.dbsync.binlog.LogBuffer: /**      * Return next 16-bit unsigned int from buffer. (little-endian)      *       * @see mysql-5.1.60/include/my_global.h - uint2korr      */     public final int getUint16() {         if (position + 1 >= origin + limit) throw new IllegalArgumentException("limit excceed: "                                                                                + (position - origin + 1));         byte[] buf = buffer;         return (0xff & buf[position++]) | ((0xff & buf[position++]) << 8);     } 这段代码获取到45  不是33  想问下大神这里解析跟mysql版本有关吗？本地使用mysql版本5.6.27 charsetId	45	 javaCharset	"MacCentralEurope" (id=77)	 mysqlCharset	"utf8mb4" (id=80)	 mysqlCollation	"utf8mb4_general_ci" (id=81)	 麻烦问下最后怎么解决的，我也存在同样的现象。。。
459,如何重启实例而不重启server 请问如何重启实例，因为想要应用position 的位置， 如果stop 再startup 的话 会导致其他实例也会收到影响 移除文件夹，然后再加回来 @agapple 嗯 好的 我去试试看， 能否把这个方法写到wiki 里面 测试有效 关闭issue
458,如何设置 position 位置 我再实例里面设置 position 但是，实际获取的都是 最新的位置， 然后看了一下日志 show master status 这样一定是获取最新的日志，那么如何设置position ？ ![image](https://user-images.githubusercontent.com/11556152/34143020-7cd03918-e4c5-11e7-9d17-cf606f8bf555.png) ![image](https://user-images.githubusercontent.com/11556152/34143026-8cbb85f8-e4c5-11e7-8fbe-c2cb78226649.png) 我设置了timestmp 好了  我囧了 还有就是不要给position 后面有空格， 我就是这样好了的。。。 
457,Client中 subscribe  筛选失效了 你好，我使用的是canal 1.0.24 deployer 和 client 都是这个版本， 第一次我设置client 中subscribe  是生效的，获取数据是可以正常筛选的，但是我改了deployer 中positon 并且删除了 meta.dat 想重新插入一下数据，发现我能下载到其他库的数据，其实我只要两张表。 请问大神这是什么问题 客户端筛选为：dm_data.person dm_data.student 结果我下载到了其他数据库的数据 现在meta 数据可以看到 我只要dm_data 库的数据 ![image](https://user-images.githubusercontent.com/11556152/34139754-70e7f47e-e4b1-11e7-8a96-133c720f9a58.png) 现在下载到其他库的数据了 ![image](https://user-images.githubusercontent.com/11556152/34140115-7e1973be-e4b3-11e7-9150-d4e5f4e83c99.png)dm_ 试过几次可以稳定重现， 删除conf 中实例中的meta.dat   然后重启 重新指定instance.properties 中的position 就失效了， 现在workaround 就是在 instance.properties 再写一个筛选 client的subscribe的条件是每次都发送的吗？ 遇到类似的问题，筛选一大串表，个别表就监测不到数据，但，设置为只监测这个表时，可以顺利监测到数据 参考：https://github.com/alibaba/canal/wiki/FAQ
456,server ha 模拟 宕机 something goes wrong when getWithoutAck data from server:null something goes wrong when getWithoutAck data from server:null 请问怎么破，谢谢 选择用canal1.0.25最新版吗？ canal1.0.25现在也有这个问题。在canal的客户端报错。 我也碰到这个问题了   1.0.25 看一下服务端对应的异常 昨天测试的时候在客户端出现异常，没有发现服务器端的异常。测试的过程中主要在两个方面客户端出现异常1.客户端启动并多次连接服务器端 2 服务器端在进行HA切换的时候客户端出现这个异常。
455,mysql semi support and mariadb gtid parse mysql半同步semi协议包解析  mariadb gtid 解析 如果mysql  mariadb启动了semi plugin  程序启动时候-Ddb.semi=1 启用semi解析模式 [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=455) <br/>All committers have signed the CLA. tks
454,cancal bind ip address error Something goes wrong when starting up the canal Server: org.jboss.netty.channel.ChannelException: Failed to bind to xxxx 请问canal只能绑定本机ip吗？ 如果是台vm，分配了一个映射的ip，指定绑定此IP就会报如上的错误。 可以指定绑定的ip 1.0.25 绑定vm映射IP失败。只能绑定本机vm的私网ip。 ![canal_err01](https://user-images.githubusercontent.com/34462344/34280405-6547bbc0-e6f2-11e7-9f3f-a08c14c2ceec.png) 
453,在master/standby切换后，client端解析重复是否正常？ 如题，添加master/standby的配置后，之前解析一次，停止master mysql后切换到standby，解析sql又出现一次，是否正常？ ![tim 20171215145732](https://user-images.githubusercontent.com/34462344/34030567-942a6efa-e1a8-11e7-9943-604a372e450f.png) 如图，binlog是不一样的，但是解析了相同的sql，请指点是否正常？ 抱歉，请关闭，测试不正常。
452,Galera Mariadb cluster 配置多节点解析配置  请问，如果使用galera mariadb cluster的话，作为canal的master，根据文档 我需要写master和standby的信息，那么只需要配置各自的binlog file-name和position? 因为canal不支持GTID，3台的binlog信息不统一，能否达到正常切换的效果？ 可以使用时间戳
451,canal这个报错是什么没搞懂 canal版本  canal.deployer-1.0.23 mysql版本5.7.18-log 对方非要设置log-bin=master-bin并非log_bin=mysql-bin 不知道是不是这个引起，然后报错为 2017-12-14 18:25:40.835 [destination = cmp_source   address = /100.100.30.203:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position  {"identity":{"slaveId":-1 "sourceAddress":{"address":"100.100.30.203" "port":3306}} "postion":{"included":false "journalName":"master-bin.000001" "position":92307730 "serverId":312 "timestamp":1513246966000}} 2017-12-14 18:25:41.257 [destination = cmp_source   address = /100.100.30.203:3306   EventParser] WARN  com.taobao.tddl.dbsync.binlog.LogDecoder - Decoding Query failed from: master-bin.000001:92307826 java.io.IOException: Read Q_FLAGS2_CODE error: limit excceed: 71 	at com.taobao.tddl.dbsync.binlog.event.QueryLogEvent.unpackVariables(QueryLogEvent.java:650) ~[canal.parse.dbsync-1.0.23.jar:na] 	at com.taobao.tddl.dbsync.binlog.event.QueryLogEvent.<init>(QueryLogEvent.java:477) ~[canal.parse.dbsync-1.0.23.jar:na] 	at com.taobao.tddl.dbsync.binlog.LogDecoder.decode(LogDecoder.java:154) ~[canal.parse.dbsync-1.0.23.jar:na] 	at com.taobao.tddl.dbsync.binlog.LogDecoder.decode(LogDecoder.java:106) ~[canal.parse.dbsync-1.0.23.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:123) [canal.parse-1.0.23.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) [canal.parse-1.0.23.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_111] Caused by: java.lang.IllegalArgumentException: limit excceed: 71 	at com.taobao.tddl.dbsync.binlog.LogBuffer.getUint32(LogBuffer.java:561) ~[canal.parse.dbsync-1.0.23.jar:na] 	at com.taobao.tddl.dbsync.binlog.event.QueryLogEvent.unpackVariables(QueryLogEvent.java:563) ~[canal.parse.dbsync-1.0.23.jar:na] 	... 6 common frames omitted binlog解析错误，升级到1.0.25吧
449,当指定的pos刚好是rowdata(TableMap的下一条)数据时，canal server 端会陷入死循环BUG 测试版本：canal 版本 1.0.24 1. Canal在实现该功能时，通过LogEventConvert.parseRowsEvent 解析event时抛出 TableIdNotFoundException， 但是parseRowsEvent 直接捕获了所有Exception，并重新抛出 CanalParseException，这样AbstractEventParser的parseThread 工作线程获取的一直都是Throwable的异常，无法感知到TableIdNotFoundException，导致陷入死循环； 2. 修改LogEventConvert.parseRowsEvent 使其抛出TableIdNotFoundException后，下次循环线程会感知needTransactionPosition变量已经会true，进入findTransactionBeginPosition方法后，会调用seek方法，但seek方法只解析事务头/尾，那么当前的seek方法中实现的sink的event事件就解析不到(因为rowdata数据被过滤了)，故reDump 一直都不会为true，从而也不会进入从头读取binlog的逻辑内，就又进入了主线程的死循环内。 PS: 个人简单分析，不知是否理解有误，抑或是findTransactionBeginPosition不是为了解决这个场景的?@agapple 修复建议1： 1. 将LogEventConvert.parseRowsEvent 在catch CanalParseException前加个catch TableIdNotFoundException并将其抛出，让上层继续感知； 2. 将MysqlEventParser.findTransactionBeginPosition方法中的第一个mysqlConnection.seek改为mysqlConnection.dump 即可解决这个BUG 修复建议2：因为MysqlEventParser.findTransactionBeginPosition只在needTransactionPosition为true时调用，故删除MysqlEventParser.findTransactionBeginPosition方法中的第一个mysqlConnection.seek逻辑，直接让其进入reDump逻辑内也可解决这个问题，减少一次dump请求； 这个1.0.25我记得有修复过，可以先尝试一下 ok，我切换到1.0.25重新试一下 @agapple 我测了1.0.25的代码，通过以下场景仍然可以非常容易复现： 1. binlog文件内容 ![image](https://user-images.githubusercontent.com/33280738/33978900-d4cbc89a-e0dc-11e7-9104-bd4b3c083a0b.png) 图上标红的是rowdata 数据所在pos: 981 2. 指定 canal的启动位点即为 981这个位置 3. 启动server 观察日志如下： ![image](https://user-images.githubusercontent.com/33280738/33979023-720fb1de-e0dd-11e7-8baf-0c29da157c32.png) 可以看到一直处于循环启动的状态； 通过findTransactionBeginPosition的第一个mysqlconnection.seek方法是找不到 981这个event的，必须要让981 这个event 重新发生TableIdNotFoundException，reDump的逻辑才可执行。 这块我关注一下
448,请问mysql和mariadb的binlog有啥区别，canal对mysql和mariadb的binlog解析有啥区别 如题，请问mysql和mariadb的binlog有啥区别，canal对mysql和mariadb的binlog解析有啥区别？ canal/dbsync/src/main/java/com/taobao/tddl/dbsync/binlog/event/mariadb/BinlogCheckPointLogEvent.java在这个类下，  public BinlogCheckPointLogEvent(LogHeader header  LogBuffer buffer  FormatDescriptionLogEvent descriptionEvent){         super(header  buffer  descriptionEvent);         // do nothing   just mariadb binlog checkpoint     }中的// do nothing   just mariadb binlog checkpoint 是啥意思，这个类是怎么处理的 多看wiki，有描述 每找到，可以发个链接或者截图吗，谢谢 @agapple  wiki上"BinlogChange(MariaDB5&amp;10)"好像已经删掉了 那你还有原版的吗 链接修复了
447,解决destination无限连接等待的bug 当有多个destination同时与数据库建立连接时（我这里是8个以上），会有大概率发生部分destination得不到数据库反馈，以致无限的等待，而无法开始数据同步。 因此在初始化destination时，加入了timeout处理，如果超时（目前是3秒），则抛出异常，重试，以尝试再次建立连接。 [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=447) <br/>All committers have signed the CLA. tks
445,请问目前canal的版本支持mariadb10.0.2以上的GTID吗？ 如题，现在mariadb的稳定版本是10.2.10，可以使用GTID和canal做binlog解析吗？ 不支持GTID 那请问支持mysql5.7的GTID吗 gtid在26版本里已支持，可以测试一下
444,canal server 异常 'show binlog events limit 1’ 上午binlog日志被删，下午重启任务，canal client 日志没有异常，canal server异常： 2017-12-11 19:35:30.882 [destination = fengdai_mqnotify   address = /10.20.21.11:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /10.20.21.11:3306 has an error  retrying. caused by  com.alibaba.otter.canal.parse.exception.CanalParseException: command : 'show binlog events limit 1' has an error! Caused by: java.io.IOException: Unexpected End Stream 	at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.readBytesAsBuffer(PacketManager.java:22) ~[canal.parse.driver-1.0.24.jar:na] 	at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.readHeader(PacketManager.java:13) ~[canal.parse.driver-1.0.24.jar:na] 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.readNextPacket(MysqlQueryExecutor.java:104) ~[canal.parse.driver-1.0.24.jar:na] 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.query(MysqlQueryExecutor.java:77) ~[canal.parse.driver-1.0.24.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:73) ~[canal.parse-1.0.24.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPosition(MysqlEventParser.java:610) ~[canal.parse-1.0.24.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findByStartTimeStamp(MysqlEventParser.java:514) ~[canal.parse-1.0.24.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPositionInternal(MysqlEventParser.java:358) ~[canal.parse-1.0.24.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPosition(MysqlEventParser.java:315) ~[canal.parse-1.0.24.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:161) ~[canal.parse-1.0.24.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_66] 想确认下为何为何上午日志还是能正常写入，而重启会出现这种情况 @agapple  com.alibaba.otter.canal.parse.exception.CanalParseException: command : 'show binlog events limit 1' has an error! Caused by: java.io.IOException: Unexpected End Stream 重启下mysql或重新开启binlog就好了嘛？
443,canal设置ZooKeeperMetaManager 1.canal在zookeeper在切换HA时，会存在重复消费问题 2.canal不能很好的修改去position 基于以上两点，想要切换至ZooKeeperMetaManager，故目前通过修改XXX-instance.xml来进行，但会报错：org.I0Itec.zkclient.exception.ZkNoNodeException: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /otter/canal/destinations/example/1001/mark，即使手动添上节点后，元数据依旧没有只存在zokeeper中，还是先存在内存然后定时刷到zk中的 @agapple  请问，解决上述两个问题有更好的方式吗？如何可以更优雅的指定position的位置来进行消费 完整的错误： exception=org.I0Itec.zkclient.exception.ZkNoNodeException: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /otter/canal/destinations/example/1001/mark 	at org.I0Itec.zkclient.exception.ZkException.create(ZkException.java:47) 	at org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:685) 	at org.I0Itec.zkclient.ZkClient.getChildren(ZkClient.java:413) 	at org.I0Itec.zkclient.ZkClient.getChildren(ZkClient.java:409) 	at com.alibaba.otter.canal.meta.ZooKeeperMetaManager.clearAllBatchs(ZooKeeperMetaManager.java:231) 	at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.rollback(CanalServerWithEmbedded.java:406) 	at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:182) 	at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:100) 	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) 	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:783) 	at org.jboss.netty.channel.SimpleChannelHandler.messageReceived(SimpleChannelHandler.java:154) 	at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:100) 	at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) 	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) 	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:783) 	at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:275) 	at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80) 	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) 	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:783) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndFireMessageReceived(ReplayingDecoder.java:525) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:506) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:443) 	at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80) 	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) 	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261) 	at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:349) 	at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:280) 	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:200) 	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) 	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:44) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) 	at java.lang.Thread.run(Thread.java:748) Caused by: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /otter/canal/destinations/example/1001/mark 	at org.apache.zookeeper.KeeperException.create(KeeperException.java:111) 	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51) 	at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1468) 	at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1496) 	at com.alibaba.otter.canal.common.zookeeper.ZooKeeperx.getChildren(ZooKeeperx.java:107) 	at org.I0Itec.zkclient.ZkClient$2.call(ZkClient.java:416) 	at org.I0Itec.zkclient.ZkClient$2.call(ZkClient.java:413) 	at org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:675) 	... 34 more 理论设计上就没法保证不重复 @agapple   如果元数据只存在zk中，在重复的这个问题上会稍微好一些，但程序整体的效率会减低，这是取舍的问题，这个理解。 另外想请问一下， canal 1.0.24 想要切换至ZooKeeperMetaManager，是否可以通过修改XXX-instance.xml来进行，虽然这样会在一定的程度上降低程序的效率(尝试过但失败了)？ 另外是否可以有办法优雅的指定位置来进行消费，不通过重启，比如只是修改zk中position，即可实现？ 目前暂时没有这样的API可以直接指定位点启动
442,table meta生成snapshot比较方法有问题 DatabaseTableMeta.applySnapshotToDB()方法中，先获取一个memoryTableMeta的副本tmpMemoryTableMeta，然后再去master库中去对比，然后如果一切正常则持久化snapshot. 但在和master库中ddl做对比时，却meta信息是从memoryTableMeta而不是tmpMemoryTableMeta，见compareTableMetaDbAndMemory()方法。 这样在多线程情况下，会导致数据不一致情况。 fix issue：对比时从tmpMemoryTableMeta中取meta信息。 >>>>>> private boolean compareTableMetaDbAndMemory(MysqlConnection connection  final String schema  final String table) {         TableMeta tableMetaFromMem = memoryTableMeta.find(schema  table); <<<<<< private boolean compareTableMetaDbAndMemory(MysqlConnection connection  final String schema  final String table，MemoryTableMeta            tmpMemoryTableMeta) {         TableMeta tableMetaFromMem = tmpMemoryTableMeta.find(schema  table); ...... 看的非常仔细，对这块TSDB比较理解，会有一个并发问题
441,canal客户端未提示错误，server端异常：batchId:414 is not the firstly:404，请问这个是什么原因导致 @agapple  想问下以下是什么情况导致，求助 2017-12-07 12:11:57.887 [New I/O server worker #1-4] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - subscribe filter change to bank_supervised_account_10.account_item_detail_rel_user bank_supervised_account_10.account_itemized_err_user bank_supervised_account_10.account_detail_err_user bank_supervised_account_10.account_detail_user bank_supervised_account_10.account_frozen_user bank_supervised_account_10.account_itemized_user bank_supervised_account_10.account_user bank_supervised_account_10.bank_user bank_supervised_account_10.company_user_info 2017-12-07 12:12:04.964 [New I/O server worker #1-4] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400   Caused by :  something goes wrong with channel:[id: 0x1fe5e36c  /10.203.151.61:38324 => /10.203.151.17:11111]  exception=com.alibaba.otter.canal.meta.exception.CanalMetaManagerException: batchId:414 is not the firstly:404 2017-12-07 12:12:04.965 [New I/O server worker #1-4] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x1fe5e36c  /10.203.151.61:38324 :> /10.203.151.17:11111]  exception=java.nio.channels.ClosedChannelException 	at org.jboss.netty.channel.socket.nio.NioWorker.cleanUpWriteBuffer(NioWorker.java:643) 	at org.jboss.netty.channel.socket.nio.NioWorker.writeFromUserCode(NioWorker.java:370) 	at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:137) 	at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) 	at org.jboss.netty.channel.Channels.write(Channels.java:611) 	at org.jboss.netty.channel.Channels.write(Channels.java:578) 	at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:28) 	at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:144) 	at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) 	at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:275) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndFireMessageReceived(ReplayingDecoder.java:525) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:506) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.cleanup(ReplayingDecoder.java:541) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.channelDisconnected(ReplayingDecoder.java:449) 	at org.jboss.netty.channel.Channels.fireChannelDisconnected(Channels.java:360) 	at org.jboss.netty.channel.socket.nio.NioWorker.close(NioWorker.java:593) 	at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:119) 	at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) 	at org.jboss.netty.channel.Channels.close(Channels.java:720) 	at org.jboss.netty.channel.AbstractChannel.close(AbstractChannel.java:200) 	at org.jboss.netty.channel.ChannelFutureListener$1.operationComplete(ChannelFutureListener.java:46) 	at org.jboss.netty.channel.DefaultChannelFuture.notifyListener(DefaultChannelFuture.java:381) 	at org.jboss.netty.channel.DefaultChannelFuture.addListener(DefaultChannelFuture.java:148) 	at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:30) 	at com.alibaba.otter.canal.server.netty.NettyUtils.error(NettyUtils.java:51) 	at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:200) 	at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) 	at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:275) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndFireMessageReceived(ReplayingDecoder.java:525) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:506) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:443) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261) 	at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:349) 	at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:280) 	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:200) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) batchId:414 is not the firstly:404，有乱序消费吗？ 这种情况怎么处理  跟之前代码上没什么太大改变  重新停掉作业开启就出现这个问题 已经找到问题了  谢谢了  自己sql解析代码上传空问题导致
440,默认开启tsdb 系统启动有异常 使用的1.0.25版本 其中默认开启了tsdb 启动后会报错 ``` 2017-12-07 17:16:23.254 [destination = example   address = /127.0.0.1:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /127.0.0.1:3306 has an error  retrying. caused by java.lang.NullPointerException: null         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPositionInternal(MysqlEventParser.java:428) ~[canal.parse-1.0.25.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPosition(MysqlEventParser.java:348) ~[canal.parse-1.0.25.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:164) ~[canal.parse-1.0.25.jar:na]         at java.lang.Thread.run(Unknown Source) [na:1.8.0_144] 2017-12-07 17:16:23.256 [destination = example   address = /127.0.0.1:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.lang.NullPointerException         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPositionInternal(MysqlEventParser.java:428)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPosition(MysqlEventParser.java:348)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:164)         at java.lang.Thread.run(Unknown Source) ``` 将 canal.instance.tsdb.spring.xml=classpath:spring/tsdb/h2-tsdb.xml 注释就好了 canal.instance.tsdb.enable设为false不起作用 另外希望有这些参数的详细说明 你初始启动的binlog位点指定了啥，照理不应该是有NPE啊，不会是指定了offest=4吧？ 配置如下: ```    canal.instance.master.journal.name= mysql-bin.000001    canal.instance.master.position= 154    canal.instance.master.timestamp= ``` 看到上面有代码修改 但是只下canal编译不通过 得有druid的preview版本 是不是要修改一下 不方便测试 https://github.com/alibaba/druid，可以临时下载druid包，执行mvn clean install -Dmaven.test.skip即可生成preview版本 基于table tsdb的配置，如果在没有对应位点的timestamp时，会尝试通过binlog查找来确定时间戳，如果找不到就是这里的NPE.    table meta的历史多版本设计，主要是基于时间戳来定位，所以启动时任何一个时间点都必须要有一个时间戳 遇到同样的问题，切到 v1.0.26 的版本上 OK
439,parse faield : CREATE TABLE `columns_priv` Example的日志发现这个错误 应该是群里反馈的set类型不支持的问题，已经反馈给druid进行修复 https://github.com/alibaba/druid/commit/f8731f9182f01353e53331cf8ad590c6ca3db416  Okay，我们暂时把这个功能给禁用掉了。
437,在LogEventConvert.parseOneRow方法中，添加扩展字段，client端，出现重复数据 在LogEventConvert.parseOneRow方法中， 添加扩展字段：rowDataBuilder.addProps(createSpecialPair("testName" "testValue")); client 端： CanalEntry.RowData接收到重复key（testName） 的 Pair  这种方式才可以，不会重复，Pair.Builder builder = rowDataBuilder.addPropsBuilder(); 没看懂具体问题
436,1.0.24版本HA模式部署，新增instance后server端异常，客户端未提示错误 2017-12-05 19:07:43.385 [New I/O server worker #1-10] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400   Caused by :  something goes wrong with channel:[id: 0x4926cac6  /192.168.182.17:2025 => /192.168.182.13:11111]  exception=com.alibaba.otter.canal.server.exception.CanalServerException: ack error   clientId:1001 batchId:1 is not exist   please check 2017-12-05 19:07:43.390 [New I/O server worker #1-10] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x4926cac6  /192.168.182.17:2025 :> /192.168.182.13:11111]  exception=java.nio.channels.ClosedChannelException 	at org.jboss.netty.channel.socket.nio.NioWorker.cleanUpWriteBuffer(NioWorker.java:643) 	at org.jboss.netty.channel.socket.nio.NioWorker.writeFromUserCode(NioWorker.java:370) 	at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:137) 	at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) 	at org.jboss.netty.channel.Channels.write(Channels.java:611) 	at org.jboss.netty.channel.Channels.write(Channels.java:578) 	at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:28) 	at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:144) 	at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) 	at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:275) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndFireMessageReceived(ReplayingDecoder.java:525) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:506) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.cleanup(ReplayingDecoder.java:541) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.channelDisconnected(ReplayingDecoder.java:449) 	at org.jboss.netty.channel.Channels.fireChannelDisconnected(Channels.java:360) 	at org.jboss.netty.channel.socket.nio.NioWorker.close(NioWorker.java:593) 	at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:119) 	at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) 	at org.jboss.netty.channel.Channels.close(Channels.java:720) 	at org.jboss.netty.channel.AbstractChannel.close(AbstractChannel.java:200) 	at org.jboss.netty.channel.ChannelFutureListener$1.operationComplete(ChannelFutureListener.java:46) 	at org.jboss.netty.channel.DefaultChannelFuture.notifyListener(DefaultChannelFuture.java:381) 	at org.jboss.netty.channel.DefaultChannelFuture.addListener(DefaultChannelFuture.java:148) 	at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:30) 	at com.alibaba.otter.canal.server.netty.NettyUtils.error(NettyUtils.java:51) 	at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:200) 	at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) 	at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:275) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndFireMessageReceived(ReplayingDecoder.java:525) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:506) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:443) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261) 	at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:349) 	at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:280) 	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:200) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) 	at java.lang.Thread.run(Thread.java:745) clientId:1001 batchId:1 is not exist   please check 估计触发instance reload了，客户端重试一下即可 重启客户端解决了，多谢
435,启动canal-client后如何关闭zookeeperDebug模式。 如题，在哪里配置关闭zookeeperDebug模式。 google搜索
434,org.jboss.netty.channel.ChannelException: Failed to bind to: 0.0.0.0/0.0.0.0:11111 canal启动之后，canal.log中报错，不知如何解决，求大神赐教~ OpenJDK 64-Bit Server VM warning: ignoring option PermSize=96m; support was removed in 8.0 OpenJDK 64-Bit Server VM warning: ignoring option MaxPermSize=256m; support was removed in 8.0 org.jboss.netty.channel.ChannelException: Failed to bind to: 0.0.0.0/0.0.0.0:11111         at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:303)         at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)         at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74) OpenJDK 64-Bit Server VM warning: ignoring option PermSize=96m; support was removed in 8.0 OpenJDK 64-Bit Server VM warning: ignoring option MaxPermSize=256m; support was removed in 8.0 OpenJDK 64-Bit Server VM warning: UseCMSCompactAtFullCollection is deprecated and will likely be removed in a future release. 2017-12-04 19:36:38.282 [main] INFO  com.alibaba.otter.canal.deployer.CanalLauncher - ## start the canal server. 2017-12-04 19:36:38.340 [main] INFO  com.alibaba.otter.canal.deployer.CanalController - ## start the canal server[10.108.211.136:11111] 2017-12-04 19:36:38.736 [main] ERROR com.alibaba.otter.canal.deployer.CanalLauncher - ## Something goes wrong when starting up the canal Server: org.jboss.netty.channel.ChannelException: Failed to bind to: 0.0.0.0/0.0.0.0:11111         at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:303)         at com.alibaba.otter.canal.server.netty.CanalServerWithNetty.start(CanalServerWithNetty.java:79)         at com.alibaba.otter.canal.deployer.CanalController.start(CanalController.java:418)         at com.alibaba.otter.canal.deployer.CanalLauncher.main(CanalLauncher.java:35) Caused by: java.net.BindException: Address already in use         at sun.nio.ch.Net.bind0(Native Method)         at sun.nio.ch.Net.bind(Net.java:433)         at sun.nio.ch.Net.bind(Net.java:425)         at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)         at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)         at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.bind(NioServerSocketPipelineSink.java:148)         at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleServerSocket(NioServerSocketPipelineSink.java:100)         at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:74)         at org.jboss.netty.channel.Channels.bind(Channels.java:468)         at org.jboss.netty.channel.AbstractChannel.bind(AbstractChannel.java:192)         at org.jboss.netty.bootstrap.ServerBootstrap$Binder.channelOpen(ServerBootstrap.java:348)         at org.jboss.netty.channel.Channels.fireChannelOpen(Channels.java:176)         at org.jboss.netty.channel.socket.nio.NioServerSocketChannel.<init>(NioServerSocketChannel.java:85)         at org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory.newChannel(NioServerSocketChannelFactory.java:142)         at org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory.newChannel(NioServerSocketChannelFactory.java:90)         at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:282)         ... 3 more Caused by: java.net.BindException: Address already in use 端口冲突
433,canal client 拿到的数据重复 cannal 客户端 拿数据 第一次拿是 batchId=1 size=1 msg=Message[id=1 exit 后第二次拿数据变成 batchId=2 size=2 msg=Message[id=2 而且第二次拿到的是 两条重复的数据  有一样的 logfileName: "mysql-bin.000012"   logfileOffset: 3207300 重启canal server后，启动 canal客户端 又变成第一次拿数据，之后再拿数据都会重复，请问这个怎么解决，总不能每次调试都 重启canal server吧 ``` CanalConnector connector = CanalConnectors.newSingleConnector(new InetSocketAddress("192.168.157.130"  11111)  "example"  ""  "");         int batchSize = 10;         int emptyCount = 0;         try {             connector.connect();             connector.subscribe(".*\\..*");             connector.rollback();             int totalEmtryCount = 120;             while (emptyCount < totalEmtryCount) {                 Message message = connector.getWithoutAck(batchSize); // 获取指定数量的数据                 long batchId = message.getId();                 int size = message.getEntries().size();                 if (batchId == -1 || size == 0) {                     emptyCount++;                     System.out.println("empty count : " + emptyCount);                     try {                         Thread.sleep(1000);                     } catch (InterruptedException e) {                     }                 } else {                     emptyCount = 0;                      System.out.printf("message[batchId=%s size=%s msg=%s] \n"  batchId  size  message.toString());                      System.exit(0);                      printEntry(message.getEntries());                 }                 connector.ack(batchId); // 提交确认                 // connector.rollback(batchId); // 处理失败  回滚数据             }             System.out.println("empty too many times  exit");         } finally {             connector.disconnect();         } ``` 重复数据无法避免
432,直接加入把db event push到kafka，最方便的做法改动哪块比较好？ 把canal当databus用。目前是写个简易的canalClient，连接canal server指定的Destination  但是这样client存在单点风险。   想直接把发送kafka消息集中到canal内，利用已有的zk 热备的模式，避免单点.    cannal源码从哪块做改动，插入这个功能比较好？ canal 的client也可以是HA的 @wingerx 谢谢，那我去看下client代码去了。这样最好了。
431,connector.getWithoutAck方法的batchSize不起作用 使用过程中发现，无论batchSize传多少，起作用的是canal.properties里配置的buffer size而非方法入参batchSize。 bufferSize是一个当前内存里的最大记录值，你是不是设置太小了？batchSize > bufferSize @agapple 感谢回复！ 我试了发现，batchSize是作用的，实际起作用的getWithoutAck batchSize * memunit。 比如 canal.instance.memory.buffer.size=16384 canal.instance.memory.buffer.memunit = 1024 canal.instance.memory.batch.mode = MEMSIZE 此时内存中的ring buffer大小应为16384 * 1024 B 如果客户端代码中getWithoutAck传2，那么每次批量获取的binlog数据量是不超过或略微超过2 * 1024 B，受server配置中的canal.instance.memory.buffer.memunit和canal.instance.memory.batch.mode配置影响。 我理解的对不对？ 是的，理解正确
430,cannal客户端阻塞 客户端在获取Message时阻塞，阻塞以后无法获取数据，设置超时时间也不管用，在重启以后才可以获取数据。 Message message = connector.getWithoutAck(batchSize) 可能是遇到大事务，canal的buffer size不够 这么解决的？ 估计还是tcp链接被mysql主动断开，canal server未感知的问题。尝试升级一下版本到26 设置超时就可以了。
429,能从canal能直接得到binlog的sql吗？ example中的例子，打印的是类似以下的形似，canal能给出执行insert，update，delete，create table等命令的具体sql语句吗？delete from sharddb. tab_shard where id =5;  BEGIN ----> Thread id: 18 ----------------> binlog[mysql-bin.000005:1200]   name[sharddb tab_shard]   eventType : DELETE   executeTime : 1512015314000   delay : 709ms id : 5    type=int(10) unsigned randomstr : 14504    type=varchar(100) create_time : 2017-11-28 05:00:35    type=timestamp ----------------  END ----> transaction id: 20565 ================> binlog[mysql-bin.000005:1251]   executeTime : 1512015314000   delay : 709ms binlog格式可以网上先了解一下
428,cannel service  can't find start position for XX 想问下这是什么原因导致 2017-11-29 16:17:36.772 [canal-instance-scan-0] INFO  c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-crm_underline_prod  2017-11-29 16:17:36.775 [canal-instance-scan-0] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - start successful.... 2017-11-29 16:17:41.864 [destination = crm_underline_prod   address = /10.203.8.48:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position ::1502794800000 2017-11-29 16:17:42.170 [destination = crm_underline_prod   address = /10.203.8.48:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - Didn't find the corresponding binlog files from binlog.000002 to binlog.000020 2017-11-29 16:17:42.171 [destination = crm_underline_prod   address = /10.203.8.48:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /10.203.8.48:3306 has an error  retrying. caused by  com.alibaba.otter.canal.parse.exception.CanalParseException: can't find start position for crm_underline_prod 2017-11-29 16:17:42.171 [destination = crm_underline_prod   address = /10.203.8.48:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:crm_underline_prod[com.alibaba.otter.canal.parse.exception.CanalParseException: can't find start position for crm_underline_prod ] 你指定的位点时间戳，找不到合适的binlog，初步怀疑位点太久了 @agepple 位点太旧 是instance.properties下的timestamp 还是zookeeper get /otter/canal/destinations/xxxx/1001/cursor的timestamp 看你使用file-instance.xml还是default-instance.xml，前者是文件后者是zk @agapple  我使用的是 default-instance.xml 处理这种zk位点过旧的情况，怎么处理，是只要更改get /otter/canal/destinations/xxxx/1001/cursor的timestamp吗？ 还有个问题想咨询下，位点时间戳必须在binlog开启之后吗？ 目前存在一个问题就是时间戳时间在binlog开启之前的时间也会提示以上问题异常 是的，给定的时间戳必须是一个binlog有的之后时间
427,首页readme.md里面的 canal的工作原理 图片没有显示完整 估计你需要翻墙看吧 like this ： ![图片](http://ww4.sinaimg.cn/large/0060lm7Tly1fm5mvsvfizj31kw0xt459.jpg) @agapple  http://dl.iteye.com/upload/attachment/0080/3107/c87b67ba-394c-3086-9577-9db05be04c95.jpg，图片源地址
426,阿里云RDS数据库切换时会报错（Connection timed out） 同步数据源时， 如果使用的是MHA的数据库，在高可用切换时， 数据库主库发生了变化  这时 canal同步日志 发生了改变， 会报错 . 报错如下： Caused by: java.io.IOException: connect rm-2zed22tz2539.mysql.rds.aliyuncs.com/100.114.42.154:3306 failure:java.net.ConnectException: Connection timed out         at sun.nio.ch.Net.connect0(Native Method)         at sun.nio.ch.Net.connect(Net.java:454)         at sun.nio.ch.Net.connect(Net.java:446)         at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648)         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:70)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:56)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:86)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:157)         at java.lang.Thread.run(Thread.java:745)         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:74)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:56)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:86)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:157)         at java.lang.Thread.run(Thread.java:745) 连不上数据库 Connection timed out
425,canal binlog 丢失 现网数据变更较大，发现数据存在遗漏的情况，定位在binlog未接收到事件信息。 对比发现binlog 29827255 未知的信息丢失 dump binlog ![image](https://user-images.githubusercontent.com/8357717/33230017-72c7cf0e-d1a0-11e7-94e6-981ab2367e25.png) canal log ![image](https://user-images.githubusercontent.com/8357717/33230032-0865fa2c-d1a1-11e7-90ae-a62fc76fbbe5.png) ![image](https://user-images.githubusercontent.com/8357717/33230062-a4101d04-d1a1-11e7-83d1-c151ca6b771f.png) 日志丢失的问题看源码找到原因了，但是binlog接收处确实未接收到，log每次会将接收到的binlog打印出来，29827255未被接收，处理代码如下 ``` void execute() {         long batchId;         LOGGER.debug("execute destination : " + destination);         while (true) {             try {                 connector.connect();                 connector.subscribe(filter);                 while (true) {                     Message message = connector.getWithoutAck(BATCH_SIZE);                     batchId = message.getId();                     int size = message.getEntries().size();                     if (batchId == -1 || size == 0) {                         try {                             Thread.sleep(SLEEP_TIME);                         } catch (InterruptedException ignored) {                             // ignored                         }                     } else {                         process(message.getEntries());                     }                     connector.ack(batchId);                 }             } catch (Exception e) {                 LOGGER.error("canal connect error  destination : " + destination  e);                 AlarmUtil.dcAlarm(App.DC_ID  "canal_connect_error"                         "canal connect error  destination : " + destination);             } finally {                 connector.disconnect();             }             try {                 Thread.sleep(1000 * 60);             } catch (InterruptedException e) {                 // ignored             }         }     }     private void process(List<CanalEntry.Entry> entryList) {         for (CanalEntry.Entry entry : entryList) {             if (entry.getEntryType() == CanalEntry.EntryType.TRANSACTIONBEGIN                     || entry.getEntryType() == CanalEntry.EntryType.TRANSACTIONEND) {                 continue;             }             if (entry.getEntryType() != CanalEntry.EntryType.ROWDATA) {                 continue;             }             CanalEntry.RowChange rowChange;             try {                 rowChange = CanalEntry.RowChange.parseFrom(entry.getStoreValue());             } catch (Exception e) {                 throw new RuntimeException("parse event has an error   data:" + entry.toString()  e);             }             CanalEntry.EventType eventType = rowChange.getEventType();             if (eventType == CanalEntry.EventType.DELETE) {                 return;             }             if (eventType != CanalEntry.EventType.INSERT && eventType != CanalEntry.EventType.UPDATE) {                 return;             }             List<CanalEntry.RowData> rowDataList = rowChange.getRowDatasList();             if (rowDataList == null || rowDataList.size() == 0) {                 continue;             }             List<BinlogColumnDTO> columns = new ArrayList<>();             try {                 for (CanalEntry.RowData rowData : rowDataList) {                     columns = convertColumnList(rowData.getAfterColumnsList());                     long updatedCount = columns.stream().filter(BinlogColumnDTO::getUpdated).count();                     if (updatedCount < 1) {                         return;                     }                     String binlogFileOffset = entry.getHeader().getLogfileName() + ":"                             + entry.getHeader().getLogfileOffset() + ":"                             + DateUtil.timestamp2DateTime(entry.getHeader().getExecuteTime());                     LOGGER.log(ACCESS  binlogFileOffset + "  eventType : {}  data : {}"                             eventType  toJsonString(columns));                     if (eventType == CanalEntry.EventType.INSERT) {                         syncService.insert(columns  binlogFileOffset);                     }                     else if (eventType == CanalEntry.EventType.UPDATE){                         syncService.update(columns  binlogFileOffset);                     }                 }             }             catch (Exception e) {                 LOGGER.error("binlog handle error  data : " + JSONArray.toJSONString(columns)  e);                 AlarmUtil.dcAlarm(App.DC_ID  "binlog_handle_error"  e.toString());             }         }     } ``` 跟踪源码，canal server打印debug日志，发现client发请求过来的时候是有读取到对应postion的，但就是没有返回事件内容过去。 另一个案例的截图  binlog file ![image](https://user-images.githubusercontent.com/8357717/33230711-53efdf28-d1ae-11e7-87ac-229d440857f0.png) canal server log ![image](https://user-images.githubusercontent.com/8357717/33230713-6129ee18-d1ae-11e7-899c-58cdc8d1e51c.png) 1.  你这最后的日志是在位点定位的时候得，不是数据读取的日志 2.  canal log里记录的位点是以批次为单位，并没有精确到一条记录.   确认过滤条件没问题吧？ 只过滤了一个表，如果是过滤条件的问题，那应该一条记录都获取不到啊，目前的情况时部分binlog丢失 instance.properties # table regex canal.instance.filter.regex = p2p.t_loan # table black regex canal.instance.filter.black.regex = 传递的filter：p2p.t_loan 有结论吗？我这边最新版26也发现更新记录有丢失的问题。 @JasonHuangHuster 请问解决了么，我1.1.0也出现1000条数据丢一两条的问题 建议你们先用标准的example工程打印接收到的数据，如果有能复现的方式最好能提供一下 有解决吗？我这边也遇到binlog丢失的情况 我用的是canal-1.0.24，数据库用的是MariaDB 10.1.19  @agapple 我发现当数据丢失的时候，canal日志会报解析异常 我的部署情况： 1. 2台canal HA模式 2.  2个实例监控两个mysql master 3. 使用mycat分片，每个master上有8个片 4. mysql使用GTID ROW 这是错误日志： 2018-09-12 17:08:12.989 [New I/O server worker #1-7] INFO  c.a.otter.canal.instance.core.AbstractCan alInstance - subscribe filter change to immig([1-8]|1[0-8])\.immig_enc_text 2018-09-12 17:09:37.077 [destination = IMMIG_200   address = /172.16.40.200:3306   EventParser] ERRO R c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address 172.16.40.200/172.16.40.200:3 306 has an error  retrying. caused by  com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception .CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: java.util.ConcurrentModificationException: null 	at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:909) ~[na:1.8.0_171] 	at java.util.ArrayList$Itr.next(ArrayList.java:859) ~[na:1.8.0_171] 	at com.alibaba.otter.canal.parse.driver.mysql.packets.UUIDSet.toString(UUIDSet.java:125) ~[c anal.parse.driver-1.1.0.jar:na] 	at com.alibaba.otter.canal.parse.driver.mysql.packets.MysqlGTIDSet.toString(MysqlGTIDSet.jav a:111) ~[canal.parse.driver-1.1.0.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.createHeader(LogEventC onvert.java:849) ~[canal.parse-1.1.0.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEven tConvert.java:561) ~[canal.parse-1.1.0.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$DmlParserStage.onE vent(MysqlMultiStageCoprocessor.java:302) ~[canal.parse-1.1.0.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$DmlParserStage.onE vent(MysqlMultiStageCoprocessor.java:288) ~[canal.parse-1.1.0.jar:na] 	at com.lmax.disruptor.WorkProcessor.run(WorkProcessor.java:143) ~[disruptor-3.4.2.jar:na] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8. 0_171] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8. 0_171] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171] 2018-09-12 17:09:37.078 [destination = IMMIG_200   address = /172.16.40.200:3306   EventParser] ERRO R com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:IMMIG_200[com.alibaba.otter.can al.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException:  parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: java.util.ConcurrentModificationException 	at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:909) 	at java.util.ArrayList$Itr.next(ArrayList.java:859) 	at com.alibaba.otter.canal.parse.driver.mysql.packets.UUIDSet.toString(UUIDSet.java:125) 	at com.alibaba.otter.canal.parse.driver.mysql.packets.MysqlGTIDSet.toString(MysqlGTIDSet.jav a:111) 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.createHeader(LogEventC onvert.java:849) 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEven tConvert.java:561) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$DmlParserStage.onE vent(MysqlMultiStageCoprocessor.java:302) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$DmlParserStage.onE vent(MysqlMultiStageCoprocessor.java:288) 	at com.lmax.disruptor.WorkProcessor.run(WorkProcessor.java:143) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) 	at java.lang.Thread.run(Thread.java:748) ] 2018-09-12 17:09:51.781 [destination = IMMIG_200   address = /172.16.40.200:3306   EventParser] WARN   c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - find start position : EntryPosition[includ ed=false journalName=mysql-bin.000012 position=46893782 serverId=200 gtid=85506e6a-a0f8-11e8-96b3-00 5056b70bff:1-85344 timestamp=1536743377000] 2018-09-12 17:09:53.682 [New I/O server worker #1-9] INFO  c.a.otter.canal.instance.core.AbstractCan alInstance - subscribe filter change to immig([1-8]|1[0-8])\.immig_enc_text 这是我的binlog：（4757那条是丢失的） ### INSERT INTO `immig2`.`immig_enc_text` ### SET ###   @1='4756' ###   @2='Name-20180912171327-3073F1CC4D611dd0b13565e94969af0b4fe0972d58bf' ###   @3='DETAIL-20180912171327-73F1CC4D611dd0b13565e94969af0b4fe0972d58bf' ###   @4='BIRTHDATE-20180912171327-1CC4D611dd0b13565e94969af0b4fe0972d58bf' ###   @5='1dd0b13565e94969af0b4fe0972d58bf' ###   @6='1' ###   @7='00' ###   @8='1' ###   @9='1' ###   @10='20180912171327' ###   @11='CHN' ###   @12=1536743376 # at 46871947 #180912 17:09:36 server id 200  end_log_pos 46872041 CRC32 0xf43de30e 	Table_map: `immig2`.`immig_enc_text` mapped to number 111 # at 46872041 #180912 17:09:36 server id 200  end_log_pos 46872344 CRC32 0x32dac44c 	Write_rows: table id 111 flags: STMT_END_F ### INSERT INTO `immig2`.`immig_enc_text` ### SET ###   @1='4757' ###   @2='Name-20180912171327-3073F1CC4D615b1e326471284984b522636e96b55fc3' ###   @3='DETAIL-20180912171327-73F1CC4D615b1e326471284984b522636e96b55fc3' ###   @4='BIRTHDATE-20180912171327-1CC4D615b1e326471284984b522636e96b55fc3' ###   @5='5b1e326471284984b522636e96b55fc3' ###   @6='1' ###   @7='00' ###   @8='1' ###   @9='1' ###   @10='20180912171327' ###   @11='CHN' ###   @12=1536743376 # at 46872344 #180912 17:09:36 server id 200  end_log_pos 46872438 CRC32 0x4dd666d7 	Table_map: `immig2`.`immig_enc_text` mapped to number 111 # at 46872438 #180912 17:09:36 server id 200  end_log_pos 46872741 CRC32 0x86092308 	Write_rows: table id 111 flags: STMT_END_F ### INSERT INTO `immig2`.`immig_enc_text` ### SET ###   @1='4758' ###   @2='Name-20180912171327-3073F1CC4D618b85988e6a054822800bf861691f6e13' ###   @3='DETAIL-20180912171327-73F1CC4D618b85988e6a054822800bf861691f6e13' ###   @4='BIRTHDATE-20180912171327-1CC4D618b85988e6a054822800bf861691f6e13' ###   @5='8b85988e6a054822800bf861691f6e13' ###   @6='1' ###   @7='00' ###   @8='1' ###   @9='1' ###   @10='20180912171327' ###   @11='CHN' ###   @12=1536743376 # at 46872741 #180912 17:09:36 server id 200  end_log_pos 46872835 CRC32 0xeb520dc4 	Table_map: `immig2`.`immig_enc_text` mapped to number 111 # at 46872835 #180912 17:09:36 server id 200  end_log_pos 46873138 CRC32 0x31e40985 	Write_rows: table id 111 flags: STMT_END_F @xesygao ConcurrentModificationException的问题: https://github.com/alibaba/canal/pull/902 @lcybo 由于串行的性能对我来说已经够用了，我暂时先用串行解析，测试了十几次没出现丢数据的问题了。tks 可以先尝试用一下1.1.1的alpha版本
424,1.0.23里面修复的KILL DUMP exception问题在(#334)里面被覆盖回去了 如题，(#334)里的改动较多，很可能是一个误操作。 tks
423,支持Mariadb 10.0.17吗 支持Mariadb 10.0.17吗 理论上支持mariadb 10.x系列，可以跑一下测试给个反馈
422,canal 在阿里云 RDS-Mysql 环境中运行 由于阿里云的mysql存在自动清理binlog机制   当该机制启动时   当前有正在处理的任务ACK会产生异常(因为log文件已经不存在) ;或者剩余还未处理的任务也会被丢弃 .  针对这个场景 是否有优化? java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Could not find first log file name in binary log index file
421,一个canal下destinations多个instance监听同一个库的问题 一个canal下destinations写了2个instance，监听的库一样。我发现库里面的数据变更后，我的client会接收到两条一模一样的变更，这是怎么回事？有人遇到过吗？ 一个instance一份数据 你说的 一份数据，是指一个数据库吗，一个instance对应一个数据库？ @loveluckystar 我也是写了三个instance对应一个库，然后通过filter过滤就不会有重复的数据，不过总是会有一个instance 的client 取不到变更数据。batchId为-1，size为0
420,mysql5.7.20 使用canal 1.0.24 对mysql 5.7.20监控时 出现表名、库名为空的情况，有没有官方的canal对mysql 5.7.20版本监控的 canal 和mysql配置 注意一下binlog格式
419,canal1.0.24，以前好好的，最近新加了一个实例之后，就一直报这个错，client能消费到（不知道数据是否丢失一部分） 2017-11-21 11:40:16.215 [destination = example31   address = /localhost:30701   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position  {"identity":{"slaveId":-1 "sourceAddress":{"address":"localhost" "port":30701}} "postion":{"included":false "journalName":"mysql-bin.000220" "position":3614315 "serverId":1 "timestamp":1511235396000}} 2017-11-21 11:40:46.230 [destination = example31   address = /localhost:30701   EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Unexpected End Stream 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) ~[canal.parse-1.0.24.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70) ~[canal.parse-1.0.24.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122) [canal.parse-1.0.24.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) [canal.parse-1.0.24.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_111] 2017-11-21 11:40:46.230 [destination = example31   address = /localhost:30701   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address localhost/localhost:30701 has an error  retrying. caused by java.io.IOException: Unexpected End Stream 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) ~[canal.parse-1.0.24.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70) ~[canal.parse-1.0.24.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122) ~[canal.parse-1.0.24.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) ~[canal.parse-1.0.24.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_111] 2017-11-21 11:40:46.230 [destination = example31   address = /localhost:30701   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example31[java.io.IOException: Unexpected End Stream 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) 	at java.lang.Thread.run(Thread.java:745) ] 是否是slaveId出现重复，被mysql端kill了链接，可以尝试一下最新的版本，增加了随机slaveId的能力
418,编译报错，isHeartBeat这个变量找不到 [ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.2:compile (default-compile) on project canal.parse: Compilation failure: Compilation failure: [ERROR] /Users/yes/Documents/workspace/idea/canal/parse/src/main/java/com/alibaba/otter/canal/parse/inbound/mysql/dbsync/LogEventConvert.java:[390 24] 找不到符号 [ERROR] 符号:   变量 isHeartBeat [ERROR] 位置: 类 com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert [ERROR] /Users/yes/Documents/workspace/idea/canal/parse/src/main/java/com/alibaba/otter/canal/parse/inbound/mysql/dbsync/LogEventConvert.java:[390 23] 非法的类型开始 [ERROR] -> [Help 1] 同上   ``` [INFO] ------------------------------------------------------------- [ERROR] /root/canal/parse/src/main/java/com/alibaba/otter/canal/parse/inbound/mysql/dbsync/LogEventConvert.java:[390 24] cannot find symbol   symbol:   variable isHeartBeat   location: class com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert [ERROR] /root/canal/parse/src/main/java/com/alibaba/otter/canal/parse/inbound/mysql/dbsync/LogEventConvert.java:[390 23] illegal start of type [INFO] 2 errors  ``` 环境 ``` ~/canal# mvn -version Apache Maven 3.3.9 Maven home: /usr/share/maven Java version: 1.8.0_151  vendor: Oracle Corporation Java home: /usr/lib/jvm/java-8-oracle/jre Default locale: en_US  platform encoding: UTF-8 OS name: "linux"  version: "4.4.0-62-generic"  arch: "amd64"  family: "unix" ~/canal# java -version java version "1.8.0_151" Java(TM) SE Runtime Environment (build 1.8.0_151-b12) Java HotSpot(TM) 64-Bit Server VM (build 25.151-b12  mixed mode) ~/canal# lsb_release -a LSB Version:	core-9.20160110ubuntu0.2-amd64:core-9.20160110ubuntu0.2-noarch:security-9.20160110ubuntu0.2-amd64:security-9.20160110ubuntu0.2-noarch Distributor ID:	Ubuntu Description:	Ubuntu 16.04.2 LTS Release:	16.04 Codename:	xenial ``` 把最新那次的提交revert掉吧 应该是提交时漏掉了代码 下面的命令可以解决编译的问题. ``` git reset --hard a3b9f6f1ebb21dd528effcceba2ac207f40b15e8 mvn clean install -Dmaven.test.skip -Denv=release ``` a9284b1b3967917ee6cef0e85787575e9b121e1e，已经修复
417,canal解析库A，但当同实例下另一个库B删除了大量数据时，导致canal卡死 一个canal-server，一个instance，解析库A的变化，但是同实例下库B做了大量删除操作，导致canal解析库A卡死（可能不是卡死，日志一直显示正常，但是库A的变化一直解析不到，且日志中位点信息不再发生变化） mysql版本：5.6 canal版本：1.0.16 ps：otter遇到过同样的问题，日志无异常，但是解析不到源库的数据 有没有人遇到过这个问题，该如何解决？ 建议升级一下canal版本
416,cannal  mysql to hive cannal是否可以直接把mysql数据同步到hive 需要自己接收到binlog变更消息之后，写代码同步到hive
415,求助，启动canal server后报错 2017-11-13 10:41:41.588 [destination = example   address = /127.0.0.1:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : flush privileges com.alibaba.druid.sql.parser.ParserException: syntax error  expect TABLES  actual IDENTIFIER  pos 16  line 1  column 6  token IDENTIFIER privileges 	at com.alibaba.druid.sql.parser.SQLParser.acceptIdentifier(SQLParser.java:60) ~[druid-1.1.5.jar:1.1.5] 	at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseFlush(MySqlStatementParser.java:915) ~[druid-1.1.5.jar:1.1.5] 	at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseStatementListDialect(MySqlStatementParser.java:762) ~[druid-1.1.5.jar:1.1.5] 	at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:388) ~[druid-1.1.5.jar:1.1.5] 	at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.5.jar:1.1.5] 	at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:461) ~[druid-1.1.5.jar:1.1.5] 	at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.5.jar:1.1.5] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:67) ~[canal.parse-1.0.25-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.applyHistoryOnMemory(DatabaseTableMeta.java:387) [canal.parse-1.0.25-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:121) [canal.parse-1.0.25-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.25-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.25-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_121] 2017-11-13 10:41:41.590 [destination = example   address = /127.0.0.1:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : flush privileges com.alibaba.druid.sql.parser.ParserException: syntax error  expect TABLES  actual IDENTIFIER  pos 16  line 1  column 6  token IDENTIFIER privileges 	at com.alibaba.druid.sql.parser.SQLParser.acceptIdentifier(SQLParser.java:60) ~[druid-1.1.5.jar:1.1.5] 	at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseFlush(MySqlStatementParser.java:915) ~[druid-1.1.5.jar:1.1.5] 	at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseStatementListDialect(MySqlStatementParser.java:762) ~[druid-1.1.5.jar:1.1.5] 	at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:388) ~[druid-1.1.5.jar:1.1.5] 	at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.5.jar:1.1.5] 	at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:461) ~[druid-1.1.5.jar:1.1.5] 	at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.5.jar:1.1.5] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:67) ~[canal.parse-1.0.25-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.applyHistoryOnMemory(DatabaseTableMeta.java:387) [canal.parse-1.0.25-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:121) [canal.parse-1.0.25-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.25-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.25-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_121] 看样子像是权限的问题。 我是先启动了canal server，然后我重新执行了一次flush privileges给canal用户赋权，就一直出现这个报错。 楼兄 这个项目活跃度还是挺高的， 我准备详细看下源码实现 临时解决方案，com.alibaba.otter.canal.parse.inbound.mysql.tsdb.apply()中将ddl中的alter user 、flush privileges的语句过滤掉即可。 这块只会记录异常日志，本身是try catch不影响功能 druid 1.1.6暂时不支持flush privileges的解析，临时先绕过，已经反馈给druid的作者
414,关于heartbeat机制的优化 现在canal与mySQL 数据库之间的heartbeat机制  在打开detecting机制的时候(即通过发送sql语句查询的方式来探测远端是否活着) 是一直在以固定的间隔循环发送  而没有根据现在是否有traffic的情况来决定发送  即和binlog的heartbeat机制一样  只有在没有traffic一段时间后才发送heartbeat消息 会以固定的频率发送heartbeat消息 
413,ErrorPacket 错误是什么情况 INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - KILL DUMP 151432475 failure:java.io.IOException: ErrorPacket [errorNumber=1094  fieldCount=-1  message=Unknown thread id: 151432475  sqlState=HY000  sqlStateMarker=#]  with command: KILL CONNECTION 151432475         at com.alibaba.otter.canal.parse.driver.mysql.MysqlUpdateExecutor.update(MysqlUpdateExecutor.java:49)         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.disconnect(MysqlConnector.java:104)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.disconnect(MysqlConnection.java:64)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:241)         at java.lang.Thread.run(Thread.java:748) 升级canal版本即可
412,发生主备数据库切换之后  在disconnect()里会发生exception 发生主备数据库切换之后  因为地址发生了变化  所以在找start position的时候会用timestamp  从拿到的binlog文件里根据timestamp找对应的position  这时候会把connection的dumping标致设为true  在后面reconnect()  disconnect()里面会因为现在是dumping状态  所以在关掉channel之后会执行一次"KILL CONNECTION"  而这时候用的connection id是之前的已经关闭的  所以会抛异常. 建议先执行"KILL CONNECTION"再close channel     public void disconnect() throws IOException {         if (connected.compareAndSet(true  false)) {             try {                 if (channel != null) {                     channel.close();                 }                 logger.info("disConnect MysqlConnection to {}..."  address);             } catch (Exception e) {                 throw new IOException("disconnect " + this.address + " failure:" + ExceptionUtils.getStackTrace(e));             }             // 执行一次quit             if (dumping && connectionId >= 0) {                 MysqlConnector connector = null;                 try {                     connector = this.fork();                     connector.connect();                     MysqlUpdateExecutor executor = new MysqlUpdateExecutor(connector);                     executor.update("KILL CONNECTION " + connectionId);                 } catch (Exception e) {                     throw new IOException("KILL DUMP " + connectionId + " failure:" + ExceptionUtils.getStackTrace(e));                 } finally {                     if (connector != null) {                         connector.disconnect();                     }                 }                 dumping = false;             }         } else {             logger.info("the channel {} is not connected"  this.address);         }     } 在新版本里已经修了  请关闭  谢谢
411,canal和mysql连接，如何获取当前操作mysql用户 binlog里没有记录
410,canal怎么监控其运行状态 有时候canal挂了，开发都不清楚，引起业务数据错误，我就想怎么把canal的监控体系搭建起来，有好的建议么？ 监控canal的进程或者zookeeper的业务消费位点
409,JsonUtils中InetAddress序列化问题 JsonUtils中的InetAddressSerializer，序列化InetAddress对象是 ``` java InetAddress address = (InetAddress)object; serializer.write(address.getHostName()) ``` 是获取getHostName来获取的，如果hosts中配置了主机名，那么这个值解析到的主机名 建议修改为 `address.getHostAddress()`直接获取IP PS:JsonUtils中 ``` java  static {         SerializeConfig.getGlobalInstance().put(InetAddress.class  JsonUtils.InetAddressSerializer.instance);         SerializeConfig.getGlobalInstance().put(Inet4Address.class  JsonUtils.InetAddressSerializer.instance);         SerializeConfig.getGlobalInstance().put(Inet6Address.class  JsonUtils.InetAddressSerializer.instance);     } ``` 这种方式是否太黑魔法，这样的话整个fastJson框架，在解析InetAddress对象的时候，全部是按照这种方式序列化的，而影响嵌入canal的应用程序？ 希望保留hostname，而不是ip，特别针对是域名+vip模式的时候，域名会保持不变
408,canal监控mysql主备服务 canal监控mysql主备服务要怎么配置，如果只配置主数据库地址，在主数据库服务器挂掉的情况下可以继续采集binlog吗 需要配置主备库的信息
407,在manager管理界面看不到Zookeeper下面挂的node节点，但是运行正常 如题，这个问题怎么解决，我看教程截图上是可以看到Zookeeper下面挂的节点个数的 提问到otter吧
406,binlog-format 在 MIXED 下如何获取 tableName 如题，因为公司已经运营很长时间了，修改的话不知道会出多少问题（服务重启，从数据库更新）。 MIXED 下的信息实在是太少了，我只能拿到原声的SQL语句，是不是意味着只有对原声的SQL语句作语法分析拿出 tableName？ 可以的，需要依赖一个sql parser即可 最新的26版本，已经通过完整SQL解析对应的表，可以完整获取 thanks  感谢回答
405,ClusterCanalClient  针对一个canal server 集群  部署多个canal client  client 监听数据会两台上监听到一样的更新数据嘛，怎么做到集群消费呢 集群消费建议投递到MQ系统中进行处理
404,canal HA zookeeper不更新position请大神指教 如题，试了很多方式，虽然不更新zookeeper但是我不切换HA的时候，重启client端能够按照顺序执行，但是当我停掉一个切换server的时候，client又自动从之前的position运行。不明白是怎么回事，求大神指教。 不会，看下zk上的offset变化。 不变化配置就不对 server 都重启下，我刚开始也有这个问题，后来没有了
403,预计什么时候发布下个版本呢？ 
402,otter node进程fullgc，导致同步点位不更新，一直死循环     我们现在使用otter，遇到一个问题，我们把otter 部署到阿里云ECS机器上，同步RDS，发现一个奇怪的问题，当表（有几百万数据）添加一个字段后，并更新这个字段的默认值（1条update语句，更新几百万数据），发现otter一直在死循环的更新这几百万数据，查看select ，load详细日志也看到一直更新，并且是死循环，界面同步进度的点位一直固定在一个点，不前进，在这个过程中出现了很多次fullgc，不知道是否有影响。 看一下FAQ
401,获取不到ROWDATA记录 @agapple  canal server: 1.0.24 使用的是阿里云RDS，部署结构： ``` master --> slave --> canal server --> 消费程序 ``` ``` MySQL [zz]> show variables like '%binlog_format%'; +---------------+-------+ | Variable_name | Value | +---------------+-------+ | binlog_format | ROW   | +---------------+-------+ 1 row in set (0.00 sec) ``` 消费程序日志基本都是这样的： ``` [2017-10-18 11:51:18 398] (CanalMessageHandlerONSImpl.java:35) DEBUG  - handleMessage batchId[46] entrys[2] [2017-10-18 11:51:18 398] (CanalMessageHandlerONSImpl.java:42) DEBUG  - entryType[TRANSACTIONBEGIN] schema[] tablename[] [2017-10-18 11:51:18 399] (CanalMessageHandlerONSImpl.java:42) DEBUG  - entryType[TRANSACTIONEND] schema[] tablename[] ``` 消费程序: ``` connector.connect(); connector.subscribe(null); ``` instance.properties ``` ## mysql serverId canal.instance.mysql.slaveId = 42341 # position info canal.instance.master.address = xxxxx canal.instance.master.journal.name =  canal.instance.master.position =  canal.instance.master.timestamp =  #canal.instance.standby.address =  #canal.instance.standby.journal.name = #canal.instance.standby.position =  #canal.instance.standby.timestamp =  # username/password canal.instance.dbUsername = xx canal.instance.dbPassword = xx canal.instance.defaultDatabaseName = zz canal.instance.connectionCharset = UTF-8 # table regex canal.instance.filter.regex = .*\\..* #canal.instance.filter.regex = .*order_info.* # table black regex canal.instance.filter.black.regex =   ``` canal.properties ``` ################################################# ######### 		common argument		#############  ################################################# canal.id= 101 canal.ip= canal.port= 11111 canal.zkServers= 127.0.0.1:2181 # flush data to zk canal.zookeeper.flush.period = 1000 # flush meta cursor/parse position to file canal.file.data.dir = ${canal.conf.dir} canal.file.flush.period = 1000 ## memory store RingBuffer size  should be Math.pow(2 n) canal.instance.memory.buffer.size = 16384 ## memory store RingBuffer used memory unit size   default 1kb canal.instance.memory.buffer.memunit = 1024  ## meory store gets mode used MEMSIZE or ITEMSIZE canal.instance.memory.batch.mode = MEMSIZE ## detecing config canal.instance.detecting.enable = false #canal.instance.detecting.sql = insert into retl.xdual values(1 now()) on duplicate key update x=now() canal.instance.detecting.sql = select 1 canal.instance.detecting.interval.time = 3 canal.instance.detecting.retry.threshold = 3 canal.instance.detecting.heartbeatHaEnable = false # support maximum transaction size  more than the size of the transaction will be cut into multiple transactions delivery canal.instance.transaction.size =  1024 # mysql fallback connected to new master should fallback times canal.instance.fallbackIntervalInSeconds = 60 # network config canal.instance.network.receiveBufferSize = 16384 canal.instance.network.sendBufferSize = 16384 canal.instance.network.soTimeout = 30 # binlog filter config canal.instance.filter.query.dcl = false canal.instance.filter.query.dml = false canal.instance.filter.query.ddl = false canal.instance.filter.table.error = false canal.instance.filter.rows = false # binlog format/image check canal.instance.binlog.format = ROW STATEMENT MIXED  canal.instance.binlog.image = FULL MINIMAL NOBLOB # binlog ddl isolation canal.instance.get.ddl.isolation = false ################################################# ######### 		destinations		#############  ################################################# canal.destinations= zz # conf root dir canal.conf.dir = ../conf # auto scan instance dir add/remove and start/stop instance canal.auto.scan = true canal.auto.scan.interval = 5 canal.instance.global.mode = spring  canal.instance.global.lazy = false #canal.instance.global.manager.address = 127.0.0.1:1099 #canal.instance.global.spring.xml = classpath:spring/memory-instance.xml #canal.instance.global.spring.xml = classpath:spring/file-instance.xml canal.instance.global.spring.xml = classpath:spring/default-instance.xml ``` 关注是否有打开log slave参数 @agapple  是否是参数：`log_slave_updates` ``` MySQL [zaozuo]> show variables like '%slave%'; +---------------------------------------+-----------------------+ | Variable_name                         | Value                 | +---------------------------------------+-----------------------+ | init_slave                            |                       | | log_slave_updates                     | ON                    | | log_slow_slave_statements             | OFF                   | | pseudo_slave_mode                     | OFF                   | | rpl_semi_sync_master_wait_no_slave    | ON                    | | rpl_semi_sync_slave_delay_master      | OFF                   | | rpl_semi_sync_slave_enabled           | ON                    | | rpl_semi_sync_slave_kill_conn_timeout | 5                     | | rpl_semi_sync_slave_trace_level       | 1                     | | rpl_stop_slave_timeout                | 31536000              | | slave_allow_batching                  | OFF                   | | slave_checkpoint_group                | 512                   | | slave_checkpoint_period               | 300                   | | slave_compressed_protocol             | OFF                   | | slave_exec_mode                       | STRICT                | | slave_load_tmpdir                     |                       | | slave_max_allowed_packet              | 1073741824            | | slave_net_timeout                     | 60                    | | slave_parallel_workers                | 16                    | | slave_pending_jobs_size_max           | 167772160             | | slave_pr_mode                         | TABLE                 | | slave_rows_search_algorithms          | TABLE_SCAN INDEX_SCAN | | slave_skip_errors                     | OFF                   | | slave_sql_verify_checksum             | ON                    | | slave_transaction_retries             | 10                    | | slave_type_conversions                |                       | | sql_slave_skip_counter                | 0                     | +---------------------------------------+-----------------------+ 27 rows in set (0.00 sec) ``` ``` MySQL [zaozuo]> show variables like '%log%'; +-----------------------------------------+----------------------+ | Variable_name                           | Value                | +-----------------------------------------+----------------------+ | back_log                                | 3000                 | | binlog_cache_size                       | 131072               | | binlog_checksum                         | CRC32                | | binlog_direct_non_transactional_updates | OFF                  | | binlog_format                           | ROW                  | | binlog_order_commits                    | OFF                  | | binlog_row_image                        | FULL                 | | binlog_rows_query_log_events            | OFF                  | | binlog_stmt_cache_size                  | 32768                | | expire_logs_days                        | 0                    | | failover_log_verbose                    |                      | | general_log                             | OFF                  | | general_log_file                        |                      | | innodb_api_enable_binlog                | OFF                  | | innodb_flush_log_at_timeout             | 1                    | | innodb_flush_log_at_trx_commit          | 2                    | | innodb_locks_unsafe_for_binlog          | OFF                  | | innodb_log_buffer_size                  | 8388608              | | innodb_log_compressed_pages             | OFF                  | | innodb_log_file_size                    | 1048576000           | | innodb_log_files_in_group               | 2                    | | innodb_log_group_home_dir               |                      | | innodb_mirrored_log_groups              | 1                    | | innodb_online_alter_log_max_size        | 134217728            | | innodb_undo_logs                        | 128                  | | log_bin                                 | ON                   | | log_bin_basename                        |                      | | log_bin_index                           |                      | | log_bin_trust_function_creators         | ON                   | | log_bin_use_v1_row_events               | ON                   | | log_error                               |                      | | log_output                              | TABLE                | | log_queries_not_using_indexes           | OFF                  | | log_slave_updates                       | ON                   | | log_slow_admin_statements               | ON                   | | log_slow_slave_statements               | OFF                  | | log_throttle_queries_not_using_indexes  | 0                    | | log_warnings                            | 2                    | | max_binlog_cache_size                   | 18446744073709547520 | | max_binlog_size                         | 524288000            | | max_binlog_stmt_cache_size              | 18446744073709547520 | | max_relay_log_size                      | 0                    | | relay_log                               |                      | | relay_log_basename                      |                      | | relay_log_index                         |                      | | relay_log_info_file                     |                      | | relay_log_info_repository               | TABLE                | | relay_log_purge                         | ON                   | | relay_log_recovery                      | OFF                  | | relay_log_space_limit                   | 0                    | | slow_query_log                          | ON                   | | slow_query_log_file                     |                      | | sql_log_bin                             | ON                   | | sql_log_off                             | OFF                  | | sync_binlog                             | 1000                 | | sync_relay_log                          | 10000                | | sync_relay_log_info                     | 10000                | | tokudb_checkpoint_on_flush_logs         | OFF                  | | tokudb_fsync_log_period                 | 0                    | | tokudb_log_buffer_size                  | 16777216             | | tokudb_log_dir                          |                      | | tokudb_log_file_size                    | 104857600            | +-----------------------------------------+----------------------+ 62 rows in set (0.00 sec) ``` @agapple  比较诡异的是mysql的慢日志语句居然能进来 ``` [2017-10-19 01:47:52 343] (CanalMessageHandlerONSImpl.java:42) DEBUG  - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 02:05:15 708] (CanalMessageHandlerONSImpl.java:42) DEBUG  - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 02:19:55 133] (CanalMessageHandlerONSImpl.java:42) DEBUG  - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 02:37:49 861] (CanalMessageHandlerONSImpl.java:42) DEBUG  - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 02:48:20 351] (CanalMessageHandlerONSImpl.java:42) DEBUG  - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 03:07:13 337] (CanalMessageHandlerONSImpl.java:42) DEBUG  - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 03:17:14 421] (CanalMessageHandlerONSImpl.java:42) DEBUG  - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 03:35:21 371] (CanalMessageHandlerONSImpl.java:42) DEBUG  - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 03:45:55 885] (CanalMessageHandlerONSImpl.java:42) DEBUG  - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 04:04:42 615] (CanalMessageHandlerONSImpl.java:42) DEBUG  - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 04:23:22 967] (CanalMessageHandlerONSImpl.java:42) DEBUG  - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 05:07:06 185] (CanalMessageHandlerONSImpl.java:42) DEBUG  - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 05:17:21 915] (CanalMessageHandlerONSImpl.java:42) DEBUG  - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 05:36:32 541] (CanalMessageHandlerONSImpl.java:42) DEBUG  - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 05:46:33 606] (CanalMessageHandlerONSImpl.java:42) DEBUG  - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 06:04:54 056] (CanalMessageHandlerONSImpl.java:42) DEBUG  - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 06:17:09 120] (CanalMessageHandlerONSImpl.java:42) DEBUG  - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 06:37:21 586] (CanalMessageHandlerONSImpl.java:42) DEBUG  - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 06:45:09 582] (CanalMessageHandlerONSImpl.java:42) DEBUG  - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 06:49:18 785] (CanalMessageHandlerONSImpl.java:42) DEBUG  - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 07:04:10 957] (CanalMessageHandlerONSImpl.java:42) DEBUG  - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 07:13:34 243] (CanalMessageHandlerONSImpl.java:42) DEBUG  - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 07:35:16 863] (CanalMessageHandlerONSImpl.java:42) DEBUG  - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 07:43:58 415] (CanalMessageHandlerONSImpl.java:42) DEBUG  - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 08:05:37 009] (CanalMessageHandlerONSImpl.java:42) DEBUG  - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 08:23:29 424] (CanalMessageHandlerONSImpl.java:42) DEBUG  - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 09:06:56 832] (CanalMessageHandlerONSImpl.java:42) DEBUG  - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 09:18:18 433] (CanalMessageHandlerONSImpl.java:42) DEBUG  - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 09:36:09 787] (CanalMessageHandlerONSImpl.java:42) DEBUG  - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 09:48:05 170] (CanalMessageHandlerONSImpl.java:42) DEBUG  - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 10:07:39 216] (CanalMessageHandlerONSImpl.java:42) DEBUG  - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 10:19:40 631] (CanalMessageHandlerONSImpl.java:42) DEBUG  - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 10:37:33 833] (CanalMessageHandlerONSImpl.java:42) DEBUG  - entryType[ROWDATA] schema[mysql] tablename[slow_log] ``` 直接连主库，问题解决了。 提阿里云工单，他们说官方不支持自主搭建复制系统去同步RDS，推荐去用DTS。。。 推测可能是阿里云备库做了什么限制 嗯，直连主库进行链接吧，DTS原理也是连主库，原理都一样 x > 嗯，直连主库进行链接吧，DTS原理也是连主库，原理都一样 现在我遇到的问题，canal [v.1.1.0] 连接阿里的rds   logs/**-instance/meta.log 一直在有新日志，但是库户端一直接收不到数据，接收的数据都是这个样子滴： ` Message[id=444 entries=[header {   version: 1   logfileName: "*****"   logfileOffset: *****   serverId: *****   serverenCode: "UTF-8"   executeTime: 1539572438000   sourceType: MYSQL   schemaName: ""   tableName: ""   eventLength: 76 } entryType: TRANSACTIONBEGIN storeValue: " \242\206\356\001"  header {   version: 1   logfileName: "*****"   logfileOffset: *****   serverId: *****   serverenCode: "UTF-8"   executeTime: 1539572438000   sourceType: MYSQL   schemaName: ""   tableName: ""   eventLength: 31 } entryType: TRANSACTIONEND storeValue: "\022\n7768450739" ]] ` 数据的类型为啥都是：entryType.TRANSACTION呢 大家不是用的RDS只读实例吧？RDS只读实例是没开log_update_slave，不会有主库的binlog，所以没法开级联同步，直接操作RDS主实例即可
400,canal可以支持grpc吗？ canal可以支持grpc吗？这样就可以用任何语言写客户端和canal交互了 可以提交一个PR给我
399,求助，， 2017-10-16 10:44:47.424 [destination = example   address = /127.0.0.1:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just show master status 2017-10-16 10:44:47.427 [destination = example   address = /127.0.0.1:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /127.0.0.1:3306 has an error  retrying. caused by  java.lang.NullPointerException: null 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.loadBinlogChecksum(MysqlConnection.java:284) ~[canal.parse-1.0.23.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) ~[canal.parse-1.0.23.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) ~[canal.parse-1.0.23.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_111] 2017-10-16 10:44:47.427 [destination = example   address = /127.0.0.1:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.lang.NullPointerException 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.loadBinlogChecksum(MysqlConnection.java:284) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) 	at java.lang.Thread.run(Thread.java:745) ] 这个错误是什么原因造成的，，， 升级到1.0.24版本，已修复
398,LOST_EVENTS 支持 我用的Canal版本是 1.0.22。 我们在分析MySQL replication时可能出现的LOST_EVENTS。我查看了一下源码，感觉Canal不支持LOST_EVENTS的处理，而是在读取到这个event后，在将其转换成Entry时给丢掉了。请问Canal是特意设计成这样的行为的吗，或是我们理解有误？ 我们分析的如下代码： 在LogDecoder.java中， INCIDENT_EVENT 是会被读取到的。 case LogEvent.INCIDENT_EVENT: {                 IncidentLogEvent event = new IncidentLogEvent(header  buffer  descriptionEvent);                 /* updating position in context */                 logPosition.position = header.getLogPos();                 return event;             } 但是在 LogEventConvert.java 中的 parse 方法，并没有处理这个INCIDENT_EVENT。 public Entry parse(LogEvent logEvent) throws CanalParseException {         if (logEvent == null || logEvent instanceof UnknownLogEvent) {                 return null;         }                  int eventType = logEvent.getHeader().getType();         switch (eventType) {             ...... ( 这里没有 case LogEvent.INCIDENT_EVENT)         } } 是不是 Canal 认为，即使读取到这个事件，consumer拿到也无能为力，所以就主动丢弃了？ 如果能主动报一个告警会不会有帮助？ 谢谢！ Best Regards Stone 当时刻意没加，canal的设计是尽可能抽取DML/DDL/DCL的部分内容，并没有全处理所有mysql事件  @agapple 谢谢你的即时回复！ 目前有考虑扩展Canal，以涵盖这类（或者尽量多的）事件吗？ Best Regards Stone 接口协议设计里不支持，所以暂时不会考虑 谢谢你！ 我先close这个问题了。如果有新的问题，我再开一个新的ticket。 谢谢！ Best Regards Stone
397,RowChange中isDdl字段值的问题 使用官方提供的clent example，RowChange.parseFrom(entry.getStoreValue()); update、insert、delete语句转换结果 isDdl的值为true，且rowChage.getRowDatasList()的list长度为0.当前使用canal版本是1.0.24 mysql版本是5.6.26-log。请问是什么原因导致的解析问题。 确认使用为ROW模式，还有一点mysql 5.6会有rows query对象，可以canal.instance.filter.query.dml改为true
396,group-instance.xml 如何配置 想使用group模式处理多库合并的问题，但是wiki找不到配置的方法？ canal.properties  只更改了 canal.destinations= food mdm ``` ################################################# ######### 		common argument		#############  ################################################# canal.id= 1 canal.ip= canal.port= 11111 canal.zkServers= # flush data to zk canal.zookeeper.flush.period = 1000 # flush meta cursor/parse position to file canal.file.data.dir = ${canal.conf.dir} canal.file.flush.period = 1000 ## memory store RingBuffer size  should be Math.pow(2 n) canal.instance.memory.buffer.size = 16384 ## memory store RingBuffer used memory unit size   default 1kb canal.instance.memory.buffer.memunit = 1024  ## meory store gets mode used MEMSIZE or ITEMSIZE canal.instance.memory.batch.mode = MEMSIZE ## detecing config canal.instance.detecting.enable = false #canal.instance.detecting.sql = insert into retl.xdual values(1 now()) on duplicate key update x=now() canal.instance.detecting.sql = select 1 canal.instance.detecting.interval.time = 3 canal.instance.detecting.retry.threshold = 3 canal.instance.detecting.heartbeatHaEnable = false # support maximum transaction size  more than the size of the transaction will be cut into multiple transactions delivery canal.instance.transaction.size =  1024 # mysql fallback connected to new master should fallback times canal.instance.fallbackIntervalInSeconds = 60 # network config canal.instance.network.receiveBufferSize = 16384 canal.instance.network.sendBufferSize = 16384 canal.instance.network.soTimeout = 30 # binlog filter config canal.instance.filter.query.dcl = false canal.instance.filter.query.dml = false canal.instance.filter.query.ddl = false canal.instance.filter.table.error = false canal.instance.filter.rows = false # binlog format/image check canal.instance.binlog.format = ROW STATEMENT MIXED  canal.instance.binlog.image = FULL MINIMAL NOBLOB # binlog ddl isolation canal.instance.get.ddl.isolation = false ################################################# ######### 		destinations		#############  ################################################# canal.destinations= food mdm # conf root dir canal.conf.dir = ../conf # auto scan instance dir add/remove and start/stop instance canal.auto.scan = true canal.auto.scan.interval = 5 canal.instance.global.mode = spring  canal.instance.global.lazy = false #canal.instance.global.manager.address = 127.0.0.1:1099 #canal.instance.global.spring.xml = classpath:spring/memory-instance.xml canal.instance.global.spring.xml = classpath:spring/file-instance.xml #canal.instance.global.spring.xml = classpath:spring/default-instance.xml ``` group-instance.xml 使用默认的内容，未做修改 ``` <?xml version="1.0" encoding="UTF-8"?> <beans xmlns="http://www.springframework.org/schema/beans" 	xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:tx="http://www.springframework.org/schema/tx" 	xmlns:aop="http://www.springframework.org/schema/aop" xmlns:lang="http://www.springframework.org/schema/lang" 	xmlns:context="http://www.springframework.org/schema/context" 	xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.0.xsd            http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-2.0.xsd            http://www.springframework.org/schema/lang http://www.springframework.org/schema/lang/spring-lang-2.0.xsd            http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-2.0.xsd            http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-2.5.xsd" 	default-autowire="byName"> 	 	<!-- properties --> 	<bean class="com.alibaba.otter.canal.instance.spring.support.PropertyPlaceholderConfigurer" lazy-init="false"> 		<property name="ignoreResourceNotFound" value="true" /> 		<property name="systemPropertiesModeName" value="SYSTEM_PROPERTIES_MODE_OVERRIDE"/><!-- 允许system覆盖 --> 		<property name="locationNames"> 			<list> 				<value>classpath:canal.properties</value> 				<value>classpath:${canal.instance.destination:}/instance.properties</value> 			</list> 		</property> 	</bean> 	 	<bean id="socketAddressEditor" class="com.alibaba.otter.canal.instance.spring.support.SocketAddressEditor" /> 	<bean class="org.springframework.beans.factory.config.CustomEditorConfigurer">  		<property name="propertyEditorRegistrars"> 			<list> 				<ref bean="socketAddressEditor" /> 			</list> 		</property> 	</bean> 	<bean id="instance" class="com.alibaba.otter.canal.instance.spring.CanalInstanceWithSpring"> 		<property name="destination" value="${canal.instance.destination}" /> 		<property name="eventParser"> 			<ref local="eventParser" /> 		</property> 		<property name="eventSink"> 			<ref local="eventSink" /> 		</property> 		<property name="eventStore"> 			<ref local="eventStore" /> 		</property> 		<property name="metaManager"> 			<ref local="metaManager" /> 		</property> 		<property name="alarmHandler"> 			<ref local="alarmHandler" /> 		</property> 	</bean> 	 	<!-- 报警处理类 --> 	<bean id="alarmHandler" class="com.alibaba.otter.canal.common.alarm.LogAlarmHandler" /> 	 	<bean id="metaManager" class="com.alibaba.otter.canal.meta.MemoryMetaManager" /> 	 	<bean id="eventStore" class="com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer"> 		<property name="bufferSize" value="${canal.instance.memory.buffer.size:16384}" /> 		<property name="bufferMemUnit" value="${canal.instance.memory.buffer.memunit:1024}" /> 		<property name="batchMode" value="${canal.instance.memory.batch.mode:MEMSIZE}" /> 		<property name="ddlIsolation" value="${canal.instance.get.ddl.isolation:false}" /> 	</bean> 	 	<bean id="eventSink" class="com.alibaba.otter.canal.sink.entry.EntryEventSink"> 		<property name="eventStore" ref="eventStore" /> 	</bean> 	 	<bean id="eventParser" class="com.alibaba.otter.canal.parse.inbound.group.GroupEventParser"> 		<property name="eventParsers"> 			<list> 				<ref bean="eventParser1" /> 				<ref bean="eventParser2" /> 			</list> 		</property> 	</bean> 	<bean id="eventParser1" class="com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser"> 		<property name="destination" value="${canal.instance.destination}" /> 		<property name="slaveId" value="${canal.instance.mysql.slaveId:1234}" /> 		<!-- 心跳配置 --> 		<property name="detectingEnable" value="${canal.instance.detecting.enable:false}" /> 		<property name="detectingSQL" value="${canal.instance.detecting.sql}" /> 		<property name="detectingIntervalInSeconds" value="${canal.instance.detecting.interval.time:5}" /> 		<property name="haController"> 			<bean class="com.alibaba.otter.canal.parse.ha.HeartBeatHAController"> 				<property name="detectingRetryTimes" value="${canal.instance.detecting.retry.threshold:3}" /> 				<property name="switchEnable" value="${canal.instance.detecting.heartbeatHaEnable:false}" /> 			</bean> 		</property> 		 		<property name="alarmHandler" ref="alarmHandler" /> 		 		<!-- 解析过滤处理 --> 		<property name="eventFilter"> 			<bean class="com.alibaba.otter.canal.filter.aviater.AviaterRegexFilter" > 				<constructor-arg index="0" value="${canal.instance.filter.regex:.*\..*}" /> 			</bean> 		</property> 		 		<property name="eventBlackFilter"> 			<bean class="com.alibaba.otter.canal.filter.aviater.AviaterRegexFilter" > 				<constructor-arg index="0" value="${canal.instance.filter.black.regex:}" /> 				<constructor-arg index="1" value="false" /> 			</bean> 		</property> 		<!-- 最大事务解析大小，超过该大小后事务将被切分为多个事务投递 --> 		<property name="transactionSize" value="${canal.instance.transaction.size:1024}" /> 		 		<!-- 网络链接参数 --> 		<property name="receiveBufferSize" value="${canal.instance.network.receiveBufferSize:16384}" /> 		<property name="sendBufferSize" value="${canal.instance.network.sendBufferSize:16384}" /> 		<property name="defaultConnectionTimeoutInSeconds" value="${canal.instance.network.soTimeout:30}" /> 		 		<!-- 解析编码 --> 		<!-- property name="connectionCharsetNumber" value="${canal.instance.connectionCharsetNumber:33}" /--> 		<property name="connectionCharset" value="${canal.instance.connectionCharset:UTF-8}" /> 	 		<!-- 解析位点记录 --> 		<property name="logPositionManager"> 			<bean class="com.alibaba.otter.canal.parse.index.MemoryLogPositionManager" /> 		</property> 		 		<!-- failover切换时回退的时间 --> 		<property name="fallbackIntervalInSeconds" value="${canal.instance.fallbackIntervalInSeconds:60}" /> 		 		<!-- 解析数据库信息 --> 		<property name="masterInfo"> 			<bean class="com.alibaba.otter.canal.parse.support.AuthenticationInfo"> 				<property name="address" value="${canal.instance.master1.address}" /> 				<property name="username" value="${canal.instance.dbUsername:retl}" /> 				<property name="password" value="${canal.instance.dbPassword:retl}" /> 				<property name="defaultDatabaseName" value="${canal.instance.defaultDatabaseName:retl}" /> 			</bean> 		</property> 		<property name="standbyInfo"> 			<bean class="com.alibaba.otter.canal.parse.support.AuthenticationInfo"> 				<property name="address" value="${canal.instance.standby1.address}" /> 				<property name="username" value="${canal.instance.dbUsername:retl}" /> 				<property name="password" value="${canal.instance.dbPassword:retl}" /> 				<property name="defaultDatabaseName" value="${canal.instance.defaultDatabaseName:retl}" /> 			</bean> 		</property> 		 		<!-- 解析起始位点 --> 		<property name="masterPosition"> 			<bean class="com.alibaba.otter.canal.protocol.position.EntryPosition"> 				<property name="journalName" value="${canal.instance.master1.journal.name}" /> 				<property name="position" value="${canal.instance.master1.position}" /> 				<property name="timestamp" value="${canal.instance.master1.timestamp}" /> 			</bean> 		</property> 		<property name="standbyPosition"> 			<bean class="com.alibaba.otter.canal.protocol.position.EntryPosition"> 				<property name="journalName" value="${canal.instance.standby1.journal.name}" /> 				<property name="position" value="${canal.instance.standby1.position}" /> 				<property name="timestamp" value="${canal.instance.standby1.timestamp}" /> 			</bean> 		</property> 		<property name="filterQueryDml" value="${canal.instance.filter.query.dml:false}" /> 		<property name="filterQueryDcl" value="${canal.instance.filter.query.dcl:false}" /> 		<property name="filterQueryDdl" value="${canal.instance.filter.query.ddl:false}" /> 		<property name="filterTableError" value="${canal.instance.filter.table.error:false}" /> 		<property name="supportBinlogFormats" value="${canal.instance.binlog.format}" /> 		<property name="supportBinlogImages" value="${canal.instance.binlog.image}" /> 	</bean> 	 	<bean id="eventParser2" class="com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser"> 		<property name="destination" value="${canal.instance.destination}" /> 		<property name="slaveId" value="${canal.instance.mysql.slaveId:1234}" /> 		<!-- 心跳配置 --> 		<property name="detectingEnable" value="${canal.instance.detecting.enable:false}" /> 		<property name="detectingSQL" value="${canal.instance.detecting.sql}" /> 		<property name="detectingIntervalInSeconds" value="${canal.instance.detecting.interval.time:5}" /> 		<property name="haController"> 			<bean class="com.alibaba.otter.canal.parse.ha.HeartBeatHAController"> 				<property name="detectingRetryTimes" value="${canal.instance.detecting.retry.threshold:3}" /> 				<property name="switchEnable" value="${canal.instance.detecting.heartbeatHaEnable:false}" /> 			</bean> 		</property> 		 		<property name="alarmHandler" ref="alarmHandler" /> 		 		<!-- 解析过滤处理 --> 		<property name="eventFilter"> 			<bean class="com.alibaba.otter.canal.filter.aviater.AviaterRegexFilter" > 				<constructor-arg index="0" value="${canal.instance.filter.regex:.*\..*}" /> 			</bean> 		</property> 		 		<property name="eventBlackFilter"> 			<bean class="com.alibaba.otter.canal.filter.aviater.AviaterRegexFilter" > 				<constructor-arg index="0" value="${canal.instance.filter.black.regex:}" /> 				<constructor-arg index="1" value="false" /> 			</bean> 		</property> 		<!-- 最大事务解析大小，超过该大小后事务将被切分为多个事务投递 --> 		<property name="transactionSize" value="${canal.instance.transaction.size:1024}" /> 		 		<!-- 网络链接参数 --> 		<property name="receiveBufferSize" value="${canal.instance.network.receiveBufferSize:16384}" /> 		<property name="sendBufferSize" value="${canal.instance.network.sendBufferSize:16384}" /> 		<property name="defaultConnectionTimeoutInSeconds" value="${canal.instance.network.soTimeout:30}" /> 		 		<!-- 解析编码 --> 		<!-- property name="connectionCharsetNumber" value="${canal.instance.connectionCharsetNumber:33}" /--> 		<property name="connectionCharset" value="${canal.instance.connectionCharset:UTF-8}" /> 	 		<!-- 解析位点记录 --> 		<property name="logPositionManager"> 			<bean class="com.alibaba.otter.canal.parse.index.MemoryLogPositionManager" /> 		</property> 		 		<!-- failover切换时回退的时间 --> 		<property name="fallbackIntervalInSeconds" value="${canal.instance.fallbackIntervalInSeconds:60}" /> 		 		<!-- 解析数据库信息 --> 		<property name="masterInfo"> 			<bean class="com.alibaba.otter.canal.parse.support.AuthenticationInfo"> 				<property name="address" value="${canal.instance.master2.address}" /> 				<property name="username" value="${canal.instance.dbUsername:retl}" /> 				<property name="password" value="${canal.instance.dbPassword:retl}" /> 				<property name="defaultDatabaseName" value="${canal.instance.defaultDatabaseName:retl}" /> 			</bean> 		</property> 		<property name="standbyInfo"> 			<bean class="com.alibaba.otter.canal.parse.support.AuthenticationInfo"> 				<property name="address" value="${canal.instance.standby2.address}" /> 				<property name="username" value="${canal.instance.dbUsername:retl}" /> 				<property name="password" value="${canal.instance.dbPassword:retl}" /> 				<property name="defaultDatabaseName" value="${canal.instance.defaultDatabaseName:retl}" /> 			</bean> 		</property> 		 		<!-- 解析起始位点 --> 		<property name="masterPosition"> 			<bean class="com.alibaba.otter.canal.protocol.position.EntryPosition"> 				<property name="journalName" value="${canal.instance.master2.journal.name}" /> 				<property name="position" value="${canal.instance.master2.position}" /> 				<property name="timestamp" value="${canal.instance.master2.timestamp}" /> 			</bean> 		</property> 		<property name="standbyPosition"> 			<bean class="com.alibaba.otter.canal.protocol.position.EntryPosition"> 				<property name="journalName" value="${canal.instance.standby2.journal.name}" /> 				<property name="position" value="${canal.instance.standby2.position}" /> 				<property name="timestamp" value="${canal.instance.standby2.timestamp}" /> 			</bean> 		</property> 		<property name="filterQueryDml" value="${canal.instance.filter.query.dml:false}" /> 		<property name="filterQueryDcl" value="${canal.instance.filter.query.dcl:false}" /> 		<property name="filterQueryDdl" value="${canal.instance.filter.query.ddl:false}" /> 		<property name="filterRows" value="${canal.instance.filter.rows:false}" /> 		<property name="filterTableError" value="${canal.instance.filter.table.error:false}" /> 		<property name="supportBinlogFormats" value="${canal.instance.binlog.format}" /> 		<property name="supportBinlogImages" value="${canal.instance.binlog.image}" /> 	</bean> </beans> ``` client接入的时候怎么配置destination呢？使用逗号分隔是不行的，会当做一个来处理，无法找到 ``` String destination = "food";         String ip = "127.0.0.1";         CanalConnector connector = CanalConnectors.newSingleConnector(new InetSocketAddress(ip  11111)             destination             ""             ""); ``` # 另外QQ群是否可以处理一下？161559791  ？ @xiaopan0513 我也想知道这个，请问相应的代码怎么写？ @xiaopan0513 我也面临同样的问题，想请教下这块怎么配置呢？
395,使用GroupEventSink时，数据变更不频繁同步卡住的解决方案 hi，agapple： 当使用GroupEventSink时，如果有一个或者多个分库一直没有binlog，那么所有paser线程都会卡在doSink方法上，导致有binlog产生的分库数据也无法消费。从代码注释上看，这么设计的原因，是为了相互协同，防止某个分库出现问题时，被落下，这样可以避免instance重启时丢数据。但同时也增大了Group模式的局限性，我们现在就有类似的场景，项目预期会有很大的数据量，提前进行了分库（8个库），但是还没有大批量推广，每天的数据量只有几千条，某些分库会长时间没有数据。 为了解决这个问题，我想到的方案有两个： 1、一个是利用心跳Sql，类似“insert into retl.xdual values(1 now()) on duplicate key update x=now()“，保证分库源源不断的有binlog产生，这样就不会卡住了。缺点是对业务系统有侵入，必须在业务库上创建一个辅助表 2、另外想到的一个方案是，改造MysqlEventParser，在内部类MysqlDetectingTimeTask的run方法中，当没有出现异常时，构造一个Heartbeat类型的Event，然后调用consumeTheEventAndProfilingIfNecessary方法，类似AbstractEventParser的buildHeartBeatTimeTask方法 你看还有没有更好的方案？ 以前group模式的设计不太好，基于锁的方式性能太差 你们内部没怎么使用这种方案吗？性能差也是相对的，这种方式还是很方便的，之前8个分库需要创建8个实例，然后每个实例一堆映射配置，运维起来还是挺麻烦的。有时间的话，您那边可以想一想，看能不能有一个更好的方案，解决这种协同卡住的问题 @agapple 对于group这种模式有什么好的思路吗？对于MySQL 分库分表的场景还是相当多的，但对于应用端其实不关心底层到底分了多少个库，不知道这块有什么好的建议？ 参考：https://github.com/alibaba/canal/pull/522 升级一下canal 26版本试试 感谢。比较关心26正式版什么时候发布。 目前我们在1.0.24上，通过patch解决死锁问题。 ps 先前1.0.25总是会出现线程卡死在connect于reconnect上。貌似在26版本里也解决了。
394,TimelineTransactionBarrier存在死锁bug TimelineTransactionBarrier的isPermit方法中有如下代码              `if (isTransactionBegin(event)) {                     if (txState.compareAndSet(0  1)) {                         inTransaction.set(true);                         return true; // 事务允许通过                     }                 } else if (txState.compareAndSet(0  2)) { // 非事务保护中                     return true; // DDL/DCL允许通过                 }` 对于最后一个else if分支，不仅ddl和dcl的场景会进入，TransactionEnd时也可能会进入（当基于上次Position启动EventParser时，拿到的第一个事件类型是TransactionEnd） 再看TimelineTransactionBarrier的clear方法 `public void clear(Event event) {         super.clear(event);         if (isTransactionEnd(event)) {             inTransaction.set(false); // 事务结束并且已经成功写入store，清理标记，进入重新排队判断，允许新的事务进入             txState.compareAndSet(1  0);             // if (txState.compareAndSet(1  0) == false) {             // throw new             // CanalSinkException("state is not correct in transaction");             // }         } else if (txState.intValue() == 2) {// 非事务中             txState.compareAndSet(2  0);             // if (txState.compareAndSet(2  0) == false) {             // throw new             // CanalSinkException("state is not correct in non-transaction");             // }         }     }` 第一个if判断，应该多加一个条件“&& txState.intValue() != 2”，以应对上面说的TransactionEnd问题，否则txState的状态将一直为2，isPermit方法始终返回false，系统出现死锁 或者是把clear方法中的两个if的顺序调换一下 方便提交一个pull request给我？ 好的，这两天忙完，就提
393,Table meta support [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=393) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=393) before we can accept your contribution.<br/>**0** out of **2** committers have signed the CLA.<br/><br/>:x: KaimingWan<br/>:x: wanshao<br/><hr/>**wanshao** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=393) it.</sub>
392,table meta support support table meta  [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=392) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=392) before we can accept your contribution.<br/>**0** out of **2** committers have signed the CLA.<br/><br/>:x: KaimingWan<br/>:x: wanshao<br/><hr/>**wanshao** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=392) it.</sub>
391,主库是mysql 5.7，主库编码UTF8MB4 用canal-1.0.24这个版本，位点初始值为123 执行时间1506482123，启动报错 主库是mysql 5.7，主库编码UTF8MB4 用canal-1.0.24这个版本，位点初始值为123 执行时间1506482123，canal配置了timestamp 没有配置位点，提示以下异常 2017-09-29 14:20:32.614 [main] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - start successful.... 2017-09-29 14:20:32.892 [destination = example_wxprober   address = /XXXXXXX:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position mysql-bin.000013::1506482123000 2017-09-29 14:20:32.934 [destination = example_wxprober   address = /XXXXXXX:3306   EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Could not find first log file name in binary log index file Could not find first log file name in binary log index file，位点不对
390,ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:xxx[java.io.IOException: Unexpected End Stream 在canal实例日志中偶尔会报这个错误，请问怎么解决 mysql断链问题，目前是一些已知问题，主干版本有人提交了修复 @agapple  ``` 2017-11-06 15:00:03.937 [destination = action_log   address = /127.0.0.2:2020   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address 127.0.0.2/127.0.0.2:2020 has an error  retrying. caused by  java.io.IOException: Unexpected End Stream         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) ~[canal.parse-1.0.24.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70) ~[canal.parse-1.0.24.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122) ~[canal.parse-1.0.24.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) ~[canal.parse-1.0.24.jar:na]         at java.lang.Thread.run(Thread.java:745) [na:1.7.0_80] 2017-11-06 15:00:03.938 [destination = action_log   address = /127.0.0.2:2020   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:action_log[java.io.IOException: Unexpected End Stream         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209)         at java.lang.Thread.run(Thread.java:745) ] 2017-11-06 15:00:22.587 [destination = action_log   address = /127.0.0.2:2020   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position  {"identity":{"slaveId":-1 "sourceAddress":{"address":"127.0.0.2" "port":2020}} "postion":{"included":false "journalName":"mysql-bin.001213" "position":638501339 "serverId":1037 "timestamp":1509951603000}} 2017-11-06 17:00:22.588 [destination = action_log   address = /127.0.0.2:2020   EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Unexpected End Stream         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) ~[canal.parse-1.0.24.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70) ~[canal.parse-1.0.24.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122) [canal.parse-1.0.24.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) [canal.parse-1.0.24.jar:na]         at java.lang.Thread.run(Thread.java:745) [na:1.7.0_80] 2017-11-06 17:00:22.588 [destination = action_log   address = /127.0.0.2:2020   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address 127.0.0.2/127.0.0.2:2020 has an error  retrying. caused by  java.io.IOException: Unexpected End Stream         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) ~[canal.parse-1.0.24.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70) ~[canal.parse-1.0.24.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122) ~[canal.parse-1.0.24.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) ~[canal.parse-1.0.24.jar:na]         at java.lang.Thread.run(Thread.java:745) [na:1.7.0_80] 2017-11-06 17:00:22.588 [destination = action_log   address = /127.0.0.2:2020   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:action_log[java.io.IOException: Unexpected End Stream         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209)         at java.lang.Thread.run(Thread.java:745) ] 2017-11-06 17:00:34.936 [destination = action_log   address = /127.0.0.2:2020   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position  {"identity":{"slaveId":-1 "sourceAddress":{"address":"127.0.0.2" "port":2020}} "postion":{"included":false "journalName":"mysql-bin.001214" "position":269841413 "serverId":1037 "timestamp":1509958822000}} (END)  ``` 大约每两小时就报一次，但是貌似并没有丢数据，请问对线上业务有影响吗？新版canal修复该问题了吗？ 亲，这个问题你们解决了嘛？ @tdytaylor  至今没有解决，如果你解决了，吼一吼 mysql server服务端主动断链接，尝试用最新版本试试 这个问题会造成丢失数据吗...重启了后遇到的   升级版本有什么简便方法吗
389,canal is not run any in node  failed to connect to:/1.0.0.1:7536 after retry 1 times 2017-09-27/14:33:30.751||||^_^|uid:|clientIp:|deviceId:|^_^|[binlog main] ERROR c.a.o.c.client.impl.running.ClientRunningMonitor 137 - There is an error when execute initRunning method  with destination [es1]. 434654-com.alibaba.otter.canal.protocol.exception.CanalClientException: java.net.ConnectException: Connection refused 434655-	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.doConnect(SimpleCanalConnector.java:171) 434656-	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.access$000(SimpleCanalConnector.java:48) 434657-	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector$1.processActiveEnter(SimpleCanalConnector.java:396) 434658-	at com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor.processActiveEnter(ClientRunningMonitor.java:207) 434659-	at com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor.initRunning(ClientRunningMonitor.java:118) 434660-	at com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor.start(ClientRunningMonitor.java:92) 434661-	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.connect(SimpleCanalConnector.java:93) 434662-	at com.alibaba.otter.canal.client.impl.ClusterCanalConnector.connect(ClusterCanalConnector.java:63) 434663-	at com.alibaba.otter.canal.client.impl.ClusterCanalConnector.restart(ClusterCanalConnector.java:275) 434664-	at com.alibaba.otter.canal.client.impl.ClusterCanalConnector.getWithoutAck(ClusterCanalConnector.java:187) 434665-	at com.domain.es.canal.Consumer$HandlerThread.run(BinLogConsumer.java:194) 434666-Caused by: java.net.ConnectException: Connection refused 434667-	at sun.nio.ch.Net.connect0(Native Method) 434668-	at sun.nio.ch.Net.connect(Net.java:454) 434669-	at sun.nio.ch.Net.connect(Net.java:446) 434670-	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) 434671-	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.doConnect(SimpleCanalConnector.java:132) 434672-	... 10 common frames omitted 434673-2017-09-27/14:33:30.759||||^_^|uid:|clientIp:|deviceId:|^_^|[binlog main] WARN  c.a.otter.canal.client.impl.ClusterCanalConnector 66 - failed to connect to:/1.0.0.1:7536 after retry 1 times 434674-2017-09-27/14:33:30.761||||^_^|uid:|clientIp:|deviceId:|^_^|[binlog main] WARN  c.a.o.c.client.impl.running.ClientRunningMonitor 174 - canal is not run any in node Caused by: java.net.ConnectException: Connection refused
388,Canal会不定期的报no data的情况 我们像otter那样使用canal来获取binlog数据，但是运行了一段时候之后有个别channel任务的canal会不定时报长时间no data的情况，告警内容如：pid:24 nid:4 exception:mainstem:pid:24 canal elapsed 5000 seconds no data ，切换mainstem也不能解决问题，重启整个服务会恢复一段时间，不知道大神们有没有遇到类似问题，或者有啥解决方案？ 我们也有一样的问题，不过好像不影响同步 应该是canal早期版本的源端库无数据的问题，尝试升级一下otter和canal版本吧
387,canal server重启，会从上次位点解析吗 开启了HA的话，上次解析的位点应该会存在zk中，是会从上次位点解析的。 我是用file持久化的，不支持HA，这样是否重启的时候也能从上次位点解析，另外，我结合otter使用，目前otter好像只支持内存模式，这样我重启后，是否会丢失服务断开期间的数据呢 不会丢，多看看WIKI和FAQ @zhumt file模式会的在instance的配置下，会生产一个meta.dat文件记录上一次解析点
386,压力大的情况下binlog解析变慢 在测试的时候发现，当产生binlog大概超过3万qps的时候，canal解析产生的event变的特别慢，服务器有个cpu跑到100%，这是什么情况，如何解决呢？ 参考：https://github.com/alibaba/canal/wiki/FAQ
384,windows中startup.bat启动报错 Error: missing `server' JVM at `C:\Program Files (x86)\Java\jre1.8.0_111\bin\server\jvm.dll'. Please install or use the JRE or JDK that contains these missing components. 检查下自己的jvm，使用1.6、1.7版本试试
383,canal postion重置问题 @agapple  @lulu2panpan 请两位有空看下，谢谢啦！ - 上一次成功的ack日志： [     batchId: 906915     position: PositionRange[         start=LogPosition[             identity=LogIdentity[                 sourceAddress=192.168.10.180/192.168.10.180: 3306                 slaveId=-1             ]             postion=EntryPosition[                 included=false                 journalName=mysql-bin.000940                 position=150869837                 serverId=180                 timestamp=1505671087000             ]         ]         ack=<null>         end=LogPosition[             identity=LogIdentity[                 sourceAddress=192.168.10.180/192.168.10.180: 3306                 slaveId=-1             ]             postion=EntryPosition[                 included=false                 journalName=mysql-bin.000940                 position=151977852                 serverId=180                 timestamp=1505671093000             ]         ]     ] ] - rollback失败的ack日志： e[     start=LogPosition[         identity=LogIdentity[             sourceAddress=192.168.10.180/192.168.10.180: 3306             slaveId=-1         ]         postion=EntryPosition[             included=false             journalName=mysql-bin.000940             position=151979096             serverId=180             timestamp=1505671093000         ]     ]     ack=LogPosition[         identity=LogIdentity[             sourceAddress=192.168.10.180/192.168.10.180: 3306             slaveId=-1         ]         postion=EntryPosition[             included=false             journalName=mysql-bin.000940             position=138659218             serverId=180             timestamp=1505670995000         ]     ]     end=LogPosition[         identity=LogIdentity[             sourceAddress=192.168.10.180/192.168.10.180: 3306             slaveId=-1         ]         postion=EntryPosition[             included=false             journalName=mysql-bin.000940             position=138740531             serverId=180             timestamp=1505670996000         ]     ] ] 我们对比两次的日志，发现ack节点的内容发生了变化，正常情况下，ack的值是null。但是异常情况下，附带了信息，且position的位置比start 和 end节点的都要靠前，不知道为什么出现这种情况。线上环境已经遇到很多次了。因为position变更，数据就会重复，主键冲突无法insert操作，都是手动修改的。 目前我们的解决办法是： 1. 比对zk的position是否是最新的，如果是的话就重启server，之后可以正常读取 2. 如果zk的position也是之前的位点，只能手动修改zk的postion为最新的位点来解决。 到目前为止都没有找到具体的原因，也不知道什么什么引起的。 我们是多个instance的方式部署。jdk 1.7 canal parser在主备切换或者链接断开时就会有重复消费，导致位点回溯，重复是难免的
382,canal 指定timestamp 1）请问为什么只指定timestamp，启动的时候找不到binlog位置？必须得要binlog filename一起指定才能找到位置呢？ 我看文档里面是这样写着： canal.instance.master.timestamp : 指定一个时间戳，canal会自动遍历mysql binlog，找到对应时间戳的binlog位点后，进行启动 2）还有我用binlog filename+ position指定时，他只会遍历这个binlog，不会执行下一个binlog是吗？ 请大神帮忙解答一下，谢谢 timestamp 没用过， binlog filename+ position也是会自动取下一个binlog的 LS正解 单纯使用timestamp 时我试了也不行。但使用binlog filename+ position没问题。
381,启动某个channel就一直报错 pid:9 nid:1 exception:canal:tdesk_test_canal:java.lang.NoClassDefFoundError: Could not initialize class com.alibaba.otter.canal.common.utils.JsonUtils      at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPositionInternal(MysqlEventParser.java:415)      at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPosition(MysqlEventParser.java:315)      at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:161)      at java.lang.Thread.run(Thread.java:748)  请问是什么原因啊？是没有位点的原因的吗？ 启动了多一段时间也可能有这个错。 请使用最新版测试
380,延迟情况下多条ddl语句引发schema不匹配的bug 在有多条ddl语句时 如果binlog解析存在延时 会引发如下错误： com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: column size is not match for table:`xxdb`.`xxxtable` 39 vs 37。 原因如下： binlog中有前后两条ddl语句a和b，由于解析落后，canal才解析到a，从db中获取最新的schema，得到了b操作后的schema。a和b之间的消息的表结构跟最新的schema不匹配。 我也遇到了同样的问题。。。求大神解答。。 下一个版本会解决 根据源代码。canal取表结构的元信息不是从binlog中解析的，而是从mysql数据库中使用desc db1.table1去获取的，这样必然存在隐患。 使用下面的case即可重现问题。 先停止canal。 再在mysql主库上运行下列sql语句 use dbt1; CREATE TABLE Usert1 (   `userId` BIGINT NOT NULL AUTO_INCREMENT   `userNickName` varchar(20) NOT NULL DEFAULT ''   `isSystem` TINYINT NOT NULL DEFAULT 0   `updateTime` INT NOT NULL DEFAULT 0   PRIMARY KEY (`userId`) ) ENGINE=InnoDB AUTO_INCREMENT=100000; CREATE INDEX IDX_Usert1_mobile on `Usert1` (`mobile`);  INSERT INTO Usert1(`userId` `userNickName` `isSystem`  `updateTime`)     VALUES (101 'u101' 1  UNIX_TIMESTAMP()); INSERT INTO Usert1(`userId` `userNickName` `isSystem`  `updateTime`)     VALUES (201 'u201' 1  UNIX_TIMESTAMP()); UPDATE Usert1 SET isSystem=0 WHERE userId=201; DELETE FROM Usert1 WHERE userId=101; SELECT * FROM Usert1; ALTER TABLE Usert1 ADD COLUMN age INT DEFAULT 0; ALTER TABLE Usert1 DROP COLUMN updateTime; ALTER TABLE Usert1 change userNickName nickName varchar(20); INSERT INTO Usert1(`userId` `nickName` `isSystem`  `age`)     VALUES (301 'u301' 1  301); INSERT INTO Usert1(`userId` `nickName` `isSystem`  `age`)     VALUES (302 'u302' 1  302); UPDATE Usert1 SET isSystem=3 WHERE userId=301; DELETE FROM Usert1 WHERE userId=302; SELECT * FROM Usert1; 再启动 canal.deployer-1.0.24 和 canal.example-1.0.24 。然后可以从canal.example-1.0.24的log输出看到如下输出。下面的输出中，nickName已经是新列名，而且本是updateTime列的值变成了age列的值。 ================> binlog[log-bin.000007:5281]   executeTime : 1507805075000   delay : -17643ms  BEGIN ----> Thread id: 17 ----------------> binlog[log-bin.000007:5407]   name[dbt1 Usert2]   eventType : INSERT   executeTime : 1507805075000   delay : -17638ms userId : 101    type=bigint(20)    update=true nickName : u101    type=varchar(20)    update=true isSystem : 1    type=tinyint(4)    update=true age : 1507805075    type=int(11)    update=true ----------------  END ----> transaction id: 105 新版本会支持ddl版本变更，可以随意回切到任意位点，DDL表结构也会同步回滚 新版本代码已经提交到master @agapple master分支编译有点问题，啥时候出新版本1.0.25？ ``` [ERROR] Failed to execute goal on project canal.parse: Could not resolve dependencies for project com.alibaba.otter:canal.parse:jar:1.0.25-SNAPSHOT: Failed to collect dependencies at com.alibaba:druid:jar:1.1.5-preview_14: Failed to read artifact descriptor for com.alibaba:druid:jar:1.1.5-preview_14: Could not transfer artifact com.alibaba:druid:pom:1.1.5-preview_14 from/to alibaba (http://code.alibabatech.com/mvn/releases/): Connect to code.alibabatech.com:80 [code.alibabatech.com/119.38.217.15] failed: 拒绝连接 (Connection refused) -> [Help 1] [ERROR]  [ERROR] To see the full stack trace of the errors  re-run Maven with the -e switch. [ERROR] Re-run Maven using the -X switch to enable full debug logging. [ERROR]  [ERROR] For more information about the errors and possible solutions  please read the following articles: [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException ``` 切换到druid 1.1.5正式版了 master 编译无法成功             } else if (isHeartBeat) {                 // 处理alisql模式的test.heartbeat心跳数据 已经修复 请问老版本怎么跳过这个问题呢？ 老版本只能跳过数据了 尝试26最新版本
379,canal有考虑事务回滚的Event吗 mysql的binlog文档在介绍XID_EVENT时有这么一段 > XID_EVENT > Generated for a commit of a transaction that modifies one or more tables of an XA-capable storage engine. Normal transactions are implemented by sending a QUERY_EVENT containing a BEGIN statement and a QUERY_EVENT containing a COMMIT statement (or a ROLLBACK statement if the transaction is rolled back). 但是我看canal1.24代码中解析QUERY_EVENT时，只考虑了BEGIN和COMMIT，并没有考虑ROLLBACK，请问这样做的原因是什么呢？ https://dev.mysql.com/doc/refman/5.7/en/innodb-and-mysql-replication.html 这里又有这么一段叙述： Transactions that fail on the master do not affect replication at all. MySQL replication is based on the binary log where MySQL writes SQL statements that modify data. A transaction that fails (for example  because of a foreign key violation  or because it is rolled back) is not written to the binary log   貌似并不会记录回滚的binlog...有没有特别清楚的人呢 rollback不会记录binlog，只有提交的事务会记录
378,可配置mysql ssl连接么？  see title 暂时不支持
377,BINARY类型解析 如果数据库中有一列使用的是BINARY类型，column.getValue()获得的是乱码，请问怎么处理？数据库里的BINARY是用hex处理过的 column.getValue().getBytes("ISO-8859-1")
376,指定binlog起始位置 你们好。 我现在想要启动canal后，把现有的binlog全部读取，所以我需要指定binlog的位置，但是因为权限问题，我无法使用show binary logs语句，请问有没有什么办法，来使canal自动从第一个binlog文件开始dump呢？ @agapple @luyee  @agapple  可能比较忙 说下使用过的姿势 从第一个开始需要保留所有的binlog文件 这个不现实吧。。。 用过的方案： 1，无更新的时段（比如凌晨3-4点）直接全量导入+自动获取位点就好了。 2，有数据更新的时段： 2.1  现在一般是小表直接mysqldump（mydumper+myloader） + 指定位点增量 2.2 xbackup +指定位点增量，可以指定表或者db进行部份恢复，不过这玩意有点坑是，指定某些表恢复也不简单，貌似db级别恢复是需要db server的datadir必须为空 2.3  3-4点直接etl工具抽全量+自动获取位点 3，最近发现了[debezium](https://github.com/debezium/debezium) 这个支持全量增量直接导入kafka，正在研究 关于权限，权限这玩意找dba 运维开吧。。大不了完了在收回去呗。。或者要什么(一般是位点，全量导出导入) 让dba操作，提供 拉取binlog  都是要开启权限的，这个是必须的，参考文档： ``` GRANT SELECT  REPLICATION SLAVE  REPLICATION CLIENT ON *.* TO 'canal'@'%'; ``` 嗯，确实不现实，决定采用直接全量导入+自动获取位点的方案了 感谢回答
375,master分支源码编译报错 看了下，master分支把netty 版本从3.x升到了4.x，netty的包名发生了改变，从org.jboss.netty-->io.netty。但是canal中使用到netty api的地方包名并没改过来，导致报错。 1.023 版本用的netty4.  这个版本不稳定，    1.0.24版本又回退了。。。 目前是 netty3和netty4都依赖了
374,try to avoid use multiline exception directly 想要把canal放到docker里面，现在canal打印异常的方法使得不太利于docker的fluentd收集，很多异常log的打印直接使用logback自带的功能就可以了，没有必要自己显式转换，这也使得canal与docker的结合更容易一点。 @agapple  [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=374) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=374) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=374) it.</sub> tks
373,meta.bat 的位置 meta.bat 存储位置在哪里？ canal service连接zookeeper，使用spring.xml  classpath:spring/default-instance.xml 如果使用default，数据是存储在zookeeper的
372,canal 订阅模式数据量太大，把内存撑爆了，有canal的最佳实践么？ canal 订阅模式数据量太大，把内存撑爆了，有canal的最佳实践么？ batchsize 太大? 调整服务端的ringbuffer大小
371,spring版本2.5.6和应用的版本冲突了，有什么办法兼容下吗？ 应用用canal-client的方式，应用用的是4.x的spring  而canal在用的是2.5.6 spring boot会报：   ``` java.lang.NoSuchMethodError: org.springframework.core.annotation.AnnotationAwareOrderComparator.sort   ```   有办法绕过吗 自问自答。用 exclusions 应该可以解决。 @liutaihua 请问去掉依赖后，用别的版本的spring，canal不会出莫名奇妙的bug吗？ @agapple 请问去掉依赖后，用别的版本的spring，canal不会出莫名奇妙的bug吗？ 不会的，canal用的spring功能非常少
370,请问，canal支持定时消费吗？ 请问，canal支持定时消费吗？ 没有吧 定时rocketmq可以
369,canal消费端到某个时间点不消费 用的canal/canal-deployer-1.0.24  最新版本   订阅的mysql版本是mariadb的10.0.22 发现消费端运行到某一时刻 zk的位点和时间戳就不在变化 而且消费端的进程并没有挂掉 也没有发现错误日志 重新启动进程不好使 必须重新置成instance.properties的时间戳 删掉zk里位点 重新消费才好使 盼回复 谢谢!  没日志[允悲] 是不是阻塞了？ 这么解决的？遇到同样问题， 没有日志很恼火。可以确定的是 client和server直接的问题 +1
367,关于解析位点和消费位点的zk存储未到flush时间数据丢失？ 在default-instance.xml  中配置为周期性的输入zk，如果在未到flush时间，server挂掉，另外的备机启动，读取的位点落后于最新的位点（因为主server未flush到zk），这样会造成重复读取数据？？怎么办？？ 分布式高可用系统，不可能设计做到数据不重复
366,这是BUG吗：canal解析错误fetch failed by table meta:`schemeName`.`tableName` 这个问题和#358类似 如果canal从一个数据库binglog的第一条记录开始读取，那么canal在解析过程中，可能会因为在解析到某条binlog记录时，该记录对应的表被删掉了，而在binlog解析过程中canal会发送一条desc "schemeName.tableName"的sql： private TableMeta getTableMeta0(String fullname) throws IOException {     ResultSetPacket packet = connection.query("desc " + fullname);     return new TableMeta(fullname  parserTableMeta(packet)); } 而对应的表已经被删掉，这时候就会抛出这个异常，导致binlog读取无法继续下去 不知道这是不是个bug？或者是通过配置的方法解决？有没有阿里的大佬出来解答一下？ 具体日志是这样的： 2017-08-10 14:26:56.593 [destination = 10.160.246.137-1379   address = /10.160.246.137:1379   EventParser]369 WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - ERROR ## parse this event has an error   last position : [EntryPosition[included=false journalName= mysql-bin.000003 position=46788142 serverId=1092 timestamp=1493132987000]] com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.collect.ComputationException: com.alibaba.otter.canal.parse.exception.CanalParseException: fetch failed by table meta:`expert`.`bak_TB_ZXJ_ADVICE_COLLECT_ACTIVITY` Caused by: com.google.common.collect.ComputationException: com.alibaba.otter.canal.parse.exception.CanalParseException: fetch failed by table meta:`expert`.`bak_TB_ZXJ_ADVICE_COLLECT_ACTIVITY`         at com.google.common.collect.MapMaker$ComputingMapAdapter.get(MapMaker.java:889)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:78)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:677)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:362)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:108)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:62)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:326)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:176)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:130)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209)         at java.lang.Thread.run(Thread.java:745) Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: fetch failed by table meta:`expert`.`bak_TB_ZXJ_ADVICE_COLLECT_ACTIVITY` Caused by: java.io.IOException: ErrorPacket [errorNumber=1146  fieldCount=-1  message=Table 'expert.bak_TB_ZXJ_ADVICE_COLLECT_ACTIVITY' doesn't exist  sqlState=42S02  sqlStateMarker=#]  with command: desc `expert`.`bak_TB_ZXJ_ADVICE_COLLECT_ACTIVITY`         at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.query(MysqlQueryExecutor.java:60)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:73)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta0(TableMetaCache.java:105)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:26)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.apply(TableMetaCache.java:51)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.apply(TableMetaCache.java:42)         at com.google.common.collect.ComputingConcurrentHashMap$ComputingValueReference.compute(ComputingConcurrentHashMap.java:356)         at com.google.common.collect.ComputingConcurrentHashMap$ComputingSegment.compute(ComputingConcurrentHashMap.java:182)         at com.google.common.collect.ComputingConcurrentHashMap$ComputingSegment.getOrCompute(ComputingConcurrentHashMap.java:151)         at com.google.common.collect.ComputingConcurrentHashMap.getOrCompute(ComputingConcurrentHashMap.java:67)         at com.google.common.collect.MapMaker$ComputingMapAdapter.get(MapMaker.java:885)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:78)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:677)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:362)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:108)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:62)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:326)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:176)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:130)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209)         at java.lang.Thread.run(Thread.java:745) 什么问题 这是 解决了吗？ 现在解决了，可以通过配置filterTableError=true来忽略掉这个异常，这样instance的线程就可以继续运行下去 我也遇到这个问题了 请问这个filterTableError=true 在哪个地方加的 直接添加filterTableError=true? @liutizhong 我使用的是CanalInstanceWithManager，只要在它的配置CanalParameter中设置就行了 代码大体上是这样： CanalParameter parameter = new CanalParameter; parameter.setFilterTableError(true); Canal canal = new Canal(); canal.setCanalParameter(parameter); 最后在new CanalInstance时： new CanalInstanceWithManager(canal  filter); @liutizhong 可以直接在配置文件配置 canal.instance.filter.table.error =true 6666666666😁 2017-11-22 11:41 GMT+08:00 yuxie <notifications@github.com>: > @liutizhong <https://github.com/liutizhong> 可以直接在配置文件配置 > canal.instance.filter.table.error =true > > — > You are receiving this because you commented. > Reply to this email directly  view it on GitHub > <https://github.com/alibaba/canal/issues/366#issuecomment-346234373>  or mute > the thread > <https://github.com/notifications/unsubscribe-auth/AWRZFY2hQZ-z-4GZKhxsDL64GgwgKF5Dks5s45fSgaJpZM4OzTks> > . > 牛逼 我也遇到这个问题了，otter配置文件里面该加在哪？ canal.instance.filter.table.error =true去忽略表错误 谢谢分享，直接修改配置文件canal.instance.filter.table.error =true 忽略了异常
365,heartbeatHaEnable设置成true，但是没有standby的mysql配置的时候，成功启动 heartbeatHaEnable设置成true，但是没有standby的mysql配置的时候，成功启动。但是做mysql stop & start动作的时候，由于parser会做switch操作，这个操作会造成空指针退出。由于这个空指针是停止了心跳线程和解析线程之后，启动新的心跳和解析线程之前，所以canal没法继续dump消息。 版本是多少？
364,maven update Missing artifact com.alibaba.otter:canal.instance.core:jar:1.0.25-SNAPSHOT 请问这个如何解决，我从master抓了最新的代码，进行maven的update的时候，提示 Missing artifact com.alibaba.otter:canal.instance.core:jar:1.0.25-SNAPSHOT Missing artifact com.alibaba.otter:canal.instance.manager:jar:1.0.25-SNAPSHOT Missing artifact com.alibaba.otter:canal.instance.spring:jar:1.0.25-SNAPSHOT 本来就是1.0.25-SNAPSHOT 何来Miss？ 先在 canal  根目录 run ``` mvn clean mvn install ```
363,canal有计划支持类似mysql5.7中组提交类似的功能么？ mysql升级5.7后，由于主从同步性能提高， 主库写入qps的限制也随之降低，这样带来一个问题就是业务写入速度加快，可能会快于我们收集的速度，这样可能无法作到实时收集了。canal有计划支持mysql5.7提到的组提交吗？ canal是以binlog写入之后为准，不关心mysql的组提交的
362,maven查看源码乱码 所有jar包通过maven查看源码时其中文注释全部乱码，能解决此乱码问题吗？ utf8编码
361,即使设定了defaultDatabaseName依然从客户端获取其他数据库的增量？ defaultDatabaseName不是用来过滤数据用的，目前是无意义的参数
360,SocketChannelPool bug 运行日志，请关注最后一行 ```` 2017-08-08 09:59:07.224 [destination = example   address = /127.0.0.1:3306   EventParser] DEBUG io.netty.util.NetUtil - -Djava.net.preferIPv4Stack: false 2017-08-08 09:59:07.224 [destination = example   address = /127.0.0.1:3306   EventParser] DEBUG io.netty.util.NetUtil - -Djava.net.preferIPv6Addresses: false 2017-08-08 09:59:07.313 [destination = example   address = /127.0.0.1:3306   EventParser] DEBUG io.netty.util.NetUtil - Loopback interface: lo (Software Loopback Interface 1  127.0.0.1) 2017-08-08 09:59:07.317 [destination = example   address = /127.0.0.1:3306   EventParser] DEBUG io.netty.util.NetUtil - \proc\sys\net\core\somaxconn: 200 (non-existent) 2017-08-08 09:59:07.403 [destination = example   address = /127.0.0.1:3306   EventParser] DEBUG io.netty.channel.DefaultChannelId - -Dio.netty.machineId: 00:00:00:00:00:00:00:e0 (auto-detected) 2017-08-08 09:59:07.404 [destination = example   address = /127.0.0.1:3306   EventParser] DEBUG io.netty.util.internal.ThreadLocalRandom - -Dio.netty.initialSeedUniquifier: 0xfe50d10b389fdd4b 2017-08-08 09:59:07.482 [destination = example   address = /127.0.0.1:3306   EventParser] DEBUG io.netty.buffer.ByteBufUtil - -Dio.netty.allocator.type: pooled 2017-08-08 09:59:07.483 [destination = example   address = /127.0.0.1:3306   EventParser] DEBUG io.netty.buffer.ByteBufUtil - -Dio.netty.threadLocalDirectBufferSize: 65536 2017-08-08 09:59:07.483 [destination = example   address = /127.0.0.1:3306   EventParser] DEBUG io.netty.buffer.ByteBufUtil - -Dio.netty.maxThreadLocalCharBufferSize: 16384 2017-08-08 09:59:17.547 [destination = example   address = /127.0.0.1:3306   EventParser] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - connect MysqlConnection to /127.0.0.1:3306... ```` ````java public static SocketChannel open(SocketAddress address) throws Exception {         final SocketChannel socket = new SocketChannel();         boot.connect(address).addListener(new ChannelFutureListener() {             @Override             public void operationComplete(ChannelFuture arg0) throws Exception {                 if (arg0.isSuccess()) socket.setChannel(arg0.channel());                 synchronized (socket) {                     socket.notify();    // <--------------------                 }             }         });         synchronized (socket) {             socket.wait();  // <--------------------         }         if (null == socket.getChannel()) {             throw new IOException("can't create socket!");         }         chManager.put(socket.getChannel()  socket);         return socket;     } ```` notify是有可能会比wait提前执行的，建议优化下 
359,关于mysql切主的问题 我们这边有一种场景，mysql是一主两从，canal从主dump binlog，但是切主的话我们是切域名，域名不变，把域名挂的ip改变，这种情况下canal  是要重启？？ 需要。mysql 主从  位点 ，文件名 都不一样，需要人肉修改到 从库的 文件名 ，位点。注意要保证 canal 连接到 新ip 上，确保主库 无写入 而且同步到 从库后 才能 切换。很多细节要注意。 新版本已经支持单vip的模式，会记录上一次的serverId，数据库主备切换时会判断serverId是否相同，不同则会自动进行位点切换 @agapple 请问非gtid的情况下，canal怎么进行自动找到正确的binlog file&pos？ 基于时间戳 具体是什么原理呢？同一个event，在新master上的timestamp，已经和旧master上的已经不同的吧，只有evnet的serverid还是保持不变。 同问，在新master上的timestamp，已经和旧master上的已经不同的吧？怎么能确定log position？ 会通过timestmap在备库上二分查找到对应的binlog，最接近该timestamp的binlog事件，以它为起始位点进行备库消费。 这里的一个假定主备库的时间是有NTP同步保证的，不会相差超过60秒(默认值) 我看代码主备库切换，根据timestmap前推了60s，这样可能会重复消费吧？ 是否可以消费的时候记录最新的entry，在主备库切换的时候，前推60s，再通过MD5或者hash（entry)找到备库最新消费的entry，确定点位，然后再消费，这样是不是就不会重复消费了？
358,canal解析错误fetch failed by table meta:`test`.`test` 请问这种错误有什么办法可以解决呢？谢谢！ com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.collect.ComputationException: com.alibaba.otter.canal.parse.exception.CanalParseException: fetch failed by table meta:`test`.`test` Caused by: com.google.common.collect.ComputationException: com.alibaba.otter.canal.parse.exception.CanalParseException: fetch failed by table meta:`test`.`test` 	at com.google.common.collect.MapMaker$ComputingMapAdapter.get(MapMaker.java:889) ~[guava-18.0.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:78) ~[canal.parse-1.0.24.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:677) ~[canal.parse-1.0.24.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:362) ~[canal.parse-1.0.24.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:108) ~[canal.parse-1.0.24.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:62) ~[canal.parse-1.0.24.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:326) ~[canal.parse-1.0.24.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:176) ~[canal.parse-1.0.24.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:130) [canal.parse-1.0.24.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) [canal.parse-1.0.24.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_91] 我也碰到这个错误了 我找到了原因，这是因为在解析某条binlog记录时，该记录对应的表被删掉了，而在binlog解析过程中canal会发送一条desc `schemeName`.`tableName`的sql语:     private TableMeta getTableMeta0(String fullname) throws IOException {         ResultSetPacket packet = connection.query("desc " + fullname);         return new TableMeta(fullname  parserTableMeta(packet));     } 而对应的表已经被删掉，这时候就会抛出这个异常 但是没找到如何解决这个方法，不知道这是不是个bug？有没有阿里的大佬出来解答一下？ 试试 canal/conf 下的 canal.instance.filter.black.regex = test.* 尝试一下最新版的，启用TSDB可以解决这个问题
357,多客户端订阅问题请教 多个canal客户端订阅一个canal服务，其中一个客户端进行取消订阅操作，其他canal客户端抛异常，会不会影响其他客户端的订阅、消息接收。 @agapple  建议只有一个客户端进行消费，否则数据顺序没法保证 @agapple 如果客户端能自己保证顺序消费，对于多客户端订阅同一个服务，其中一个客户端调用unsubscribe() 导致其他客户端抛异常了，此时会影响其他客户端的订阅及消息获取吗？ 目前是会影响其他客户端的 @agapple  实测过，确实影响，感谢。 @agapple canal能从RDS中获取数据吗 可以订阅RDS @agapple 感谢。161559791这个群不能加入了，我的QQ：295826395，能否把我拉进去呢，谢啦。 @NPCSun 群已经可以加了 我现在使用方式是先写入 Redis 再进行消费订阅。
356,canal异常求解 com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.collect.ComputationException: com.alibaba.otter.canal.parse.exception.CanalParseException: fetch failed by table meta:`mysql`.`ha_health_check` Caused by: com.google.common.collect.ComputationException: com.alibaba.otter.canal.parse.exception.CanalParseException: fetch failed by table meta:`mysql`.`ha_health_check` 	at com.google.common.collect.MapMaker$ComputingMapAdapter.get(MapMaker.java:889) ~[guava-18.0.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:70) ~[canal.parse-1.0.22.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:645) ~[canal.parse-1.0.22.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:357) ~[canal.parse-1.0.22.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:111) ~[canal.parse-1.0.22.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:62) ~[canal.parse-1.0.22.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:327) ~[canal.parse-1.0.22.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:177) ~[canal.parse-1.0.22.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:121) ~[canal.parse-1.0.22.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210) ~[canal.parse-1.0.22.jar:na] 	at java.lang.Thread.run(Thread.java:722) [na:1.7.0_10] canal版本是1.0.22，不知道这个异常是什么意思，求大神帮忙解答下 求解，我也是遇到这种问题 升级版本吧，修复过 我使用最新版本也是报这样的错误，请问有什么解决办法吗
355,当数据库有大量的删除时，canal解析变慢 当数据库有大量的删除时，canal解析变慢，之前每个小时可以解析5万，有大量删除后每个小时才100条，请问有什么可以解决方法吗？谢谢 这个和#267 是同一类问题 mysql配置如下参数：innodb_flush_log_at_trx_commit 在提交事务的时候是否提交缓冲? 有3个值： 0：不会主动触发日志缓冲写入磁盘 1（默认）：每次提交事务的时候，同时会把日志缓冲刷新到磁盘 2：每次提交事务的时候，会把日志缓冲刷新到磁盘，但是他不是同时进行的 而是每秒钟刷新一次 建议配置成2 ，对性能影响较大。 可以试试这个
354,启动cancl，连接数据库连接不上 2017-08-01 09:27:36.766 [main] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2017-08-01 09:27:36.770 [main] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties] 2017-08-01 09:27:36.850 [main] WARN  org.springframework.beans.TypeConverterDelegate - PropertyEditor [com.sun.beans.editors.EnumEditor] found through deprecated global PropertyEditorManager fallback - consider using a more isolated form of registration  e.g. on the BeanWrapper/BeanFactory! 2017-08-01 09:27:36.971 [main] INFO  c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-example  2017-08-01 09:27:36.987 [main] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - start successful.... 2017-08-01 09:27:44.004 [destination = example   address = /192.168.**.**:****   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /192.168.30.46:3306 has an error  retrying. caused by  com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: connect /192.168.**.**:**** failure:java.net.ConnectException: Connection timed out 	at sun.nio.ch.Net.connect0(Native Method) 	at sun.nio.ch.Net.connect(Net.java:364) 	at sun.nio.ch.Net.connect(Net.java:356) 	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:623) 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:70) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:56) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:86) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:157) 	at java.lang.Thread.run(Thread.java:722) 报错信息如上，我不知道是不是这个/的问题，但是我在配置文件并没有/的，看了源码也没有，不知道是什么原因，求助！！！！万分感谢 这个肯定是自己连接有问题，另外，1.0.24 还是别用的好。。。
353,canal如何配置多个数据库下的表 请问下，canal如何指定订阅几个数据库的若干表，比如我只订阅 oschina库下的order表和oschina_user下的user表 翻翻之前的issue 正确的姿势应该是client写入filter see https://github.com/alibaba/canal/issues/311  多个库？ 那是canal server 配置 多个instance吧
352,canal  报错：  Read Q_FLAGS2_CODE error: limit excceed: 63 大数据量的批量插入 类似select into 的sql （10w条记录），canal 报错： pid:1 nid:2 exception:canal:canal_master:java.io.IOException: Read Q_FLAGS2_CODE error: limit excceed: 63 at com.taobao.tddl.dbsync.binlog.event.QueryLogEvent.unpackVariables(QueryLogEvent.java:650) at com.taobao.tddl.dbsync.binlog.event.QueryLogEvent.(QueryLogEvent.java:477) at com.taobao.tddl.dbsync.binlog.LogDecoder.decode(LogDecoder.java:154) at com.taobao.tddl.dbsync.binlog.LogDecoder.decode(LogDecoder.java:106) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:123) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) at java.lang.Thread.run(Thread.java:745) Caused by: java.lang.IllegalArgumentException: limit excceed: 63 at com.taobao.tddl.dbsync.binlog.LogBuffer.getUint32(LogBuffer.java:561) at com.taobao.tddl.dbsync.binlog.event.QueryLogEvent.unpackVariables(QueryLogEvent.java:563) ... 6 more 报告一下mysql和canal版本 我这边也有这个问题 canal是1.0.22，Mysql是5.7.12-5-log  @jwcjlu 先测试一下1.0.26版本试试 测试过了1.0.25就没有问题了
351,batchSize指什么 您好，试了下 有阻塞的getWithoutAck，发现在更新到很多次的时候才会打印(example)，但是batchSize设置的是1. 有点不太明白，batchSize到底指的是什么，我以为是一次修改，目前看应该不是的。 Batch Id: [3]  count : [33]   memsize : [1050]   Time : 2017-07-29 23:52:08 * Start : [mysql-bin.000003:17576:1501343167000(2017-07-29 23:46:07)]  * End : [mysql-bin.000003:19943:1501343529000(2017-07-29 23:52:09)]  这是打印的日志。  麻烦~ batchSize 是在client端设置的，这个参数作用是控制 每次从server端的store中get数据的size 那这个并不等于   数据库的一次修改是把？因为上面看的，一次有33个entries。  这个是在什么情况下server才会有一条数据啊 是啊，我也不是很明白batchSize具体的作用。当我设置batchSize=1的时候，每次拉取到的Message里面，是有多个entry的。 batchSize是刚好超过这个大小的，最合适的记录数.  最小是1个Message @agapple 嗯嗯～谢谢啦
350,canal解析json类型出错 和#330中提到的问题一致，mysql版本是5.7 sql如下： a='"a"'+' "a"'*30000 sql="insert into canal_json_test(j_json) values('[%s]')" % (a) 即一个json字段的值为'["a" "a" "a"……]' 共30001个a，可以正常插入mysql，但是canal解析这个出错了 和#330中提到的问题一致， sql如下： a='"a"'+' "a"'*30000 sql="insert into canal_json_test(j_json) values('[%s]')" % (a) 即一个json字段的值为'["a" "a" "a"……]' 共30001个a，可以正常插入mysql，但是canal解析这个出错了 我尽快修复一下，应该不难 我已经试着修复了，应该是获取json值长度的时候用了getUnit16导致的，应该是用getUnit32吧
349,关于解析Mysql5.7中json类型的一些疑问 case LogEvent.MYSQL_TYPE_JSON: {                 len = buffer.getUint16();                 buffer.forward(meta - 2);                 int position = buffer.position();                 JsonConversion.Json_Value jsonValue = JsonConversion.parse_value(buffer.getUint8()  buffer  len - 1  charsetName);                 StringBuilder builder = new StringBuilder();                 jsonValue.toJsonString(builder  charsetName);                 value = builder.toString();                 buffer.position(position + len);                 // byte[] binary = new byte[len];                 // buffer.fillBytes(binary  0  len);                 // value = binary;                 javaType = Types.VARCHAR;                 length = len;                 break;             } 这部分是解析json类型的代码，其中不理解： 1.buffer.forward(meta - 2);这里为什么要回退 2.JsonConversion.Json_Value jsonValue = JsonConversion.parse_value(buffer.getUint8()  buffer  len - 1  charsetName);  这里为什么是len-1。 望大神解答啊 @agapple
348,canal server停用一个月后，重新启用报错    canal server停用一个月后，重新启用报错，找不到之前解析文件位置，请问canal当找不到原来已解析过的文件位置时，不能从最新位置开始解析吗？谢谢！ [destination = xxx   address = /1.1.1.1:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:xxx [java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Could not find first log file name in binary log index file         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209)         at java.lang.Thread.run(Thread.java:745) 指定了位点不能吧，要从最新开始消费，直接删除位点试试 或者不指定位点就是从最新开始 没有指定位点，请问如何删除位点，谢谢！ 我那个问题应该是binlog清理掉之前的文件，然后导致找不到从位点解析binlog的问题吧 是的 指定位点了就会，不指定应该是从最新开始消费的 嗯嗯，谢谢！修改位点是不是只能修改zookeeper里存放offset的数据呀！
347,mysql使用utf8mb4出现中文乱码 ![image](https://user-images.githubusercontent.com/5357638/28571083-c7fade36-7173-11e7-8cef-2484de5b1b04.png) 具体乱码的部分是这样的。因为有一些业务敏感字段，所以打了码，见谅。 找到问题了。不是canal的问题、。 原因是什么呢，我也遇到类似的问题 记不太清楚了。好像是我们用的druid版本太低还是什么。我再想想……
346,AbstractEventParser设置parseThread.setName的时候报空   parseThread.setName(String.format("destination = %s   address = %s   EventParser"             destination             runningInfo == null ? null : runningInfo.getAddress().toString())); 应该去掉toString()方法 已提交
345,canal为什么获取tableName和schemaName为空  header {   version: 1   logfileName: "mysql-bin.000068"   logfileOffset: 719945754   serverId: 125   serverenCode: "UTF-8"   executeTime: 1500862055000   sourceType: MYSQL   schemaName: ""   tableName: ""   eventLength: 217   eventType: QUERY } entryType: ROWDATA storeValue: "\020\aZ\301\001insert into productinfotemp(SysId Price RealityPrice SkuCount Name Code SaleDate Status) values (\'eab6d30a-2436-4c3b-b493-3c997fd8d8fb\' 10.00 10.00 1.00 \'\' \'E0205004\' \'2017-07-24 00:00:00 \' 99)" QUERY语句，不是ROW模式的行记录吧 我也有很多这样的数据 insert，update都有 2017-09-12 14:09:57 417[INFO]yike.canal.AbstractCanalClientTest.printEntry(AbstractCanalClientTest.java:206) ----------------> binlog[mysql-bin.002059:3086158]   name[xmall ]   eventType : UPDATE   executeTime : 1505196597000   delay : 417ms 2017-09-12 14:09:57 417[INFO]yike.canal.AbstractCanalClientTest.printEntry(AbstractCanalClientTest.java:206) ----------------> binlog[mysql-bin.002059:3086397]   name[xmall ]   eventType : INSERT   executeTime : 1505196597000   delay : 417ms 2017-09-12 14:09:57 606[INFO]yike.canal.AbstractCanalClientTest.printSummary(AbstractCanalClientTest.java:155) canal新版本已经通过SQL解析工具，完整提取tableName，schemaName在binlog里无记录
344,canal如何只注册insert的事件？update，delete，select都不关心，不需要发送到客户端 canal如何只注册insert的事件？update，delete，select都不关心，不需要发送到客户端 ``` if (!(eventType == EventType.DELETE|EventType.SELECT|EventType.UPDATE)) {} ```
343,Bugfix when filterqueryddl 如果ddl被过滤掉了。meta cache就不会发生变化。 补充的字段名就会发生错乱。 [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=343) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=343) before we can accept your contribution.<br/>**1** out of **2** committers have signed the CLA.<br/><br/>:white_check_mark: toruneko<br/>:x: jianhao.dai<br/><hr/>**jianhao.dai** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=343) it.</sub> tks. 先合并 ps. 近期会对meta cache做一个比较大的调整，支持一下表结构版本的归档存储，解决删列、删表的问题
342,怎样才能把数据同步到从库 本人新手一个，有问题想请教各位  环境：v1.0.24  mysql：5.6  问题：客户端已经运行成功，能捕获到主数据的二进制文件，但是怎样把二进制文件还原成从库的数据？ 我理解是你需要自己解析读到的entry，然后把它转成SQL在从库中执行。 Canal 并没有提供数据存储的。 现成的是otter   db-->db
341,zookeeper挂了重启后，canal server的同步停止了      canal运行中，zookeeper不知什么原因挂了，然后canal server开始报连不上zookeeper的错误， 于是stop了canal server，然后重启zookeeper、canal server和canal_client。      重启后发现canal_client在getWithoutAck()这个地方卡住了，一直获取不到Message，所以同步 也就卡住了，经检查主库的日志，发瑞已经到了binlog.000446，而canal_server从zookeeper中 获取到的日志为binlog.000440，已经差了6个日志没有同步了。     后来发现，cannal每同步一段时间，就会停住，过一段时间后，会继续同步，然后周而复始的 卡主的异常原因要看看，是否是表结构删除性质导致
339,the max size of buffer must not more than 1G!!! if buffer = bufferSize * bufferMemUnit = 2G than  (bufferSize * bufferMemUnit) = 2097152*1024 > max length of int;  cause endless loop in MemoryEventStoreWithBuffer.tryPut & checkFreeSlotAt so that  the max size of buffer = 1G!!! why not use long type? bufferSize = 2097152 ?  过大的bufferSize，并不会对性能有很大的帮助哦，建议bufferSize控制在32768，基本就有几万的TPS
338,rowChage.getRowDatasList()数据为空 mysql版本5.6.19 开启binlog  ROW 类型 解析文件事件类型都是eventType: QUERY，无update insert事件类型 ，请问是什么原因 canal.zkServers=192.168.0.14:2181 canal.instance.global.spring.xml  配置  file-instance.xml 内存+文件就有数据 canal.instance.global.spring.xml   使用default-instance.xml HA 方式无数据
337,mixed模式下，如何获取sql语句    请问一下，在mixed模式下，怎么获取sql语句
336,异步ack的时候，ack线程出现空指针异常，get线程阻塞在BooleanMutex，是否与设置ring buffer size有关？ client端ack线程出现空指针异常，get线程阻塞在BooleanMutex `"SyncThread5" #27 prio=5 os_prio=0 tid=0x00007fc880f62800 nid=0x549 waiting on condition [0x00007fc854302000]    java.lang.Thread.State: WAITING (parking)         at sun.misc.Unsafe.park(Native Method)         - parking to wait for  <0x0000000735ee1df0> (a com.alibaba.otter.canal.common.utils.BooleanMutex$Sync)         at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)         at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)         at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)         at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)         at com.alibaba.otter.canal.common.utils.BooleanMutex$Sync.innerGet(BooleanMutex.java:123)         at com.alibaba.otter.canal.common.utils.BooleanMutex.get(BooleanMutex.java:53)         at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.waitClientRunning(SimpleCanalConnector.java:425)         at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:256)         at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:252)         at com.alibaba.otter.canal.client.impl.ClusterCanalConnector.getWithoutAck(ClusterCanalConnector.java:180)         at com.dfire.obd.core.Launcher$SyncThread.run(Launcher.java:105)` server端出现：2017-07-04 14:28:15.068 ERROR com.alibaba.otter.canal.server.netty.NettyUtils [New I/O server worker #1-4]；ErrotCode:400   Caused by :  something goes wrong with channel:[id: 0x2ce9c057  /10.26.0.247:41053 :> /10.26.0.9:11111]  exception=com.alibaba.otter.canal.server.exception.CanalServerException: ack error   clientId:1001 batchId:14800952 is not exist   please check 尝试一下canal新版本
335,一个canal sever上instance太多影响性能怎么办? 因为instance可能很多  canal能实现部署多套主备吗? 可以的，将instance分散到多台server中
334,针对使用netty取代JDK的socketchannel实现的优化 1，SocketChannel由wait(time)取代wait-notify 2，读写直接采用byte[]，取代之前的ByteBuff转byte[] 3，修改PacketManager的方法，并调整相关的引用 4，至于使用了Jboss.netty，可以进行exclude，这里没有提交（不影响） [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=334) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=334) before we can accept your contribution.<br/>**1** out of **2** committers have signed the CLA.<br/><br/>:white_check_mark: agapple<br/>:x: luoyaogui<br/><hr/>**luoyaogui** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=334) it.</sub> tks，我验证一下
333,fix bug：异常网络连接导致canal server 无法关闭 现实运用中出现：client已经不存在但server端的连接仍然是存活状态。 在此种网络异常状态时，stop canal server会一直hang住。 原因：异常网络连接由于无数据传输，导致 io worker线程一直loop selector而无法退出。 解决方案：1. 设置keepalive 由os 监控。         2. 设置netty idle 机制，定时检测并主动close socket。(fix 时间单位bug)         3. 每次新创建socket channel时存储在一个容器内，在stop canal server时，从容器内主动释放所有socket channel。 [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=333) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=333) before we can accept your contribution.<br/><hr/>**Jason Huang** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=333) it.</sub> tks
332,HA方式，kill掉进程不能切换 hi，HA方式时，我手工kiil掉正在运行的进程，客户端程序不能切换，在zookeeper查看已经服务已经切换，请问是什么回事呢？谢谢！ 另：正常关闭服务时，程序可以切换服务 DEBUG org.apache.zookeeper.ClientCnxn - Got ping response for sessionid: 0x35ca6b0127d780e after 0ms 一直都在打印这样的语句 这个是DEBUG输出，忽略吧
331,update zkclient to 0.10 没有做过多的改动，发现zkclient原包里提供的ZkConnection跟canal自己实现的ZooKeeperx有很多代码上的重叠，由于不是太清楚这部分的代码的历史，所以简单的继承了ZkConnection， 还是留给canal维护者自己决定下，这里代码的去留吧。 @agapple  ## Update: 第一个commit还是有bug的， 修改了之后提了第二个commit。 目前的测试没有问题。 [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=331) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=331) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=331) it.</sub> tks @agapple   下次发版什么时候？
330,MySQL5.7 JSON解析问题 canal 版本1.0.24 写入MySQL的JSON数据，Canal解析失败 2017-06-22 11:39:25.104 [destination = example   address = /127.0.0.1:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: java.lang.IllegalArgumentException: illegal json data         at com.taobao.tddl.dbsync.binlog.JsonConversion.parse_array_or_object(JsonConversion.java:81)         at com.taobao.tddl.dbsync.binlog.JsonConversion.parse_value(JsonConversion.java:61)         at com.taobao.tddl.dbsync.binlog.event.RowsLogBuffer.fetchValue(RowsLogBuffer.java:955)         at com.taobao.tddl.dbsync.binlog.event.RowsLogBuffer.nextValue(RowsLogBuffer.java:99)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseOneRow(LogEventConvert.java:495)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:376)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:108)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:62)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:326)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:176)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:130)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209)         at java.lang.Thread.run(Thread.java:745) 另外如果json中包含双引号，canal解析之后丢失了转义符号。 ![image](https://user-images.githubusercontent.com/5357638/27416733-e8f73ba8-5741-11e7-96c0-2a0de5ddc8af.png) 提供一下测试SQL 两个问题，第一个问题还没找到出问题的SQL， 第二个是canal已经解析了，并且canal client可以拿到数据。 +-------+-------------+------+-----+-------------------+-------+ | Field | Type        | Null | Key | Default           | Extra | +-------+-------------+------+-----+-------------------+-------+ | id    | int(11)     | NO   | PRI | NULL              |       | | dt    | timestamp   | NO   |     | CURRENT_TIMESTAMP |       | | json1 | json        | YES  |     | NULL              |       | | aa    | varchar(50) | YES  |     | NULL              |       | +-------+-------------+------+-----+-------------------+-------+ `insert into t1(id  json1  aa) values(3 "{\"test\":\"aa\\\"aa\"}"  "a");` 查回来是这样的 +----+---------------------+--------------------+------+ | id | dt                  | json1              | aa   | +----+---------------------+--------------------+------+ |  3 | 2017-06-22 14:34:19 | {"test": "aa\\\"aa"} | a    | +----+---------------------+--------------------+------+ canal client取出的json如图。 变成了`{"test":"aa"aa"}`。 找到导致illegal json data 的SQL了。 json大概有25000个字节，就补贴出来了。 会不会是因为json太大，导致bytes和len长度计算错误。 找到导致illegal json data 的SQL了。 json大概有25000个字节，就补贴出来了。 会不会是因为json太大，导致bytes和len长度计算错误。 ---------------------------------------------------------------- @toruneko   你这个问题怎么解决的？ @lan1994 最后我们没有用json了。改了方案。。 遇到同样问题，求解。 我尽快修复一下，应该不难 @agapple   技术专家，您好，有个问题请教一下： 最近我们项目改造，数据库postgresql切换到mysql用到了json类型的字段，mysql版本升级到5.7才能支持，想问一下canal升级到v1.1.0，是否支持json类型的字段？ canal是向前兼容的，支持json解析 @agapple  没想到这么快就回复了，谢谢大咖！
329,canal在同步报错的情况下，可以跳过这个错误么？ 据测试，canal在同步sql报错的时候，会一直重复的去执行这个sql，导致position位置停止，假如由于手误导致有一条binlog的sql，比如表不存在这种，就会导致canal停滞，请问大神有跳过同步错误的方法么？ 目前只能重置一下位点，后续可以考虑增加DDL变更能力的支持
328,canal对MySQL5.7有支持吗？ canal对MySQL5.7有支持吗？谢谢 可以的，除了json对象支持会有部分缺陷 当前主干已修复json对象过大的解析问题
327,canal-server最近会莫名挂掉，没有任何日志的那种。 我的canal-server配置了四个destination，莫名挂掉之后，所有的canal-client就开始疯狂刷这样的错误： 2017-06-17 00:21:19.110 [Thread-1] ERROR com.alibaba.otter.canal.example.AbstractCanalClientTest - process error! com.alibaba.otter.canal.protocol.exception.CanalClientException: failed to subscribe with reason: something goes wrong with channel:[id: 0x1e023246  /**.**.**.**:39396 => /**.**.**.**:1 1111]  exception=com.alibaba.otter.canal.server.exception.CanalServerException: destination:*** should start first 然后机器的负载就飙到特别高。 请问有大神遇到过类似的问题么？
326,如果我要同步好几个不同地址下的数据库的数据，是不是要一个canal-server，和多个canal-client？ canal server 没关系吧，canalclient 跟destination是一对一的 otter+canal LS正解
325,canal-server能否直接接入MQ直接发送消息，而不通过canal-client 希望通过直接改造canal-server直接接入公司的消息队列，binlog直接通过消息负载到集群节点。canal仅需要服务端就行？这种应该如何做呢？   [maxwell](https://github.com/zendesk/maxwell)      [debezium](https://github.com/debezium/debezium)可以支持全量+增量
324,升级zkclient到0.10 @agapple   希望把zkclient升级到0.10 # 优点： 1. 在0.10版本里， Switch from Log4j to Slf4j，这方便我们把debug log打开调试 1. 一些bug fix # 缺点 1.  0.10 对一些接口增加一些方法，canal要适当的做些调整 如果方便，给我提交一个PR吧 #142 ， 不升级的话，还会遇到142的问题 #331 ， 提交了一个PR，还请review一下， @agapple 
323,ClusterCanalConnector——zookeeper模式下连接canal过慢 前提：集群搭建没有问题，java直接连接zk速度也很快，用Simple模式也很快，但是， CanalConnector connector = CanalConnectors.newClusterConnector("200.200.200.64:2181"  "es"  ""  "");         int batchSize = 1000;         int emptyCount = 0;         try {             System.out.println("start connect");             connector.connect();             System.out.println("connect successed!");             connector.subscribe("\\..*");             System.out.println("subscribeed !!"); ... 在connector.subscribe("\\..*"); 非常慢，有时候还需要重启两次才能正常连接！！ debug查看是 public void subscribe(String filter) throws CanalClientException {         this.waitClientRunning(); 发生问题，有什么办法可以解决？ 你这是基于zookeeper的集群模式启动，上一次异常退出时尽量使用平滑退出，不要kill -9，正确调用disconnect
322,怎么在docker容器里面启动canal的服务端 怎么在docker容器里面启动canal的服务端，求指导方便简单的方式；
321,指定位点启动出错 版本：1.0.24 问题： 你好，canal本身是支持指定位点开始解析的，但是解析时会报TableIdNotFoundException，我看源码有对这个异常进行处理，但是实际上抛出的异常是CanalParseException，导致捕捉并不成功。 如截图： ![rtx](https://user-images.githubusercontent.com/29394152/27125044-9910f768-5125-11e7-9971-9309d016f512.jpg) 抛出exception的类型是**CanalParseException**， 其cause才是**TableIdNotFoundException** 
320,关于解析位点与消费位点 canal真好用，谢谢你们的付出。但是我发现一个问题（不知道是不是真的问题） 场景： canal server和client都在运行中，此时往数据库变更很多数据（假设十万） 现在假设parser已经解析完，位点处于binlog最新的位置，但client消费很慢，只消费了一万条数据。此时如果server挂了，然后重新启动，读取zk上的解析位点（binlog最新位置），此时server上的binlog数据只有最新的，而client还没消费完，此时client再获取数据，是不是就获取不到那九万条没消费的数据？ https://github.com/alibaba/canal/blob/master/parse/src/main/java/com/alibaba/otter/canal/parse/inbound/mysql/MysqlEventParser.java#L314 https://github.com/alibaba/canal/blob/master/parse/src/main/java/com/alibaba/otter/canal/parse/inbound/mysql/MysqlEventParser.java#L330 @luyee 你好，你贴这个不是取解析位点吗，然后呢。。 @xzhhh canal重启后是从上一次的读取位点开始，该binlog位置信息存放于conf/\<instance\>/meta.dat文件中。文件内容类似： `{"clientDatas":[{"clientIdentity":{"clientId":1001 "destination":"example_db" "filter":""} "cursor":{"identity":{"slaveId":-1 "sourceAddress":{"address":"127.0.0.1" "port":3306}} "postion":{"included":false "journalName":"mysql-bin.000032" "position":18169170 "serverId":1 "timestamp":1497426548000}}}] "destination":"trc_account_db"}` 其中binlog位置字段 为position 但可能会存在这样的问题，server长时间没有更新binlog位置后（如停机/或长期无消费），mysql服务器由于某种原因清除了之前的binlog文件，则会导致读取异常，需要重新定位binlog位置从指定位置读取 @mfkugergvh 我知道canal重启会从上一次位点开始读取，但我问题是，如果解析的位点在消费位点后面很多的时候canal挂了，重启之后那些未消费的数据不是已经不在canal里了吗？因为目前eventStore只有基于内存版的实现，重启之后那些原来存在内存里的未消费的event也已经没了吧。 每次从mysql获取binlog不是，都要从指定起始位置开始嘛， 所以我贴的是解析起始位点代码，大概是每次canal server 启动都会去拿上次客户端消费后（ack）的positon（如果有）去mysql获取binlog  所以即使canal server 获取了10w 条记录，canal client 只消费了1000条后canal server挂了， 如果ack成功更新了这1000条记录，下次从1000开始消费，如果没有ack成功，应该从头开始消费，就会有重复！至于你说的消费不到，除非journalname，postion  timestamp 都没配置，就是从mysql binlog(show master status)最近的开始消费 LS理解正确 
319,fixbug: soTimeout doesn't work. 描述： 当网络不好或其他情况，Server关闭连接，但Client未收到Fin包，连接将长时间处于半连接状态。 原因： SocketChanal 中设置timeout是不生效，改用socket读取即可。见：http://bugs.java.com/bugdatabase/view_bug.do?bug_id=4614802 [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=319) <br/>All committers have signed the CLA. 你的soTimeout参数，没看到有在client层面进行设置吧？ @agapple 设置了，但是没有效果，需要改用socket处理。 你确认都提交全了么？我感觉只改动了client的代码，server和mysql之间的soTimeout的socket替换貌似没提交么？ tks  Hi Thanks for the commit. I have met the same issue but it happens between canal server instance and mysql replication master. Any plan to fix it? Thanks mailzyok
318,canal example报错：something goes wrong when getWithoutAck data from server:null canal example 启动了两台 相当于主从把 一台挂了会自动切换到另一台 运行中出现如下异常 2017-06-08 02:39:42.964 [Thread-3] WARN  c.alibaba.otter.canal.client.impl.ClusterCanalConnector - something goes wrong when getWithoutAck data from server:null com.alibaba.otter.canal.protocol.exception.CanalClientException: java.nio.channels.AsynchronousCloseException 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:281) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:252) 	at com.alibaba.otter.canal.client.impl.ClusterCanalConnector.getWithoutAck(ClusterCanalConnector.java:180) 	at com.alibaba.otter.canal.example.AbstractCanalClientTest.process(AbstractCanalClientTest.java:131) 	at com.alibaba.otter.canal.example.AbstractCanalClientTest$2.run(AbstractCanalClientTest.java:98) 	at java.lang.Thread.run(Thread.java:748) Caused by: java.nio.channels.AsynchronousCloseException 	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:205) 	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:407) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.read(SimpleCanalConnector.java:376) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.readNextPacket(SimpleCanalConnector.java:366) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.receiveMessages(SimpleCanalConnector.java:286) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:279) 	... 5 more 2017-06-08 02:39:43.088 [ZkClient-EventThread-17-10.1.102.181:3001] ERROR c.a.otter.canal.client.impl.running.ClientRunningMonitor - There is an error when execute initRunning method  with destination [example]. com.alibaba.otter.canal.protocol.exception.CanalClientException: java.nio.channels.ClosedChannelException 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.doConnect(SimpleCanalConnector.java:171) ~[canal.client-1.0.24.jar:na] 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.access$000(SimpleCanalConnector.java:48) ~[canal.client-1.0.24.jar:na] 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector$1.processActiveEnter(SimpleCanalConnector.java:396) ~[canal.client-1.0.24.jar:na] 	at com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor.processActiveEnter(ClientRunningMonitor.java:207) ~[canal.client-1.0.24.jar:na] 	at com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor.initRunning(ClientRunningMonitor.java:118) ~[canal.client-1.0.24.jar:na] 	at com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor$1.handleDataDeleted(ClientRunningMonitor.java:71) [canal.client-1.0.24.jar:na] 	at org.I0Itec.zkclient.ZkClient$6.run(ZkClient.java:549) [zkclient-0.1.jar:na] 	at org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71) [zkclient-0.1.jar:na] Caused by: java.nio.channels.ClosedChannelException: null 	at sun.nio.ch.SocketChannelImpl.ensureReadOpen(SocketChannelImpl.java:257) ~[na:1.8.0_131] 	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:300) ~[na:1.8.0_131] 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.read(SimpleCanalConnector.java:376) ~[canal.client-1.0.24.jar:na] 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.readNextPacket(SimpleCanalConnector.java:369) ~[canal.client-1.0.24.jar:na] 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.doConnect(SimpleCanalConnector.java:133) ~[canal.client-1.0.24.jar:na] 	... 7 common frames omitted 2017-06-08 02:39:43.114 [ZkClient-EventThread-17-10.1.102.181:3001] WARN  c.a.otter.canal.client.impl.running.ClientRunningMonitor - canal is not run any in node 2017-06-08 02:39:48.103 [Thread-3] INFO  c.alibaba.otter.canal.client.impl.ClusterCanalConnector - restart the connector for next round retry. 2017-06-08 11:34:56.529 [main-SendThread(10.1.102.181:3001)] INFO  org.apache.zookeeper.ClientCnxn - Unable to read additional data from server sessionid 0x15c83dad6dc0000  likely server has closed socket  closing socket connection and attempting reconnect 2017-06-08 11:34:59.029 [main-SendThread(10.1.102.181:3001)] INFO  org.apache.zookeeper.ClientCnxn - Opening socket connection to server 10.1.102.181/10.1.102.181:3001. Will not attempt to authenticate using SASL (unknown error) 2017-06-08 11:34:59.310 [main-SendThread(10.1.102.181:3001)] INFO  org.apache.zookeeper.ClientCnxn - Socket connection established to 10.1.102.181/10.1.102.181:3001  initiating session 2017-06-08 11:35:00.488 [main-SendThread(10.1.102.181:3001)] WARN  org.apache.zookeeper.ClientCnxn - Session 0x15c83dad6dc0000 for server 10.1.102.181/10.1.102.181:3001  unexpected error  closing socket connection and attempting reconnect java.io.IOException: Broken pipe 	at sun.nio.ch.FileDispatcherImpl.write0(Native Method) ~[na:1.8.0_131] 	at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47) ~[na:1.8.0_131] 	at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93) ~[na:1.8.0_131] 	at sun.nio.ch.IOUtil.write(IOUtil.java:65) ~[na:1.8.0_131] 	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:471) ~[na:1.8.0_131] 	at org.apache.zookeeper.ClientCnxnSocketNIO.doIO(ClientCnxnSocketNIO.java:117) ~[zookeeper-3.4.5.jar:3.4.5-1392090] 	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:355) ~[zookeeper-3.4.5.jar:3.4.5-1392090] 	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1068) ~[zookeeper-3.4.5.jar:3.4.5-1392090] 这个我也在1.0.24中发现了。在执行过程中，把canal server停掉，然后canal client就一直会有这个问题，把canal server重启启动，canal client还是在那里一直报错，不会选择新的server重连，具体的问题我也还在调试中。 @agapple  调试了一下，是因为自己的用法导致的，我把我创建的cluster client放到了一个pool里面，注意下面的code ```                    CanalConnector connector = CanalConnectors.newClusterConnector(connectionPath  destination  ""  "");             connector.connect();             connector.subscribe(); ``` 这里会抛出CanalClientException， 这是个runtime exception 所以当`connect`抛出异常的时候，记得要捕获一下，然后`disconnect`掉。 ```         try {             connector.connect();             connector.subscribe();         } catch (Throwable t) {             logger.error("failed to connect to canal server"  t);             connector.disconnect();             throw t;         } ``` @xuliangyong ， 不知道你的是不是跟我一样的问题，希望对你有帮助。 @alexandnpu  我也碰到过这种情况，而且检查TCP连接时，发现很多事CLOSE_WAIT状态的。 最后也是通过catch异常然后disconnect解决的。
317,把配置文件移动到另一个目录，需要修改哪些内容？   你好！请问我是想把conf目录下的配置文件移动到其它目录，需要修改哪些内容呢？谢谢 关注instance.properties
316,MySQL5.6.10连接失败 Exception in thread "main" com.alibaba.otter.canal.protocol.exception.CanalClientException: something goes wrong with reason: something goes wrong with channel:[id: 0x6ac75c6a  /127.0.0.1:51803 => /127.0.0.1:11111]  exception=java.lang.NullPointerException 	at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.calculateSize(MemoryEventStoreWithBuffer.java:535) 	at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.doGet(MemoryEventStoreWithBuffer.java:312) 	at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.tryGet(MemoryEventStoreWithBuffer.java:251) 	at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getEvents(CanalServerWithEmbedded.java:460) 	at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:296) 	at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:259) 	at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:123) 	at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:100) 	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) 	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:783) 	at org.jboss.netty.channel.SimpleChannelHandler.messageReceived(SimpleChannelHandler.java:154) 	at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:100) 	at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) 	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) 	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:783) 	at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:275) 	at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80) 	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) 	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:783) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndFireMessageReceived(ReplayingDecoder.java:525) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:506) 	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:443) 	at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80) 	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) 	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261) 	at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:349) 	at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:280) 	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:200) 	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) 	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:44) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:748) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.receiveMessages(SimpleCanalConnector.java:302) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:279) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:252) 具体版本是？ 
315,canal client可以做集群负载均衡方式吗？ 你好！请问canal client可以做集群负载均衡方式吗？是否能有个例子简单说明一下，谢谢！ ClientRunningMonitor / ServerRunningMonitors  maybe useful 你说的应该是HA方式吧，但是我想实现，比如当一个节点负载太多，然后就会自动分配到另一个节点来运行，这可以吗？谢谢  不支持LB吧，其实是standby模式吧 see  关于canal高可用的问题https://github.com/alibaba/canal/issues/147 谢谢
314,ClientRunningMonitor    ConnectException 在CentOS7.2的服务器上，运行着运行着就发现抛出了ConnectException，提示连接异常。请问这个是哪部分出了问题？ ``` [2017-06-02 17:01:03.295] [ERROR] [ZkClient-EventThread-20-10.10.10.81:2181 10.10.10.82:2181 10.10.10.33:2181] [c.a.o.c.client.impl.running.ClientRunningMonitor] >>> There is an error when execute initRunning method  with destination [db10101067002]. com.alibaba.otter.canal.protocol.exception.CanalClientException: java.net.ConnectException: Connection timed out         at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.doConnect(SimpleCanalConnector.java:171) ~[canal.client-1.0.23.jar:na]         at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.access$000(SimpleCanalConnector.java:48) ~[canal.client-1.0.23.jar:na]         at com.alibaba.otter.canal.client.impl.SimpleCanalConnector$1.processActiveEnter(SimpleCanalConnector.java:396) ~[canal.client-1.0.23.jar:na]         at com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor.processActiveEnter(ClientRunningMonitor.java:207) ~[canal.client-1.0.23.jar:na]         at com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor.initRunning(ClientRunningMonitor.java:118) ~[canal.client-1.0.23.jar:na] ``` 环境问题，自行排查
313,可以instance列表进行连接吗？ canal.destinations=example，example2，...... 服务器可以配置多个实例列表 而连接时只支持单个实例，为什么不支持多个？ String destination=example; connector = CanalConnectors.newClusterConnector(addresses  destination  username  password); connector = CanalConnectors.newSingleConnector(new InetSocketAddress(ip  port)  destination username  password);   destination参数为什么不能和配置参数一样支持多个，比如: destination=example，example2，...... 好像文档有说吧 一个client只支持单个instance @liyong2008   这样的话  假设20个instance  需要部署至少20 个client？ group-instance.xml介绍： 主要针对需要进行多库合并时，可以将多个物理instance合并为一个逻辑instance，提供客户端访问。 场景：分库业务。 比如产品数据拆分了4个库，每个库会有一个instance，如果不用group，业务上要消费数据时，需要启动4个客户端，分别链接4个instance实例。使用group后，可以在canal server上合并为一个逻辑instance，只需要启动1个客户端，链接这个逻辑instance即可.
312,canal对MySQL5.7有支持吗？ canal版本：1.0.24 MySQL版本：5.7.18 对以上版本不支持 目前已知对mysql 5.7json类型在转义字符上有一些bug，其余的暂没人反馈有问题 我这边的版本是 ` mysql  Ver 14.14 Distrib 5.7.17  for osx10.12 (x86_64) using  EditLine wrapper ` 从log里面看是 `subscribe successfully  ClientIdentity[destination=task-4 clientId=1001 filter=] with first position:null` 但是对mysql的数据表做更新，canal一直收不到数据 @alexandnpu  检查Canal配置是否正确，检查MySQL是否开启binlong 如果都没有问题而无法监控到数据表的变化，我遇到过这样情况，下载一个干净的canal版本然后进行配置(配置MySQL地址端口，以及destination[不要使用example])，就可以啦 @kervin521   谢谢你的回复。 我现在的问题是在mac上面做测试，启动了vagrant用作测试环境，我担心的是canal在往mysql做slave注册时候使用了虚拟机的ip，而mysql往这个ip发送binglog是找不到的 @alexandnpu  这个你不用担心，因为canal服务端口是一定的，即使借用虚拟机IP 同样的问题，我用的 canal版本：1.0.24 MySQL版本：5.7.16 请问Canal支持吗？？？ @shijiebei2009 可以的，如果不行，配置自己的destination不要用样本，并删除日志和数据记录在Canal安装目录下 
311,canal怎么只监听一个数据库 1.假如mysql中有多个数据库，只监听test数据库怎么配置？ 2.假如可以的话，怎么此库下的配置某一张表？ 3.canal.instance.defaultDatabaseName = 怎么使用 在配置文件里设置 如conf\example\instance.properties里有2个配置选项 canal.instance.filter.regex = .*\\..* 这个是白名单 如test\\..*只监听test数据库 test\\.test监控test库里的test表 多个的话用逗号隔开 但是注意这个配置是无效的 必须在客户端subscribe方法里配置 因为会覆盖 canal.instance.filter.black.regex =  这个是黑名单 排除库表 配置同上 这个配置有效 具体配置示例参照源码目录里filter部分 canal.instance.defaultDatabaseName = 这个配置项也是无效的 配不配没有关系 
310,canal同步的时候，有些binlog没有同步过来 我使用的集群模式，同步一个数据库的数据，有更新的binlog同志同步过来，但是我自己修改数据库的数据，确没有同步过来，不知道啥情况~ 关注一下你的操作binlog是否有生成 嗯，是我的问题，同步的位点太前了，一直以为没有同步过来，非常感谢！
309,ERROR ## parse this event has an error   last position : [mysql-bin.000001 1199597] java.lang.NullPointerException: null ``` 2017-05-27 16:53:34.167 [destination = example   address = /127.0.0.1:3306   EventParser] INFO  c.a.otter.canal.parse.inbound.mysql.MysqlConnection - COM_BINLOG_DUMP with position:BinlogDumpCommandPacket[binlogPosition=1199597 slaveServerId=1234 binlogFileName=mysql-bin.000001 command=18] 2017-05-27 16:53:34.168 [destination = example   address = /127.0.0.1:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - ERROR ## parse this event has an error   last position : [mysql-bin.000001 1199597] java.lang.NullPointerException: null 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:178) ~[canal.parse-1.0.25-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:130) [canal.parse-1.0.25-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210) [canal.parse-1.0.25-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_77] 2017-05-27 16:53:34.168 [destination = example   address = /127.0.0.1:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /127.0.0.1:3306 has an error  retrying. caused by  com.alibaba.otter.canal.parse.exception.CanalParseException: java.lang.NullPointerException Caused by: java.lang.NullPointerException: null 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:178) ~[canal.parse-1.0.25-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:130) ~[canal.parse-1.0.25-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210) ~[canal.parse-1.0.25-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_77] 2017-05-27 16:53:34.169 [destination = example   address = /127.0.0.1:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: java.lang.NullPointerException Caused by: java.lang.NullPointerException 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:178) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:130) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210) 	at java.lang.Thread.run(Thread.java:745) ] 2017-05-27 16:53:34.169 [destination = example   address = /127.0.0.1:3306   EventParser] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - disConnect MysqlConnection to /127.0.0.1:3306... 2017-05-27 16:53:34.169 [destination = example   address = /127.0.0.1:3306   EventParser] INFO  c.alibaba.otter.canal.pars ``` show binlog events; ``` | mysql-bin.000001 | 1199597 | Xid            |         1 |     1199628 | COMMIT /* xid=5909 */  ``` 
308,配置文件中怎么没有canal.properties 配置文件中怎么没有canal.properties
307,client的例子中，取到的ddl语句sql为什么是乱码 看了下底层的实现是先回转成utf-8编码的，但是取出来的还是乱码 那是你mysql客户端 编码不是utf8造成的。  你这个问题解决了吗？是什么原因？ 我现在也遇到类似的问题 https://github.com/alibaba/canal/issues/610 
306,怎么解析date类型 怎么解析date类型 client 都是string了，只能自己转java  type？ 是的，mysql的时间类型范围会比java Date对象定义的要大，所以用了string的泛类型 Canal 有提供 JDBC 多种类型通用的转换函数么？ 没有吧， jdbc的化， 可以看看otter项目里的  com.alibaba.otter.node.etl.common.db.utils.SqlUtils.java
305,canal+mysql5.5.35报错：can't find start position pid:1 nid:2 exception:canal:CTSDB-CANAL-master:com.alibaba.otter.canal.parse.exception.CanalParseException: can't find start position for CTSDB-CANAL-master 目标数据库是mysql-community-server-5.7.18-1 正式业务环境的数据库是mysql-5.5.35 之前在测试环境源和目的都是mysql-community-server-5.7.18-1时，成功，换到正式业务环境的mysql5.5时报错。 canal.deployer-1.0.24 node.deployer-4.2.14 manager.deployer-4.2.14 zookeeper-3.4.10 aria2-1.19.0 +1 can't find start position for CTSDB-CANAL-master，清空一下位点或者设置一个存在的位点
304,canal.destinations支持两个，CanalConnectors.newClusterConnector里的destinations只支持一个是吗 RT CanalConnectors.newClusterConnector 调用两次，就支持两个了。
303,一段时间没有去拉binlog，直至报错的疑问 18:23:55.605收到最后一个binlog请求，下一个日志就是2017-05-18 20:35:27.539的日志， 两个疑问： 1、中间两个小时的binlog为什么没有去拉 2、2017-05-18 20:35:27.539  的报错不像是找不到binlog文件，这个错误有什么经验能分享下吗？ 下面的是昨晚的日志： 2017-05-18 18:23:55.605 [New I/O server worker #1-6] INFO  c.a.otter.canal.server.embedded.CanalServerWithEmbedded - ack successfully  clientId:1001 batchId:7193361 position:PositionRange[start=LogPosition[identity=LogIdentity[sourceAddress=x/y:3306 slaveId=-1] postion=EntryPosition[included=false journalName=mysql-bin.006011 position=409700943 serverId=262651658 timestamp=1495103035000]] ack=LogPosition[identity=LogIdentity[sourceAddress=x:3306 slaveId=-1] postion=EntryPosition[included=false journalName=mysql-bin.006011 position=409702008 serverId=262651658 timestamp=1495103035000]] end=LogPosition[identity=LogIdentity[sourceAddress=x/10.157.81.57:3306 slaveId=-1] postion=EntryPosition[included=false journalName=mysql-bin.006011 position=409702008 serverId=262651658 timestamp=1495103035000]]] 2017-05-18 20:35:27.539 [destination = db50-59   address = x/y:3306   EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Connection timed out         at sun.nio.ch.FileDispatcherImpl.read0(Native Method) ~[na:1.7.0_45]         at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39) ~[na:1.7.0_45]         at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223) ~[na:1.7.0_45]         at sun.nio.ch.IOUtil.read(IOUtil.java:197) ~[na:1.7.0_45]         at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379) ~[na:1.7.0_45]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:154) ~[canal.parse-1.0.22.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70) ~[canal.parse-1.0.22.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) [canal.parse-1.0.22.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210) [canal.parse-1.0.22.jar:na]         at java.lang.Thread.run(Thread.java:744) [na:1.7.0_45] 2017-05-18 20:35:27.599 [destination = db50-59   address = x/y:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address x/y:3306 has an error  retrying. caused by java.io.IOException: Connection timed out         at sun.nio.ch.FileDispatcherImpl.read0(Native Method) ~[na:1.7.0_45]         at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39) ~[na:1.7.0_45]         at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223) ~[na:1.7.0_45]         at sun.nio.ch.IOUtil.read(IOUtil.java:197) ~[na:1.7.0_45]         at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379) ~[na:1.7.0_45]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:154) ~[canal.parse-1.0.22.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70) ~[canal.parse-1.0.22.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) ~[canal.parse-1.0.22.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210) ~[canal.parse-1.0.22.jar:na] 2017-05-18 20:35:27.599 [destination = db50-59   address = mangoerpdb1.mysql.rds.aliyuncs.com/10.157.81.57:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address mangoerpdb1.mysql.rds.aliyuncs.com/10.157.81.57:3306 has an error  retrying. caused by java.io.IOException: Connection timed out 会不会是这段时间数据库连接异常？ 中间没有日志不是代表没有拉取binlog啊
302,从mysql的从库中同步数据该怎么解析？ 手上有两台mysql，一台作为master，一台作为slave。想把canal接到slave上。但是slave机器为了和master机器同步，产生的是[hostname]-relay-bin命名的log。这种情况canal该如何和slave机器对接呢。官方文档中只有对接mysql-bin这种命名的方式。有配置的地方可以自己调整么？ 看看下面这段话对你有没有帮助。 Slave的配置与master类似，你同样需要重启slave的MySQL。如下： log_bin           = mysql-bin server_id         = 2 relay_log         = mysql-relay-bin log_slave_updates = 1 read_only         = 1 server_id是必须的，而且唯一。slave没有必要开启二进制日志，但是在一些情况下，必须设置，例如，如果slave为其它slave的master，必须设置bin_log。在这里，我们开启了二进制日志，而且显示的命名(默认名称为hostname，但是，如果hostname改变则会出现问题)。 relay_log配置中继日志，log_slave_updates表示slave将复制事件写进自己的二进制日志(后面会看到它的用处)。 有些人开启了slave的二进制日志，却没有设置log_slave_updates，然后查看slave的数据是否改变，这是一种错误的配置。所以，尽量使用read_only，它防止改变数据(除了特殊的线程)。但是，read_only并是很实用，特别是那些需要在slave上创建表的应用。 canal 是模拟MySQL的主从  你从slave 中同步  ，slave 相对于canal 来说就是master 解析slave中的二进制文件。 多谢大家，已搞定！ 请问是怎么搞定的？
301,无法使用stop.sh ``` Distributor ID:	Ubuntu Description:	Ubuntu 16.04.2 LTS Release:	16.04 Codename:	xenial ``` ``` stop.sh: 47: [: 2211: unexpected operator -e xiaojie-VirtualBox: stopping canal 2211 ...  stop.sh: 58: [: unexpected operator stop.sh: 63: stop.sh: let: not found stop.sh: 58: [: unexpected operator stop.sh: 63: stop.sh: let: not found stop.sh: 58: [: unexpected operator stop.sh: 63: stop.sh: let: not found ``` 是我的问题吗?  参考： https://github.com/alibaba/canal/issues/228 应该类似
299,日志异常dataType=DB_BATCH，求解决办法 日志错误信息： pid:5 nid:1 exception:setl:load miss data with keys:[MemoryPipeKey[identity=Identity[channelId=2 pipelineId=5 processId=702747] time=1493745579135 dataType=DB_BATCH]] pid:5 nid:1 exception:canal:canal_题库:java.io.IOException: Connection reset by peer     at sun.nio.ch.FileDispatcherImpl.read0(Native Method)     at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)     at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)     at sun.nio.ch.IOUtil.read(IOUtil.java:197)     at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)     at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:154)     at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70)     at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:121)     at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209)     at java.lang.Thread.run(Thread.java:744) Connection reset by peer，网络被mysql给断开了吧，重试一下就好
298,startup.sh脚本有错误 在linux环境中执行启动命令时报下面错误，可以推测startup.sh脚本有错误。 [root@localhost bin]# sh startup.sh : command not found  'tartup.sh: line 4: syntax error near unexpected token `in 'tartup.sh: line 4: `case "`uname`" in [root@localhost bin]#  每人的环境不同，尝试调整一下shell脚本，可以提交一个PR给我 :set ff=unix
297,发生网络抖动后，canalserver会卡住，不能自动恢复 问题描述： 当发生网络抖动时，canalserver会被卡住，网络恢复正常后canalserver不能自动恢复正常，需要手动重启instance才能恢复。 心跳检查是开启的，异常栈如下： 2017-04-21 00:29:16.425 [destination = xxxx  address = xxxx   HeartBeatTimeTask] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - connect failed by java.io.IOException: Connection reset by peer         at sun.nio.ch.FileDispatcherImpl.read0(Native Method)         at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)         at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)         at sun.nio.ch.IOUtil.read(IOUtil.java:197)         at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)         at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.readBytesAsBuffer(PacketManager.java:20)         at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.readHeader(PacketManager.java:13)         at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.readNextPacket(MysqlQueryExecutor.java:104)         at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.query(MysqlQueryExecutor.java:55)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:68)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser$MysqlDetectingTimeTask.run(MysqlEventParser.java:191)         at java.util.TimerThread.mainLoop(Timer.java:555)         at java.util.TimerThread.run(Timer.java:505) 除此之外没有其它的异常信息，有人遇到这种情况么 @agapple 要么是mysql binlogdump的tcp链接被异常断开了，canal没有感知到 代码中，有异常应该是能捕捉到的，捕捉到异常会重连重试的，但是我这里没有重试。你说canal没有感知到是指异常没有捕捉到吗 应该是socket一直阻塞在读操作上，没有感知到异常 @lan1994   今天也遇到了这种情况，而且sotimeout默认是30秒，当网络不稳定的情况下，更拖慢了binlog的消费。  @agapple ，不知道这里的30秒有什么讲究 我的栈好像不太一样  canal 1.0.24 ``` 2017-06-27 19:50:47.285 [destination = xxxxx   address = xxxxx   EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Connection reset by peer         at sun.nio.ch.FileDispatcherImpl.read0(Native Method) ~[na:1.8.0_131]         at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39) ~[na:1.8.0_131]         at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223) ~[na:1.8.0_131]         at sun.nio.ch.IOUtil.read(IOUtil.java:197) ~[na:1.8.0_131]         at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380) ~[na:1.8.0_131]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:154) ~[canal.parse-1.0.24.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70) ~[canal.parse-1.0.24.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122) [canal.parse-1.0.24.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) [canal.parse-1.0.24.jar:na]         at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131] ``` 你的这个栈我也遇到过，那个sotimeout 30s没什么用，因为这个设置对于nio没有作用，产生这个问题应该是网络出现故障后，canal 的解析线程一直在read，没有感知到故障    我们可能需要一个类似sotimeout 超时重连的功能 @alexandnpu  当前master版本已经提供，可以关注 @agapple ， 有相关的PR吗？ 别人提交的一个MR，主要是基于netty改造了，增加了超时读取，可以关注一下代码里的SocketChannel 好的，回头找找，只是觉得，你能提供一个pr号，或者一个commit id，会更容易一些 https://github.com/alibaba/canal/pull/334 Thanks
296,单Server情况下ClusterCanalConnector在与server断开重连时NullPointException导致restart无法执行问题 以getWithoutAck方法举例： ```java while (times < retryTimes) {     try {         Message msg = currentConnector.getWithoutAck(batchSize);         return msg;     } catch (Throwable t) {         logger.warn("something goes wrong when getWithoutAck data from server:{}\n{}"             currentConnector.getAddress()             ExceptionUtils.getFullStackTrace(t));         times++;         restart();         logger.info("restart the connector for next round retry.");     } } ``` 当与server连接断开（网络原因或server被停止）时currentConnector.getWithoutAck抛出异常，进入restart方法，在调用connect方法依然连接失败时，会将currentConnector置为null，超过重试次数后抛出Exception。此时再次尝试调用getWithoutAck方法时，因currentConnector为null，currentConnector.getWithoutAck抛出NullPotionException，然后被catch住，在 logger.warn打印日志的时候使用了currentConnector，又会抛出NullPotionException，从而导致restart方法无法被执行。在server恢复后，由于相同原因，客户端依然无法连接上server。 1.0.26-SNAPSHOT 版本上，getWithoutAck并没有对这个问题做修改，请问是出于什么考虑？ 估计遗漏了
295,canal客户端或服务端基于zookeeper跑一段时间挂了 直接重启无效，必须要把zookeeper里canal相关的节点都删除，再重启canal服务端和客户端，才可以正常运行，还有我开启了HA模式，但为了测试只运行了一个canal服务端 看下ＪＶＭ内存和ＧＣ情况
294,Exception in thread "main" java.lang.UnsupportedOperationException: This is supposed to be overridden by subclasses. 运行ClientSample，提示This is supposed to be overridden by subclasses. 环境： canal server在linux机器侠监听远程linux机器mysql的binlog 在win7机器使用ClientSample报错 protobuf版本冲突吧
293,fix bug of repeat log manager [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=293) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=293) before we can accept your contribution.<br/><hr/>**yinxiu** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=293) it.</sub>
292,怎么在docker中启动canal server 怎么在docker中启动canal server 问题解决了
291,重构LogPositionManager并适配XML配置 修改内容主要是使用组合的方式来代替原先MixedLogPositionManager继承MemoryLogPositionManager的方式。核心逻辑保持不变（修改只是使用组合来代替继承让逻辑更加清晰）。 [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=291) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=291) before we can accept your contribution.<br/><hr/>**yinxiu** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=291) it.</sub>
290,生产配置咨询 线上环境，通过canal嵌入式方式使用，现在想将CanalLogPositionManager配置为FailbackLogPositionManager模式，CanalMetaManager配置为PeriodMixedMetaManager，是否存在重复消费问题？比如：PeriodMixedMetaManager客户端消费某条数据后，还未更新cursor如zookeeper中，任务进行重启，客户端将按照zookeeper中cursor进行重复消费。 为了追求一致性，是否将CanalMetaManager配置为ZooKeeperMetaManager更为合适？ 重复消费这块无法避免，位点重复只是一方面，还有就是batch一批数据的处理，如果进程挂了，下次还是会整个batch进行处理。so.  这样的改造没有太多的意义
289,something goes wrong with channel ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x68949ed6  /10.253.0.5:59418 => /10.124.134.173:11111]  exception=java.io.IOException: Connection reset by peer 网络重置，客户端增加一下重连机制吧 这是服务端的出错异常，客户端没有异常，没有断连，服务端开启HA（default-instance.xml）后就出现了这种情况 我这里是多个客户端连接了服务器报的错，多地监听会有这个问题，只启一个就好了。 谢谢大牛们，问题解决了，是zookeeper的问题，重安了一个好了，新手学习，多多包涵 @agapple 我也出现了这种情况，客户端怎么增加重连机制呢？ 看一下example的client代码
288,与guava 20有兼容问题，guava 18 MapMaker.makeComputingMap方法已过时 Exception in thread "main" java.lang.NoSuchMethodError: com.google.common.collect.MapMaker.makeComputingMap(Lcom/google/common/base/Function;)Ljava/util/concurrent/ConcurrentMap; 	at com.google.common.collect.MigrateMap.makeComputingMap(MigrateMap.java:17) 	at com.alibaba.otter.canal.common.zookeeper.ZkClientx.<clinit>(ZkClientx.java:26) 	at com.alibaba.otter.canal.client.CanalConnectors.newClusterConnector(CanalConnectors.java:66) 考虑提交一个PR给我把，guava 20目前内部还没人用上 自己实现了CanalNodeAccessStrategy解决了 @Luckywb 怎么实现的？ @Luckywb 能把实现的原理发布一下吗？遇到同样的问题了
287,json类型字段bug 有个bug在1.0.23下 com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: java.lang.IllegalArgumentException: illegal json data at com.taobao.tddl.dbsync.binlog.JsonConversion.parse_scalar(JsonConversion.java:150) at com.taobao.tddl.dbsync.binlog.JsonConversion.parse_value(JsonConversion.java:67) 数据库版本 mysql-5.7.11-winx64 数据库结构 CREATE TABLE customer_data ( id bigint(19) unsigned NOT NULL AUTO_INCREMENT supplier_id bigint(19) unsigned NOT NULL COMMENT '商户id' customer_id bigint(19) unsigned NOT NULL customer_data json DEFAULT NULL COMMENT '顾客标准属性' customer_ext_data json DEFAULT NULL COMMENT '顾客扩展属性' PRIMARY KEY (id) UNIQUE KEY uk_cust (customer_id) ) ENGINE=InnoDB AUTO_INCREMENT=26036 DEFAULT CHARSET=utf8mb4; debug后就是149行str_len==0 @agapple  给出你有问题的json字符串 `{"性别": "男"  "手机号": "1860652"  "cardNumber": ""  "微信昵称": "Peter"}` 
286,refactoring log position manager module 重构log position模块，没有改变核心逻辑，采用组合的模式使代码逻辑更加清晰 [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=286) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=286) before we can accept your contribution.<br/><hr/>**yinxiu** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=286) it.</sub>
285,Canal报找不到binlog file 2017-03-17 15:17:27.000 [destination = qbo   address = /10.0.0.68:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - **Didn't find the corresponding binlog files from mysql-bin.000001 to mysql-bin.000004** 2017-03-17 15:17:27.005 [destination = qbo   address = /10.0.0.68:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /10.0.0.68:3306 has an error  retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: can't find start position for qbo 2017-03-17 15:17:27.008 [destination = qbo   address = /10.0.0.68:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:qbo[com.alibaba.otter.canal.parse.exception.CanalParseException: can't find start position for qbo ] canal 版本 1.0.24 mysql版本 5.6.33 主库状态 mysql> show master status; +------------------+----------+--------------+------------------+-------------------+ | File             | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +------------------+----------+--------------+------------------+-------------------+ | mysql-bin.000004 |     1507 |              |                  |                   | +------------------+----------+--------------+------------------+-------------------+ 1 row in set (0.00 sec) 完整日志在附件中 [qbo.log.txt](https://github.com/alibaba/canal/files/849900/qbo.log.txt) 这异常是你指定的时间戳，比现在的任何binlog文件都早，调整一下位点
284,here comes an MySQL bug between MySQL 5.6.20 & MySQL5.6.34 for MySQL5.6.20 have not compatible with MySQL5.5  so it will no kill zombie thread without the slave uuid ![mysql5 6 20](https://cloud.githubusercontent.com/assets/22068461/24030521/6586639a-0b18-11e7-8014-068073ee5408.png) ![mysql5 6 34](https://cloud.githubusercontent.com/assets/22068461/24030527/6efc0060-0b18-11e7-8484-037403014128.png) fix this bug for using set @slave_uuid="${uuid}" command before dump 我看一下，可能也是因为这个很多用户反馈链接异常断开的一个问题 可以参考下MySQL源码的实现 在建立IO线程到Master的时候 会设置一下uuid > /** >   Set user variables after connecting to the master. >  >   @param  mysql MYSQL to request uuid from master. >   @param  mi    Master_info to set master_uuid >  >   @return 0: Success  1: Fatal error  2: Network error. > */ > int io_thread_init_commands(MYSQL *mysql  Master_info *mi) > { >   char query[256]; >   int ret= 0; >  >   sprintf(query  "SET @slave_uuid= '%s'"  server_uuid); >   if (mysql_real_query(mysql  query  strlen(query)) >       && !check_io_slave_killed(mi->info_thd  mi  NULL)) >     goto err; 看了mysql5.6的源码，它应该是兼容了mysql5.5 client的serverId的dump，如果没有设置slave_uuid默认也会是基于serverId进行判断.  增加slave_uuid之后可以避免对于slaveId不同的强依赖
283,upgrade fastjson for security [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=283) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=283) before we can accept your contribution.<br/><hr/>**fengyong** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=283) it.</sub> tks
282,Update JsonUtils.java 修复升级fastjson到1.2.28后JSONUtils报错的问题 [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=282) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=282) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=282) it.</sub> pom里面的版本号也要改一下吧？ tks 
281,fastjson高危漏洞 不知道有没有影响 https://github.com/alibaba/fastjson/wiki/security_update_20170315 see #283 我关注一下
280,[bug]TableMetaCache 在连接断了后的reconnect功能无效 master上的源代码： ``` public TableMetaCache(MysqlConnection con){         this.connection = con;         tableMetaCache = MigrateMap.makeComputingMap(new Function<String  TableMeta>() {             public TableMeta apply(String name) {                 try {                     return getTableMeta0(name);                 } catch (IOException e) {                     // 尝试做一次retry操作                     try {                         connection.reconnect();                         System.out.println("cclehui 重新 connect master");                         return getTableMeta0(name);                     } catch (IOException e1) {                         throw new CanalParseException("fetch failed by table meta:" + name  e1);                     }                 }             }         }); ``` 可以看到在捕获到IOException 后会自动重连一次，然后在获取表结构。但问题在于如果mysql master主动关闭了连接，根本不会抛出这个IOException  相反抛出的是一个 RuntimeException，相应的异常抛出代码在 MysqlQueryExecutor  29行： ``` public MysqlQueryExecutor(MysqlConnector connector) throws IOException {         if (!connector.isConnected()) {//服务器关闭连接后会走这个逻辑抛出异常             throw new RuntimeException("should execute connector.connect() first");         }         this.channel = connector.getChannel();     } ``` 如上所述，重连逻辑没有生效。 bug复现方法： 把 master mysql 的 wait_timeout 设置成20秒， 启动canal后等20秒， 去master做一些insert或update操作， canal slave得到这个binlog后，解析，如果本地没有标结构信息，就会通过一个非slave连接去master获取表结构信息， 但20秒后master已关闭该连接，这时候就抛出了RuntimeException ， 然后这个binlog事件就解析失败了。 我的解决方法:把RuntimeException 改成 IOException 抛出，但不知道会不会有什么别的问题 当然我自己测试OK ``` public MysqlQueryExecutor(MysqlConnector connector) throws IOException {         if (!connector.isConnected()) { //            throw new RuntimeException("should execute connector.connect() first");             throw new IOException("should execute connector.connect() first");         }         this.channel = connector.getChannel();     } ``` 我也遇到了同样的问题，根据楼主的方法也可以重现 还有另外一个信息，理论上canal server在遇到异常之后是会重试的，如下： ![retry](https://cloud.githubusercontent.com/assets/3198806/23950250/4cf424ba-09c5-11e7-8439-a683e064a3ee.png) 从日志上来看也确实重试了，并且恢复了正常： ![retry_log_1](https://cloud.githubusercontent.com/assets/3198806/23950630/937491f8-09c6-11e7-83c6-bd15f16b34a7.png) 但是后面过段时间DB链接再次断掉之后却没有重试成功日志： ![retry_log_2](https://cloud.githubusercontent.com/assets/3198806/23950657/b04fb302-09c6-11e7-919a-61e1f6b488cf.png) 然后手动停止canal server，会报以下错误： ![retry_log_3](https://cloud.githubusercontent.com/assets/3198806/23950700/d4f989a8-09c6-11e7-9e19-ed071a0caf11.png) 期待完美解决方案..................
279,运行客户端  batchId偶尔出现冲突问题  batchId:2176966 is not the firstly:2176965         at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.receiveMessages(SimpleCanalConnector.java:302)     at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:279)         at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:252)         at com.yagou.yggx.provider.datasync.start.AbstractCanalStart.process(AbstractCanalStart.java:115) 下面是代码： int batchSize = 5 * 1024; 		while (running) { 			try { 				MDC.put("destination"  destination); 				connector.connect(); 				connector.subscribe(); 				while (running) {				 					// 获取指定数量的数据 					Message message = connector.getWithoutAck(batchSize); 					long batchId = message.getId(); 					int size = message.getEntries().size(); 					List<Entry> entrys = message.getEntries(); 					if (batchId == -1 || size == 0) { 						// try {Thread.sleep(1000); 						// } catch (InterruptedException e) { } 					} else { 						DataSourceListenerInformation.comeOutEntry(entrys); 					} 					connector.ack(batchId); // 提交确认 同样碰到这个问题，请问你这个最后是怎么解决的 一个destination 对应一个消费端。多个消费端同时消费同一个destination，就会报batchId不存在或不是位点
278,canal server cpu 100% @agapple  版本是最新的master，我是从4天前的一个binlog pos点开始同步的，发现同步速度跟之前相比慢很多，top发现canal server cpu一直100%，经过定位找到如下栈： ![cpu](https://cloud.githubusercontent.com/assets/3198806/23615030/0adf5ef2-02c0-11e7-9334-fc3c3e399d6e.png) 内存当时是：              total       used       free     shared    buffers     cached Mem:         32047      24349       7698          0        274      13740 -/+ buffers/cache:      10333      21713 Swap:          511        189        322 这块是有新同学提交的netty4的实现，可能存在一些问题 可以尝试用最新的1.1.1版本，已优化
277,配置自动更新功能不稳定 ``` scan reload found[local] but reload failed ``` 重启就可以 把完整的异常贴清楚
276,canal 服务端显示某个配置成功，但client端异常 ``` 2017-03-06 15:19:16.800 [canal-instance-scan-0] INFO  c.a.o.canal.deployer.monitor.SpringInstanceConfigMonitor - auto notify reload local successful. ``` 日志显示 local 这个instance成功，但client报异常： ``` com.alibaba.otter.canal.protocol.exception.CanalClientException: something goes wrong with reason: something goes wrong with channel:[id: 0x3f655fa5  /127.0.0.1:65443 => /127.0.0.1:11111]  exception=com.alibaba.otter.canal.server.exception.CanalServerException: destination:local should start first 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.receiveMessages(SimpleCanalConnector.java:302) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:279) 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:252) 	at com.alibaba.otter.canal.example.RockyDemo.main(RockyDemo.java:38) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.lang.reflect.Method.invoke(Method.java:498) 	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147) ``` 服务端可能发生了重启，client多重试
275,canal.instance.filter.regex 参数不起作用 canal.instance.filter.regex = demodbname\\\\.user 这样设置后，client端还是会接受到接收到非user表的binlog日志信息。 使用版本： 1.0.23 [zk: localhost:2181(CONNECTED) 44] get /otter/canal/destinations/example/1001/filter .*\..* cZxid = 0x1000013e8 ctime = Fri Mar 03 11:26:12 CST 2017 ..................... .................... [zk: localhost:2181(CONNECTED) 48] set /otter/canal/destinations/example/1001/filter demodbname\\.user   试试，zk 中貌似记录了初始值，修改配置文件后，重启服务，貌似zk中的值没有修改过来，导致的 参数不起作用，个人猜测的  
274,解析binlog日志失败            canal运行后解析binlog日志的时候出现错误，提示Read Q_SQL_MODE_CODE error: limit excceed: 67 我看了canal的部分源码，貌似是sql_mode设置的不对？SQL_mode在canal里定义是64位的所以用getLong64，但是实际是67导致越界了？没看出数据库的数据有啥问题 PS: mysql版本是5.7.14 mysqlbinlog的日志： ------------------------------------------------------------------- 见附件 ![binlog](https://cloud.githubusercontent.com/assets/2059502/23546331/104ef6f0-003a-11e7-8f0c-40f50a85356c.png) [binlog.txt](https://github.com/alibaba/canal/files/816403/binlog.txt) canal实例日志： ----------------------------------------------------------------- 2017-03-03 17:17:19.952 [destination = gene_cachenotice_play3   address = /172.16.10.213:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position  {"identity":{"slaveId":-1 "sourceAddress":{"address":"172.16.10.213" "port":3306}} "postion":{"included":false "journalName":"mysql-bin.000016" "position":24856433 "serverId":213 "timestamp":1488484784000}} 2017-03-03 17:17:19.959 [destination = gene_cachenotice_play3   address = /172.16.10.213:3306   EventParser] WARN  com.taobao.tddl.dbsync.binlog.LogDecoder - Decoding Query failed from: mysql-bin.000016:24858343 java.io.IOException: Read Q_SQL_MODE_CODE error: limit excceed: 67 	at com.taobao.tddl.dbsync.binlog.event.QueryLogEvent.unpackVariables(QueryLogEvent.java:650) ~[canal.parse.dbsync-1.0.23.jar:na] 	at com.taobao.tddl.dbsync.binlog.event.QueryLogEvent.<init>(QueryLogEvent.java:477) ~[canal.parse.dbsync-1.0.23.jar:na] 	at com.taobao.tddl.dbsync.binlog.LogDecoder.decode(LogDecoder.java:154) ~[canal.parse.dbsync-1.0.23.jar:na] 	at com.taobao.tddl.dbsync.binlog.LogDecoder.decode(LogDecoder.java:106) ~[canal.parse.dbsync-1.0.23.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:123) [canal.parse-1.0.23.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) [canal.parse-1.0.23.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_25] Caused by: java.lang.IllegalArgumentException: limit excceed: 67 	at com.taobao.tddl.dbsync.binlog.LogBuffer.getLong64(LogBuffer.java:873) ~[canal.parse.dbsync-1.0.23.jar:na] 	at com.taobao.tddl.dbsync.binlog.event.QueryLogEvent.unpackVariables(QueryLogEvent.java:566) ~[canal.parse.dbsync-1.0.23.jar:na] 	... 6 common frames omitted 2017-03-03 17:17:19.960 [destination = gene_cachenotice_play3   address = /172.16.10.213:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address 172.16.10.213/172.16.10.213:3306 has an error  retrying. caused by  java.io.IOException: Read Q_SQL_MODE_CODE error: limit excceed: 67 	at com.taobao.tddl.dbsync.binlog.event.QueryLogEvent.unpackVariables(QueryLogEvent.java:650) ~[canal.parse.dbsync-1.0.23.jar:na] 	at com.taobao.tddl.dbsync.binlog.event.QueryLogEvent.<init>(QueryLogEvent.java:477) ~[canal.parse.dbsync-1.0.23.jar:na] 	at com.taobao.tddl.dbsync.binlog.LogDecoder.decode(LogDecoder.java:154) ~[canal.parse.dbsync-1.0.23.jar:na] 	at com.taobao.tddl.dbsync.binlog.LogDecoder.decode(LogDecoder.java:106) ~[canal.parse.dbsync-1.0.23.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:123) ~[canal.parse-1.0.23.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) ~[canal.parse-1.0.23.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_25] Caused by: java.lang.IllegalArgumentException: limit excceed: 67 	at com.taobao.tddl.dbsync.binlog.LogBuffer.getLong64(LogBuffer.java:873) ~[canal.parse.dbsync-1.0.23.jar:na] 	at com.taobao.tddl.dbsync.binlog.event.QueryLogEvent.unpackVariables(QueryLogEvent.java:566) ~[canal.parse.dbsync-1.0.23.jar:na] 	... 6 common frames omitted 2017-03-03 17:17:19.960 [destination = gene_cachenotice_play3   address = /172.16.10.213:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:gene_cachenotice_play3[java.io.IOException: Read Q_SQL_MODE_CODE error: limit excceed: 67 	at com.taobao.tddl.dbsync.binlog.event.QueryLogEvent.unpackVariables(QueryLogEvent.java:650) 	at com.taobao.tddl.dbsync.binlog.event.QueryLogEvent.<init>(QueryLogEvent.java:477) 	at com.taobao.tddl.dbsync.binlog.LogDecoder.decode(LogDecoder.java:154) 	at com.taobao.tddl.dbsync.binlog.LogDecoder.decode(LogDecoder.java:106) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:123) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) 	at java.lang.Thread.run(Thread.java:745) Caused by: java.lang.IllegalArgumentException: limit excceed: 67 	at com.taobao.tddl.dbsync.binlog.LogBuffer.getLong64(LogBuffer.java:873) 	at com.taobao.tddl.dbsync.binlog.event.QueryLogEvent.unpackVariables(QueryLogEvent.java:566) 	... 6 more ] mysql 5.7.14版本还没测试过，可能是有协议变更 改了一个代码，帮我验证一下mysql 5.7.14环境是否可以正常解析，谢谢。验证ok之后回复一下信息
273,报错，转换异常 java.lang.ClassCastException: com.alibaba.otter.canal.parse.inbound.AbstractEventParser$4 cannot be cast to com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser$MysqlDetectingTimeTask 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.stopHeartBeat(MysqlEventParser.java:186) ~[canal.parse-1.0.23.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.stop(AbstractEventParser.java:282) ~[canal.parse-1.0.23.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.stop(MysqlEventParser.java:165) ~[canal.parse-1.0.23.jar:na] 	at com.alibaba.otter.canal.instance.core.AbstractCanalInstance.stop(AbstractCanalInstance.java:104) ~[canal.instance.core-1.0.23.jar:na] 	at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.stop(CanalServerWithEmbedded.java:85) ~[canal.server-1.0.23.jar:na] 	at com.alibaba.otter.canal.server.netty.CanalServerWithNetty.stop(CanalServerWithNetty.java:95) [canal.server-1.0.23.jar:na] 	at com.alibaba.otter.canal.deployer.CanalController.stop(CanalController.java:422) [canal.deployer-1.0.23.jar:na] 	at com.alibaba.otter.canal.deployer.CanalLauncher$1.run(CanalLauncher.java:42) [canal.deployer-1.0.23.jar:na] 新的master代码已经修复
272,server启动后 example instance 和master 一会就莫名的断开连接 ， 没有任何Log 我是2017-02-27从master上拉取的分支编译的， Mysql版本：5.5.32 启动服务：正常 ``` 2017-03-02 17:27:58.627 [main] INFO  com.alibaba.otter.canal.deployer.CanalLauncher - ## start the canal server. 2017-03-02 17:27:58.924 [main] INFO  com.alibaba.otter.canal.deployer.CanalController - ## start the canal server[my ip :11111] 2017-03-02 17:28:00.401 [main] INFO  com.alibaba.otter.canal.deployer.CanalLauncher - ## the canal server is running now ...... ``` example instance 启动失败， 该instance启动连接上了mysql， 然后莫名奇妙的做了一个 disConnect  断开了链接 ， 没有然后就没有任何log了 ， 服务还是在运行，但该instance没有起来 ， 和mysql的链接也没有建立起来 。而且很奇怪的是连接了两次mysql 。 instance的启动log如下  ``` 2017-03-02 17:27:59.019 [main] INFO  o.s.context.support.ClassPathXmlApplicationContext - Refreshing org.springframework.context.support.ClassPathXmlApplicationContext@68bbe345: display name [org.springframework.context.support.ClassPathXmlApplicationContext@68bbe345]; startup date [Thu Mar 02 17:27:59 CST 2017]; root of context hierarchy 2017-03-02 17:27:59.133 [main] INFO  o.s.beans.factory.xml.XmlBeanDefinitionReader - Loading XML bean definitions from class path resource [spring/file-instance.xml] 2017-03-02 17:27:59.516 [main] INFO  o.s.context.support.ClassPathXmlApplicationContext - Bean factory for application context [org.springframework.context.support.ClassPathXmlApplicationContext@68bbe345]: org.springframework.beans.factory.support.DefaultListableBeanFactory@78186a70 2017-03-02 17:27:59.642 [main] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2017-03-02 17:27:59.643 [main] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties] 2017-03-02 17:27:59.697 [main] INFO  o.s.beans.factory.support.DefaultListableBeanFactory - Pre-instantiating singletons in org.springframework.beans.factory.support.DefaultListableBeanFactory@78186a70: defining beans [com.alibaba.otter.canal.instance.spring.support.PropertyPlaceholderConfigurer#0 socketAddressEditor org.springframework.beans.factory.config.CustomEditorConfigurer#0 instance alarmHandler metaManager eventStore eventSink eventParser]; root of factory hierarchy 2017-03-02 17:27:59.851 [main] WARN  org.springframework.beans.TypeConverterDelegate - PropertyEditor [com.sun.beans.editors.EnumEditor] found through deprecated global PropertyEditorManager fallback - consider using a more isolated form of registration  e.g. on the BeanWrapper/BeanFactory! 2017-03-02 17:28:00.131 [main] INFO  c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-example  2017-03-02 17:28:00.158 [main] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - start successful.... 2017-03-02 17:28:00.158 [main] INFO  c.a.otter.canal.server.embedded.CanalServerWithEmbedded - start CanalInstances[example] successfully 2017-03-02 17:28:00.177 [destination = example   address = /127.0.0.1:3306   EventParser] INFO  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - start heart beat....  2017-03-02 17:28:00.936 [destination = example   address = /127.0.0.1:3306   EventParser] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - connect MysqlConnection to /127.0.0.1:3306... 2017-03-02 17:28:00.944 [destination = example   address = /127.0.0.1:3306   EventParser] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - handshake initialization packet received  prepare the client authentication packet to send 2017-03-02 17:28:00.973 [destination = example   address = /127.0.0.1:3306   EventParser] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - client authentication packet is sent out. 2017-03-02 17:28:00.989 [destination = example   address = /127.0.0.1:3306   EventParser] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - connect MysqlConnection to /127.0.0.1:3306... 2017-03-02 17:28:00.991 [destination = example   address = /127.0.0.1:3306   EventParser] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - handshake initialization packet received  prepare the client authentication packet to send 2017-03-02 17:28:00.991 [destination = example   address = /127.0.0.1:3306   EventParser] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - client authentication packet is sent out. 2017-03-02 17:28:00.993 [destination = example   address = /127.0.0.1:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just show master status 2017-03-02 17:28:01.013 [destination = example   address = /127.0.0.1:3306   EventParser] INFO  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000004 position=121250 serverId=<null> timestamp=<null>] 2017-03-02 17:28:01.023 [destination = example   address = /127.0.0.1:3306   EventParser] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - disConnect MysqlConnection to /127.0.0.1:3306... ``` mysql 的 tcp连接情况： 刚启动的时候： ``` tcp        0      0 127.0.0.1:3306          127.0.0.1:60383         ESTABLISHED 18565/mysqld               tcp        0      0 127.0.0.1:60383         127.0.0.1:3306          ESTABLISHED 1273/java        tcp        0      0 127.0.0.1:3306          127.0.0.1:60381         ESTABLISHED 18565/mysqld     tcp        0      0 127.0.0.1:60381         127.0.0.1:3306          ESTABLISHED 1273/java ``` 过一会这两个连接都断了 这个问题已经找到原因了，我自己的测试mysql服务器配置的 interactive_timeout 和 wait_timeout 分别为30、20 ， 过了20秒后mysql 主动关闭不活跃的连接
271,fetch failed by table meta:`mysql`.`user` 昨晚上11：50之前都正常，之后突然出现了问题，看起来和权限相关 线上用的是RDS， 测试线自己的机器canal没有异常 2017-02-28 23:49:51.829 [destination = example   address = /XXX.178.61.230:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEvent Parser - ERROR ## parse this event has an error   last position : [EntryPosition[included=false journalName=mysql-bin.000302 position=5492156 serve rId=3071795599 timestamp=1488296991000]] com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.collect.ComputationException: com.alibaba.otter.canal.parse.exception.CanalParseException: fetch failed by table meta:`mysql`.`user` Caused by: com.google.common.collect.ComputationException: com.alibaba.otter.canal.parse.exception.CanalParseException: fetch failed by table meta: `mysql`.`user`         at com.google.common.collect.MapMaker$ComputingMapAdapter.get(MapMaker.java:889) ~[guava-18.0.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:78) ~[canal.parse-1.0.23-SNAPSHOT.jar :na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:677) ~[canal.parse-1.0.23-SNAPSHOT. jar:na]         ...........                 at java.lang.Thread.run(Thread.java:745) [na:1.8.0_102] Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: fetch failed by table meta:`mysql`.`user` Caused by: java.io.IOException: connect XXX.178.61.230/118.178.61.230:3306 failure:java.io.IOException: Error When doing Client Authentication:ErrorPacket [errorNumber=2811  fieldCount=-1  message=Authentication Failed For RDS backend  sqlState=RDS00  sqlStateMarker=#]         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:208)         ........... Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: fetch failed by table meta:`mysql`.`user` Caused by: java.io.IOException: ErrorPacket [errorNumber=1142  fieldCount=-1  message=SELECT command denied to user 'app_prod_crawler'@'116.62.43.4 1' for table 'user'  sqlState=42000  sqlStateMarker=#]  with command: desc `mysql`.`user`         at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.query(MysqlQueryExecutor.java:60) ~[canal.parse.driver-1.0.23-SNAPSHOT.jar:na] 我在使用canal监听库的时候删除了一个表之后也包类似的错误  这个问题解决了吗?  之前监听了所有的库，看文档可以用配置只监听指定库，canal.instance.filter.regex 过滤一下mysql.user表 最新版本24已经支持
270,接收不到全部的insert事件 我所在的公司会不定时发放优惠券，每次从几百张到几万张不等，都是批量把优惠券数据插入到数据库中，但是client只能接收到部分insert事件，举例如下： 2017-02-28 11:08:47往数据库中插入了400条记录，但是client只接收到4条insert事件，不知道什么原因，以前我以为是内存的原因，把各个指标都调大了，还是不行。 mysql版本：5.6 canal版本：1.0.20 canal配置： ################################################# ######### 		common argument		#############  ################################################# canal.id= 1 canal.ip= ******** canal.port= 11111 canal.zkServers= # flush data to zk canal.zookeeper.flush.period = 1000 # flush meta cursor/parse position to file canal.file.data.dir = ${canal.conf.dir} canal.file.flush.period = 1000 ## memory store RingBuffer size  should be Math.pow(2 n) canal.instance.memory.buffer.size = 1048576 ## memory store RingBuffer used memory unit size   default 1kb canal.instance.memory.buffer.memunit = 1024 (这里开始改为了4096，服务端和客户端都启动正常，也可以正常连接，但就是没有任何事件产生，也不报错，恢复为1024后正常)  ## meory store gets mode used MEMSIZE or ITEMSIZE canal.instance.memory.batch.mode = MEMSIZE ## detecing config canal.instance.detecting.enable = false #canal.instance.detecting.sql = insert into retl.xdual values(1 now()) on duplicate key update x=now() canal.instance.detecting.sql = select 1 canal.instance.detecting.interval.time = 3 canal.instance.detecting.retry.threshold = 3 canal.instance.detecting.heartbeatHaEnable = false # support maximum transaction size  more than the size of the transaction will be cut into multiple transactions delivery canal.instance.transaction.size =  16384 # mysql fallback connected to new master should fallback times canal.instance.fallbackIntervalInSeconds = 60 # network config canal.instance.network.receiveBufferSize = 104857600 canal.instance.network.sendBufferSize = 104857600 canal.instance.network.soTimeout = 30 # binlog filter config canal.instance.filter.query.dcl = false canal.instance.filter.query.dml = false canal.instance.filter.query.ddl = false canal.instance.filter.table.error = false # binlog format/image check canal.instance.binlog.format = ROW STATEMENT MIXED  canal.instance.binlog.image = FULL MINIMAL NOBLOB # binlog ddl isolation canal.instance.get.ddl.isolation = false ################################################# ######### 		destinations		#############  ################################################# canal.destinations= ******** # conf root dir canal.conf.dir = ../conf # auto scan instance dir add/remove and start/stop instance canal.auto.scan = true canal.auto.scan.interval = 5 canal.instance.global.mode = spring  canal.instance.global.lazy = false #canal.instance.global.manager.address = 127.0.0.1:1099 #canal.instance.global.spring.xml = classpath:spring/memory-instance.xml canal.instance.global.spring.xml = classpath:spring/file-instance.xml #canal.instance.global.spring.xml = classpath:spring/default-instance.xml 同步别的表的时候没有这个问题，那些表没有同时批量插入很多数据的情况。 一个event事件可能包含多条变更记录 最近专门跟踪了这个问题，原因是RowChange返回的是个List，包含多条记录，但只处理了最后一条记录。严重鄙视这样写的那个人，^_^
269,canal 服务端运行一段时间就报如下错误 2017-02-28 17:12:37.470 [destination = example   address = /10.10.10.193:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.collect.ComputationException: java.lang.RuntimeException: should execute connector.connect() first Caused by: com.google.common.collect.ComputationException: java.lang.RuntimeException: should execute connector.connect() first         at com.google.common.collect.MapMaker$ComputingMapAdapter.get(MapMaker.java:889)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:78)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:677)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:362)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:108)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:62)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:326)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:176)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:129)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209)         at java.lang.Thread.run(Thread.java:724) Caused by: java.lang.RuntimeException: should execute connector.connect() first         at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.<init>(MysqlQueryExecutor.java:29)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:71)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta0(TableMetaCache.java:105)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:26)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.apply(TableMetaCache.java:46)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.apply(TableMetaCache.java:42)         at com.google.common.collect.ComputingConcurrentHashMap$ComputingValueReference.compute(ComputingConcurrentHashMap.java:356)         at com.google.common.collect.ComputingConcurrentHashMap$ComputingSegment.compute(ComputingConcurrentHashMap.java:182)         at com.google.common.collect.ComputingConcurrentHashMap$ComputingSegment.getOrCompute(ComputingConcurrentHashMap.java:151)         at com.google.common.collect.ComputingConcurrentHashMap.getOrCompute(ComputingConcurrentHashMap.java:67)         at com.google.common.collect.MapMaker$ComputingMapAdapter.get(MapMaker.java:885)         ... 10 more ] canal server在重启估计，client多做一下重试即可
268,可以只同步特定类型的日志吗？比如新增数据 RT，只同步新增数据，其它不要，可以做到吗？ 你先了解一下mysql binlog的基本概念
267,canal收集 由一条update语句更新多行 产生的binlog的速度很慢 什么原因呢？ 我们有一个场景，一条update语句更新3000行（update tablename set x='xxx' where id>0 and id <3001），然后提交。发现canal收集速度明显慢于正常情况。 从mysql中查看binlog，发现一个事务里，分成了47个update_row event（因此一个update_row event包含64个行变化）， 在我们的消费场景中消费时发现一个entry（rowDatas）中有64个BinlogRow。47*64正好对应3000行变化。 做了一些测试，发现当所更新的mysql行的大小比较小的时候，一条语句更新多行时，会有这种情况，收集速度也很慢。 但是mysql行比较大的时候，（更新3000行，binlog原始文件中有3000个update_row event），这个时候收集速度就正常了。 现在想了解： 1.是否是因为更新多行产生的binlog内容（一个entry包含多个行变化信息）导致 后续解析速度慢，从而收集速度变慢 2.具体什么情况下，一个update_row event会包含多个行变化 3.针对一条sql语句更新多行的场景，有没有办法提高处理性能 这个之前没有刻意关注过，你关注到慢有一些具体的数据么？比如评估tps大概是多少？ 相同配置的情况下，一条sql语句只影响一行的情况下，大概每秒能收集7MB原始binlog文件，这种一条sql语句影响多行的会特别慢，我当时测一条sql语句更新3000行，大概每秒只能收集0.5MBbinlog文件。。 我测试的时候发现，只有原先mysql单条记录比较小的时候，这样批量更新会出现这种情况。对于批量更新单条mysql记录比较大的操作（我尝试的是各个字段加起来超过1KB），收集速度和正常情况差不多。 @lan1994 你那个问题解决了吗？我们也遇到类似的问题 @pan289091315 没解决 mysql配置如下参数：innodb_flush_log_at_trx_commit 在提交事务的时候是否提交缓冲? 有3个值： 	0：不会主动触发日志缓冲写入磁盘 	1（默认）：每次提交事务的时候，同时会把日志缓冲刷新到磁盘 	2：每次提交事务的时候，会把日志缓冲刷新到磁盘，但是他不是同时进行的 	   而是每秒钟刷新一次 建议配置成2 ，对性能影响较大。 可以试试这个 @wufengbin 你提的这个参数是解决mysql批量写入慢的问题，但是这里提到的是canal解析慢的问题。在预写入mysql完毕后，才测试解析性能的。 @lan1994 请问解决了吗？ 倒是可以先执行一个大事务，然后canal回溯到事务执行之前进行验证，从我这边的测试来看，基本都是可以满足基本性能 @lan1994 问题原因找到了没？兄弟 参考文档：https://github.com/alibaba/canal/wiki/Performance
266,canal 1.0.16 在批量刷数据库数据出现错误    canal 1.0.16 在批量刷数据库数据出现错误，请问下这个版本有什么BUG。 需要把具体错误内容描述清楚，版本变更你关注一下release note
265,ErrotCode:400   Caused by : packet type=CLIENTAUTHENTICATION is NOT supported! 启动canal，报这样的异常，但是数据库我用msyql客户端，账号密码可以正常登录，mysql从库和canal是部署在同一台服务器上的，请问是什么原因，有碰到相似问题的吗？ 2017-02-22 10:53:15.093 [New I/O server worker #1-2] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400   Caused by : packet type=CLIENTAUTHENTICATION is NOT supported! 2017-02-22 10:53:45.014 [Thread-3] INFO  com.alibaba.otter.canal.deployer.CanalLauncher - ## stop the canal server 2017-02-22 10:53:45.315 [Thread-3] INFO  com.alibaba.otter.canal.deployer.CanalController - ## stop the canal server[192.168.1.20:11111] 2017-02-22 10:53:45.315 [Thread-3] INFO  com.alibaba.otter.canal.deployer.CanalLauncher - ## canal server is down. OpenJDK 64-Bit Server VM warning: ignoring option PermSize=96m; support was removed in 8.0 OpenJDK 64-Bit Server VM warning: ignoring option MaxPermSize=256m; support was removed in 8.0 OpenJDK 64-Bit Server VM warning: UseCMSCompactAtFullCollection is deprecated and will likely be removed in a future release. 2017-02-22 10:53:59.987 [main] INFO  com.alibaba.otter.canal.deployer.CanalLauncher - ## start the canal server. 2017-02-22 10:54:00.151 [main] INFO  com.alibaba.otter.canal.deployer.CanalController - ## start the canal server[192.168.1.20:11111] 2017-02-22 10:54:01.278 [main] INFO  com.alibaba.otter.canal.deployer.CanalLauncher - ## the canal server is running now ...... 看日志最后一行和使用ps查询，canal是启动起来了，但是没有事件发生，但binlog日志是一直增加的。 请问这个你怎么解决的啊... 2462962670
264,binlog解析问题 canal 解析binlog日志 ，有时候获取不到主键是什么原因？ mysql 版本mysql-5.7.14 没遇到过用户反馈，最好能明确一下复现的场景，比如mysql表，canal版本等一些信息
263,[updated]binlog解析疑问 canal能够解析现在mysql中的Percona分支吗 如果不能 有支持的计划吗 支持的 不过这种问题你试一下不就知道了，来问一下至少等1天才会有答复，何必呢 我表述不够清楚 是这样的 由percona的源库 同步至mysql主分支的目的库 需同步的源表既有innodb 也有tokudb 同步至的目的表均为innodb 源表中均无blob等大字段类型 同步时发生了解析错误(更详细的错误堆栈没有打印) pid:17 nid:8 exception:canal:test_canal:com.alibaba.otter.canal.parse.exception.CanalParseException: Unsupported BinlogImage NOBLOB 配置上并没有修改使用默认配置 即支持的BinlogImage为[FULL MINIMAL NOBLOB] @agapple 求解答 otter不支持image模式为NOBLOB
262,关于 SlaveId 碰撞问题 Mysql 会认为拥有同一个 SlaveId 的 connection 都来自同一个从库，因而在两个具有相同 SlaveId 的 canal instance 对同一个 Mysql Master 发起 dump 请求时，必定会有一个 instance 会失败 && 反复重试。 这个错误在 Mysql 中报出来的是 1236，但是并不能通过查找 binlog 异常来解决。 这种情况下，只能重新生成新的 SlaveId。例如这样： canal.parse: MysqlEventParser.java ```     @Override     protected void processDumpError(Throwable e) {         if (e instanceof IOException) {             String message = e.getMessage();             if (StringUtils.contains(message  "errno = 1236")) {                 // 1236 errorCode 代表 ER_MASTER_FATAL_ERROR_READING_BINLOG                 // 但也有可能是 slaveId 冲突  在这里作预先补救                 if (StringUtils.contains(message  "errmsg = A slave with the same server_uuid/server_id as this slave has connected to the master")) {                     // 重新生成 slaveId                     long new_slave_id = new Random().nextLong();                     logger.warn("SlaveId Conflict is detected: Old slave id = [{}]  new slave id = [{}]"  this.slaveId  new_slave_id);                     this.slaveId = new_slave_id;                     return;                 }                 dumpErrorCount++;             }         }         super.processDumpError(e);     } ``` 可能能够减少一点手工改配置的工作？ 你能提交一个pull request给我不？ 最新的版本已经设置了slave_uuid
261,java客户端CanalConnector.subscribe()使用以下第二种正则过滤无效 使用canal的java api， 为什么用第三 四种有效；第二种正则无效，过滤全部呢 ps:canal server的conf/example/instance.properties的canal.instance.filter.regex没有配置 1.  所有表：.*   or  .*\\..* 2.  canal schema下所有表： canal\\..* 3.  canal下的以canal打头的表：canal\\.canal.* 4.  canal schema下的一张表：canal.test1 5.  多个规则组合使用：canal\\..* mysql.test1 mysql.test2 (逗号分隔) 朋友，你这个 Issue 后来有什么结论么？
260,生产环境版本考量 我们准备在生产环境使用`canal`，之前在测试环境同事搭建的是`1.0.22`版本，测试使用过程中发现了一些bug（如kill connection的问题），目前看基本上都在`1.0.23`修复了。后来注意到官方wiki推荐的是`1.0.19`版本，请问@agapple基于稳定性而言是推荐使用`1.0.19`版本吗？ 估计是还没有及时更新文档，可以使用最新版本 多谢多谢~ 我们也选了最新版本，这几天正在测试 文档更新及时一些就更好啦~
259,fix bug for #202（canal.instance.filter.regex 修改后未生效） 这个不能加meta配置修改生效，因为这文件位点会不停的更新，会导致频繁的reload 没有meta呀，只比较对应目录下的instance.properties文件～ subscribe自动生成meta文件？
258,fix bug for: #202 canal.instance.filter.regex 修改后未生效 之前提的issue（#202 canal.instance.filter.regex 修改后未生效），这是我打的patch～ 不好意思，才看到回复～ [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=258) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=258) before we can accept your contribution.<br/><hr/>**玉麒麟** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).
257,MysqlEventParser里发现一个bug 1.0.23版里MysqlEventParser的stopHeartBeat方法里没有判断heartBeatTimerTask的类型就直接做强制转换，导致下面 java.lang.ClassCastException: com.alibaba.otter.canal.parse.inbound.AbstractEventParser$4 cannot be cast to com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser$MysqlDetectingTimeTask 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.stopHeartBeat(MysqlEventParser.java:186) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.stop(AbstractEventParser.java:282) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.stop(MysqlEventParser.java:165) 	at com.alibaba.otter.canal.instance.core.AbstractCanalInstance.stop(AbstractCanalInstance.java:104) 	at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.stop(CanalServerWithEmbedded.java:85) @agapple  大神，这个bug会导致eventparser线程释放不了，是不是意味着 canalserver stop 其实好多stop的事情都没能做到，啥时候能上 1.0.24版本到mvn仓库呢，坐等谢谢大神
256,使用netty重构socketChannel（jdk内置），修改了PacketManager实现，以及其他相关引用的类。 [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=256) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=256) before we can accept your contribution.<br/>**1** out of **2** committers have signed the CLA.<br/><br/>:white_check_mark: agapple<br/>:x: luoyaogui<hr/>**luoyaogui** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user). tks @luoyaogui 用户反馈比较多的网络问题，我暂时先回滚去掉代码，后面优化验证稳定性之后再更新 
255,BUG，在Mysql 5.5.28版本下报错 日志： 2017-01-12 12:06:16.107 [destination = example   address = /127.0.0.1:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /127.0.0.1:3306 has an error  retrying. caused by  java.lang.NullPointerException: null 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.loadBinlogChecksum(MysqlConnection.java:309) ~[canal.parse-1.0.24-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:120) ~[canal.parse-1.0.24-SNAPSHOT.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) ~[canal.parse-1.0.24-SNAPSHOT.jar:na] 	at java.lang.Thread.run(Unknown Source) [na:1.7.0_80] 原因：程序BUG，缺少null判断 if (columnValues != null && columnValues.size() >= 1 				&& columnValues.get(0).toUpperCase().equals("CRC32")) 改为： if (columnValues != null && columnValues.size() >= 1 				&& columnValues.get(0) != null   //缺少的判断 				&& columnValues.get(0).toUpperCase().equals("CRC32")) 症状消除！ 请更新当前master版本 感觉这个问题比较严重啊，建议发一个release比较好，对于低版本mysql，canal直接无法正常启动了
254,ll 
253,prepare to find start position just last position 部署了两台机器 A，机器 和B 机器，A 机器先启动成功，然后看日志 一直处于 prepare to find start position just last position，之后我把 A 机器关闭了，然后去观察B 机器，然后迅速 获取到postion ，这是问题难道每次都要关闭一台机器吗？ 我遇到了类似的问题，我只部署了一个canal节点 ，一直卡在定位这一步 @boboChina  单机的话你就不用用zk 了，就可以用了，我怀疑是zk  有问题日志上面显示一直ping  zk  我怀疑过，我后台做过几次重试： 1先把canal停了，把zk上面数据删了 这种不行 2 设置binlog文件、位点、时间戳 也不行 根据打印的日志，去对照配置项，和源码里面走到的if else分支对不上 不用zk也试了下，没效果，怀疑和mysql设置有关，换了其他的mysql有的很快恨得定位就卡死了，但是具体不知道是哪些配置项会有影响 按照你这样的逻辑应该是数据库的问题咯，但是我把Zk关闭了，然后单机模式就可以，数据库没有变化啊 prepare to find start position just last position   这个是代表用最后的位点已经启动成功的意思 可是启动成功过后，一直就停在那里了，然后客户端去消费的时候，也会一直卡在哪里的
252,com.alibaba.otter.canal.common.utils.JsonUtils 相关 客户端启动抛出异常 推测是FastJSON版本问题 <img width="1051" alt="2017-01-06 14 32 56" src="https://cloud.githubusercontent.com/assets/22702893/21709589/0e616c68-d41d-11e6-9af3-88e79a5af79b.png"> 
251,请问支持10.0.10版本的MariaDB吗 > ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.  alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: java.lang.IllegalArgumentException: bad format  x exceed: 1207926783  999999999 mysql使用10.0.10版本的MariaDB，canal启动后解析binlog就会报以上错误。试了1.0.22，1.0.23版本的canal都不行 这个估计要看看mariadb在10.0.10上改动了哪些 @minotaursu 可以试试maxscale的binlogserver组件。 @zhongjimax binlog server组件只是解决binlog远程下载吧，具体解析工作还得自己做 试过了，无法解析mariadb的10.0.10版本。
250,[BUGFIX] 在MySQL5.6做主备库切换时出现的filename乱码修复 多次测试发现问题出现在seek方法上，相较于dump，缺少了checksum，导致搜索binlog位点时，拿到的filename多了四字节。 [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=250) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=250) before we can accept your contribution.<br/><hr/>**jianhao.dai** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user). tks
249,主备切换的问题 主备切换的过程中出现错误，log如下（注意mysql-bin.000029=7¾以及Could not find first log file name in binary log index） 2017-01-03 22:09:16.930 [destination = xiaopang3   address = /192.168.6.123:3306   EventParser] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - client authentication packet is sent out. 2017-01-03 22:09:16.934 [destination = xiaopang3   address = /192.168.6.123:3306   EventParser] INFO  c.a.otter.canal.parse.inbound.mysql.MysqlConnection - COM_BINLOG_DUMP with position:BinlogDumpCommandPacket[binlogPosition=21725485 slaveServerId=1237 binlogFileName=mysql-bin.000029=7¾ command=18] 2017-01-03 22:09:16.935 [destination = xiaopang3   address = /192.168.6.123:3306   EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Could not find first log file name in binary log index file 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) ~[canal.parse-1.0.23.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:121) [canal.parse-1.0.23.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) [canal.parse-1.0.23.jar:na] 	at java.lang.Thread.run(Thread.java:744) [na:1.7.0_45] 2017-01-03 22:09:16.935 [destination = xiaopang3   address = /192.168.6.123:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /192.168.6.123:3306 has an error  retrying. caused by  java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Could not find first log file name in binary log index file 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) ~[canal.parse-1.0.23.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:121) ~[canal.parse-1.0.23.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) ~[canal.parse-1.0.23.jar:na] 	at java.lang.Thread.run(Thread.java:744) [na:1.7.0_45] 2017-01-03 22:09:16.935 [destination = xiaopang3   address = /192.168.6.123:3306   EventParser] ERROR com.alibaba.otter.canal.common. 感觉是binlog名字出现了乱码，试试当前主干 @agapple 这个问题在1.0.26版本上还是存在，知道什么原因吗？ 我也遇到同样的问题，canal  版本 v1.0.26 alpha 1， mariadb 版本用的 10.2.11，切换的时候 发现binlog文件名字乱码。 @lijie1992066 提供一下对应的binlog文件给我 我也遇到了同样的问题，主备切换时老是报binlog找不到（mysql中binlog日志是存在的），断点调试如下： ![image](https://user-images.githubusercontent.com/20380664/38121113-d6675676-33ff-11e8-9341-6703fa332bd6.png) 感觉后缀上有乱码 能debug看看，这个乱码的位点是从哪里获取到的？@ZhiXingHeYiApple 
248,修复mysql5.6以下报java.lang.NullPointerException 修复mysql5.6以下报java.lang.NullPointerException [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=248) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=248) before we can accept your contribution.<br/> tks
247,example无法链接到mysql，报java.lang.NullPointerException 2017-01-03 16:58:21.099 [destination = example   address = /127.0.0.1:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /127.0.0.1:3306 has an error  retrying. caused by  java.lang.NullPointerException: null 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.loadBinlogChecksum(MysqlConnection.java:284) ~[canal.parse-1.0.23.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) ~[canal.parse-1.0.23.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) ~[canal.parse-1.0.23.jar:na] 	at java.lang.Thread.run(Unknown Source) [na:1.8.0_111] 2017-01-03 16:58:21.100 [destination = example   address = /127.0.0.1:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.lang.NullPointerException 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.loadBinlogChecksum(MysqlConnection.java:284) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) 	at java.lang.Thread.run(Unknown Source) ] 定位到MysqlConnection.java:284代码为 if (columnValues != null && columnValues.size() >= 1 && columnValues.get(0).toUpperCase().equals("CRC32")) MySQL版本5.5.14 ，前面查询 select @master_binlog_checksum 为null 升级到最新版本，代码前面加了判断 当前主干已经有人提了PR修复了 你好，你解决了吗，我也碰到了同样的问题，用了1.0.23，不是已经是最新版本了吗？ @CONANLMN 这个问题我已经修复提交了，Release里面应该是没有重新打包，你去把最新的源码下下来编译了，替换掉MysqlConnection.class就行
246,UPDATE 事件如何直接获取SQL UPDATE 事件如何直接获取SQL。 当前都是获取更新的字段值。 只能自己拼sql
245,issues#244 [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=245) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=245) before we can accept your contribution.<br/> tks
244,不能正常关闭Mysql心跳连接 ## MysqlEventParser.stopHeartBeat ![screenshot_20161230_112811](https://cloud.githubusercontent.com/assets/1852661/21559222/32776da4-ce83-11e6-9cdb-e0671014de96.png) ## AbstractEventParser.stopHeartBeat ![screenshot_20161230_112756](https://cloud.githubusercontent.com/assets/1852661/21559229/5bae9b34-ce83-11e6-8de1-258aac961da3.png) tks
243,原来的example内的startup.bat脚本直接运行时报语法错误 [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=243) <br/>All committers have signed the CLA. tks
242,Client requested master to start replication from impossible position 完整的报错信息： 2016-12-22 00:00:54.195 [destination = ixtrade   address = /172.30.10.109:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position 2016-12-22 00:00:54.199 [destination = ixtrade   address = /172.30.10.109:3306   EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Client requested master to start replication from impossible position; the first event 'mysql-bin.000024' at 533920026  the last event read from 'mysql-bin.000024' at 4  the last byte read from 'mysql-bin.000024' at 4. 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) ~[canal.parse-1.0.22.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) [canal.parse-1.0.22.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210) [canal.parse-1.0.22.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_25] 2016-12-22 00:00:54.199 [destination = ixtrade   address = /172.30.10.109:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /172.30.10.109:3306 has an error  retrying. caused by  java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Client requested master to start replication from impossible position; the first event 'mysql-bin.000024' at 533920026  the last event read from 'mysql-bin.000024' at 4  the last byte read from 'mysql-bin.000024' at 4. 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) ~[canal.parse-1.0.22.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) ~[canal.parse-1.0.22.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210) ~[canal.parse-1.0.22.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_25] 2016-12-22 00:00:54.200 [destination = ixtrade   address = /172.30.10.109:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:ixtrade[java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Client requested master to start replication from impossible position; the first event 'mysql-bin.000024' at 533920026  the last event read from 'mysql-bin.000024' at 4  the last byte read from 'mysql-bin.000024' at 4. 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210) 	at java.lang.Thread.run(Thread.java:745) ] 2016-12-22 00:00:54.202 [destination = ixtrade   address = /172.30.10.109:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - disconnect address /172.30.10.109:3306 has an error  retrying.  caused by  java.io.IOException: KILL DUMP 7726 failure:java.io.IOException: ErrorPacket [errorNumber=1094  fieldCount=-1  message=Unknown thread id: 7726  sqlState=HY000  sqlStateMarker=#]  with command: KILL CONNECTION 7726 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlUpdateExecutor.update(MysqlUpdateExecutor.java:49) 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.disconnect(MysqlConnector.java:104) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.disconnect(MysqlConnection.java:60) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:242) 	at java.lang.Thread.run(Thread.java:745) 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.disconnect(MysqlConnector.java:106) ~[canal.parse.driver-1.0.22.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.disconnect(MysqlConnection.java:60) ~[canal.parse-1.0.22.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:242) ~[canal.parse-1.0.22.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_25] 这个异常应该是mysql非正常关闭后产生的，请问该怎么处理？ sqlstate = HY000 errmsg = Client requested master to start replication from impossible position; the first event 'mysql-bin.000024' at 533920026  the last event read from 'mysql-bin.000024' at 4  the last byte read from 'mysql-bin.000024' at 4. 估计位点错误 清除meta.dat中内容 或者清楚zk 中报错的postion 信息 重启server 端就可以了
241,ClusterCanalClientTest运行后server端频繁刷日志： [New I/O server worker #1-2] INFO  c.a.otter.canal.server.netty.handler.SessionHandler - message receives in session handler clone的最新的canal项目，用ClusterCanalClientTest能正常获取DB的binlog。但是server端疯狂刷日志。虽然是INFO，但是感觉不是很正常。我logback那边开启了<appender-ref ref="STDOUT"/>这个appender 2016-12-22 18:07:31.810 [New I/O server worker #1-2] INFO  c.a.otter.canal.server.netty.handler.SessionHandler - message receives in session handler... 2016-12-22 18:07:31.810 [New I/O server worker #1-2] INFO  c.a.otter.canal.server.netty.handler.SessionHandler - message receives in session handler... 2016-12-22 18:07:31.810 [New I/O server worker #1-2] INFO  c.a.otter.canal.server.netty.handler.SessionHandler - message receives in session handler... 2016-12-22 18:07:31.810 [New I/O server worker #1-2] INFO  c.a.otter.canal.server.netty.handler.SessionHandler - message receives in session handler... 2016-12-22 18:07:31.810 [New I/O server worker #1-2] INFO  c.a.otter.canal.server.netty.handler.SessionHandler - message receives in session handler... 2016-12-22 18:07:31.810 [New I/O server worker #1-2] INFO  c.a.otter.canal.server.netty.handler.SessionHandler - message receives in session handler... 2016-12-22 18:07:31.810 [New I/O server worker #1-2] INFO  c.a.otter.canal.server.netty.handler.SessionHandler - message receives in session handler... 2016-12-22 18:07:31.810 [New I/O server worker #1-2] INFO  c.a.otter.canal.server.netty.handler.SessionHandler - message receives in session handler... 2016-12-22 18:07:31.810 [New I/O server worker #1-2] INFO  c.a.otter.canal.server.netty.handler.SessionHandler - message receives in session handler... 2016-12-22 18:07:31.810 [New I/O server worker #1-2] INFO  c.a.otter.canal.server.netty.handler.SessionHandler - message receives in session handler... 开启了这个STDOUT的appender INFO日志？只是打印了一下日志 哦哦，知道了。netty里面的长连接，server端一直长轮询在拉消息，所以sessionHandler被频繁调用了~
240,切换成group-instance.xml 时，消费不到数据 2016-12-19 17:28:58.734 [main] INFO  com.alibaba.otter.canal.deployer.CanalLauncher - ## start the canal server. log4j:WARN No appenders could be found for logger (org.I0Itec.zkclient.ZkEventThread). log4j:WARN Please initialize the log4j system properly. 2016-12-19 17:28:58.991 [main] INFO  com.alibaba.otter.canal.deployer.CanalController - ## start the canal server[10.97.14.145:11111] 2016-12-19 17:28:59.527 [main] ERROR c.a.o.c.common.zookeeper.running.ServerRunningMonitor - processActiveEnter failed java.lang.NullPointerException: null 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.start(AbstractEventParser.java:274) ~[canal.parse-1.0.22.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.start(MysqlEventParser.java:145) ~[canal.parse-1.0.22.jar:na] 	at com.alibaba.otter.canal.parse.inbound.group.GroupEventParser.start(GroupEventParser.java:24) ~[canal.parse-1.0.22.jar:na] 	at com.alibaba.otter.canal.instance.core.AbstractCanalInstance.start(AbstractCanalInstance.java:91) ~[canal.instance.core-1.0.22.jar:na] 	at com.alibaba.otter.canal.instance.spring.CanalInstanceWithSpring.start(CanalInstanceWithSpring.java:30) ~[canal.instance.spring-1.0.22.jar:na] 	at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.start(CanalServerWithEmbedded.java:102) ~[canal.server-1.0.22.jar:na] 	at com.alibaba.otter.canal.deployer.CanalController$2$1.processActiveEnter(CanalController.java:121) ~[canal.deployer-1.0.22.jar:na] 	at com.alibaba.otter.canal.common.zookeeper.running.ServerRunningMonitor.processActiveEnter(ServerRunningMonitor.java:234) [canal.common-1.0.22.jar:na] 	at com.alibaba.otter.canal.common.zookeeper.running.ServerRunningMonitor.initRunning(ServerRunningMonitor.java:138) [canal.common-1.0.22.jar:na] 	at com.alibaba.otter.canal.common.zookeeper.running.ServerRunningMonitor.start(ServerRunningMonitor.java:98) [canal.common-1.0.22.jar:na] 	at com.alibaba.otter.canal.deployer.CanalController.start(CanalController.java:396) [canal.deployer-1.0.22.jar:na] 	at com.alibaba.otter.canal.deployer.CanalLauncher.main(CanalLauncher.java:35) [canal.deployer-1.0.22.jar:na] java.lang.NullPointerException: null at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.start(AbstractEventParser.java:274) ~ 要学会看日志 关注下最新版1.0.23 关注下最新版1.0.23
239,使用CanalConnectors.newClusterConnector报错。 使用zk获取canal连接报错 语句：  CanalConnector simpleCanalConnector= CanalConnectors.newClusterConnector("10.100.103.19:2181" "example" "" ""); 报错信息 Exception in thread "main" java.lang.IllegalAccessError: tried to access method com.google.common.collect.MapMaker.makeComputingMap(Lcom/google/common/base/Function;)Ljava/util/concurrent/ConcurrentMap; from class com.alibaba.otter.canal.common.zookeeper.ZkClientx 	at com.alibaba.otter.canal.common.zookeeper.ZkClientx.<clinit>(ZkClientx.java:26) 	at com.alibaba.otter.canal.client.CanalConnectors.newClusterConnector(CanalConnectors.java:60) 	at com.foundation.test.canal.ClientCluster.main(ClientCluster.java:25) 解决了。依赖更新到最新的1.0.22就ok了。老版本的参数不一致！ <dependency>   <groupId>com.alibaba.otter</groupId>   <artifactId>canal.client</artifactId>   <version>1.0.22</version> </dependency>
238,java.io.IOException: end of stream when reading header 异常 前两天还正常，今天突然报了这个异常 18:08:13.923 [im_message-****] ERROR  ***.mysql.binlog.CanalClient - process error! com.alibaba.otter.canal.protocol.exception.CanalClientException: java.io.IOException: end of stream when reading header 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:281) ~[canal.client.jar:na] 	at cn.ishow.db.mysql.binlog.CanalClient.process(CanalClient.java:160) [Subscriber.jar:na] 	at cn.ishow.db.mysql.binlog.CanalClient.access$100(CanalClient.java:27) [Subscriber.jar:na] 	at cn.ishow.db.mysql.binlog.CanalClient$2.run(CanalClient.java:101) [Subscriber.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_74] Caused by: java.io.IOException: end of stream when reading header 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.read(SimpleCanalConnector.java:378) ~[canal.client.jar:na] 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.readNextPacket(SimpleCanalConnector.java:369) ~[canal.client.jar:na] 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.receiveMessages(SimpleCanalConnector.java:286) ~[canal.client.jar:na] 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:279) ~[canal.client.jar:na] 	... 4 common frames omitted 18:08:13.923 [im_message-****] INFO  ***.mysql.binlog.CanalClient - disconnect 随后canal就连不上了，一直报错 com.alibaba.otter.canal.protocol.exception.CanalClientException: java.net.ConnectException: Connection refused: connect 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.doConnect(SimpleCanalConnector.java:171) ~[canal.client.jar:na] 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.connect(SimpleCanalConnector.java:97) ~[canal.client.jar:na] 	at cn.ishow.db.mysql.binlog.CanalClient.process(CanalClient.java:151) [Subscriber.jar:na] 	at cn.ishow.db.mysql.binlog.CanalClient.access$100(CanalClient.java:27) [Subscriber.jar:na] 	at cn.ishow.db.mysql.binlog.CanalClient$2.run(CanalClient.java:101) [Subscriber.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_74] Caused by: java.net.ConnectException: Connection refused: connect 	at sun.nio.ch.Net.connect0(Native Method) ~[na:1.8.0_74] 	at sun.nio.ch.Net.connect(Net.java:454) ~[na:1.8.0_74] 	at sun.nio.ch.Net.connect(Net.java:446) ~[na:1.8.0_74] 	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[na:1.8.0_74] 	at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.doConnect(SimpleCanalConnector.java:132) ~[canal.client.jar:na] 	... 5 common frames omitted sorry 自己的问题 什么原因，我也遇到了 什么问题，我也遇到了，突然就有这个问题 @AnakinSkyW 
237,关于canal嵌入式模式重启后数据丢失问题 大神好：  本人使用的canal-1.0.22版本，参照otter写了一个数据订阅平台，由于目前canal的event store只有memory模式，当eventstore中的数据还没有来得及消费时，进行重启后，canal会从上次解析的位置进行数据重新装载消费，于是导致了重启前未消费eventstore的数据丢失。请问针对这个问题目前有没有好的处理思路，或者新的版本规划? 不会丢失数据的，下次重启前会以你最后消费ack的位点进行重拉binlog，包括之前内存里没消费的数据 看了一下代码，ack只是清空了store中数据，没有看到写zookeeper，下次重启后，读取到了zookeeper parse节点的binlog位置，导致数据丢失的 你要选择default-instance.xml模式，就可以保证刷到zookeeper，可以先看下adminGuide wiki 大概发现问题了，我在初始化CanalLogPositionManager时使用的ZOOKEEPER模式，导致并不会解析MetaLogPosition，另外我的MetaManager刚好也是MEMORY模式，也并不写MetaLogPosition，多谢！
236,为什么运行好好的，突然报错Could not find first log file name in binary log index file 双向同步中，出现的。而且还影响了单向同步。 指定了位点，同步看起来正常，但是始终处于定位中，数据不同步。 我这边也是，运行着 突然报这个  canal 1.0.22    rds  我记得canal1.0.22已经支持了rds(myql)的主备切换时的binlog自动切换的问题。但是还是会有这个问题 ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Could not find first log file name in binary log index file         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) ~[canal.parse-1.0.22.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) [canal.parse-1.0.22.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210) [canal.parse-1.0.22.jar:na]         at java.lang.Thread.run(Thread.java:744) [na:1.7.0_45] rds会有删除binlog的操作，如果消费有延迟就容易出现位点不在的情况 请问这个要怎么处理
235,多表进行数据聚合 出现源库表的id 会覆盖 目的表库的id 导致多条只有一条记录 有什么解决方案~? 源和目标设置不同的id段 这样就很不合理了 设置不同的id段 是源1-100目标是100-200吗? 我有设置过把id映射到mid 然后就会多出记录来 而不是在值相同的地方进行覆盖~ 聚合方面的不能整合吗? 2016-12-20 22:45 GMT+08:00 agapple <notifications@github.com>: > 源和目标设置不同的id段 > > — > You are receiving this because you authored the thread. > Reply to this email directly  view it on GitHub > <https://github.com/alibaba/canal/issues/235#issuecomment-268260730>  or mute > the thread > <https://github.com/notifications/unsubscribe-auth/AILd__h0spaA-5aLl4sX14smS1DpcYa6ks5rJ-obgaJpZM4LGToY> > . > 分布式场景下，不同分区的id都要求唯一，否则落到一个库上怎么判断是否是一条记录
234,Merge pull request #1 from alibaba/master full pull [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=234) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=234) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=234) it.</sub> 主要变更了啥？没看懂提交的内容  @agapple 这是在老版本上从上游做了一次合并，生成了 git no-ff 的一个节点。其实里面啥内容都没有，关掉吧。
233,canal基于timestamp位点回抽报错 canal版本：1.0.22 测试时基于timestamp位点信息配置数据重抽，发现日志里不断报错，请问是什么原因，该如何解决？报错信息如下： 2016-12-05 19:24:29.049 [destination = oms_orders   address = /172.172.230.36:3307   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position ::1480780800000 2016-12-05 19:24:49.062 [destination = oms_orders   address = /172.172.230.36:3307   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - ERROR ## findAsPerTimestampInSpecificLogFile has an error java.io.IOException: KILL DUMP 19915370 failure:java.io.IOException: ErrorPacket [errorNumber=1094  fieldCount=-1  message=Unknown thread id: 19915370  sqlState=HY000  sqlStateMarker=#]  with command: KILL CONNECTION 19915370         at com.alibaba.otter.canal.parse.driver.mysql.MysqlUpdateExecutor.update(MysqlUpdateExecutor.java:49)         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.disconnect(MysqlConnector.java:104)         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.reconnect(MysqlConnector.java:82)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.reconnect(MysqlConnection.java:56)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findAsPerTimestampInSpecificLogFile(MysqlEventParser.java:674)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findByStartTimeStamp(MysqlEventParser.java:519)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPositionInternal(MysqlEventParser.java:356)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPosition(MysqlEventParser.java:313)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:162)         at java.lang.Thread.run(Thread.java:745)         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.disconnect(MysqlConnector.java:106) ~[canal.parse.driver-1.0.22.jar:na]         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.reconnect(MysqlConnector.java:82) ~[canal.parse.driver-1.0.22.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.reconnect(MysqlConnection.java:56) ~[canal.parse-1.0.22.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findAsPerTimestampInSpecificLogFile(MysqlEventParser.java:674) [canal.parse-1.0.22.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findByStartTimeStamp(MysqlEventParser.java:519) [canal.parse-1.0.22.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPositionInternal(MysqlEventParser.java:356) [canal.parse-1.0.22.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPosition(MysqlEventParser.java:313) [canal.parse-1.0.22.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:162) [canal.parse-1.0.22.jar:na]         at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101] ![image](https://cloud.githubusercontent.com/assets/7370509/20909493/1b10a274-bb97-11e6-8098-6fa85a4ab56e.png) 我用的1.0.22版本里的代码，发现异常不是记录日志 已修改，使用当前master打个包试试 重新基于master做了编译，替换了driver下的mysqlconnector类，不影响抽取流程了
232,ErrorPacket [errorNumber=1094  fieldCount=-1  message=Unknown thread id: 335944433  sqlState=HY000  sqlStateMarker=#] canal版本:1.0.22 mysql版本：阿里云rds mysql5.6.16-log 报如下异常: 2016-12-05 10:05:19.828 [destination = example   address = xxx.mysql.rds.aliyuncs.com/xxx:3306   EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Unexpected End Stream 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) ~[canal.parse-1.0.22.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70) ~[canal.parse-1.0.22.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) [canal.parse-1.0.22.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210) [canal.parse-1.0.22.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_111] 2016-12-05 10:05:19.837 [destination = example   address = xxx.mysql.rds.aliyuncs.com/xxx:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address xxx.mysql.rds.aliyuncs.com/xxx:3306 has an error  retrying. caused by  java.io.IOException: Unexpected End Stream 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) ~[canal.parse-1.0.22.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70) ~[canal.parse-1.0.22.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) ~[canal.parse-1.0.22.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210) ~[canal.parse-1.0.22.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_111] 2016-12-05 10:05:19.841 [destination = example   address = xxx.mysql.rds.aliyuncs.com/xxx:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: Unexpected End Stream 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210) 	at java.lang.Thread.run(Thread.java:745) ] 2016-12-05 10:05:19.858 [destination = example   address = xxx.mysql.rds.aliyuncs.com/xxx:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - disconnect address xxx.mysql.rds.aliyuncs.com/xxx:3306 has an error  retrying.  caused by  java.io.IOException: KILL DUMP 335944433 failure:java.io.IOException:   with command: KILL CONNECTION 335944433 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlUpdateExecutor.update(MysqlUpdateExecutor.java:49)ErrorPacket [errorNumber=1094  fieldCount=-1  message=Unknown thread id: 335944433  sqlState=HY000  sqlStateMarker=#] 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.disconnect(MysqlConnector.java:104) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.disconnect(MysqlConnection.java:60) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:242) 	at java.lang.Thread.run(Thread.java:745) 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.disconnect(MysqlConnector.java:106) ~[canal.parse.driver-1.0.22.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.disconnect(MysqlConnection.java:60) ~[canal.parse-1.0.22.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:242) ~[canal.parse-1.0.22.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_111] 2016-12-05 10:05:33.210 [destination = example   address = xxx.mysql.rds.aliyuncs.com/xxx:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position 2016-12-05 10:05:33.249 [destination = example   address = xxx.mysql.rds.aliyuncs.com/xxx:3306   EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Could not find first log file name in binary log index file 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) ~[canal.parse-1.0.22.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) [canal.parse-1.0.22.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210) [canal.parse-1.0.22.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_111] 2016-12-05 10:05:33.250 [destination = example   address = xxx.mysql.rds.aliyuncs.com/xxx:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address xxx.mysql.rds.aliyuncs.com/xxx:3306 has an error  retrying. caused by  java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Could not find first log file name in binary log index file 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) ~[canal.parse-1.0.22.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) ~[canal.parse-1.0.22.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210) ~[canal.parse-1.0.22.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_111] 2016-12-05 10:05:33.250 [destination = example   address = xxx.mysql.rds.aliyuncs.com/xxx:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Could not find first log file name in binary log index file 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210) 	at java.lang.Thread.run(Thread.java:745) ] destination:example[java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Could not find first log file name in binary log index file 是出现java.io.IOException: Unexpected End Stream之后发送KILL DUMP 335944433 之后 ，然后接收到了 [errorNumber=1094  fieldCount=-1  message=Unknown thread id: 335944433  sqlState=HY000  sqlStateMarker=#]，再重连就出现Could not find first log file name in binary log index file了，想问一下是不是那个java.io.IOException: Unexpected End Stream造成的 Could not find first log file name in binary log index file，这个是主因
231,fixed issue #227   fixed Mysql5.1.73 throw NullPointerException `select @master_binlog_checksum` 在mysql5.1版本中返回null 会触发NullPointerException [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=231) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=231) before we can accept your contribution.<br/> tks
230,canal监听问题 canal服务，我设置监听了一个数据库的**tjxt-busi-cs**表，但是程序连接到此canal上，却能打印出别的数据库**tjxt-busi-ln**的表数据变化。两个数据库在同一个机器的mysql中。 这是我canal instance的配置信息 ``` canal.instance.mysql.slaveId = 2283306 canal.instance.master.address = 192.168.10.100:3306 canal.instance.master.journal.name =  canal.instance.master.position =  canal.instance.master.timestamp =  #canal.instance.standby.address =  #canal.instance.standby.journal.name = #canal.instance.standby.position =  #canal.instance.standby.timestamp =  canal.instance.dbUsername = canal canal.instance.dbPassword = canal canal.instance.defaultDatabaseName = tjxt-busi-cs canal.instance.connectionCharset = UTF-8 canal.instance.filter.regex = .*\\..* canal.instance.filter.black.regex =   ``` 我程序里连接这个caanl，打印binlog信息，居然监听到了别的数据库的binlog日志 c.f.s.c.listener.BusiAnscWatcherThread - ================> binlog[mysqld-bin.000015:687343087]   name[**tjxt-busi-ln** busi_wife_b_value]   eventType : INSERT 不清楚为什么这样。 canal.instance.defaultDatabaseName = tjxt-busi-cs 这个属性不是用来指定canal只监听这一个数据库的。你想一下binlog是针对一个DB实例的。不是针对一个数据库的。 @AllenRay 嗯嗯。知道原理了。谢谢。。我想想通过别的方式进行解决一下
229,java.io.IOException: Unexpected End Stream异常 canal版本 1.0.16 偶发性的报如下异常： 2016-11-30 12:56:17.588 [destination = example   address = /xxx:3306   EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Unexpected End Stream 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) ~[canal.parse-1.0.16.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:78) ~[canal.parse-1.0.16.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:116) [canal.parse-1.0.16.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:208) [canal.parse-1.0.16.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.7.0_67] 2016-11-30 12:56:17.588 [destination = example   address = /xxx:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /172.16.81.97:23306 has an error  retrying. caused by  java.io.IOException: Unexpected End Stream 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) ~[canal.parse-1.0.16.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:78) ~[canal.parse-1.0.16.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:116) ~[canal.parse-1.0.16.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:208) ~[canal.parse-1.0.16.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.7.0_67] 2016-11-30 12:56:17.588 [destination = example   address = /xxx:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: Unexpected End Stream 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) 	at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:78) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:116) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:208) 	at java.lang.Thread.run(Thread.java:745) ] 且伴随着丢失binlog，偶发的时间没有规律性，有没有遇到过这个问题？ 几个建议： 1. 升级下canal版本到最新版 2. 开启心跳SQL 定时产生心跳事件 保证TCP链接一致有包传递 我也遇到了这个问题，具体场景是，一张60多万记录的表，有批量更新操作（update语句没有where条件），这样基于row模式的binlog就特别多，每次执行到30w数据时，就报这个错误了。我现在使用的最新版本，并且加上了心跳sql，还是出现同样的问题，有没有其他解决方案了？ 附异常日志： 2017-03-03 09:14:29.255 [destination = shuangshi   address = /10.3.250.93:3306   EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Unexpected End Stream         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) ~[canal.parse-1.0.23.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:78) ~[canal.parse-1.0.23.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:121) [canal.parse-1.0.23.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) [canal.parse-1.0.23.jar:na]         at java.lang.Thread.run(Thread.java:745) [na:1.8.0_111] 2017-03-03 09:14:29.255 [destination = shuangshi   address = /10.3.250.93:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address 10.3.250.93/10.3.250.93:3306 has an error  retrying. caused by java.io.IOException: Unexpected End Stream         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) ~[canal.parse-1.0.23.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:78) ~[canal.parse-1.0.23.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:121) ~[canal.parse-1.0.23.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) ~[canal.parse-1.0.23.jar:na]         at java.lang.Thread.run(Thread.java:745) [na:1.8.0_111] 2017-03-03 09:14:29.256 [destination = shuangshi   address = /10.3.250.93:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:shuangshi[java.io.IOException: Unexpected End Stream         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:78)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:121)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209)         at java.lang.Thread.run(Thread.java:745) ] 测试了一下，貌似是客户端处理时间过长导致的，如果客户端处理很快，就没有这个问题。所以我目前的解决方案是在客户端中将接收message和处理通过文件队列异步处理。但这样带来一个麻烦，就是失败的逻辑，得在客户端在实现一次。 各位前辈，这个问题我也是时不时的出现，有实用的解决方案吗，谢谢啦 不知道你解决了吗？我一样的问题，数据堆积在master就报错 @tutulvlv 
228,启动startup.sh报错 下载的master版本1.0.23-SNAPSHOT。 根据QuickStart中启动：sh bin/startup.sh 报错如下： : not found 2: startup.sh:  startup.sh: 4: startup.sh: Syntax error: word unexpected (expecting "in") 估计是和你环境所有的shell环境不兼容导致，你可以本地测试修复一下，提交一个PR吧 我也遇到同样的问题，不过最后排查下来是换行符导致的 下载一个dos2unix转换一下就搞定了 @keyganker  我的不是这个原因，是shell环境不兼容导致的。 我的linux版本是ubuntu 4.4.0-36-generic，这个版本自带的sh估计和官方的不一样。 命令改为用bash stop.sh就没有问题了。 :set ff=unix
227,canal仓库代码编译之后 不支持mysql5.1的问题 hi canal开发团队 我使用canal仓库中master分支的代码编译. Mysql Server version: 5.1.73 错误信息: > ERROR: [destination = example   address = test/192.168.10.16:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlConnection - java.io.IOException: ErrorPacket [errorNumber=1193  fieldCount=-1  message=Unknown system variable 'binlog_checksum'  sqlState=HY000  sqlStateMarker=#]  with command: set @master_binlog_checksum= @@global.binlog_checksum changed file:https://github.com/alibaba/canal/commit/0c9eecd758a8cba0f218a121f83d3e8f01c4dee8 ![image](https://cloud.githubusercontent.com/assets/9983697/20620006/5bffa586-b332-11e6-80b6-3dd104c87c00.png) 这篇答案说`binlog_checksum`变量是从mysql5.6才开始支持.[参考](http://dba.stackexchange.com/questions/66226/mysql-slave-database-wont-start) 如果是这样的话 时候意味着canal以后不会支持mysql5.6以下版本? thx 可以增加try lock机制忽略异常设置的情况.  Mysql Server version: 5.1.73 [MysqlConnection.java#L282](https://github.com/alibaba/canal/blob/3f38ad6f4927e1d3bcf43faca8501df1e706405c/parse/src/main/java/com/alibaba/otter/canal/parse/inbound/mysql/MysqlConnection.java#L282 )   抛出 NullPointerException 遇到了同样的问题，老版本的5.1.73 ，楼上最终的解决方案？ 使用当前主干试试
226,清理了一下 主mysql主的binlog日志，然后重启了canal，canal 服务端启动报错 我清理了一下 主mysql主的binlog日志，然后重启了canal  canal 服务端启动报错。 ``` 2016-11-22 15:41:55.513 [destination = tj-172.18.100.97   address = /172.18.100.97:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - disconnect address /172.18.100.97:3306 has an error  retrying.  caused by  java.io.IOException: KILL DUMP 1299 failure:java.io.IOException: ErrorPacket [errorNumber=1094  fieldCount=-1  message=Unknown thread id: 1299  sqlState=HY000  sqlStateMarker=#]  with command: KILL CONNECTION 1299 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlUpdateExecutor.update(MysqlUpdateExecutor.java:49) 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.disconnect(MysqlConnector.java:104) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.disconnect(MysqlConnection.java:60) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:242) 	at java.lang.Thread.run(Thread.java:745) 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.disconnect(MysqlConnector.java:106) ~[canal.parse.driver-1.0.22.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.disconnect(MysqlConnection.java:60) ~[canal.parse-1.0.22.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:242) ~[canal.parse-1.0.22.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.7.0_79] ``` 这大概是什么原因导致的呢？ 我主要是把主mysql的binlog日志清理了，binlog从头开始了。但meta.dat里面记录的却不是。所以启动canal报错了，清理了一下 instance的meta.dat 文件，就可以了 
225,canal 服务端运行一段时间会报如下错。不知道如何解决。 我程序启动连接canal 报错： something goes wrong with reason: something goes wrong with channel  com.alibaba.otter.canal.server.exception.CanalServerException: ack error   clientId:1001 batchId:47 is not exist   please check 怎么解决？canal服务端我也重启了。运行了一下，过一阵程序启动还是继续会报错 下面是我的代码 ``` while (true) {                 Message message = connector.getWithoutAck(1000); // 获取指定数量的数据                 long batchId = message.getId();                 int size = message.getEntries().size();                 if (batchId == -1 || size == 0) {                     try {                         Thread.sleep(1000);                     } catch (InterruptedException e) {}                 } else {                     long beginTime = DateUtils.nowDateToTimestamp();                     //逻辑:1先判断插入与更新2如果是状态已完成4，则获取数据。进行保存，存mongdb                     try {                         switchHandle(message.getEntries());                         connector.ack(batchId); // 提交确认                     } catch (Exception e) {                         //System.out.println("同步错误，需要回滚" + e.getMessage());                         logger.error("同步错误，需要回滚"  e);                         connector.rollback(batchId);                         continue;                     }                     long endTime = DateUtils.nowDateToTimestamp();                     logger.info("此次同步时间是:" + (endTime - beginTime) + " ms");                 } } ``` 出现这个问题，怀疑是server端发生了instance重启，导致内存里的batchId丢失.  检查一下服务端的日志 @agapple 还真是这个问题。 server端开启了scan=true扫描？ 我也遇到这个问题，请问是怎么解决的啊？ 客户端增加重试，或者服务端关闭scan=false @SHUlibee 两种思路。一个是确认一下是否是canal server进行了重启，再就是确认mysql的binlog是否进行了清理，canal的meta.data是否进行了清理。。 同时如果进行优化的话，可以跟agepple说的那样client端增加重试机制，并且关闭把canal Server的scan设置成false 一样的错，求解 我这边是切换了kakfa的版本 修改了kafka和zookeeper的地址 之后就一直报这个错误 请教下怎么处理? ``` 2018-05-29 11:00:08.799 [Thread-4] WARN  c.alibaba.otter.canal.client.impl.ClusterCanalConnector - something goes wrong when rollbacking data from server:null com.alibaba.otter.canal.protocol.exception.CanalClientException: java.io.IOException: Broken pipe         at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.rollback(SimpleCanalConnector.java:342)         at com.alibaba.otter.canal.client.impl.ClusterCanalConnector.rollback(ClusterCanalConnector.java:216)         at com.xiaomatech.dbevent.canal.dbkafka.AbstractCanalClient.process(AbstractCanalClient.java:203)         at com.xiaomatech.dbevent.canal.dbkafka.AbstractCanalClient$2.run(AbstractCanalClient.java:99)         at java.lang.Thread.run(Thread.java:745) Caused by: java.io.IOException: Broken pipe         at sun.nio.ch.FileDispatcherImpl.write0(Native Method)         at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47)         at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)         at sun.nio.ch.IOUtil.write(IOUtil.java:65)         at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:470)         at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.writeWithHeader(SimpleCanalConnector.java:359)         at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.rollback(SimpleCanalConnector.java:336)         ... 4 more 2018-05-29 11:00:13.869 [Thread-4] INFO  c.alibaba.otter.canal.client.impl.ClusterCanalConnector - restart the connector for next round retry. 2018-05-29 11:00:13.902 [Thread-4] WARN  c.alibaba.otter.canal.client.impl.ClusterCanalConnector - something goes wrong when getWithoutAck data from server:null com.alibaba.otter.canal.protocol.exception.CanalClientException: something goes wrong with reason: something goes wrong with channel:[id: 0x513ca1d3  /10.20.105.201:39029 => /10.20.105.201:11112]  exception=com.alibaba.otter.canal.server.exception.CanalServerException: rollback error  clientId:1001 batchId:3 is not exist   please check         at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.receiveMessages(SimpleCanalConnector.java:302)         at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:279)         at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:252)         at com.alibaba.otter.canal.client.impl.ClusterCanalConnector.getWithoutAck(ClusterCanalConnector.java:180)         at com.xiaomatech.dbevent.canal.dbkafka.AbstractCanalClient.process(AbstractCanalClient.java:186)         at com.xiaomatech.dbevent.canal.dbkafka.AbstractCanalClient$2.run(AbstractCanalClient.java:99)         at java.lang.Thread.run(Thread.java:745) ``` @agapple  一样的错，关闭scan=false也不行。
224,client获取不到更新事件 update操作，对应的binlog如下： ![image](https://cloud.githubusercontent.com/assets/3926181/20453252/ab62af26-ae58-11e6-8c27-2c14ea9a441b.png) connector.getWithoutAck(batchSize)只能获取到TRANSACTIONBEGIN和TRANSACTIONEND，如图 ![image](https://cloud.githubusercontent.com/assets/3926181/20453255/c911f216-ae58-11e6-90a4-e77bfee52348.png) db的配置如下： ``` [mysqld] pid-file	= /var/run/mysqld/mysqld.pid socket		= /var/run/mysqld/mysqld.sock datadir		= /data/mysql log-error	= /var/log/mysql/error.log # By default we only accept connections from localhost #bind-address	= 127.0.0.1 # Disabling symbolic-links is recommended to prevent assorted security risks symbolic-links=0 #自定义参数 explicit_defaults_for_timestamp lower_case_table_names=1 max_connections = 2000 # Recommended in standard MySQL setup sql_mode=NO_ENGINE_SUBSTITUTION STRICT_TRANS_TABLES innodb_open_files=65535 max_connect_errors=3500 max_connections=3100 max_user_connections=3000 open_files_limit=65535 #connect_timeout=30 #interactive_timeout=200 #wait_timeout=200 log-bin=mysql-bin #添加这一行就ok binlog-format=ROW #选择row模式 server_id=1 #配置mysql replaction需要定义，不能和canal的slaveId重复 #skip-name-resolve innodb_log_file_size=1000M innodb_buffer_pool_size=2g innodb_log_buffer_size=10M innodb_sync_array_size=16 thread_cache_size=256 max_binlog_size=500M binlog_cache_size=1M key_buffer_size=8M #开启慢查询 slow_query_log值为1或on表示开启，为0或off为关闭 slow_query_log=on #设置慢查询日志放在哪里 slow_query_log_file=mysql-slow #设置sql执行时间多长为慢查询 long_query_time=2 #表示没有使用索引的sql查询也会记录下来 #log-queries-not-using-indexes #支持emoji character-set-server=utf8mb4 collation-server=utf8mb4_unicode_ci ``` 经过验证，当canal部署到mysql同一台机器时可以获取到 ROWDATA事件（canal.instance.master.address = 使用 127.0.0.1:3306或者 机器真实地址:3306都可以） 如果使用阿里云rds，这该怎么办- - 没这回事，不需要部署到同一台mysql上.  检查下过滤条件，会以client提交的为准 截图红色框住的状况很奇怪， 检查过两个canal的环境，配置是一样。 client是同一个，测试的方式就是通过同一个client切换canal地址 debug看看server端的过滤条件，可以试试使用RDS的情况 过滤条件两个环境是一样的  ``` #canal.instance.filter.regex = test\\..*   //这个是之前的 canal.instance.filter.regex =     //换成这个远程canal可以工作，但本机部署的canal并会受到影响 # table black regex canal.instance.filter.black.regex = ``` 这样的格式。 把过滤条件删除（空白），远程部署的canal就可以获取到正常的Event了。但和mysql同机的canal，不受这个影响 进一步对比测试，发现是canal.instance.filter.regex的问题，总结如下： > 1、没有命中canal.instance.filter.regex的SQL操作，不管canal部署在哪，client只能获取到TRANSACTIONBEGIN和TRANSACTIONEND (为什么没有命中也会获取到这两个事件 也是很不解) > 2、部署在DB本机和远程的canal，对于regex的解析不同 > > 2.1、和DB同机部署canal的regx=test\\\\..*和 test.*都能获取到row event > >  2.2、和DB远程部署的cancal  regx=test\\\\..*这种方式获取不到row event 但  regx=test.*可以正常工作 以上现象，在rds和mysql都一样。 otter在配置canal时有个【过滤表达式】，但那里我填的是 test\\\\..* 这种格式， 同步数据好像没问题。otter node节点和DB都不是同一台机器。 这个表达式，还要看一下你client调用subscribe里指定的参数
223,canal.instance.master配置未生效 您好，请问一下 我配置一下参数（~/conf/example/instance.properties里） canal.instance.master.journal.name canal.instance.master.position canal.instance.master.timestamp 操作 ./bin/stop.sh  ./bin/start.sh 没有生效啊。 例如，我配置 timestamp=1479370130，在这个时间戳之前的update，也在这之后的update都会得到message不知道什么原因啊？ 时间戳定位默认会回溯60秒 
222,RDS mysql.mysql ha_health_check表找不到 - 在canal启动时报mysql.mysql ha_health_check表找不到这个错 - 去binlog查看了events，确实有ha_health_check这张表的记录 - 然而，通过show tables等sql语句却查不到这张表 估计是表权限的问题 2016-11-14 10:07 GMT+08:00 iadmirezhe notifications@github.com: >    - >  >    在canal启动时报mysql.mysql ha_health_check表找不到这个错 >    - >  >    去binlog查看了events，确实有ha_health_check这张表的记录 >    - >  >    然而，通过show tables等sql语句却查不到这张表 >  > — > You are receiving this because you are subscribed to this thread. > Reply to this email directly  view it on GitHub > https://github.com/alibaba/canal/issues/222  or mute the thread > https://github.com/notifications/unsubscribe-auth/AAy8txbjG_zt6LSby8EsUa3SQeXKXCOmks5q98JugaJpZM4Kw46d > . https://github.com/alibaba/canal/commit/96dca0b9e6b6dcc12fc439d2f17d85a4656485df   特别支持了下阿里云的RDS 这个不是➕个过滤就可以了吗 是的，现在默认识别为RDS增加了过滤机制 嗯，加了过滤，把mysql过滤掉 
221,canal client获取字段值为空字符串（本身应为null） 在使用canal的过程中发现一个问题，我在master机器上插入一条数据 比如说insert into test1 values(17 NULL);  在获取canal解析结果 column的值为''空字符串  采用的方法是 String columnValue=column.getValue();   这个是canal框架的问题 还是我本身获取字段值的方法不对。 column.getValue()中column对象是import com.alibaba.otter.canal.protocol.CanalEntry.Column对象 column.isNull()使用这个进行判断是否null 
220,canal client中Entry未包括操作用户 一个问题想请教一下。 canal client中Entry未包括操作用户，这个原因是因为binlog里面本身就没有operator user，还是canal没qe去封装解析？mysql binlog包含的具体内容，有相关的资料吗？在网上都说的不全。如果有，我想尝试着增加。 个人觉得，这部分信息不应该放在binlog里，对于备份完全多余，mysql应该有权限管理相关的处理，操作日志等等。 没有在binlog里 
219,Canal如何支持阿里云RDS的订阅？ 目前使用canal去订阅阿里RDS的增量变化数据，出现如下报错： com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.collect.ComputationException: com.alibaba.otter.canal.parse.exception.CanalParseException: fetch failed by table meta:`mysql`.`ha_health_check` Caused by: com.google.common.collect.ComputationException: com.alibaba.otter.canal.parse.exception.CanalParseException: fetch failed by table meta:`mysql`.`ha_health_check` 已经解决：阿里云RDS的账号缺少读取mysql.ha_health_check的权限 这应该是一个阿里用来监控的心跳表，将账号升级为高权限账号，并赋予canal账号SELECT mysql.ha_health_check权限后，不再报错，问题解决。 由此衍生的一个问题是：似乎当解析一行失败并抛出异常后，会影响后续的其他正常行解析，导致canal客户端无法接收到数据。这一点是我的猜测，并没有去细读源码，如果作者看见了这条issue可以适当关注一下。 你的猜测是对的，不过不用获取那么多权限，你把那个表过滤掉就可以了 是的，你可以设置白名单表，针对黑名单表binlog协议只看一个header头，判断不命中直接丢弃 https://github.com/alibaba/canal/commit/96dca0b9e6b6dcc12fc439d2f17d85a4656485df  特别支持了下阿里云的RDS 
218,一次更新，两次消费？ 你好，问题如下： 复现步骤：  + 现有两个库USERS_01，USERS_02，需要订阅这两个库中的表users的DML；  + canal.properties配置文件：              canal.id= 1         canal.ip=127.0.0.1         canal.port= 11111         canal.zkServers=localhost:2181         canal.destinations= users_building         canal.instance.global.spring.xml = classpath:spring/group-instance.xml         # 其他配置默认...  + group-instance.xml：                  <!-- 其他配置保持不变 -->         <bean id="eventParser1" class="com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser">             <property name="eventFilter"> 			<bean class="com.alibaba.otter.canal.filter.aviater.AviaterRegexFilter" > 				<constructor-arg index="0" value="${canal.instance.master1.filter.regex:.*\..*}" /> 			</bean> 		</property>         <!-- 其他配置保持不变 -->         </bean>         <bean id="eventParser2" class="com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser">             <property name="eventFilter"> 			<bean class="com.alibaba.otter.canal.filter.aviater.AviaterRegexFilter" > 				<constructor-arg index="0" value="${canal.instance.master2.filter.regex:.*\..*}" /> 			</bean> 		</property>         <!-- 其他配置保持不变 -->         </bean> + users_building/instance.properties配置：         canal.instance.mysql.slaveId = 1000         canal.instance.master1.address = 127.0.0.1:3306         canal.instance.master1.journal.name =          canal.instance.master1.position =          canal.instance.master1.timestamp =          canal.instance.master2.address = 127.0.0.1:3306         canal.instance.master2.journal.name =          canal.instance.master2.position =          canal.instance.master2.timestamp =          canal.instance.master1.filter.regex = USERS_01.users  # 对应于group-instance.xml的eventParser1的canal.instance.master1.filter.regex         canal.instance.master2.filter.regex = USERS_02.users # 对应于group-instance.xml的eventParser2的canal.instance.master2.filter.regex         # 其他配置默认... + 测试代码：         connector = CanalConnectors.newClusterConnector("127.0.0.1:2181"  "users_building"  ""  "");         // 其他代码为ClusterCanalClientTest + 当在mysql terminal里执行一次更新操作`update users set sex = 1 where id = 1;`，console里会有两次连续一样的订阅消息如：          ![users_update](https://cloud.githubusercontent.com/assets/8662695/19843667/b9e8fc24-9f5f-11e6-910f-c2755de0461d.png) 请问，这是有什么配置问题吗？还是其他问题？谢谢。          测试发现只有客户端设置filter表达式才能过滤，deployer设置不能生效？？ 明白了 ，demo里client.subcribe时设置了filter 
217,canal 客户端接收blob字段解析成 Google Protocol Buffer协议失败 数据库的表中存的是Google Protocol Buffer协议，然而在canal接收后，解析报错。 想问一下，canal支不支持非基本类型解析 blob类型为new String("ISO-8859-1")处理后的字符串，可以通过string.getBytes("ISO-8859-1")还原出来 谢谢，已经解决 
216,canal如何支持sql server 目前，我们有个项目，通过监听binlog变化来更新索引，上线前发现有的客户的数据库用的是sql server的，canal目前不支持sql server，不知道作者有没有思路来支持sql server？ 网上找了下，sql server有个cdc模式（https://msdn.microsoft.com/en-us/library/cc645937.aspx），开启这个模式就能track变更记录 对于sql server没啥研究，抱歉 
215,canal-server1.0.22基于mariadb10.1.14测试HA功能的BUG 测试描述：canalserver1.0.22基于mariadb10.1.14的HA功能不好用，部署两台canaserver当其中一台server挂掉后，zookeeper已经切换到新的canalserver，但新的canalserver获取的binlogname不正确导致读取mariadb的binlog错误。 mariadb中binlog名字： (canal@127.0.0.1) [canal_test]> show binary logs; +------------------+-----------+ | Log_name         | File_size | +------------------+-----------+ | mysql-bin.000001 |     25487 | | mysql-bin.000002 |       358 | | mysql-bin.000003 |      1518 | | mysql-bin.000004 |  29137967 | +------------------+-----------+ canalserver获取的binlogname:【binlog[mysql-bin.000004$^Nk<U+0081>:29138083]】中后半段有乱码 解决方法： 修改canalserver1.0.22源码com.alibaba.otter.canal.protocol.CanalEntry中对获取到的binlogname进行过滤，添加如下代码： 【public String filterLogFileName(String logfileName) {       System.out.println("开始过滤logfileName:"+logfileName);       Pattern p = Pattern.compile("(mysql-bin\.[0-9]_)._");       Matcher m = p.matcher(logfileName);       while(m.find()){         if(m.groupCount() >= 1) {           System.out.println("过滤logfileName为:"+m.group(1));           return m.group(1);         }       }       return logfileName;     } 】 修改代码： 【public String getLogfileName() {       java.lang.Object ref = logfileName_;       if (ref instanceof String) {         return filterLogFileName((String) ref);       } else {         com.google.protobuf.ByteString bs =              (com.google.protobuf.ByteString) ref;         String s = bs.toStringUtf8();         if (com.google.protobuf.Internal.isValidUtf8(bs)) {           logfileName_ = s;         }         return filterLogFileName(s);       }     } 】 重新打包测试验证HA功能可以自动切换，但是存在canalserver切换时client消费端重复消费的问题。 但是在mysql5.6中测试未出现此问题 现在有测试mariadb10.1.14的验证版本吗，看源码好像是从protocolbuffer读取的时候binglogname就存在问题了，对binlog协议格式不是很了解，这个是protocolbuffer定义的协议格式问题吗 实际代码分析：出现在【com.alibaba.otter.canal.parse.inbound.mysql.dbsync】中，int netlen = getUint24(PACKET_LEN_OFFSET);代码获取的包长度不一致造成的，maraidb和mysql获取的长度为：47和43，mariadb多获取4字节导致后续处理logfilename读取多4个字节；您好问下作者什么情况导致netlen获取的不一致? 最后4位可能是checksum，你试着找一下mariadb关闭一下mariadb的binlog checksum机制看看.   ps. 之前有支持过mysql5.6的checksum，可能mariadb协议上有点不一样，没完全兼容，先帮忙验证一下是否为checksum问题 if (FormatDescriptionLogEvent.versionProduct(versionSplit) >= FormatDescriptionLogEvent.checksumVersionProduct) { 之前canal代码是限制了>=5.6.1版本号才会开启checksum检查，这是以前mysql5.6的机制，mariadb估计有不一样的行为 已经确定是checksum的问题，但是测试的1.0.22稳定版，去查源码也是看的1.0.22的稳定版代码，后来看到canal1.0.23快照版代码已经修复这个问题了，在mariadb10.1.14验证通过，多谢指教。 ps: xiaolinziemail@163.com 发件人： agapple 发送时间： 2016-11-03 13:49 收件人： alibaba/canal 抄送： linwenxue; Author 主题： Re: [alibaba/canal] canal-server1.0.22基于mariadb10.1.14测试HA功能的BUG (#215) 最后4位可能是checksum，你试着找一下mariadb关闭一下mariadb的binlog checksum机制看看. ps. 之前有支持过mysql5.6的checksum，可能mariadb协议上有点不一样，没完全兼容，先帮忙验证一下是否为checksum问题 — You are receiving this because you authored the thread. Reply to this email directly  view it on GitHub  or mute the thread. 
214,canal 一个server怎么能监听一个数据库的不同表？ canal 一个server怎么能监听一个数据库的不同表？复制多份example文件夹，然后配置文件里面分别配置不同的连接表信息？麻烦作者看看有没有好的解决方案？ 一个配置里写多张表的匹配正则 
213,修复issue #212 1、BinLogFileQueue的listBinlogFiles方法中对binlog文件名进行更严格的验证，必须以数字结尾 2、LocalBinLogConnection类中解决如下bug     2.1第二个dump方法中增加代码"decoder.handle(LogEvent.FORMAT_DESCRIPTION_EVENT);" 解决"((QueryLogEvent)event).getQuery()"语句出现乱码的问题(BEGIN后面多了4个字节) 出现乱码不会进入if语句，导致找到的binlog位点不准确     2.2第一个dump方法中，被注释的decode代码才是正确的，和DirectLogFetcher不同，此处需要构造子循环进行遍历     2.3第一个dump方法中，将每个binlog-file的首个event设置为RotateLogEvent，否则会导致所有event的binlogFileName都是默认的mysql-bin.000001。这么改完之后，和DirectLogFetcher的行为相契合，后者在每个binlogfile的首个event就是RotateLogEvent 3、LocalBinlogEventParser中增加元数据处理的相关代码，不加的话，所有的event都不带列名，基本上没法儿使用 [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=213) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=213) before we can accept your contribution.<br/><hr/>**lubiao** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user). 补充：修复#206 对应类文件：SessionHandler、CanalServerWithEmbedded、MysqlEventParser 关于206，后来没有见到你的回复，但肯定是有问题的，我们在线上已经复现有两三次了。 导致该问题的最主要原因在于：在某个时间点没有running节点的时候，客户端通过ClusterNodeAccessStrategy的nextNode方法拿到的可能是未来standby的ip，如果是这样的话，可能会出现在备机上subscribe成功而getWithoutAck失败的场景，然后客户端最终切换到了主机。但是如果某个时刻，instance切换到备机了，那就有问题了，因为备机的metaManger已经初始化过了，并且缓存了以前的position。 #205没有修复，我们内部直接把lazy的功能全部禁用了，所以代码没有提交 tks 
212,关于binlog离线消费的bug 设计binlog离线消费(LocalBinlogEventParser)的初衷是什么？在阿里内部有没有大批量使用过这个功能？ 我们在尝试使用的过程中，发现有很多问题，如： 1、离线消费时tableMetaCache为null，构造出来的event不带元数据，这样的event应该是没法儿用 2、事件解析也存在bug       \* 比如LocalBinLogConnection中的dump方法，被注释的代码才是正确的代码       \* LocalBinLogConnection中第二个dump方法，应该增加decoder.handle(LogEvent.FORMAT_DESCRIPTION_EVENT);语句，否则((QueryLogEvent) event).getQuery()返回的字符串带乱码       \* local模式下获取不到ROTATE_EVENT类型的事件，导致构造的event的binlogFileName始终都是mysql-bin.000001 LocalBinlogEventParser主要用来做日志重发解析用的，内部用的少.  已有人提交PR解决 
211,canal客户端认为insert\update\delete是DDL语句了 如题，今天做了个测试，连接本机mysql，测试insert等语句，客户端example执行后，将insert、update、delete等语句看成了DDL语句，但是，连接开发服务器上的mysql就正常，请问这是为什么呢？ 估计是mysql5.6的rows query log，canal不会转换为sql 有具体信息再reopen 
210,不好的设计 MysqlEventParser 是 AbstractEventParser 抽象类的实现，怎么可以在父类中显示的依赖自类呢？可以使用Object 锁  另外，不太清楚抽象类的类锁是否可以？ [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=210) <br/>All committers have signed the CLA. tks 
209,Fix a minor fault: Canal instance reload again after startup. 说一下大致的场景： 用户在使用增量同步之前会先进行全量初始化。在初始化的过程中，可以随时中断并重新初始化。我们会将原来instance的config文件夹删除重新生成instance文件夹和配置文件（内容一致）。这时候，启动成功后的下一次扫描间隔（5s）会又reload一次，因为初始化transaction比较大，所以会同步给client一些duplicate的binlog。日志如下： 2016-10-02 13:28:35.103 [canal-instance-scan-0] INFO  c.a.o.canal.deployer.monitor.SpringInstanceConfigMonitor - auto notify start example successful. 2016-10-02 13:28:40.138 [canal-instance-scan-0] INFO  c.a.o.canal.deployer.monitor.SpringInstanceConfigMonitor - auto notify reload example successful. 所以，stop成功以后是不是应该把lastFiles里的remove掉？ 若有不对的地方请指教，谢谢！ 你们重复删除和增加文件的确是会有问题，tks 
208,mysql5.6.29'@@global.binlog_checksum'带单引号导致master退出 优化binlogFormat判断 mysql5.6.29'@@global.binlog_checksum'带单引号导致master退出 优化binlogFormat判断 Assertion failed: (ret <= BINLOG_CHECKSUM_ALG_CRC32)  function get_binlog_checksum_value_at_connect [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=208) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=208) before we can accept your contribution.<br/> tks 
207,一个日志疑问 首先看日志： `2016-09-27 11:01:06:com.alibaba.otter.canal.client.impl.ClusterCanalConnector.subscribe(ClusterCanalConnector.java:110) - something goes wrong when subscribing from server:null com.alibaba.otter.canal.protocol.exception.CanalClientException: failed to subscribe with reason: something goes wrong with channel:[id: 0x111fe04e  /10.204.12.79:47450 => /10.204.246.102:11111]  exceptio n=com.alibaba.otter.canal.server.exception.CanalServerException: destination:ucarobd should start first ```     at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.subscribe(SimpleCanalConnector.java:203)     at com.alibaba.otter.canal.client.impl.ClusterCanalConnector.subscribe(ClusterCanalConnector.java:106)     at com.alibaba.otter.canal.client.impl.ClusterCanalConnector.subscribe(ClusterCanalConnector.java:99)     at com.ucar.dep.exchange.binlog.AbstractCanalClient$ProcessThread.process(AbstractCanalClient.java:142)     at com.ucar.dep.exchange.binlog.AbstractCanalClient$ProcessThread.run(AbstractCanalClient.java:134)` ``` 通过分析代码，“should start first”这样的异常信息在subscribing的时候是不会触发的，可是我的日志文件里出现了这样的日志，分析了一天代码了，让我百思不得其解，请问作者能提供点儿什么思路吗？ 这异常一般是server端出现了重启导致，比如开启了scan=true，扫描到了文件变化，自动重载，客户端增加重试机制就好了 这个异常产生的原因我清楚 我感到奇怪的是：通过查看堆栈信息，是在subscribe方法中触发的这个异常，我查看了客户端和server端的代码，在subscribe的时候没有checkStart的判断，也就是说这个“should start first”异常不应该在此出现 客户端的第一次subscribe会触发服务端进行start操作 
206,SUBSCRIPTION存在bug 直接上SessionHandler的代码段 `case SUBSCRIPTION:                     Sub sub = Sub.parseFrom(packet.getBody());                     if (StringUtils.isNotEmpty(sub.getDestination()) && StringUtils.isNotEmpty(sub.getClientId())) {                         clientIdentity = new ClientIdentity(sub.getDestination()                             Short.valueOf(sub.getClientId())                             sub.getFilter());                         MDC.put("destination"  clientIdentity.getDestination());                         embeddedServer.subscribe(clientIdentity); ```                     // 尝试启动，如果已经启动，忽略                     if (!embeddedServer.isStart(clientIdentity.getDestination())) {                         ServerRunningMonitor runningMonitor = ServerRunningMonitors.getRunningMonitor(clientIdentity.getDestination());                         if (!runningMonitor.isStart()) {                             runningMonitor.start();                         }                     }                     ctx.setAttachment(clientIdentity);// 设置状态数据                     NettyUtils.ack(ctx.getChannel()  null);                 } else {                     NettyUtils.error(401                         MessageFormatter.format("destination or clientId is null"  sub.toString()).getMessage()                         ctx.getChannel()                         null);                 }                 break; ``` ` 问题代码为：embeddedServer.subscribe(clientIdentity); instance还没有启动就可以执行subscribe？？？ 个人认为正确的逻辑应该是：先启动runningMonitor，再判断instance是否启动，最后再执行subscribe。 按目前的代码逻辑，会产生如下的问题： 1.两台canalserver和两台canalclient正在运行，其中有个instance名称为xxx 2.将xxx的Active置为false，xxx会被release掉，然后触发重新抢占 3.在抢占的过程中，zk上xxx/cluster节点和xxx/running节点的注册是有时间间隔的 4.因为有时间间隔，所以，按照ClusterNodeAccessStrategy的逻辑，canalclient在进行connect时拿到的ip不一定是以后抢占到running节点的那个canalserver的ip 5.即使ip有问题，仍然可以进行connect和subscribe操作，但在get的时候会报错，然后重连 6.虽然重连了，但是subscribe时产生了脏数据，position信息被缓存到metaManager了 7.过了几天xxx处于running状态的那台canal关了，此时slave-canal接管，会启动instance，但是metaManager已经启动过了，postion定位到了几天前 8.悲剧了 备注： 1.此处判断runningMonitor的目的是想触发lazy-start，但是lazy-start在HA模式下不成立，请参见 https://github.com/alibaba/canal/issues/205 2.强烈建议改造一下ClusterNodeAccessStrategy，HA模式下取Address的时候只认running节点，就甭支持lazy了，按现在的设计为了支持lazy徒增了复杂度，导致客户端的高可用切换的时间有时非常长 我看了下代码，embeddedServer.subscribe(clientIdentity);订阅时会提前启动metaManager来记录订阅的信息，后启动CanalInstance，在拿数据的时候才会去校验是否拿到了锁启动了服务 恩，即使不是lazy模式，当 1，Active置为false的时候 2，或者，instance重启的时候(autoscan) 会出现只有cluster节点，没有running节点的情况，此时客户端会随机取cluster节点下的一个ip进行重连，而这个ip很可能是本次抢占的slave，那么就会触发subscribe的bug 需要改两处代码： 1、SessionHander中变更【启动runningmonitor】和【调用subscribe】的顺序 2、CanalServerWithEmbedded中的subscribe方法中第一行增加checkStart方法 checkStart方法增加会有风险是失败的，即使你SessionHander调用了start方法，走到subscribe方法时可能还没初始化完成 我记得以前我是特意去掉的，不知道这个没有checkstart校验，对于你们的影响是？ 
205,instance的lazy启动在HA模式下不成立 在HA模式下，instance的lazy启动不成立。 **首先看CanalController的代码** for (Map.Entry<String  InstanceConfig> entry : instanceConfigs.entrySet()) {             final String destination = entry.getKey();             InstanceConfig config = entry.getValue();             // 创建destination的工作节点             if (!config.getLazy() && !embededCanalServer.isStart(destination)) {                 // HA机制启动                 ServerRunningMonitor runningMonitor = ServerRunningMonitors.getRunningMonitor(destination);                 if (!runningMonitor.isStart()) {                     runningMonitor.start();                 }             } ```         if (autoScan) {             instanceConfigMonitors.get(config.getMode()).register(destination  defaultAction);         }     } ``` lazy模式下，ServerRunningMonitor不会启动，那么在zookeeper的/otter/canal/destinations/destination/cluster节点下不会有数据，running节点也不会有数据 **再看ClusterNodeAccessStrategy的代码**     public SocketAddress nextNode() {         if (runningAddress != null) {// 如果服务已经启动，直接选择当前正在工作的节点             return runningAddress;         } else if (!currentAddress.isEmpty()) { // 如果不存在已经启动的服务，可能服务是一种lazy启动，随机选择一台触发服务器进行启动             return currentAddress.get(0);// 默认返回第一个节点，之前已经做过shuffle         } else {             throw new CanalClientException("no alive canal server");         }     } 由于cluster和running节点都不存在，此处会进入else分支，客户端会一直报错，没办法触发laze启动。 个人认为lazy启动是个比较鸡肋的功能，增加了代码的复杂度，性价比不高。 我们内部使用时，完全屏蔽了这种模式，把ClusterNodeAccessStrategy类中的这段代码直接注释掉了 `else if (!currentAddress.isEmpty()) { // 如果不存在已经启动的服务，可能服务是一种lazy启动，随机选择一台触发服务器进行启动 return currentAddress.get(0);// 默认返回第一个节点，之前已经做过shuffle }` 找不到running节点就直接抛错，免去了无谓的网络交互，使高可用场景下的切换时间尽可能的小 server端改造的话，想到的一个思路是 为ServerRunningMonitor的start方法增加一个 boolean lazy 参数 ServerRunningMonitor只是检测running节点的变化，不管是否lazy模式，都会为每个instance创建一个cid节点，这里的currentAddress拿的应该是这个list. 看了下代码，目前cid节点的注册是依赖了ServerRunningMonitor.start()方法，逻辑上这里的确会有一点问题。   改进办法：ServerRunningMonitor可以增加init方法，init方法调用创建cid，init方法不受lazy模式控制 恩   这个可行   monitor的生命周期和instance并不在一条线上 init中只负责启动monitor 这个问题倒是没那么严重，估计lazy也极少有人用 subscribe那个问题建议还是及时处理下，我们在生产环境已经遇见过两次问题了：维护的时候，停掉一台canal-server，另外一台接管instance，然后position重置为几天前的了 position为啥会是几天前？第一次subscrite拿到了历史store的数据更新掉了？ 如有新信息，再reopen该issue 
204,replace into 操作的语句filter无效 replace into table 不订阅这个表，客户端也能收到消息 query语句没有严格按表进行过滤 
203,fixed issue # 201 [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=203) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=203) before we can accept your contribution.<br/><hr/>**zikaifeng** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user). tks 
202,canal.instance.filter.regex 修改后未生效 我们这边通过程序实现自动添加新的同步表（修改instance.properties下的canal.instance.filter.regex配置项），发现有时canal.instance.filter.regex修改后并未生效，对应的destination并为reload。 查看com.alibaba.otter.canal.deployer.monitor.SpringInstanceConfigMonitor，发现reload逻辑存在漏洞。 > 因第一次启动时并未记录配置文件的信息，所以在下次扫描前（默认5秒后）配置文件发生变更将无法被识别到，自然也无法进行reload操作。 下面是我打的patch～ ``` Index: deployer/src/main/java/com/alibaba/otter/canal/deployer/monitor/SpringInstanceConfigMonitor.java IDEA additional info: Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP <+>UTF-8 =================================================================== --- deployer/src/main/java/com/alibaba/otter/canal/deployer/monitor/SpringInstanceConfigMonitor.java    (revision f6cf88a6bd887afc56203bf9c4fa2aaccfd76e85) +++ deployer/src/main/java/com/alibaba/otter/canal/deployer/monitor/SpringInstanceConfigMonitor.java    (revision ) @@ -49 6 +49 8 @@      private ScheduledExecutorService         executor             = Executors.newScheduledThreadPool(1                                                                        new NamedThreadFactory("canal-instance-scan")); +    private volatile boolean isFirst = true; +      public void start() {          super.start();          Assert.notNull(rootConf  "root conf dir is null!"); @@ -58 6 +60 7 @@              public void run() {                  try {                      scan(); +                    if (isFirst) isFirst = false;                  } catch (Throwable e) {                      logger.error("scan failed"  e);                  } @@ -122 13 +125 18 @@              if (!actions.containsKey(destination) && instanceConfigs.length > 0) {                  // 存在合法的instance.properties，并且第一次添加时，进行启动操作 -                notifyStart(instanceDir  destination); +                notifyStart(instanceDir  destination  instanceConfigs);              } else if (actions.containsKey(destination)) {                  // 历史已经启动过                  if (instanceConfigs.length == 0) { // 如果不存在合法的instance.properties                      notifyStop(destination);                  } else {                      InstanceConfigFiles lastFile = lastFiles.get(destination); +                    // TODO 历史启动过 所以配置文件信息必然存在 +                    if (!isFirst && CollectionUtils.isEmpty(lastFile.getInstanceFiles())) { +                        logger.error("### [{}] is started  but not found instance file info."  destination); +                    } +                      boolean hasChanged = judgeFileChanged(instanceConfigs  lastFile.getInstanceFiles());                      // 通知变化                      if (hasChanged) { @@ -161 10 +169 19 @@          }      } -    private void notifyStart(File instanceDir  String destination) { +    private void notifyStart(File instanceDir  String destination  File[] instanceConfigs) {          try {              defaultAction.start(destination);              actions.put(destination  defaultAction); + +            // TODO 启动成功后记录配置文件信息 +            InstanceConfigFiles lastFile = lastFiles.get(destination); +            List<FileInfo> newFileInfo = new ArrayList<FileInfo>(); +            for (File instanceConfig : instanceConfigs) { +                newFileInfo.add(new FileInfo(instanceConfig.getName()  instanceConfig.lastModified())); +            } +            lastFile.setInstanceFiles(newFileInfo); +              logger.info("auto notify start {} successful."  destination);          } catch (Throwable e) {              logger.error("scan add found[{}] but start failed"  destination  ExceptionUtils.getFullStackTrace(e)); ``` 你打的patch能提交一个PR给我？我好合并到master 怎么感觉配置了canal.instance.filter.regex没有生效呢，所有库任何表更新都会被消费呢？ 表数据过滤，只针对row模式会有效 你好，针对CanalServerWithEmbedded_StandaloneTest，CanalServerWithEmbedded_StandbyTest有以下问题： - 修改**MetaMode**为**MetaMode.ZOOKEEPER**，发现同时启动两个，数据修改后会两个实例都会得到更新通知，或者说**嵌入式**不支持**Server**模式那样，一个instance只会一个Server实例在监听呢？ - 使用**嵌入式**对于配置多库配置，设置Group时比较麻烦(相对于xml)； - 生产环境的Server模式有具体的一些案例测试，和参考的benchmark吗？ 1.  HA的机制是在deployer实现，迁入式时需要自行解决HA 2. Group模式可以选择API创建的方式，可以看下otter和canal结合的部分 3. canal大概的性能TPS在2.5w+左右，和不同用户的机器环境和网络都有一些略微变化 @ihaolin `检查下CanalConnector是否调用subscribe(filter)方法；有的话，filter需要和instance.properties的canal.instance.filter.regex一致，否则subscribe的filter会覆盖instance的配置，如果subscribe的filter是.*\\..*，那么相当于你消费了所有的更新数据。`
201,MysqlConnection的seek方法和dump方法会报NullPointerException seek方法和dump方法都有初始化LogContext ``` java LogContext context = new LogContext(); ``` 但是在decode的时候会报NullPointerException 建议分别添加 ``` java context.setLogPosition(new LogPosition(binlogfilename)); ``` 能给更详细的异常堆栈不？ 我运行的是MysqlEventParserTest的test_timestamp方法 ``` java 16:06:16.145 [destination = null   address = /10.4.0.20:3306   EventParser] INFO  c.a.o.c.p.i.mysql.MysqlConnection - COM_BINLOG_DUMP with position:BinlogDumpCommandPacket[binlogPosition=4 slaveServerId=3344 binlogFileName=mysql-bin.000047 command=18] 16:06:16.146 [destination = null   address = /10.4.0.20:3306   EventParser] WARN  c.a.o.c.p.i.mysql.MysqlEventParser - the binlogfile:mysql-bin.000047 doesn't exist  to continue to search the next binlogfile   caused by java.lang.NullPointerException     at com.taobao.tddl.dbsync.binlog.LogDecoder.decode(LogDecoder.java:162)     at com.taobao.tddl.dbsync.binlog.LogDecoder.decode(LogDecoder.java:106)     at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.seek(MysqlConnection.java:96)     at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findAsPerTimestampInSpecificLogFile(MysqlEventParser.java:1101)     at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findByStartTimeStamp(MysqlEventParser.java:539)     at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPositionInternal(MysqlEventParser.java:375)     at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPosition(MysqlEventParser.java:310)     at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:162)     at java.lang.Thread.run(Thread.java:745) ``` https://github.com/alibaba/canal/pull/203 
200,项目使用log4j2与canal的logback发生冲突 `SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/Users/raoshaoquan/.m2/repository/org/slf4j/slf4j-log4j12/1.6.1/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/Users/raoshaoquan/.m2/repository/ch/qos/logback/logback-classic/1.1.3/logback-classic-1.1.3.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory] log4j:WARN No appenders could be found for logger (cn.mwee.search.job.misc.MyApplicationContext). log4j:WARN Please initialize the log4j system properly. log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.` canal默认依赖的是slf4j，你可以排除logback来启用你自己的logger选择 谢谢 已解决 
199,upgrade protobuf version 版本太老了，在依赖canal做二次开发的时候和其他开源项目依赖的版本冲突，提升canal的protobuf的版本 [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=199) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=199) before we can accept your contribution.<br/><hr/>**yinxiu** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user). tks 
198,mysql 5.5.52下canal解析bin-log问题 mysql 5.5 my.cnf配置如下：     [mysqld]       log-bin=mysql-bin #添加这一行就ok       binlog-format=ROW #选择row模式       server_id=1 #配置mysql replaction需要定义，不能和canal的slaveId重复   canal日志：    2016-09-12 16:22:04.994 [destination = example   address = localhost/127.0.0.1:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - Didn't find the corresponding binlog files from mysql-bin.000001 to mysql-bin.000009 2016-09-12 16:22:04.994 [destination = example   address = localhost/127.0.0.1:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address localhost/127.0.0.1:3306 has an error  retrying. caused by  com.alibaba.otter.canal.parse.exception.CanalParseException: can't find start position for example 2016-09-12 16:22:04.995 [destination = example   address = localhost/127.0.0.1:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: can't find start position for example    mysql 5.6是正常的，能帮看下是啥问题吗？ can't find start position for example 找不到比指定时间戳更早之前的时间 请教具体如何解决，我也有这个问题，感谢！ updated: 发现删掉canal.deployer-1.0.22\conf\example\meta.dat就可以了！ CanalParseException: can't find start position for example 发生上面的错误原因是时间戳的问题，可能导致的原因有几个： - conf/${instance}/meta.dat 文件记录了时间戳，删掉它尝试下。 - 此外可能是之前的instance名在Zookeeper中已经有binlog时间戳记录，但你之后的instance修改了数据库连接，但是instance名还是相同，因此会导致这个问题。我修改instance名之后就可以了。
197,Unexpected End Stream Exception 数据库用的阿里云rds， canal版本1.0.22 最近隔几天就会报下面的错误，导致接收不到binlog。 能看出是啥问题吗？ 2016-08-30 11:00:27.364 [destination = prodb2   address = x/y:3306   EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Unexpected End Stream         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) ~[canal.parse-1.0.22.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70) ~[canal.parse-1.0.22.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) [canal.parse-1.0.22.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210) [canal.parse-1.0.22.jar:na]         at java.lang.Thread.run(Thread.java:744) [na:1.7.0_45] 2016-08-30 11:00:27.370 [destination = prodb2   address = x/y:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address x/y:3306 has an error  retrying. caused by java.io.IOException: Unexpected End Stream         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) ~[canal.parse-1.0.22.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70) ~[canal.parse-1.0.22.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) ~[canal.parse-1.0.22.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210) ~[canal.parse-1.0.22.jar:na]         at java.lang.Thread.run(Thread.java:744) [na:1.7.0_45] 2016-08-30 11:00:27.371 [destination = prodb2   address = x/y:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:prodb2[java.io.IOException: Unexpected End Stream         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210)         at java.lang.Thread.run(Thread.java:744) ] 2016-08-30 11:00:28.554 [destination = prodb2   address = x/y:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - disconnect address x/y:3306 has an error  retrying.  caused by java.io.IOException: KILL DUMP 369615002 failure:java.io.IOException: ErrorPacket [errorNumber=1094  fieldCount=-1  message=Unknown thread id: 369615002  sqlState=HY000  sqlStateMarker=#]  with command: KILL CONNECTION 369615002         at com.alibaba.otter.canal.parse.driver.mysql.MysqlUpdateExecutor.update(MysqlUpdateExecutor.java:49)         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.disconnect(MysqlConnector.java:104)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.disconnect(MysqlConnection.java:60)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:242)         at java.lang.Thread.run(Thread.java:744) KILL CONNECTION 建议使用主干打包试试 @agapple 您好，我最近也发现了这个问题，您说得主干打包是什么意思，没明白，这个问题您在另一个相同issue里提出的解决方案是打开心跳sql，有具体可行的方案吗，刚接触不久，谢谢啦 @tutulvlv 使用1.0.24新版试试 用的1.0.24也是偶尔报这个错误。 `2017-09-15 16:33:17.786 [destination = action_log   address = /127.0.0.2:2020   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address 127.0.0.2/127.0.0.2:2020 has an error  retrying. caused by  java.io.IOException: Unexpected End Stream         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) ~[canal.parse-1.0.24.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70) ~[canal.parse-1.0.24.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122) ~[canal.parse-1.0.24.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) ~[canal.parse-1.0.24.jar:na]         at java.lang.Thread.run(Thread.java:745) [na:1.7.0_80] 2017-09-15 16:33:17.786 [destination = action_log   address = /127.0.0.2:2020   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:action_log[java.io.IOException: Unexpected End Stream         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209)         at java.lang.Thread.run(Thread.java:745) ] 2017-09-15 16:33:34.137 [destination = action_log   address = /127.0.0.2:2020   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position  {"identity":{"slaveId":-1 "sourceAddress":{"address":"127.0.0.2" "port":2020}} "postion":{"included":false "journalName":"mysql-bin.000876" "position":1064558972 "serverId":1037 "timestamp":1505464397000}} 2017-09-15 18:33:34.137 [destination = action_log   address = /127.0.0.2:2020   EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Unexpected End Stream         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) ~[canal.parse-1.0.24.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70) ~[canal.parse-1.0.24.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122) [canal.parse-1.0.24.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) [canal.parse-1.0.24.jar:na]         at java.lang.Thread.run(Thread.java:745) [na:1.7.0_80] 2017-09-15 18:33:34.137 [destination = action_log   address = /127.0.0.2:2020   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address 127.0.0.2/127.0.0.2:2020 has an error  retrying. caused by  java.io.IOException: Unexpected End Stream         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) ~[canal.parse-1.0.24.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70) ~[canal.parse-1.0.24.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122) ~[canal.parse-1.0.24.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) ~[canal.parse-1.0.24.jar:na]         at java.lang.Thread.run(Thread.java:745) [na:1.7.0_80] 2017-09-15 18:33:34.137 [destination = action_log   address = /127.0.0.2:2020   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:action_log[java.io.IOException: Unexpected End Stream         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209)         at java.lang.Thread.run(Thread.java:745) ] 2017-09-15 18:33:46.970 [destination = action_log   address = /127.0.0.2:2020   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position  {"identity":{"slaveId":-1 "sourceAddress":{"address":"127.0.0.2" "port":2020}} "postion":{"included":false "journalName":"mysql-bin.000877" "position":459898074 "serverId":1037 "timestamp":1505471614000}}` 大概几个小时就会重现，不知道是不是客户端引起的？客户端是拉一次->消费->确认，再循环，如果数据在master堆积会有问题吗？ @agapple 
196,eclipse导入项目编译报错 问题描述： 代码版本: 1.0.22 maven 版本: 3.3.3 jdk版本: 1.7.0_80 在根目录下执行mvn eclipse:eclipse 命令后，将项目导入eclipse，大面积报错，大概原因是：driver项目依赖canal.common  但是eclipse中导入的common项目的项目名是"common"  项目名不匹配。大家有碰到类似问题吗？ ![qq20160828-1](https://cloud.githubusercontent.com/assets/13866684/18031262/31068e26-6d0a-11e6-8257-f8b21a2e4681.png) 我工程依赖树定义都是canal.xxx，全名的，你看下其他工程的pom.xml定义 @agapple  我看了下其他工程的pom.xml定义，依赖也都是全名依赖，但是被依赖的项目导入eclipse后就不是全名了，可以见问题中的截图“common”。 是不是该项目对maven版本有要求还是我执行的mvn eclipse:eclipse命令缺少参数。 我自己在eclipse中将项目名手工改了，现在编译已经没问题了。 
195,对position内存的竞争导致的定位错误问题 前提：使用PeriodMixedMetaManager.java作为metaManager 问题描述： 在CanalServerWithEmbedded.java的subscribe()方法中存在这样一段代码： ```         Position position = canalInstance.getMetaManager().getCursor(clientIdentity);         if (position == null) {             position = canalInstance.getEventStore().getFirstPosition();// 获取一下store中的第一条             if (position != null) {                 canalInstance.getMetaManager().updateCursor(clientIdentity  position); // 更新一下cursor             }             logger.info("subscribe successfully  {} with first position:{} "  clientIdentity  position); ``` 根据上下文，当内存中的position为null时，这段代码会先取出eventStore中的position，再根据此position更新zk中的cursor位置。但是在PeriodMixedMetaManager中更新方法的代码如下： ```     public void updateCursor(ClientIdentity clientIdentity  Position position) throws CanalMetaManagerException {         updateCursorTasks.add(clientIdentity);// 添加到任务队列中进行触发         super.updateCursor(clientIdentity  position);     } ``` 此方法会先将调度任务加入任务队列，然后再更新内存中的position。而调度线程则会先去取内存中的position，再更新zookeeper中的cursor。两个线程处于并发竞争态，可能会发生读后写情况。 问题解决办法（建议）： 如果您认可这里的确可能有风险，把updateCursor里的两句话调换一下位置 问题背景： 在切换canal时发生了cursor定位错误（从0开始）的情况。我抱着“有问题”的心态来找问题，初步将问题定位在这里。如果存在风险更大的地方，请作者指点一二 updateCursorTasks.add(clientIdentity);// 添加到任务队列中进行触发 这个只是一个标致位而已，具体更新的时候还是获取以下当前内存中的值进行写出，不会出现读到旧版本数据的问题，一般也建议subscribe是单客户端调用，避免并发 我说的并不是多客户端调用subscribe()的问题，而是单次subscribe()执行时可能会发生的问题。<定时将内存位点推到zk>和<更新位点到内存>是两个并行的过程，而定时推送任务的启动可能早于更新位点到内存的操作完成，因此是有可能读到旧数据的，谁先谁后完全取决于调度情况 对于代码的细致要求，修改风险可控 
194,server与client部署在同机上的高可用性问题 问题情景： server与client部署在同机上的高可用性问题 canal server版本为1.0.21，client版本为1.0.22 问题描述： 现有两台机器pc1  pc2  分别部有server与client各1个，名为s1  c1  s2  c2。s1和c1在pc1上正常运行，s2和c2在pc2上作为备份hanging。 现将pc1进行reboot，pc2上的s2可以切换到正常运行态，但c2会卡在connector.subscribe()方法上。 问题分析： subscribe()在内部(SimpleCanalConnector.java)实现时，会一直等待获取锁(mutex.get())，而未采用超时返回TimeoutException的方法(mutex.get(long timeout  TimeUnit unit))。 在只有server发生切换的时候，原有的client会在server更新位点时抛出异常，从而跳出收发binlog的循环，重新订阅server节点。而当server与client同时发生切换时，由于备份client一直处于等待锁状态中，所以未能更新获取node的参数或者重新调用subscribe()方法。 问题hack： 修改了一些源码，使其暴露了带有TimeoutException的subscribe()方法，从而用重试的方法hack此bug。尚有以下几点问题： 1、 由于在底层加入了超时机制，connect()与disconnect()方法也会抛出超时异常，从而不能保证正确开启/关闭连接。（未验证，只是觉得有风险） 2、 其它潜在风险（我才刚刚接手这一块，源码读的不够深） 望得到作者指点一二，敬谢 看下 https://github.com/alibaba/canal/issues/171， 如果是卡在subscribe方法，得检查下initRunning方法是否有执行成功，成功之后会触发一个回调更新mutex initRunning()方法如果调用成功，会修改mutex，subscribe方法会返回重试。但是根据日志的情况，initRunning()只在reboot15分钟前有过日志记录。可能是因为reboot机器时，client端仍保有原server的黑洞状态的连接，和客户端单切造成堵塞的情况不太一样。。建议作者尝试一下，基本都可以重现，我这边说不甚清楚 
193,canal KILL DUMP  # position info canal.instance.master.address = xx.xx.xx.xx:3306 canal.instance.master.journal.name =  canal.instance.master.position =  canal.instance.master.timestamp = 1471076485000 2016-08-13 16:23:09.821 [main] WARN  org.springframework.beans.TypeConverterDelegate - PropertyEditor [com.sun.beans.editors.EnumEditor] found through deprecated global PropertyEditorManager fallback - consider using a more isolated form of registration  e.g. on the BeanWrapper/BeanFactory! 2016-08-13 16:23:09.978 [main] INFO  c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-example  2016-08-13 16:23:10.000 [main] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - start successful.... 2016-08-13 16:23:10.030 [destination = example   address = /xxxxxxxxx:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position ::1471076485000 2016-08-13 16:23:34.010 [destination = example   address = /xxxxxxxxxxx:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /xxxxxxxxxx:3306 has an error  retrying. caused by  java.io.IOException: KILL DUMP 459561 failure:java.io.IOException: ErrorPacket [errorNumber=1094  fieldCount=-1  message=Unknown thread id: 459561  sqlState=HY000  sqlStateMarker=#]  with command: KILL CONNECTION 459561         at com.alibaba.otter.canal.parse.driver.mysql.MysqlUpdateExecutor.update(MysqlUpdateExecutor.java:49)         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.disconnect(MysqlConnector.java:104)         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.reconnect(MysqlConnector.java:82)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.reconnect(MysqlConnection.java:56)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:169)         at java.lang.Thread.run(Thread.java:745) ```     at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.disconnect(MysqlConnector.java:106) ~[canal.parse.driver-1.0.22.jar:na]     at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.reconnect(MysqlConnector.java:82) ~[canal.parse.driver-1.0.22.jar:na]     at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.reconnect(MysqlConnection.java:56) ~[canal.parse-1.0.22.jar:na]     at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:169) ~[canal.parse-1.0.22.jar:na]     at java.lang.Thread.run(Thread.java:745) [na:1.7.0_80] ``` 2016-08-13 16:23:34.010 [destination = example   address = /xxxxxxxx:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: KILL DUMP 459561 failure:java.io.IOException: ErrorPacket [errorNumber=1094  fieldCount=-1  message=Unknown thread id: 459561  sqlState=HY000  sqlStateMarker=#]  with command: KILL CONNECTION 459561         at com.alibaba.otter.canal.parse.driver.mysql.MysqlUpdateExecutor.update(MysqlUpdateExecutor.java:49)         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.disconnect(MysqlConnector.java:104)         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.reconnect(MysqlConnector.java:82)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.reconnect(MysqlConnection.java:56)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:169)         at java.lang.Thread.run(Thread.java:745) ```     at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.disconnect(MysqlConnector.java:106)     at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.reconnect(MysqlConnector.java:82)     at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.reconnect(MysqlConnection.java:56)     at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:169)     at java.lang.Thread.run(Thread.java:745) ``` ] 2016-08-13 16:23:46.004 [destination = example   address = /xxxxxxxx:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position ::1471076485000 1.0.22 版本  配置起始时间信息，client启动后，/logs/example/example.log  中记录 kill dump 配置起始位置，也报相同的错误 谢谢 大神们 帮忙 我也遇到这样的问题，后来把conf/example/meta.dat删掉，再重启就好了。 @whlgh   这个方法我试了，没用，不知道我是不是还有其他地方没设置对？ 我用windows 跑，配置了canal.instance.master.timestamp 没问题，配置 canal.instance.master.journal.name 和 canal.instance.master.position  就报上面的错，  Linux下跑 2中配置方式 都报错  ################################################# ## mysql serverId canal.instance.mysql.slaveId = 1234 # position info canal.instance.master.address = 127.0.0.1:3306 canal.instance.master.journal.name = canal.instance.master.position = canal.instance.master.timestamp = #canal.instance.standby.address =  #canal.instance.standby.journal.name = #canal.instance.standby.position =  #canal.instance.standby.timestamp =  # username/password canal.instance.dbUsername =root canal.instance.dbPassword =root canal.instance.defaultDatabaseName =test canal.instance.connectionCharset = UTF-8 # table regex canal.instance.filter.regex = ._\.._ # table black regex canal.instance.filter.black.regex = ################################################# 这是我配置文件， canal.instance.master.journal.name、canal.instance.master.position、canal.instance.master.timestamp三个参数都为空也没问题。配上前两个也没问题，我是在Linux下跑的。 @whlgh   你用的什么版本？我用的1.0.22  也是1.0.22 1、你用root用户执行 2、show variables like '%log_bin%'; 看看mysql的binlog有没有开启 KILL CONNECTION异常已经处理 请问下，这个问题是怎么解决的，我怎么没有找到meta.dat文件 这个是我的异常日志信息 2016-11-29 11:17:23.465 [destination = inventory   address = /172.172.230.55:3307   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position ::1480262400000 2016-11-29 11:17:57.401 [destination = inventory   address = /172.172.230.55:3307   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /172.172.230.55:3307 has an error  retrying. caused by  java.io.IOException: KILL DUMP 6155706 failure:java.io.IOException: ErrorPacket [errorNumber=1094  fieldCount=-1  message=Unknown thread id: 6155706  sqlState=HY000  sqlStateMarker=#]  with command: KILL CONNECTION 6155706 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlUpdateExecutor.update(MysqlUpdateExecutor.java:49) 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.disconnect(MysqlConnector.java:104) 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.reconnect(MysqlConnector.java:82) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.reconnect(MysqlConnection.java:56) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:169) 	at java.lang.Thread.run(Thread.java:745) 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.disconnect(MysqlConnector.java:106) ~[canal.parse.driver-1.0.22.jar:na] 	at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.reconnect(MysqlConnector.java:82) ~[canal.parse.driver-1.0.22.jar:na] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.reconnect(MysqlConnection.java:56) ~[canal.parse-1.0.22.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:169) ~[canal.parse-1.0.22.jar:na] 	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101] 你的问题怎么解决的？能分享下么？ 我好像用时间了，不和位置较劲了
191,主备同步问题 现在有两台MySQL数据库 ：M1（主）-》M2（备） 我现在的情况是otter部署在M2上，两个有相同的数据库与一张表 条件：M1，M2均正常运行，otter未启动同步 1.在M1中insert一条数据，此时由于未启动同步，M2不会出现这条数据 2.启动otter同步 3.修改M1中刚才插入的数据，M2中不会出现数据 这种情况应该如何解决？ 谢谢各位了 update转insert，可以开启otter的行同步模式 我在测试的时候用的基于数据库反查就不能就会出现不同步，今天我使用基于当前日志变更的时候能够成功同步过来 就想问下数据库反查和日志变更在使用上分别的用途？ 2016年8月4日星期四，agapple notifications@github.com 写道： > Closed #191 https://github.com/alibaba/canal/issues/191. >  > — > You are receiving this because you authored the thread. > Reply to this email directly  view it on GitHub > https://github.com/alibaba/canal/issues/191#event-744692264  or mute > the thread > https://github.com/notifications/unsubscribe-auth/ABGYwgeN6sXmAKefU3pKSRlCkO6zylPdks5qcUUXgaJpZM4JbtQ1 > . 看到文档了，就不麻烦你了哈，谢谢 2016年8月4日星期四，刘欢 askingneil@gmail.com 写道： > 我在测试的时候用的基于数据库反查就不能就会出现不同步，今天我使用基于当前日志变更的时候能够成功同步过来 > 就想问下数据库反查和日志变更在使用上分别的用途？ >  > 2016年8月4日星期四，agapple <notifications@github.com > <javascript:_e(%7B%7D 'cvml' 'notifications@github.com');>> 写道： >  > > Closed #191 https://github.com/alibaba/canal/issues/191. > >  > > — > > You are receiving this because you authored the thread. > > Reply to this email directly  view it on GitHub > > https://github.com/alibaba/canal/issues/191#event-744692264  or mute > > the thread > > https://github.com/notifications/unsubscribe-auth/ABGYwgeN6sXmAKefU3pKSRlCkO6zylPdks5qcUUXgaJpZM4JbtQ1 > > . 
190,qq群拒绝加入，怎么才能加入qq群呢？ qq群还有空闲名额，重新尝试下 qq群还可以加人吗，有没有其它的群 qq群还有空闲名额，重新尝试下 qq群可以加入吗？
189,canal的数据库连接信息会在otter-manager内自动解析为ip的问题 canal的数据库连接信息会在otter-manager内自动解析为ip 但我们用的服务域名，会不定期更新ip，会导致canal连接不上mysql服务。 请问如何让配置中保持域名而不是解析为ip. 研究一下，提交一个PR给我吧 这个问题我们也遇到了，我们通过配置vpn解决的 能说下配置vpn如何解决otter-manager把域名解析成IP的问题吗？ https://github.com/alibaba/otter/issues/172  fix了 
188,mysql长时间没数据，可能导致canal与mysql的连接读操作永远阻塞 canal从mysql读binlog事件时，建的连接设置了读超时时间：        channel.socket().setSoTimeout(this.soTimeout);  ，soTimeout用了30s。 读binlog事件时，用了SocketChannel.read(buffer)方法，soTimeout对这个SocketChannel的read方法无效，永远不会超时，参考http://bugs.java.com/bugdatabase/view_bug.do?bug_id=4614802。 如果mysql有一段时间没数据（比如一个小时），这个连接失效了（可能因为防火墙问题，参考http://www.tldp.org/HOWTO/TCP-Keepalive-HOWTO/overview.html的2.4.  虽然设置了channel.socket().setKeepAlive(true)，但是linux系统默认的keepalive配置时间太长、不起作用。），但canal还在永远阻塞read。 所以是不是用sock.socket().getInputStream()这个来read比较好，如果在soTimeout内没读到数据，就超时了，强制重新建立连接。 目前设计时依赖于MySQL数据库的空闲链接超时机制，默认是8小时会断开链接，你的链接失效是指半开链接么？这个客户端能获取到EOFException 这个问题在我自己的测试环境中很容易重现，mysql版本:5.5.14 wait_timeout设为8小时，canal版本1.0.22，如果mysql半个小时无数据更新，canal就无法读取新的bin-log，重启canalServer后就正常了。难道跟数据库的配置有关系，还是其他原因？期望能得到指导。 PS：CanalServer没有任何的错误日志，不好分析问题。。。 暂时性解决方案，可以开启下心跳SQL. 这问题还真没遇到过 mysql的slave连接master读数据的默认超时时间（slave_net_timeout）是3600s（时间太长），超时后重建连接，参考http://dev.mysql.com/doc/refman/5.6/en/replication-options-slave.html#sysvar_slave_net_timeout mysql5.5后好像引入了master/slave之间的heartbeat binlog event，master没binlog event空闲一段时间后（超过MASTER_HEARTBEAT_PERIOD）就发送给slave heartbeat事件。参考： https://www.percona.com/blog/2011/12/29/actively-monitoring-replication-connectivity-with-mysqls-heartbeat/ http://dev.mysql.com/doc/refman/5.6/en/change-master-to.html里的MASTER_HEARTBEAT_PERIOD MASTER_HEARTBEAT_PERIOD的默认配置是slave_net_timeout/2，默认是1800s，时间都太长了。 @tianshazzq 可以测试下这些参数试试。 canal程序想要支持默认配置、不出问题，最好还是像开头说的那样自己控制超时时间、连接空闲一段时间（比如最多1min？）后重新建连接。 mysql与canl-server的tcp连接闪断，canl-server并不知情，导致后续的binlog无法通知到canal-server，这个怎么破？ 建议开启canal心跳设置，会定时更新数据库来产生心跳binlog，如果指定时间没收到心跳信息可以尝试重建链接 mysql到canal的连接异常了，比如mysql宕机这种情况，canal是被动接收数据的，自然是不会知道任何链路是否正常的状态；关键是canal是否实现了空闲后有无重连的机制或者mysql异常重连？如若实现了配置哪个参数？ canal基于被动监听式的是没法感知socket被server断开的，最重要还是开启心跳binlog，或者升级mysql5.6(mysql针对空闲链接会定时发送心跳binlog事件来维持tcp链接) 在Mysql宕机后重启这种情况下面的，开启心跳没什么用吧？我们需要在canal中实现mysql这个参数slave_net_timeout所需要实现的功能。 @agapple  把数据库断开时间改成了2分钟，并且开启了心跳检查。长时间（2分钟以上）没有数据后新插入数据，canal server会获取不到 （过不定时间后canal server又能获取到数据10~30分钟不等）。debug看心跳数据都是能正常收发。日志里没有任何异常输出 ## detecing config canal.instance.detecting.enable = true #canal.instance.detecting.sql = insert into retl.xdual values(1 now()) on duplicate key update x=now() canal.instance.detecting.sql = select 1 canal.instance.detecting.interval.time = 3 canal.instance.detecting.retry.threshold = 3 canal.instance.detecting.heartbeatHaEnable = false mysql与canl-server的tcp连接闪断，canl-server并不知情，导致后续的binlog无法通知到canal-server，这个怎么破？ --------------------------------------------------------------------------------------------------- 这个问题你怎么解决的，我现在也遇到了这个问题 @githubcjh  刚刚踩到一个坑，canal.instance.memory.buffer.size设置过小，导致主库执行大事务时，canal堵住 buffer.size建议设置>1024 确实有这个问题，不知道楼主解决了这个问题么？ 改为inputstream是可以的，sotimeout生效了，但是长时间没有数据，inputstream 阻塞在read那里时不会响应thread.interrupt()，event parser那里需要做一些改动先close socket再调用thread.interrupt() 但是总感觉这个方法和canal的nio设计有些矛盾。但是问题是现实存在的。虽然是nio但是读的时候用的是阻塞的模式  buffer size能解决这个问题？？ 解决方案就是在读取mysql binlog变更的地方加个超时就好，不然就只能依赖tcp的keeplive机制；我不太懂java，已经没用这一套了，自己开发了一套。 SocketChannel.read(buffer)此方法的确不会报超时异常（[https://stackoverflow.com/questions/2866557/timeout-for-socketchannel-doesnt-work#answer-9150513](https://stackoverflow.com/questions/2866557/timeout-for-socketchannel-doesnt-work#answer-9150513)），目前本人使用1.0.24版本，暂时将DirectLogFetcher使用的SocketChannel转为ReadableByteChannel，可使socket timeout 生效，抛出SocketTimeoutException。 @vamdt [BioSocketChannelPool.java](https://github.com/alibaba/canal/blob/master/driver/src/main/java/com/alibaba/otter/canal/parse/driver/mysql/socket/BioSocketChannelPool.java)，基于原生Socket操作，soTimeout参数验证也可以生效 @agapple OK，暂时使用体验上感觉1.0.24更稳，等1.0.26正式发布再试试看BIO的。 推荐v1.1.1版本，基于原生Socket的bio操作，增加了超时重连机制
187,支持下mysql5.7 json类型解析 RT. 支持下mysql5.7 测试数据格式: ``` +-----+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+ | uid | data                                                                                                                                                            | +-----+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+ |   1 | {"mail": "jiangchengyao@gmail.com"  "name": "David"  "address": "Shangahai"}                                                                                    | |   2 | {"mail": "test"}                                                                                                                                                | |   3 | {"int": 1  "bool": true  "long1": 4294967296  "long2": 1.8446744073709552e19  "double": 1.11012}                                                                | |   4 | {"mail": [1  true  1.11012  4294967296  1.8446744073709552e19]}                                                                                                 | |   5 | {"int": 1  "bool": true  "mail": [1  true  1.11012  4294967296  1.8446744073709552e19]  "long1": 4294967296  "long2": 1.8446744073709552e19  "double": 1.11012} | |   6 | {"time": "23:59:59"}                                                                                                                                            | |   7 | {"time": "2016-06-30 15:03:32.000000"}                                                                                                                          | |   8 | {"time": "2016-06-30 15:51:25.041000"}                                                                                                                          | |   9 | {"time": 123.123}                                                                                                                                               | |  10 | {"time": "23:23:23:19"}                                                                                                                                         | |  11 | {"time": "01:00:05.000000"}                                                                                                                                     | |  12 | {"time": "630:19:29.000000"}                                                                                                                                    | |  13 | {"time": "2016-06-30 16:08:58.000000"}                                                                                                                          | |  14 | {"n1": "v1"  "ne1": {"n2": "v2"  "ne2": {"n3": "v3"  "n32": ["v32"  "v33"  1]}}}                                                                                | +-----+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+ ``` 有个bug在1.0.23下 `com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: java.lang.IllegalArgumentException: illegal json data 	at com.taobao.tddl.dbsync.binlog.JsonConversion.parse_scalar(JsonConversion.java:150) 	at com.taobao.tddl.dbsync.binlog.JsonConversion.parse_value(JsonConversion.java:67)` 数据库版本  mysql-5.7.11-winx64 数据库结构 CREATE TABLE `customer_data` (   `id` bigint(19) unsigned NOT NULL AUTO_INCREMENT   `supplier_id` bigint(19) unsigned NOT NULL COMMENT '商户id'   `customer_id` bigint(19) unsigned NOT NULL   `customer_data` json DEFAULT NULL COMMENT '顾客标准属性'   `customer_ext_data` json DEFAULT NULL COMMENT '顾客扩展属性'   PRIMARY KEY (`id`)   UNIQUE KEY `uk_cust` (`customer_id`) ) ENGINE=InnoDB AUTO_INCREMENT=26036 DEFAULT CHARSET=utf8mb4; debug后就是149行str_len==0  @agapple  给出你测试的json字符串
186,ack 错误了 2016-06-06 19:07:07.416 [New I/O server worker #1-25] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400   Caused by :  something goes wrong with channel:[id: 0x4d32397e  /10.252.158.196:53393 => /10.1.7.109:9930]  exception=com.alibaba.otter.canal.server.exception.CanalServerException: ack error   clientId:1001 batchId:85 is not exist   please check 2016-06-06 19:07:07.418 [New I/O server worker #1-25] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x4d32397e  /10.252.158.196:53393 :> /10.1.7.109:9930]  exception=java.nio.channels.ClosedChannelException     at org.jboss.netty.channel.socket.nio.NioWorker.cleanUpWriteBuffer(NioWorker.java:643)     at org.jboss.netty.channel.socket.nio.NioWorker.writeFromUserCode(NioWorker.java:370)     at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:137)     at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76)     at org.jboss.netty.channel.Channels.write(Channels.java:611)     at org.jboss.netty.channel.Channels.write(Channels.java:578)     at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:28)     at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:144)     at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48)     at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:275)     at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302)     at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndFireMessageReceived(ReplayingDecoder.java:525)     at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:506)     at org.jboss.netty.handler.codec.replay.ReplayingDecoder.cleanup(ReplayingDecoder.java:541)     at org.jboss.netty.handler.codec.replay.ReplayingDecoder.channelDisconnected(ReplayingDecoder.java:449)     at org.jboss.netty.channel.Channels.fireChannelDisconnected(Channels.java:360)     at org.jboss.netty.channel.socket.nio.NioWorker.close(NioWorker.java:593)     at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:119)     at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76)     at org.jboss.netty.channel.Channels.close(Channels.java:720)     at org.jboss.netty.channel.AbstractChannel.close(AbstractChannel.java:200)     at org.jboss.netty.channel.ChannelFutureListener$1.operationComplete(ChannelFutureListener.java:46)     at org.jboss.netty.channel.DefaultChannelFuture.notifyListener(DefaultChannelFuture.java:381)     at org.jboss.netty.channel.DefaultChannelFuture.addListener(DefaultChannelFuture.java:148)     at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:30)     at com.alibaba.otter.canal.server.netty.NettyUtils.error(NettyUtils.java:51)     at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:200)     at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48)     at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:275)     at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302)     at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndFireMessageReceived(ReplayingDecoder.java:525)     at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:506)     at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:443)     at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274)     at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261)     at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:349)     at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:280)     at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:200)     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)     at java.lang.Thread.run(Thread.java:744) 我也遇到过这个问题，是多消费都是消费canal时报的错，你那边是啥情况产生的？ 检查一下server端，是否发生了instance重启
185,canal.instance.filter.regex无法过滤db 
184,删除表的问题 在遇到表删除后会有这种情况 Caused by: java.io.IOException: ErrorPacket [errorNumber=1146  fieldCount=-1  message=Table 'secooOrderWriter1DB.t_increment_sequence_bak' doesn't exist  sqlState=42S02  sqlStateMarker=#]  with command: desc `orderDB`.`t_increment_sequence_bak`     at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.query(MysqlQueryExecutor.java:60) ~[canal.parse.driver-1.0.21.jar:na]     at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:69) ~[canal.parse-1.0.21.jar:na]     at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta0(TableMetaCache.java:97) ~[canal.parse-1.0.21.jar:na]     at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:26) ~[canal.parse-1.0.21.jar:na]     at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.apply(TableMetaCache.java:50) ~[canal.parse-1.0.21.jar:na]     at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.apply(TableMetaCache.java:41) ~[canal.parse-1.0.21.jar:na] 翻一下历史的issue，不建议做有删除性质的DDL操作，包括删表，删字段等 已知问题，FAQ里有明确建议 
183,关于canal对于statement 格式日志的解析 请问下canal  有可以解析statement格式日志的方法和类吗？类似对row日志的解析，可以解析出数据库名称，表名，操作类型……而不是一条单纯的sql 语句 可以自行依赖sql解析器，对于输出的内容进行解析，得到你想要的结果 
182,关于MIXED日志格式数据获取 
181,Client连接问题 SimpleCanalConnector.java doConnect()方法调用的readNextPacket() int bodyLen = readHeader.getInt(0); bodyLen 的值1308622848 这个数怎么读出这么大。然后就内存溢出了 这应该是异常数据流 建议先增加客户端重试或者HA机制保证，如有新的信息再reopen issue 
180,CanalConnectors.newClusterConnector zk failover问题 CanalConnectors.newClusterConnector 用zk的时候报这个错误 new ClusterNodeAccessStrategy() 的时候clusterPath没有create zkClient.getChildren这个方法报错 Exception in thread "main" org.I0Itec.zkclient.exception.ZkNoNodeException: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /otter/canal/destinations/192.168.2.2:3306/cluster     at org.I0Itec.zkclient.exception.ZkException.create(ZkException.java:47)     at org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:685)     at org.I0Itec.zkclient.ZkClient.getChildren(ZkClient.java:413)     at org.I0Itec.zkclient.ZkClient.getChildren(ZkClient.java:409)     at com.alibaba.otter.canal.client.impl.ClusterNodeAccessStrategy.<init>(ClusterNodeAccessStrategy.java:58)     at com.alibaba.otter.canal.client.CanalConnectors.newClusterConnector(CanalConnectors.java:69)     at com.alibaba.otter.canal.example.ClusterCanalClientTest.main(ClusterCanalClientTest.java:31) 服务端有没启动？ 
179,请问目前的版本支持mysql5.7吗? 请问目前的版本支持mysql5.7吗? 没做过测试，群里有人做过类似测试，可询问 我测试了一下，没遇到什么特别的问题。 
178,关于持久化到文件或ZK的时间间隔问题 我这边测试，发现如果将canal.zookeeper.flush.period或者canal.file.flush.period这两个时间间隔设久一点，由于Client的cursor信息还在内存中没有持久化到文件或者ZK，那么重启Canal或者HA切换至另一个节点会收到重复数据。我想问的是，直连Canal，是否客户端要自己做消息去重的逻辑？我目前想到的是根据binlog的position？ 是的，会有重复，去重的思路基本正确 
177,RDS主从库切换时候异常处理 修改方案： 1、当出现IOException时retry指定次数。超过次数时获取当前最新的位点，重新连接。 2、错误日志输出和报警输出(建议以后可以实现邮件预警) [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=177) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=177) before we can accept your contribution.<br/><hr/>**Yishun.Chen** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account  please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user). tks    先合并了PR，需要做一些微调，比如精确识别binlog位点不存在的异常 
176,客户端高可用主动干预zookeeper的running会导致链接未关闭 手工登陆zookeeper，delete当前client的running节点，会触发一次客户端的高可用，上一次运行的节点会有优先选择权.  但上一次运行的节点，没有及时关闭上一次打开的socket链接.  触发条件：非正常的running节点干预，如果zookeeper出现网络抖动引起的running节点消失，理论上client的网络应该整体都有问题，会和canal server也断开链接.  解决：发现running节点消失，先执行以下本地关闭socket链接的操作，网络抖动时引起的避免socket泄漏.  
175,file-instance.xml配置模式下，设置的table过滤不起作用？ 1. 在example instance中配置    `# table regex    canal.instance.filter.regex = canal\\.ad.*`  debug调试发现变量确实被设置进了instance实例的eventParser.eventFilter里面了 2. 创建client进行数据请求 3. 修改数据库canal下面的advice表，client可以收到对应的message. 4. 修改数据库test下面的advice表，client同样可以收到对应的message.    注: 两个数据库都是创建在本地的 1. 确认设置了row模式 2. filter条件只针对row数据有效，其余的query查询不会进行过滤 我在对项目本身的 deployer.CanalLuncher进行debug的过程中，发现一个问题，在启动canal server附属的EventParser服务时，关于table过滤的表达式还是正确的，但是在处理过滤业务的线程中会发现表达式pattern被改变了，下面是我的调试过程，麻烦请看一下 1. 初始EventParser并启动    ![qq 20160509143520](https://cloud.githubusercontent.com/assets/5526657/15105668/0aa5d130-15f6-11e6-8007-b427efec5baf.png) 2. 构造Erosa连接之前    ![qq 20160509143526](https://cloud.githubusercontent.com/assets/5526657/15105742/ce1a585c-15f6-11e6-8407-a71284ce097c.png) 3. 构造Erosa连接之后    ![qq 20160509145114](https://cloud.githubusercontent.com/assets/5526657/15105758/dbd62980-15f6-11e6-8335-25b6dea0f4f2.png) client的subscribe传入的过滤条件优先级最高，会忽略掉服务端的设置 @agapple 但是我想用服务器端的设置，如果我不写subscribe在程序中就报错，写了就把server端的regex覆盖，这可如何是好啊，我就是想用instance中的内容 那你client的subscribe就传一个空值就好
174,Canal 选用group-instance 模式的话，怎么配置destination name呢？ 怎么配置逻辑上的把多个instance融合后的总得destination name吗？ 看一下group-instance.xml配置示例 项目给的example中是配置file-instance.xml做的示例，麻烦可以给下group-instance.xml配置示例的地址吗？ https://raw.githubusercontent.com/alibaba/canal/master/deployer/src/main/resources/spring/group-instance.xml 
173,Canal客户端高可用bug修复 Request原因请参见issue https://github.com/alibaba/canal/issues/171  [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=173) <br/>Thank you for your submission  we really appreciate it. Like many open source projects  we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=173) before we can accept your contribution.<br/> https://github.com/alibaba/canal/issues/171 tks   后面我调整了下部分代码，主要是SimpleCanalConnector 
172,rds配置了主从库，主从库切换时候出现异常。 异常报错如下： 16-04-22 07:11:24.236 [destination = crm_csc_03   address = rdst58y130m2pdn0th48.mysql.rds.aliyuncs.com/10.248.4.8:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:crm_csc_03[java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Could not find first log file name in binary log index file     at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95)     at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113)     at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209)     at java.lang.Thread.run(Thread.java:745) ] 主从库切换从库上没有找到binlog fetch时报错。 我看canal监听可以配置主从库地址。这个方法可以解决该问题吗？ 像这种问题有没有好的解决方案？ 错误的位点，需要按照时间戳进行订阅 大哥，Could not find first log file name in binary log index file 不是错误的位点吧！！！这是fetch时候bingo文件丢失了。重启错误依旧，只能跳过。这bingo丢失应该是主从库切换引起的。你们有没有遇到过这种问题。 问题是错误的位点没错，但是canal一直在运行。这是运行中的异常。设置时间戳可以跳过这段。但会引起数据丢失。我是想知道。这种错误应该怎么样去避免。instance.properties配置standby可以避免这种问题发生吗？或者有什么好的解决方案。 rds的vip模式，canal无法感知主备切换的操作，用之前主库的binlog位点去备库(新的主库)去找，当然会找不到 目前只能做好监控 当发现主备切换 及时在binlog被删除前 做到按时间重置 我后续版本会考虑下，可以提供一个思路，如果同学有兴趣可以按照实现之.  1. 订阅binlog之后，获取一下binlog事件的mysql server id，作为position的一部分记录到位点中 2. 节点发生binlog位点不可用时，通过新建链接判断一下mysql server id是否有所不同，如果不同则自动按照时间戳进行订阅  查询mysql的server_id的办法: ``` show variables like 'server_id'; ``` 这个修改建议非常好，不过按此修改还是会出现一些问题。rds在有些时候binlog也会被清除。如rds规格升级等，我总结了一些规律当出现IOException的时候错误基本不可恢复。企业应用很多情况。希望跳过错误继续后面的操作，并且告知相关维护人员，我现在修改的方案是： 1、当出现IOException时retry指定次数。超过次数时获取当前最新的位点，重新连接。 2、错误日志输出和报警输出(建议以后可以实现邮件预警) 3、ExceptionRetryCount可在配置文件配置。不配置的话。按以前的方式处理。 相关代码我已经修改并且已经测试通过应用到了生产环境，如果版主觉得我思路正确，我可以提交相关代码。 建议提交下代码，一起看下代码，希望canal未来能更好的支持这一类vip模式的主备集群，你和我的两个方法可以结合处理 我现在也遇到这个问题了，我们用的亚马逊的RDS，对mysql的flush logs的权限做了限制 我尝试了清空zookeeper里面的信息并重启canal，又尝试了instance.properties里面设置当前时间戳都不行，请问还有什么方式手动修复吗？ 哎 知道了问题 RDS默认只保留10分钟的binlog，如果canal中断了10分钟以上就再也起不来了。。。 希望能提供个简单的命令或者配置文件重置为最新位点，每次手工操作好麻烦 我修改了一些代码。当出现这种错误一定的次数。自动重置最新的位点。代码提交了。看看阿里的哥们会不会采用了。 PR代码已合并，会适当再精确识别一下异常内容，判断需要进行主备切换，尽可能保证不丢数据 貌似mysql GTID 有这个功能 
171,客户端高可用存在bug 问题场景 1，canal有两台服务器c1和c2，c1处于激活状态 2，客户端使用ClusterCanalConnector进行消费，也是两台服务器d1和d2，d1处于激活状态 3，c1发生宕机，此时d1会执行【ClusterCanalConnector】的restart()方法。restart中有三步： 第一步，disconnect，会释放running节点； 第二步，线程休眠5秒钟； 第三步，尝试重连，重连的时候会执行initRunning()方法。 4，在d1释放running节点后，d2会被立即触发，执行initRunning()方法，initRunning中也有两大步： 第一步，抢占running节点 第二步，执行processActiveEnter()方法，该方法肯定会报错，因为此时d2连接的canal也是c1，执行initRunning的zk线程会异常退出，此时d2的mutex变量仍然为false，并且没有释放running节点 5，d1后续的重连都没意义了，因为d2没有释放running节点，所以d1的mutext变成了false 6，最终结果是d1和d2都阻塞了 7，此时关闭d2，d1的initRunning方法会被触发，但d1的processActiveEnter方法仍然会报错，d1仍然无法恢复消费 1. 确认一下canal的版本 2. 你的第4步，d2启动链接的canal是c1是因为时效性问题么？d2未能及时发现c2已经是主？ 如果可以明确重现，是否有修复的代码？ canal版本是1.0.20 很容易重现的，出现上述现象的原因在于【ClientRunningMonitor】的【initRunning方法】没有对【processActiveEnter()】抛出的异常做处理，把Zk上的running节点占住了没有释放，导致两个客户端的mutex都为false，都没有机会执行了。 修复代码正在写，涉及到高可用的代码比较绕，还没写好，您可以看看有没有简便的修改方法。 还有就是SimpleCanalConnector类中的doConnect方法 private InetSocketAddress doConnect() throws CanalClientException {         try {             channel = SocketChannel.open();             channel.socket().setSoTimeout(soTimeout);             channel.connect(address); channel.connect(address);//此处的address应该即时取一次，否则D2永远没机会拿到最新的canal-server的ip 你的第4步，d2启动链接的canal是c1是因为时效性问题么？d2未能及时发现c2已经是主？ 是的，没能发现，原因就在于这段代码 channel.connect(address);//此处的address应该即时取一次，否则D2永远没机会拿到最新的canal-server的ip 看了下代码，如果第4步，d2启动链接时拿到的是c1，会在initRunning执行doConnect时执行失败，原本期望是通过客户端发起restart操作来处理，而d2因为是热备，可能是在异步initRunning回调时报错，无法触发client端的restart而导致长时间挂起 想了下一种比较简单的改法 1. initRunning针对执行异常时，主动释放running节点，通过异步回调来进行重试 2. SimpleCanalConnector的doConnect方法动态获取address地址 是的，就是这么改的。 机器上没装git客户端 改造的代码发你邮箱了，jianghang115@gmail.com 抽时间看下吧 
170,canal怎么和RocketMQ集成？ 因为要实现多客户端订阅的效果，貌似canal本身是不支持的。 所以考虑和RocketMQ结合使用。  能否不需要canal-client，直接canal-server中消息直接推到MQ中？ 是不是要定制canal-server才可以？ 定制的工作量大吗？ 使用canal client获取到消息后写入Rocketmq 我现在就是写了一个中间应用，使用 Redis 订阅一个 instance，然后在 Redis 里面多次消费。
169,canal-server和client能否1对多关系 启动一个canal-server和两个client，结果只有第一个启动的可以收到消息，是什么原因？ client只能一个工作，另一个是热备 
168,reset master后出现的binlog寻找不到的问题 pid:2 nid:2 exception:canal:shanghai_canal:java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) at java.lang.Thread.run(Thread.java:745) 通过mysql日志发现canal依然请求原始的binlog，请问这个问题是否有处理机制？谢谢 @ch-lgs  请问下是如何处理的呢 RESET MASTER :删除所有的二进制日志，并重新创建一个新的二进制日志 原始binlog被删除了，canal也就无法消费了 所以会报错。解决办法是重设一个消费点位   该点位之前的数据丢失。
167,canal server如何自适应数据库HA？ 当主db挂了切换到备机，这个时候canal server能自动连接备机获取binlog吗？offset怎么设置呢？ 多看下wiki
166,启动服务成功，监控不到数据库的改变 2016-04-15 15:45:39.563 [destination = image   address = /127.0.0.1:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just show master status 2016-04-15 15:45:39.564 [destination = image   address = /127.0.0.1:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /127.0.0.1:3306 has an error  retrying. caused by  com.alibaba.otter.canal.parse.exception.CanalParseException: command : 'show master status' has an error! Caused by: java.io.IOException: ErrorPacket [errorNumber=1227  fieldCount=-1  message=Access denied; you need (at least one of) the SUPER  REPLICATION CLIENT privilege(s) for this operation  sqlState=42000  sqlStateMarker=#]  with command: show master status         at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.query(MysqlQueryExecutor.java:60) ~[canal.parse.driver-1.0.21.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:69) ~[canal.parse-1.0.21.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findEndPosition(MysqlEventParser.java:548) ~[canal.parse-1.0.21.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPositionInternal(MysqlEventParser.java:352) ~[canal.parse-1.0.21.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPosition(MysqlEventParser.java:312) ~[canal.parse-1.0.21.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:162) ~[canal.parse-1.0.21.jar:na]         at java.lang.Thread.run(Thread.java:745) [na:1.7.0_79] 执行show master status的时候权限不足吧 低级问题，见1楼回复 查看过权限都是给的777不是这个问题 账号权限，不是文件权限 都给的最大权限 是zookeeper缓存节点没有清空导致的 
165,canal + zookeeper 不能正确找到点位 zookeeper启动正常 > canal配置` - canal.instance.global.spring.xml = classpath:spring/default-instance.xml > example/instance.properties canal.instance.master.address = 172.16.20.120:3306 canal.instance.master.journal.name = canal.instance.master.position = canal.instance.master.timestamp = > 使用除classpath:spring/default-instance.xml以外都能正常捕捉数据库改动，一旦换成classpath:spring/default-instance.xml就不能正常获取 1.  2. \- 2016-04-14 15:41:06.202 [Thread-4] INFO  c.a.otter.canal.instance.spring.CanalInstanceWithSpring - stop CannalInstance for 1-example  3. \- 2016-04-14 15:41:06.204 [Thread-4] INFO  c.a.otter.canal.instance.spring.CanalInstanceWithSpring - stop successful.... 4. \- [root@Note_140_canal ~]# tailf -n 300 /usr/local/canal.deployer-1.0.21/logs/example/example.log  5. \- 2016-04-14 15:41:34.578 [main] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 6. \- 2016-04-14 15:41:34.588 [main] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties] 7. \- 2016-04-14 15:41:34.645 [main] WARN  org.springframework.beans.TypeConverterDelegate - PropertyEditor [com.sun.beans.editors.EnumEditor] found through deprecated global PropertyEditorManager fallback - consider using a more isolated form of registration  e.g. on the BeanWrapper/BeanFactory! 8. \- 2016-04-14 15:41:34.759 [main] INFO  c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-example  9. \- 2016-04-14 15:41:34.788 [main] INFO  c.a.otter.canal.instance.spring.CanalInstanceWithSpring - subscribe filter change to ._.._ 10. \- 2016-04-14 15:41:34.789 [main] INFO  c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start successful.... 11. \- 2016-04-14 15:41:34.832 [destination = example   address = /172.16.20.120:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position by switch ::1460100169000 12. \- 2016-04-14 15:41:34.970 [destination = example   address = /172.16.20.120:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - Didn't find the corresponding binlog files from mysql-bin.000001 to mysql-bin.000004 13. \- 2016-04-14 15:41:34.979 [destination = example   address = /172.16.20.120:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /172.16.20.120:3306 has an error  retrying. caused by  14. \- com.alibaba.otter.canal.parse.exception.CanalParseException: can't find start position for example 15. \- 2016-04-14 15:41:34.983 [destination = example   address = /172.16.20.120:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: can't find start position for example 16. \- ] 17.  18. ``` 通过 时间戳 1460100169000。找不到 对应的 binlog 日志文件 现在已经解决了，压缩包传到linux被损坏了 
164,监听实例正则过滤的问题 有这样一个特殊的数据库，同样的table分布在了多个schema里（分库了）。我想监听所有以`order`开头，下划线加数字结尾的库名，如`order_1` `order_2` `...` `order_11`等等，**因为自动扩容这个数字累加， 同时又存在多个无关的schema，没法一一列举**。 试着写了如下的正则过滤配置，启动server后并未生效，也没有抛错，像类似的配置怎么写？ **`canal.instance.filter.regex=order_/(\d+)/\\..*`** 不知道这样的需求正则能否满足？感谢！！ 通过反复调试，这样就可以了。 `order_\\d+\\..*`   
163,CanalServer 是否存在性能瓶颈 canal上部署了51个实例，client（消费所有的实例）与canal部署在同一台服务器上，并且服务器上无其他服务。 ### client实现 - 初始化连接 ```         // 根据ip，直接创建链接，无HA的功能         CanalConnector connector = CanalConnectors.newSingleConnector(                 new InetSocketAddress(hostIp  port)  destination.generateDestination()  ""  ""); ``` - 获取数据 ```                     long t1 = System.currentTimeMillis();                     Message message = connector.getWithoutAck(10240  1000L  TimeUnit.MILLISECONDS);   //以毫秒为单位                     long t2 = System.currentTimeMillis();                     long batchId = message.getId();                     logger.info("read batchId=[{}]  time=[{}]ms"  batchId  (t2 -t1)); ``` ### 问题 发现部分实例执行getWithoutAck方法的耗时大于设置值（1000ms），大概在2000ms，数据量大时耗时也随之放大。 下面是client一次请求的分析： - client发送请求（t1） - server收到请求（t2） - embeddedServer.getWithoutAck()完成，并向client返回数据（t3=10:40:34） - client收到数据（t4=10:40:37） 根据日志可知，t4－t1＝5000ms，而t4－t3＝3000ms，而getWithoutAck耗时为1000ms，因此可以推断出t2－t1=5000－3000－1000=1000ms。 **因此，是否可以说明client与CanalServer之间的通信存在耗时？但是这个耗时在一般情况下是忽略不计的！** 如果是CanalServer 的性能瓶颈，能否优化？ 服务器24核，内存64g 优先看下canal server的jvm内存和GC状态.   a.  一次性取10MB的数据不是一种明智的做法 b. 一个server部署过多的binlog解析队列，也不明智，需要平衡jvm内存和业务' 一般我们内部使用，8个队列，4M+100ms的获取 一个canal上部署8个实例，有集群版？ 一个server本身就是8个消费队列(instance) 
162,支持下IDE启动CanalLauncher RT. 主要解决下conf目录文件的查找  规避掉scan扫描失败触发了stop机制 
161,parse row data failed 按照quick start 的步骤，使用了一下，启动 startup.sh后，观察 example.log 不停地会有如下报错： 2016-04-01 15:01:37.068 [destination = example   address = /10.0.2.40:3550   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: java.lang.IllegalArgumentException: bad format  x exceed: 1887436800  999999999     at com.taobao.tddl.dbsync.binlog.LogBuffer.getDecimal0(LogBuffer.java:1355)     at com.taobao.tddl.dbsync.binlog.LogBuffer.getDecimal(LogBuffer.java:1265)     at com.taobao.tddl.dbsync.binlog.event.RowsLogBuffer.fetchValue(RowsLogBuffer.java:339)     at com.taobao.tddl.dbsync.binlog.event.RowsLogBuffer.nextValue(RowsLogBuffer.java:96)     at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseOneRow(LogEventConvert.java:462) canal版本和mysql版本报告一下 mysql 版本： /home/rong/mysql/bin/mysql  Ver 14.14 Distrib 5.6.26  for Linux (x86_64) using  EditLine wrapper canal 就是最新的1.0.21 可以下载mysql binlog到本地，然后使用FileLogFetcherTest进行本地解析测试 好的，我试试看，多谢 估计是这个问题：https://github.com/alibaba/canal/issues/119 使用当前master分支build一个再试试 这个问题怎么解决呀？我的MySQL版本是5.6.26 类似的问题修复过一次，使用当前master分支，本地打包一个版本测试一下，如有问题请再反馈 
160,取不到ROWDATA数据 从应用日志上看取到了EntryType为 TRANSACTIONBEGIN和TRANSACTIONEND 数据 没有取得ROWDATA。请问一下这是什么原因 Canal版本 1.0.21 mysq 版本   PXC架构 5.6.20 自己回答一下，是数据库端口配置的问题 数据库是pxc，起了两个实例3306 3307. 奇怪的是连接了错误的端口，没有任何报错 你确定下你的mysql配置中binlog-format=ROW吗？ 发自我的 iPhone > 在 2016年3月31日，17:22，Song Chang notifications@github.com 写道： >  > 从应用日志上看取到了EntryType为 TRANSACTIONBEGIN和TRANSACTIONEND 数据 > 没有取得ROWDATA。请问一下这是什么原因 > Canal版本 1.0.21 > mysq 版本 PXC架构 5.6.20 >  > — > You are receiving this because you are subscribed to this thread. > Reply to this email directly or view it on GitHub canal.instance.binlog.format = ROW STATEMENT MIXED  默认的，没有改过。 原因已经查出来了，见上面的回答，主要奇怪的是canal没有任何报错.各个日志都正常 canal会默认解析所有的binlog，你这里开启了所有日志格式，当然不会报错了 我用的canal 1.0.25-SNAPSHOT对应MySQL 5.6、5.7也会出现这个问题。 端口没改，配置也是采用默认的支持所有格式 canal.instance.binlog.format = ROW STATEMENT MIXED 默认解析所有binlog也不会导致rowdata解析不了吧？ 数据库采用的是row 经过验证，filter过滤掉的库、表信息，rowdata信息收不到了，但是对应的Transaction Begin、Transaction End还是有的。 同样的问题 `connector.subscribe("actionlog\\d{8}");`我想订阅所有actionlog打头接数字的表，结果没有ROWDATA了，是正则错了，还是哪里错了？？？ 你现在本地用canal的PattenUtils跑一下testcase
159,ack 能实现对某一个 Message 中的某个 Entry 提交吗 `com.alibaba.otter.canal.client.CanalConnector.get` 方法中有 batchSize 参数。当我设置成 100 时，假如我在处理某些 `entry` 出错，然后调用 `connector.rollback(batchId)`，这就会导致此次的所有 `entry` 回滚吗？那是否会导致接下来出现重复处理相同 `entry` 的问题 ？ 我想问下 canal 有提供 "ack 提交某个 Message Entry" 的方案吗？ 我现在的思路是在记录 "库名+表名+主键" 做唯一判断，防止 rollback 后重复处理 没有单条ack的功能，get的时候设置batchSize小一点.   ps.  主要是性能考虑 那怎么处理重复呢？batchSize=1？ 需要业务上设计为可幂等执行，软件层面没法避免不重复 
158,为什么在CanalLauncher 中运行报错 java.io.IOException: connect /127.0.0.1:3306 failure:java.nio.channels.ClosedChannelException 而打包后运行startup.sh却不会呢？很奇怪 debug代码，和你自己环境有太多相关性 遇到同样问题，不知道@shuhuiguo   你是怎么解决的 没有，应该是线程模型有问题 
157,枚举类型的字段值怎么以字符串形式读取 当 mysql 字段类型为 enum 时，`column.getValue()` 返回的是 int 值，我想问下能否方便地获取和 mysql 中一样的 enum 字符串值呢？ binlog协议里数据包就是int类型，可以拿到数据库表结构定义，将int转为对应的enum值 @agapple canal 有提供获取数据库表结构定义的方法吗？我查了下 wiki 没有找到。 entry协议里有个mysql type属性 
156,Could not find first log file name in binary log index file  报错信息： `2016-03-25 17:56:48.605 [destination = mysql_56688   address = /127.0.0.1:56688   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position /data/mysql_data/data/mysql-bin.000001:4: 2016-03-25 17:56:48.607 [destination = mysql_56688   address = /127.0.0.1:56688   EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Could not find first log file name in binary log index file         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) ~[canal.parse-1.0.21.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) [canal.parse-1.0.21.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) [canal.parse-1.0.21.jar:na]         at java.lang.Thread.run(Thread.java:745) [na:1.7.0_79] 2016-03-25 17:56:48.607 [destination = mysql_56688   address = /127.0.0.1:56688   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /127.0.0.1:56688 has an error  retrying. caused by  java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Could not find first log file name in binary log index file         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) ~[canal.parse-1.0.21.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) ~[canal.parse-1.0.21.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) ~[canal.parse-1.0.21.jar:na]         at java.lang.Thread.run(Thread.java:745) [na:1.7.0_79] 2016-03-25 17:56:48.607 [destination = mysql_56688   address = /127.0.0.1:56688   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:mysql_56688[java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Could not find first log file name in binary log index file         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209)         at java.lang.Thread.run(Thread.java:745) ` MySQL:  5.6.25 canal:  1.0.21 instance.properties: canal.instance.mysql.slaveId = 3 canal.instance.master.address = 127.0.0.1:56688 canal.instance.master.journal.name = /data/mysql_data/data/mysql-bin.000001 canal.instance.master.position = 4 首先MySQL配置和文件权限都检查过了，没啥问题，这个是因为bin log目录不在常规目录引起的吗？ Could not find first log file name in binary log index file at.  估计手工清理过binlog日志，没重置mysql binlog index信息，google一大把这问题解决方案 reset master也不行，另外，如果把不指定binlog位置，instance thread会一直卡在'show master status' canal会自行做重试，清理下canal记录的历史位点信息 如何清理 下canal记录的历史位点信息？ 删除什么文件还是重启zookeeper? 删除zk节点或者meta.dat，然后重启instance otter-manager里node节点的删除链接是文字不可删除，要怎样步骤才能删除呢？ 在同步进度里把Position 状态删除，即可恢复 canal每次将解析过journalname存储到zk上面，第二次parse数据的时候会从zk去拿一次journalname，如果找不到则从master去找first index file。如果数据库管理员手动清理过日志或者磁盘过满自动清理日志都会出现这样问题，解决办法就是删除对应name的节点 一般是建议使用mysql命令行进行删除，就会同步修改index文件 @agapple 你好 我删除了instance的meta.dat 重启canal 还是爆这个错 请问还有别的解决办法吗 
155,Time Out 是版本不兼容的问题吗 hi， 我使用了server－V1.0.22最新版、client－V1.0.21 仓库取的依赖jar出现了莫名的超时问题，我的zookeeper其他应用支持的很好`zkServer=cdh3:2181 cdh1:2181 cdh5:2181` 。 **使用zookeeper调度出现超时现象，clinet一直联不通**  用Simple方式就没有问题，超时抛的错误如下。 `[2016-03-24 19:58:27] ERROR [ZkClient-EventThread-13-cdh3:2181 cdh1:2181 cdh5:2181] ZkEventThread - Error handling event ZkEvent[Data of /otter/canal/destinations/user/1001/running changed sent to com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor$1@45681b65] com.alibaba.otter.canal.protocol.exception.CanalClientException: java.net.ConnectException: Operation timed out     at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.doConnect(SimpleCanalConnector.java:167)     at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.access$000(SimpleCanalConnector.java:48)     at com.alibaba.otter.canal.client.impl.SimpleCanalConnector$1.processActiveEnter(SimpleCanalConnector.java:392)     at com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor.processActiveEnter(ClientRunningMonitor.java:188)     at com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor.initRunning(ClientRunningMonitor.java:110)     at com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor$1.handleDataDeleted(ClientRunningMonitor.java:67)     at org.I0Itec.zkclient.ZkClient$6.run(ZkClient.java:549)     at org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71) Caused by: java.net.ConnectException: Operation timed out     at sun.nio.ch.Net.connect0(Native Method)     at sun.nio.ch.Net.connect(Net.java:458)     at sun.nio.ch.Net.connect(Net.java:450)     at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648)     at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.doConnect(SimpleCanalConnector.java:128)     ... 7 more`  debug到BooleanMutex这个类 这个get没法同步导致超时的，具体原因没有找到： ` public void get() throws InterruptedException {         this.sync.innerGet();     } ` 后来把server换成V1.0.21还是不行。 是不是在zk里保留了版本不兼容的信息。 是21client不能配合22的server吗？ 谢谢。 这个错误怎么解读？ ERROR  ZkEventThread - Error handling event ZkEvent[Data of /otter/canal/destinations/user/1001/running changed sent to com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor$1@5789b4d3] zk Node [/otter/canal/destinations/user/1001/running ] 是: {"active":true "address":"10.1.193.0" "clientId":1001} 1.  java.net.ConnectException: Operation timed out at ，是连不上server或者链接超时 2.  Error handling event 异常是因为timeout引起的，先解决1的问题 谢谢，因为server网络环境下有两个IP地址 ， 如果不设置IP，默认的在client是无法连接导致的。 Server指定了IP后，Client端还是抛了类似的错误。 ` ClientRunningMonitor - canal is running in [10.1.193.0]   but not in [192.168.56.1:64327] ` client端也是两个IP，没有找到配置，不知道是否可以指定client端的IP！（10段的IP不通，都默认了，192段的IP是一个局域网的） client没有指定ip一说，这是AddressUtils.getHostIp()返回的信息和建立socket的ip不一致 先简单弄个单ip的客户端机器吧 
154,canal binlog解析报错 报错日志： Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: java.lang.IllegalArgumentException: limit excceed: 367     at com.taobao.tddl.dbsync.binlog.LogBuffer.getFullString(LogBuffer.java:1122) ~[canal.parse.dbsync-1.0.21.jar:na]     at com.taobao.tddl.dbsync.binlog.event.RowsLogBuffer.fetchValue(RowsLogBuffer.java:920) ~[canal.parse.dbsync-1.0.21.jar:na]     at com.taobao.tddl.dbsync.binlog.event.RowsLogBuffer.nextValue(RowsLogBuffer.java:96) ~[canal.parse.dbsync-1.0.21.jar:na]     at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseOneRow(LogEventConvert.java:462) ~[canal.parse-1.0.21.jar:na]     at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:378) ~[canal.parse-1.0.21.jar:na]     at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:109) ~[canal.parse-1.0.21.jar:na]     at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:62) ~[canal.parse-1.0.21.jar:na]     at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:323) ~[canal.parse-1.0.21.jar:na]     at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:176) ~[canal.parse-1.0.21.jar:na]     at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:121) ~[canal.parse-1.0.21.jar:na]     at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) ~[canal.parse-1.0.21.jar:na] mysql 5.6.27  1. 是否可以重现？比如什么样的sql可以造成这样的问题？ 2. 是否可以提供binlog文件 [root@69_114 mysql]# mysqlbinlog --no-defaults --base64-output=decode-rows -v --start-position=214215706  /data2/logs/mysql/mysql-bin.000105  |more /_!50530 SET @@SESSION.PSEUDO_SLAVE_MODE=1_/; /_!40019 SET @@session.max_insert_delayed_threads=0_/; /_!50003 SET @OLD_COMPLETION_TYPE=@@COMPLETION_TYPE COMPLETION_TYPE=0_/; DELIMITER /_!_/; # at 214215706 # at 214216435 #160324 14:16:33 server id 6915  end_log_pos 214216533  Table_map: `crm`.`t_product` mapped to number 78 # at 214216533 #160324 14:16:33 server id 6915  end_log_pos 214216647  Update_rows: table id 78 flags: STMT_END_F ### UPDATE `crm`.`t_product` ### WHERE ### @1=598342 ### SET ### @3='全车抛光' ### @4=30000 ### @5=30000 ### @6=38800 ### @7='洗车+去污+抛光' ### @11='1' ### @12='9' ### @15='5' ### @18='2016-03-24 14:16:33' ### @19='18699926655' # at 214216647 # at 214217458 #160324 14:16:33 server id 6915  end_log_pos 214217547  Table_map: `crm`.`t_sys_log` mapped to number 82 # at 214217547 #160324 14:16:33 server id 6915  end_log_pos 214218065  Write_rows: table id 82 flags: STMT_END_F ### INSERT INTO `crm`.`t_sys_log` ### SET ### @1=908095 ### @2='1458800193776' ### @3='DML日志' ### @4='1001' ### @5='修改' ### @6='modify' ### @7='商品[全车抛光]' ### @8='598342' ### @9='' ### @10='{"clearAmt":30000 "dataFlag":"" "firstCategory":"1" "marketAmt":38800 "myself":false "pageNum":1 "pageSize":10 "product4SVo":{"carTypeName":""} "productDesc":"洗车+去污+抛光" "productId":"598342" "p roductName":"全车抛光" "productStatus":"5" "saleAmt":30000 "secondCategory":"9" "storeId":"33813" "sysLogList":[] "updateUser":"18699926655"}' ### @11='操作成功' ### @12=18 ### @13='2016-03-24 14:16:33' ### @14='18699926655' ### @15='2016-03-24 14:16:33' ### @16='18699926655' [EntryPosition[included=false journalName=mysql-bin.000105 position=214215706 timestamp=1458800193000]] com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: java.lang.IllegalArgumentException: limit excceed: 3332     at com.taobao.tddl.dbsync.binlog.LogBuffer.getFullString(LogBuffer.java:1122) ~[canal.parse.dbsync-1.0.20.jar:na]     at com.taobao.tddl.dbsync.binlog.event.RowsLogBuffer.fetchValue(RowsLogBuffer.java:917) ~[canal.parse.dbsync-1.0.20.jar:na]     at com.taobao.tddl.dbsync.binlog.event.RowsLogBuffer.nextValue(RowsLogBuffer.java:96) ~[canal.parse.dbsync-1.0.20.jar:na] canal 1.0.20 my.cnf   binlog-row-image=minimal 改为full 正常了！ 印象中canal1.0.21才支持minimal模式，提供binlog的原始二进制文件而不是解析后的文本 canal1.0.21 canal1.0.22-sanpshot 都不试过 不行！ 自己做下代码debug，或者提供复现的办法，或者提供原始的binlog二进制文件，不然我也帮不了 自己做下代码debug，或者提供复现的办法，或者提供原始的binlog二进制文件，不然我也帮不了 2016-03-30 10:55:31.825 [destination = example   address = /127.0.0.1:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - ERROR ## parse this event has an error   last position : [EntryPosition[included=false journalName=mysql-bin.000052 position=35751033 timestamp=1459305662000]] com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: java.lang.IllegalArgumentException: limit excceed: 3332         at com.taobao.tddl.dbsync.binlog.LogBuffer.getFullString(LogBuffer.java:1122)  
153,canal无法关闭的问题 当开启了canal.auto.scan功能，并且实际执行过动态增减instance的操作后。 执行sh stop.sh，无法关闭canal服务器，一直处于卡死等待状态，只能通过kill结束进程 执行stop之后，执行一下jstack获取一下线程堆栈 基于当前最新版暂无发现stop失败的问题 
152,canal是否和MQ消息一样只能保证最少一次，不能保证只有一次。 在数据库主备切换，canal崩溃重启等情况，canal是否会重复发送消息？出现相同的insert语句。 涉及太多网络因素 没法保证. 目前所谓的高性能MQ也保证不了不重复，顶多是基于一些存储做了一些去重处理，要理解网络的不可确定性 除了网络因素，是否有情况一定会出现重复消息，例如kill -9 canal pid 然后重启。如果有必然出现重复的情况，可以方便进行一些测试。 kill -9就是一种网络不确定因素，我数据发给你了，你还没返回ack给我，我只能下次再发给你 
151,jdk版本 jdk使用什么版本好 我们之前都是用1.6.25 otter添加数据表的时候如何匹配全库所有的表 先把wiki和视频都看一遍 
150,Merge pull request #1 from alibaba/master Merge pull request [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=150) <br/>You should sign our Contributor License Agreement in order to get your pull request merged.<br/> 
149,关于BINLOG_SEND_ANNOTATE_ROWS_EVENT 在mysql官方文档中，基于COM_BINLOG_DUMP协议的dump请求中flags只看到BINLOG_DUMP_NON_BLOCK这一种，请问BINLOG_SEND_ANNOTATE_ROWS_EVENT代表什么意思？在官方文档的哪里可以找到相关说明？ 搜索： mysql binlog protocol 
148,canal目前支持oracle吗？ canal目前支持oracle吗？刚看了一下代码，貌似还不支持吧？ [https://github.com/alibaba/canal/blob/master/instance/manager/src/main/java/com/alibaba/otter/canal/instance/manager/CanalInstanceWithManager.java#L295](https://github.com/alibaba/canal/blob/master/instance/manager/src/main/java/com/alibaba/otter/canal/instance/manager/CanalInstanceWithManager.java#L295) 是不是意味着otter也只支持mysql，读和写都是？ 可以看下wiki，都有明确描述，目前不支持oracle，对于oracle有需求可以参考我的另一个项目：https://github.com/alibaba/yugong Yugong 支持oracle到mysql ，反过来从mysql 到oracle也支持吗 yugong开源不支持mysql->oracle 请问oracle版的canal就是yugong么？还是有另一个基于ogg实现的版本？ https://github.com/alibaba/yugong，yugong是基于oracle物化视图来做的 
146,Canal启动成功，但是查看instance发现错误 系统 CentOS6.5 我的 /etc/my.cnf 配置： _[mysqld] datadir=/var/lib/mysql socket=/var/lib/mysql/mysql.sock log-bin=mysql-bin binlog-format=ROW server_id=1 # Disabling symbolic-links is recommended to prevent assorted security risks symbolic-links=0 [mysqld_safe] log-error=/var/log/mysqld.log pid-file=/var/run/mysqld/mysqld.pid_ instance.properties 配置： ``` $ cat conf/example/instance.properties  ################################################# ## mysql serverId canal.instance.mysql.slaveId = 1234 # position info canal.instance.master.address = 127.0.0.1:3306 canal.instance.master.journal.name =  canal.instance.master.position =  canal.instance.master.timestamp =  #canal.instance.standby.address =  #canal.instance.standby.journal.name = #canal.instance.standby.position =  #canal.instance.standby.timestamp =  # username/password canal.instance.dbUsername = canal canal.instance.dbPassword = canal canal.instance.defaultDatabaseName = canal.instance.connectionCharset = UTF-8 # table regex canal.instance.filter.regex = .*\\..* # table black regex canal.instance.filter.black.regex =   ################################################# ``` canal.log 信息： ``` Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=96m; support was removed in 8.0 Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=256m; support was removed in 8.0 Java HotSpot(TM) 64-Bit Server VM warning: UseCMSCompactAtFullCollection is deprecated and will likely be removed in a future release. 2016-03-04 23:01:39.529 [main] INFO  com.alibaba.otter.canal.deployer.CanalLauncher - ## start the canal server. 2016-03-04 23:01:39.735 [main] WARN  com.alibaba.otter.canal.common.utils.AddressUtils - Failed to retriving local host ip address  try scan network card ip address. cause: scidb-work601: scidb-work601: unknown error 2016-03-04 23:01:39.743 [main] INFO  com.alibaba.otter.canal.deployer.CanalController - ## start the canal server[192.168.100.113:11111] 2016-03-04 23:01:41.046 [main] INFO  com.alibaba.otter.canal.deployer.CanalLauncher - ## the canal server is running now ...... ``` example.log信息： ``` 2016-03-04 23:01:40.378 [main] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2016-03-04 23:01:40.414 [main] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties] 2016-03-04 23:01:40.588 [main] WARN  org.springframework.beans.TypeConverterDelegate - PropertyEditor [com.sun.beans.editors.EnumEditor] found through deprecated global PropertyEditorManager fallback - consider using a more isolated form of registration  e.g. on the BeanWrapper/BeanFactory! 2016-03-04 23:01:40.840 [main] INFO  c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-example 2016-03-04 23:01:40.884 [main] INFO  c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start successful.... 2016-03-04 23:01:41.147 [destination = example   address = /127.0.0.1:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /127.0.0.1:3306 has an error  retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: connect /127.0.0.1:3306 failure:java.io.IOException: Error When doing Client Authentication:ErrorPacket [errorNumber=1045  fieldCount=-1  message=Access denied for user 'canal'@'localhost' (using password: YES)  sqlState=28000  sqlStateMarker=#]         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:174)         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:68)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:52)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:84)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:158)         at java.lang.Thread.run(Thread.java:745) Caused by: java.io.IOException: connect /127.0.0.1:3306 failure:java.io.IOException: Error When doing Client Authentication:ErrorPacket [errorNumber=1045  fieldCount=-1  message=Access denied for user 'canal'@'localhost' (using password: YES)  sqlState=28000  sqlStateMarker=#]         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:174)         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:68)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:52)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:84)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:158)         at java.lang.Thread.run(Thread.java:745)         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:71) ~[canal.parse.driver-1.0.21.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:52) ~[canal.parse-1.0.21.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:84) ~[canal.parse-1.0.21.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:158) ~[canal.parse-1.0.21.jar:na]         at java.lang.Thread.run(Thread.java:745) [na:1.8.0_73] 2016-03-04 23:01:41.150 [destination = example   address = /127.0.0.1:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: connect /127.0.0.1:3306 failure:java.io.IOException: Error When doing Client Authentication:ErrorPacket [errorNumber=1045  fieldCount=-1  message=Access denied for user 'canal'@'localhost' (using password: YES)  sqlState=28000  sqlStateMarker=#]         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:174)         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:68)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:52)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:84)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:158)         at java.lang.Thread.run(Thread.java:745) Caused by: java.io.IOException: connect /127.0.0.1:3306 failure:java.io.IOException: Error When doing Client Authentication:ErrorPacket [errorNumber=1045  fieldCount=-1  message=Access denied for user 'canal'@'localhost' (using password: YES)  sqlState=28000  sqlStateMarker=#]         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:174)         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:68)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:52)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:84)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:158)         at java.lang.Thread.run(Thread.java:745)         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:71)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:52)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:84)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:158)         at java.lang.Thread.run(Thread.java:745) ``` MySQL 的用户权限查看： ``` mysql> SELECT DISTINCT CONCAT('User: ''' user '''@''' host ''';') AS query FROM mysql.user; +-------------------------------+ | query                         | +-------------------------------+ | User: 'canal'@'%';            | | User: 'root'@'127.0.0.1';     | | User: 'root'@'::1';           | | User: ''@'localhost';         | | User: 'root'@'localhost';     | | User: ''@'scidb-work601';     | | User: 'root'@'scidb-work601'; | +-------------------------------+ 7 rows in set (0.01 sec) ``` 自己用client链接下mysql 这个问题今天解决了，是这样的，因为不同的版本问题，导致创建用户的赋予权限，密码出现了不同，设置成'%'的时候，会导致 'localhost'无法登录。。。需要用命令单独去用'localhost'在运行一次。我用的是mysql5.5版本的。。 我也遇到这样的问题，搜索资料后，通过如下方式解决 ```` 增加普通用户后，执行：  mysql> use mysql  mysql> delete from user where user='';  mysql> flush privileges;  ```` 不知道哪里来的匿名用户，好囧 改super权限，重启，必须重启啊！ 按照 @maoyikun 的方法解决了 给mysql用户添加super权限 然后 flush privileges; 2018-10-12 19:27:59.970 [destination = example   address = localhost/127.0.0.1:3306   EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address localhost/127.0.0.1:3306 has an error  retrying. caused by  java.lang.IllegalArgumentException: null 	at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1314) ~[na:1.8.0_181] 	at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1237) ~[na:1.8.0_181] 	at java.util.concurrent.Executors.newFixedThreadPool(Executors.java:151) ~[na:1.8.0_181] 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor.start(MysqlMultiStageCoprocessor.java:84) ~[canal.parse-1.1.0.jar:na] 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:238) ~[canal.parse-1.1.0.jar:na] 	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_181] 2018-10-12 19:27:59.975 [destination = example   address = localhost/127.0.0.1:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.lang.IllegalArgumentException 	at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1314) 	at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1237) 	at java.util.concurrent.Executors.newFixedThreadPool(Executors.java:151) 	at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor.start(MysqlMultiStageCoprocessor.java:84) 	at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:238) 	at java.lang.Thread.run(Thread.java:748) ] 2018-10-12 19:28:16.965 [destination = example   address = localhost/127.0.0.1:3306   EventParser] WARN  c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - prepare to find start position just show master status 服务端数据库一有变动就报这个错误，请同仁帮忙看一下 @shiyuan2he 下载最新的1.1.1版本，已解决
145,添加rowdata过滤 为parseRowsEvent方法添加rowdata过滤，可以过滤掉rowdata数据，用于仅订阅除rowdata以外的数据 麻烦再审核一下，多谢 tks 
144,注销父类已经有的属性，避免super.start()调用了没有在子类初始化的同名属性 fix issue:master分支代码在使用OTTER时报NullPointerException #143 tks 
143,master分支代码在使用OTTER时报NullPointerException ``` pid:1 nid:1 exception:setl:java.lang.NullPointerException at com.alibaba.otter.canal.instance.core.AbstractCanalInstance.start(AbstractCanalInstance.java:73) at com.alibaba.otter.canal.instance.manager.CanalInstanceWithManager.start(CanalInstanceWithManager.java:110) at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.start(CanalServerWithEmbedded.java:102) at com.alibaba.otter.node.etl.select.selector.canal.CanalEmbedSelector.start(CanalEmbedSelector.java:206) at com.alibaba.otter.node.etl.select.SelectTask.startup(SelectTask.java:170) at com.alibaba.otter.node.etl.select.SelectTask.run(SelectTask.java:126) ``` 原因是：CanalInstanceWithManager 与 AbstractCanalInstance声明了相同名字的属性， CanalInstanceWithManager.java 110行调用了super.start()，调用了父类没有初始化的属性导致的。 ``` public void start() {         // 初始化metaManager         logger.info("start CannalInstance for {}-{} with parameters:{}"  canalId  destination  parameters);         **super.start();//110行**     } ``` 
142,canal1.0.21版本中zkclient和kafka zkclient版本冲突问题 canal和kafka一块用的时候 canal1.0.21版本中zkclient(com.github.sgroschupf )和kafka中zkclient版本(com.101tec:zkclient)冲突 artifactId不一样 不好排除包  这种问题有什么解决方案吗 多谢啊  为什么当时不使用com.101tec这个稍微大众的zkclient版本 这个issue为什么关掉了呀，我今天在1.0.24版本上也遇到这个问题 这个包历史有点悠久，如果方便可以提交一个PR给我
141,请教几个Canal使用的几个问题，多谢 1、文档说多个canal实例的serverId不能重复，请问有啥影响？我试的好像没什么问题 2、instance.properties里面的正则监听一个表和多个表对canal服务端性能影响大吗，因为我们的表是动态生成的，如果影响不大在客户端动态订阅就好了 3、多个otter客户端连接同一个canal时我发现一条消息只会收到一次，请问这个特性靠谱吗？ 1. 同一个mysql主备集群serverId，出现重复的时候，mysql会随机kill一个客户端链接  2. 正则表达式会有长度限制，相比于正则计算，网络I/O会先成为瓶颈 3. 建议一个server只有一个client，server和client都使用HA的模式，可以保证只有一个client工作，并且client能收到全部数据 
140,是否支持python客户端读取同步的binlog? 是否支持python客户端读取同步的binlog? canal目前只提供了java版本，server采用了protobuf协议进行通讯，可以按照协议需求自行完成一个python client版本开发 
139,关闭canal之后mysql连接未释放 **使用canal时，发现当mysql库没有任何binlog产生时，关闭canal后部分mysql连接未释放。** mysql版本：5.6.24-72.2-log canal版本：1.0.21 关闭前的堆栈信息如下： ``` "destination = 90   address = /192.168.12.110:3309   HeartBeatTimeTask" daemon prio=10 tid=0x00007f4b2c00d000 nid=0x395b in Object.wait() [0x00007f4b51d13000]    java.lang.Thread.State: TIMED_WAITING (on object monitor)         at java.lang.Object.wait(Native Method)         at java.util.TimerThread.mainLoop(Timer.java:552)         - locked <0x000000076052ffa8> (a java.util.TaskQueue)         at java.util.TimerThread.run(Timer.java:505) "destination = 90   address = /192.168.12.110:3309   EventParser" prio=10 tid=0x00007f4bd073e800 nid=0x395a runnable [0x00007f4b51d54000]    java.lang.Thread.State: RUNNABLE         at sun.nio.ch.FileDispatcherImpl.read0(Native Method)         at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)         at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)         at sun.nio.ch.IOUtil.read(IOUtil.java:197)         at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)         - locked <0x0000000760a5efb0> (a java.lang.Object)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:154)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209)         at java.lang.Thread.run(Thread.java:745) ``` 关闭前的mysql连接信息如下： ``` mysql> show processlist; | 442033 | xxx | 192.168.201.174:47598 | NULL | Sleep       |       4 |                                                                       | NULL             |         1 |             1 | | 442035 | xxx | 192.168.201.174:47600 | NULL | Binlog Dump |       4 | Master has sent all binlog to slave; waiting for binlog to be updated | NULL             |         0 |             0 | ``` **执行./bin/stop.sh后，发现442035连接未释放！！！** ./log/90/90.log如下： ``` 2016-01-14 11:55:11.655 [Thread-5] INFO  c.a.otter.canal.instance.spring.CanalInstanceWithSpring - stop CannalInstance for 1-90 2016-01-14 11:55:11.670 [Thread-5] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - disConnect MysqlConnection to /192.168.12.110:3309... 2016-01-14 11:55:11.675 [destination = 90   address = /192.168.12.110:3309   EventParser] INFO  c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O interrupted while readi ng from client socket java.nio.channels.ClosedByInterruptException: null         at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202) ~[na:1.7.0_55]         at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:412) ~[na:1.7.0_55]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:154) ~[canal.parse-1.0.21-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70) ~[canal.parse-1.0.21-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) [canal.parse-1.0.21-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) [canal.parse-1.0.21-SNAPSHOT.jar:na]         at java.lang.Thread.run(Thread.java:745) [na:1.7.0_55] 2016-01-14 11:55:11.676 [destination = 90   address = /192.168.12.110:3309   EventParser] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - the channel /192.168.12.110 :3309 is not connected 2016-01-14 11:55:11.676 [destination = 90   address = /192.168.12.110:3309   EventParser] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - disConnect MysqlConnection to /192.168.12.110:3309... 2016-01-14 11:55:11.677 [Thread-5] INFO  c.a.otter.canal.instance.spring.CanalInstanceWithSpring - stop successful.... 2016-01-14 11:55:11.677 [Thread-5] INFO  c.a.otter.canal.server.embedded.CanalServerWithEmbedded - stop CanalInstances[90] successfully ``` 另外，偶尔还会出现以下莫名mysql连接： ![image](https://cloud.githubusercontent.com/assets/7714843/12315487/4bcd3586-bab6-11e5-8a34-4c15758cab19.png) stop执行后，jvm是否已经完整退出？ @agapple jvm进程已完整退出。 JVM退出，TCP照理应该完全退出，确认下是否是canal对应进程之前的TCP链接 确实是canal的连接 经过debug测试，canal已经正确关闭了socket，只不过部分情况下操作系统的tcp链接会一直处于close_wait状态，可以通过调整操作系统参数，加速回收这类状态的链接.  mysql binlogdump为服务端主动推送数据模式，在客户端发起socket.close之后，服务端一致为发送ACK包导致一致处于CLOSE_WAIT状态.  采取的策略：针对binlog dump链接 在socket.close完成之后 再新开一个链接发起一次KILL CONNECTION   ID请求彻底关闭binlogdump链接.  MysqlConnector 106行 fork出一个连接去KILL CONNECTION时如果发生异常，不建议抛出去，记一下log就好了，毕竟是非主要流程 今天我看日志，一直在死循环的报这个： ``` 2016-08-10 15:44:48.865 [destination = ucarorder_to_es   address = /10.101.23.101:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position by switch ::1470729600000 2016-08-10 15:44:49.366 [destination = ucarorder_to_es   address = /10.101.23.101:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - ERROR ## findAsPerTimestampInSpecificLogFile has an error java.io.IOException: KILL DUMP 7265038 failure:java.io.IOException: ErrorPacket [errorNumber=1094  fieldCount=-1  message=Unknown thread id: 7265038  sqlState=HY000  sqlStateMarker=#]  with command: KILL CONNECTION 7265038         at com.alibaba.otter.canal.parse.driver.mysql.MysqlUpdateExecutor.update(MysqlUpdateExecutor.java:49)         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.disconnect(MysqlConnector.java:104)         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.reconnect(MysqlConnector.java:82)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.reconnect(MysqlConnection.java:56)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findAsPerTimestampInSpecificLogFile(MysqlEventParser.java:674)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findByStartTimeStamp(MysqlEventParser.java:519)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPositionInternal(MysqlEventParser.java:420)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPosition(MysqlEventParser.java:313)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:162)         at java.lang.Thread.run(Thread.java:745) ``` 但是我重启后就无法重现了，所以在这里提一下，看有没有别人遇到过这个问题。 最新代码已经提交，只记录了异常，没有往外抛 2016-08-10 18:59 GMT+08:00 hechaoyi notifications@github.com: > MysqlConnector 106行 > fork出一个连接去KILL CONNECTION时如果发生异常，不建议抛出去，记一下log就好了，毕竟是非主支流程 >  > 今天我看日志，一直在死循环的报这个： >  > 2016-08-10 15:44:48.865 [destination = ucarorder_to_es   address = /10.101.23.101:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position by switch ::1470729600000 > 2016-08-10 15:44:49.366 [destination = ucarorder_to_es   address = /10.101.23.101:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - ERROR ## findAsPerTimestampInSpecificLogFile has an error > java.io.IOException: KILL DUMP 7265038 failure:java.io.IOException: ErrorPacket [errorNumber=1094  fieldCount=-1  message=Unknown thread id: 7265038  sqlState=HY000  sqlStateMarker=#] >  with command: KILL CONNECTION 7265038 >         at com.alibaba.otter.canal.parse.driver.mysql.MysqlUpdateExecutor.update(MysqlUpdateExecutor.java:49) >         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.disconnect(MysqlConnector.java:104) >         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.reconnect(MysqlConnector.java:82) >         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.reconnect(MysqlConnection.java:56) >         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findAsPerTimestampInSpecificLogFile(MysqlEventParser.java:674) >         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findByStartTimeStamp(MysqlEventParser.java:519) >         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPositionInternal(MysqlEventParser.java:420) >         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPosition(MysqlEventParser.java:313) >         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:162) >         at java.lang.Thread.run(Thread.java:745) >  > 但是我重启后就无法重现了，所以在这里提一下，看有没有别人遇到过这个问题。 >  > — > You are receiving this because you were assigned. > Reply to this email directly  view it on GitHub > https://github.com/alibaba/canal/issues/139#issuecomment-238834028  or mute > the thread > https://github.com/notifications/unsubscribe-auth/AAy8t3RasGIBbgz1EEZps-j345g6Cc3uks5qea79gaJpZM4HElfj > . 
138,make canal server become a single instance 理论上一个JVM进程只需要一个CanalServer实例，由CanalServer管理多个CanalInstance； 先将这里改成单例，后续好修改otter代码 增加了AbstractCanalInstance，移除CanalInstance子类的大量重复代码 Canal代码不应以支持Otter未前提，如果这部分代码没有问题，打算着手修改Otter的CanalServer实现 赞赞哒👍 提交PR的行为值得肯定和鼓励 
137,java.lang.ArrayIndexOutOfBoundsException: 0 com.alibaba.otter.canal.parse.inbound.mysql.dbsync.SimpleDdlParser.parseTableName(SimpleDdlParser.java:163) 版本1.0.19 错误如下： ![qq 20151218092517](https://cloud.githubusercontent.com/assets/11752250/11886867/b88a512a-a569-11e5-9dba-1feaa8a6637c.png) binlog位置如下： ![qq 20151218092527](https://cloud.githubusercontent.com/assets/11752250/11886874/c864b91e-a569-11e5-8677-c808ef22c33d.png)  使用pt-online-schema-change变更字段后出现这种情况 https://github.com/alibaba/canal/pull/128，已经有用户修复，新版本发布后即可，或者可自行打包当前master 
136,Table无主键时，insert数据会导致columnInfo和tableMeta的size不匹配，从而instance监控hang掉 问题描述如题，以下为error log 2015-12-03 16:27:55.718 [destination =    address =    EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:crm[com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: column size is not match for table:`s1`.`t1` 6 vs 5 ] column size is not match for table:s1.t1 6 vs 5.  应该是做了字段删除操作，和主键无关 未做过字段删除，而且添加主键后就没有这个问题了。 个人猜测是因为mysql对无主键的表会自动创建一个隐藏主键，binlog中带有这个隐藏主键的数据，而tableMeta中则没有此字段，两边无法对应，因此程序报错。对于这种情况可否作适当处理？ 你测试的是RDS吧？RDS的确默认会对无主键表添加隐藏字段，需要super权限才可见 是RDS，有什么推荐的解决方案吗？目前我们需监测RDS库，这样的情况常常会使canal出问题 使用新RDS的自主用户权限，对canal授权super权限 @Librazk  rds能用canal吗？怎么玩 rds主备切换目前没有很好的处理，会导致位点报错 那需要清空canal上的meta.bat文件，让canal重新自动获取最新点位？ 目前是的，对于vip模式的RDS支持不好 这样会漏数据的 因为清空meta文件 去slava上拿的是最新的position吧  在发现master被切的这一段时间的数据就没了 记录上一次meta文件里的时间戳，基于时间戳重新定位 RDS无主键表会有一个隐藏列：**#alibaba_rds_row_id#**，会在每张表的最后一列且为int类型，可以兼容一下 meta.bat  文件位置在哪里？为什么在canal目录下没有找到 > meta.bat 文件位置在哪里？为什么在canal目录下没有找到 zookeeper 模式你需要执行 zkCli 在里面 delete 掉那个 canal 的 path
135,值由Null变为空字符串时，isUpdated属性为false mysql版本是5.5 将一个varchar类型的列的值由NULL，改为''时，从canal获取的entry的column里，isupdated属性为false。看了下LogEventConverter，感觉是isUpdated方法有个bug。newValue为""，旧值为NULL时，旧的column.getValue()为""。 column.isNull()方法可以判断是否为null值 这是在otter中发现的，otter使用了canal的column.isUpdated方法来判断是否发生了变更，然后就有些列同步错误了。 多谢反馈，问题代码已经修复，下个版本可发布 不客气。期待canal与otter更加完善 
134,关于max memory size的问题 Hi jianghang： 在类MemoryEventStoreWithBuffer的checkFreeSlotAt方法中， if (batchMode.isMemSize()) {                 final long memsize = putMemSize.get() - ackMemSize.get();                 if (memsize < bufferSize \* bufferMemUnit) {                     return true;                 } else {                     return false;                 }             } else {                 return true;             } 只判断memsize < bufferSize \* bufferMemUnit还不够吧，应该用 ”当前空闲内存量+新加入事件所占内存量”和bufferSize \* bufferMemUnit进行比较才对吧？？ 望指教。 putMemSize.get() - ackMemSize.get()， 这个得出来的结果就是当前在内存中未被get的数据，也就是使用中的内存块，因为Ringbuffer的特性是lazy set模式，在几个位点之间会有部分内存数据未被及时清理，不过一般在数据高速get/put过程中，总的内存量是可控的，并不是非常精确 恩 明白 tks 
133,关于事务被截断的疑问 下面是我的实验步骤，发现除了canal.instance.transaction.size参数可能导致事务被截断外，客户端batchsize的设置也会导致事务被截断，请问下，canla在设计的时候对事务完整性是一个什么样的考虑呢？只要保证能把数据同步给客户端，能记下ack，保证最终一致？ 下面是我的实验。 1. 创建一个测试用存储过程，过程代码如下：        DELIMITER //        CREATE PROCEDURE loop_insert(IN count int)        begin            declare lp int;            declare idvalue int;            set lp=0;            select max(id)+1 into idvalue from test;            set autocommit=0;            while lp<count do                insert into test(id name)values(idvalue 'test-'+idvalue);                set idvalue=idvalue+1;                  set lp=lp+1;              end while;            commit;        end;        //        DELIMITER;    1. 测试“canal.instance.transaction.size”       > 将canal-server的“canal.instance.transaction.size”参数设置为1024(默认也是1024)，canal-client(SimpleCanalClientTest)的batchsize设置为1024*5(要足够大)       > 依序启动mysql、canal-server和canal-client（在mysql执行操作之前，canal-client必须预先启动起来，否则事务截断的现象显示不出来）       > 在mysql端执行loop_insert，参数传1022，观察canal-client端：所有event会在一个batch中拿到       > 在mysql端执行loop_insert，参数传1023，观察canal-client端：会返回两个batch，第一个包含1024条event，第二个包含1条event       上面验证了参数"canal.instance.transaction.size"的作用，为了防止事务被截断需要把参数值设置的大一些    2. 测试batchsize       3.1 场景一            > 将canal-server的参数设为ITEMSIZE(使用ITEMSIZE便于测试)，将canal-client中的batchsize设置为2           > 启动mysql，启动canal-server，在mysql端执行loop_insert，参数传10，然后再启动canal-client           > 观察canal-client端：会返回6个batch，每个batch包含2条event，事务被截断       3.4 场景二           > 将canal-server的参数设为ITEMSIZE(使用ITEMSIZE便于测试)，将canal-client中的batchsize设置为13           > 启动mysql，启动canal-server，在mysql端连续执行loop_insert两次，参数传10，然后再启动canal-client(必须上述操作完成后再启动client，保证3.4的真实性)           > 观察canal-client端：会返回两个batch，第一个batch13条event，第二个batch11条event，事务被截断       由此可见，除了需要考虑transaction size还需要考虑batch size，batchsize配置的比较小的话也可能截断事务，但是如果太大的话又会影响实时性，需要取一个折衷       注：再对2和3进行测试时，要保证相互隔离，即测试transaction.size的时候要保证batchsize的足够大，测试batchsize的时候亦然 canal.instance.transaction.size的目的是防止事务被截断，但是如果mysql的binlog-format是mixed类型，基本每个事件都会导致一次flush，canal.instance.transaction.size的设置就没什么意义了吧？？？ 说下我当时考虑的思路： 1. canal.instance.transaction.size，这个参数主要控制parser解析后，提交到event store时，如果要保证事务可见的一致性，设置的最大保证一致性的事务大小为1024. (举个例子：如果没有这一致性保护，你get频繁调用可能会拿到一次事务头，事务体数据，事务尾数据)，事务的一致性写入是事务块读取的一个基础 2. 目前get数据获取时，暂时没有考虑事务完整读取的机制，主要还是考虑业务需求，对于事务完整性不敏感.  要保证完整读取其实也不难. 你可以描述一下，你们对完整事务的需求，主要基于什么样的业务考虑 明白你的意思。 但是最后那个binlog-format的问题，不知道您试了没有。如果format是mixed类型，那么canal.instance.transaction.size应该就失效了，因为isDml返回的是false，每收到一个rowdata类型的entry就flush一次了。             case ROWDATA:                 put(entry);                 // 针对非DML的数据，直接输出，不进行buffer控制                 EventType eventType = entry.getHeader().getEventType();                 if (eventType != null && !isDml(eventType)) {                     flush();                 }                 break; 
132,filterEmtryTransactionEntry的问题 在EntryEventSink类的sinkData方法中，有对filterEmtryTransactionEntry进行判断的代码段，如下所示：             if (filterEmtryTransactionEntry && !CollectionUtils.isEmpty(events)) {                 long currentTimestamp = events.get(0).getEntry().getHeader().getExecuteTime();                 // 基于一定的策略控制，放过空的事务头和尾，便于及时更新数据库位点，表明工作正常                 if (Math.abs(currentTimestamp - lastEmptyTransactionTimestamp) > emptyTransactionInterval                     || lastEmptyTransactionCount.incrementAndGet() > emptyTransctionThresold) {                     lastEmptyTransactionCount.set(0L);                     lastEmptyTransactionTimestamp = currentTimestamp;                     return doSink(events);                 }             } 请问这里的判断基于什么目的？不太明白。 把canal.instance.transaction.size的大小设置为1024，在mysql上运行一个事务，进行1023次插入，那么canal-server会产生1025个event，按照EventTransactionBuffer的逻辑达到1024的时候会flush一次，随后剩下的一个event(Transactionend类型)会单独进行flush，但是由于上述代码的缘故，该event被忽略了，这应该不尽合理吧？？？？ 事务头和尾主要是用来区分事务边界，本身数据没啥意义.  1025的transaction end事件的确可能被忽略了。 在canal+otter产品配合实施中，otter对于过滤后的空事务事件不是强需求，反而占用了过多的canal eventStore的ringBuffer空间，导致整体业务的处理速度变慢，所以当时设计默认值为true，待考虑和优化 感谢您的回复。 的确，出现事务被截断的情况是个小概率事件，并且像1025例子这样被截断后恰好出现了一个空事务事件更是一个极端事件。 您说的“反而占用了过多的canal eventStor的ringBuffer空间”指的应该是mysql的binlog-format不等于row的情况吧。当binlog-format等于row的时候，每次EventTransactionBuffer都是攒够一个事务才flush一次，除非事务太大被截断，否则不会出现【空事务事件】的情况，那么也就没机会过滤了，这样并没有减少ringbuffer的内存使用。而当binlog-format比如等于mixed时，下面的代码段会进入if语句，基本上每条语句都会flush一次，这样才导致很多【空事务事件】。 case ROWDATA:                 put(entry);                 // 针对非DML的数据，直接输出，不进行buffer控制                 EventType eventType = entry.getHeader().getEventType();                 if (eventType != null && !isDml(eventType)) {                     flush();  //当binlog-format为mixed时，eventtyp为Query，isDml返回false，会flush                 }                 break; 您说的 “ 在canal+otter产品配置实施中，otter对于过滤后的空事务事件不是强需求 ”，这个该怎么理解呢？还是1025那个例子，如果1025对应的TRANSACTIONEND事件被过滤，otter怎么来界定事务边界呢？ otter非事务敏感性，其主要利用事务头和尾来更新位点而已，不是以事务单元进行整体提交，具体可以看下数据导入的wiki 恩 “非事务敏感性、主要利用事务头和尾来更新位点”  这个明白了。 上述关于binlog-format的表述是否正确？如果事务不被截断，并且binlog-foramt为row，那出现【空事务事件】的场景应该就没有了吧？ 关于【空事务事件】，我找到另外的场景了，当配置filter的时候，如果某个表被过滤，那这张表的rowdata event会被过滤掉，但是TRANSACTIONBEGIN和TRANSACTIONEDN并没有被过滤掉，所以还是需要通过filterEmtryTransactionEntry 进行控制。您说的“反而占用了过多的canal eventStor的ringBuffer空间”应该主要指这种情况吧？ 是的，理解正确. RingBuffer是固定数组大小的设计，所以空的事务头和尾会影响整体的吞吐量 
131,parse row data failed mysql版本：5.6.23-log， 使用otter做数据同步的时候，会报如下错误: pid:2 nid:2 exception:canal:canal_mysql:com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: java.lang.IllegalArgumentException: bad format  x exceed: 2147483638  999999999 at com.taobao.tddl.dbsync.binlog.LogBuffer.getDecimal0(LogBuffer.java:1355) at com.taobao.tddl.dbsync.binlog.LogBuffer.getDecimal(LogBuffer.java:1265) at com.taobao.tddl.dbsync.binlog.event.RowsLogBuffer.fetchValue(RowsLogBuffer.java:336) at com.taobao.tddl.dbsync.binlog.event.RowsLogBuffer.nextValue(RowsLogBuffer.java:96) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseOneRow(LogEventConvert.java:454) 麻烦报告一下canal版本和otter版本 canal: 1.0.20 otter: 4.2.12 mysql环境是一个主从集群，开启半同步插件。 连从库会报这个错误； 连主库没有问题。 测试了本地的mysql 5.6版本，针对decimal类型暂时没发现有类似错误，如果方便是否可以提供一下对应出错位点的binlog原始文件 估计是这个问题，https://github.com/alibaba/canal/issues/119 使用master分支build一个包试试 也遇到这个问题，canal在解析Decimal类型(MYSQL_TYPE_NEWDECIMAL)时似乎出现了问题，出问题的代码在LogBuffer类的getDecimal0方法； 我在本地改了一个版本，已经修复了这个问题，但可能需要进一步完善，稍后会提交PR 欢迎提交PR，方便报告一下测试mysql和canal的版本 mysql Server version: 10.2.14-MariaDB-log Columnstore 1.1.4-1 canal 1.0.24 2018-05-10 17:42 GMT+08:00 agapple <notifications@github.com>: > 欢迎提交PR，方便报告一下测试mysql和canal的版本 > > — > You are receiving this because you commented. > Reply to this email directly  view it on GitHub > <https://github.com/alibaba/canal/issues/131#issuecomment-388006054>  or mute > the thread > <https://github.com/notifications/unsubscribe-auth/ASsS_qXHnsjFiIsbJ28I1VULEZax_zqjks5txAt6gaJpZM4GX7gy> > . > --  Best Regards  Zhiyu 对应的测试decimal数据发一下吧 ​  mysql-bin.000014 <https://drive.google.com/file/d/1V9PHa3M4T4UhTPfHBElsqLFMi4ECJU2r/view?usp=drive_web> ​ google硬盘应该可以访问吧。 我昨天把我改过代码运行在mysql5.6上，发现对于mysql5.6不work，怀疑是mysql和columnstore的binlog格式在处理decimal方面有点不一样。 2018-05-10 19:14 GMT+08:00 agapple <notifications@github.com>: > 对应的测试decimal数据发一下吧 > > — > You are receiving this because you commented. > Reply to this email directly  view it on GitHub > <https://github.com/alibaba/canal/issues/131#issuecomment-388025033>  or mute > the thread > <https://github.com/notifications/unsubscribe-auth/ASsS_iT5TTAzZOB_09yhsHg3h4XE9xEzks5txCEfgaJpZM4GX7gy> > . > --  Best Regards  Zhiyu 有业务上构造测试数据的sql？ 他们是用load命令把csv文件load到columnstore里面，语句如下： LOAD DATA LOCAL INFILE '"+ ExportData[ Key ].Location +"' INTO TABLE "+ ExportData[ Key ].Table +" CHARACTER SET utf8 COLUMNS TERMINATED BY '|' IGNORE 1 LINES") 如果想要重现的话，写一个csv文件 ，再执行load应该可以 2018-05-15 17:58 GMT+08:00 agapple <notifications@github.com>: > 有业务上构造测试数据的sql？ > > — > You are receiving this because you commented. > Reply to this email directly  view it on GitHub > <https://github.com/alibaba/canal/issues/131#issuecomment-389112064>  or mute > the thread > <https://github.com/notifications/unsubscribe-auth/ASsS_qVV7534IKkprEIB-GLBjiwb-x6Oks5tyqa9gaJpZM4GX7gy> > . > --  Best Regards  Zhiyu 最好是能构造出对应的sql语句来复现这个问题，查过对应的binlog解析代码，和mysql parser的是保持一致的
130,mysql 5.6版本 datetime值为null时 sqltype解析异常 问题描述： mysql 版本为5.6.12， 在插入数据时，DATETIME类型的字段（如update_time字段）设置为NULL时，在从canal客户端获取到的数据中，对应字段的sqlType为1111（OTHER） 而在mysql 5.5.*版本中做同样的测试，DATETIME为null时，canal客户端获取到的对应sqlType为93（TIMESTAMP） 这个是BUG么？ 我先验证下 确认是一个bug，mysql5.6增加了新的LogEvent类型，增加swtich判断后即可修复 
129,无法读取Binlog 环境： Fedora 21 docker 1.8.1 docker mysql 5.5.45 mysql配置： ``` [mysqld] server-id=1042776 log_bin=mysql-bin log_error=mysql-bin.err binlog-format=ROW ``` canal配置： ``` [snowwolf@snowwolf-dev canal]$ cat conf/example/instance.properties ################################################# ## mysql serverId canal.instance.mysql.slaveId = 1234 # position info canal.instance.master.address = 127.0.0.1:33306 canal.instance.master.journal.name =  canal.instance.master.position =  canal.instance.master.timestamp =  #canal.instance.standby.address =  #canal.instance.standby.journal.name = #canal.instance.standby.position =  #canal.instance.standby.timestamp =  # username/password canal.instance.dbUsername = canal canal.instance.dbPassword = canal canal.instance.defaultDatabaseName = dbroute_01 canal.instance.connectionCharset = UTF-8 # table regex canal.instance.filter.regex = .*\\..* # table black regex canal.instance.filter.black.regex =   ################################################# ``` 启动参数： ``` docker run --name dbrouter-mysql-dev -p 33306:3306 -v /etc/mysql/conf.d:/etc/mysql/conf.d -v /var/lib/docker-mysql:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=xxx -d mysql:5.5 ``` 日志： ``` 2015-09-08 21:11:34.687 [destination = example   address = /127.0.0.1:33306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position by switch ::1441612069000 2015-09-08 21:11:34.701 [destination = example   address = /127.0.0.1:33306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - Didn't find the corresponding binlog files from mysql-bin.000001 to mysql-bin.000003 2015-09-08 21:11:34.701 [destination = example   address = /127.0.0.1:33306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /127.0.0.1:33306 has an error  retrying. caused by  com.alibaba.otter.canal.parse.exception.CanalParseException: can't find start position for example 2015-09-08 21:11:34.702 [destination = example   address = /127.0.0.1:33306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: can't find start position for example ] ``` 使用mysqlbinlog可以解析 ``` [root@snowwolf-dev ~]# mysqlbinlog -h127.0.0.1 -P33306 -ucanal -pcanal --read-from-remote-server -v --start-position=296 -d dbroute_01 mysql-bin.000003 /*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE=1*/; /*!40019 SET @@session.max_insert_delayed_threads=0*/; /*!50003 SET @OLD_COMPLETION_TYPE=@@COMPLETION_TYPE COMPLETION_TYPE=0*/; DELIMITER /*!*/; # at 296 #150908 20:29:44 server id 1042776  end_log_pos 0   Start: binlog v 4  server v 5.5.45-log created 150908 20:29:44 BINLOG ' uNTuVQ9Y6Q8AZwAAAAAAAAAAAAQANS41LjQ1LWxvZwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA AAAAAAAAAAAAAAAAAAAAAAAAEzgNAAgAEgAEBAQEEgAAVAAEGggAAAAICAgCAA== '/*!*/; # at 296 #150908 20:33:05 server id 1042776  end_log_pos 431     Write_rows: table id 3979 flags: STMT_END_F BINLOG ' gdXuVRdY6Q8AhwAAAK8BAAAAAIsPAAAAAAEAGP///0DSPgMAAACyyO5VssjuVQEAAAAKMTIzNDU2 Nzg5MgwrMTM4MTAwMDEwMDIJAHRlc3R1c2VyMyBlMTBhZGMzOTQ5YmE1OWFiYmU1NmUwNTdmMjBm ODgzZQGyyO5VAQEABGFiY2QAAAAA '/*!*/; ### Row event for unknown table #3979# at 431 #150908 20:33:05 server id 1042776  end_log_pos 458     Xid = 914 COMMIT/*!*/; # at 458 #150908 20:57:33 server id 1042776  end_log_pos 532     Query   thread_id=515   exec_time=0 error_code=0 SET TIMESTAMP=1441717053/*!*/; SET @@session.pseudo_thread_id=515/*!*/; SET @@session.foreign_key_checks=1  @@session.sql_auto_is_null=0  @@session.unique_checks=1  @@session.autocommit=1/*!*/; SET @@session.sql_mode=0/*!*/; SET @@session.auto_increment_increment=1  @@session.auto_increment_offset=1/*!*/; /*!\C utf8mb4 *//*!*/; SET @@session.character_set_client=45 @@session.collation_connection=224 @@session.collation_server=8/*!*/; SET @@session.lc_time_names=0/*!*/; SET @@session.collation_database=DEFAULT/*!*/; BEGIN /*!*/; # at 532 # at 639 #150908 20:57:33 server id 1042776  end_log_pos 639     Table_map: `dbroute_01`.`uc_member_000` mapped to number 3979 #150908 20:57:33 server id 1042776  end_log_pos 774     Delete_rows: table id 3979 flags: STMT_END_F BINLOG ' PdvuVRNY6Q8AawAAAH8CAAAAAIsPAAAAAAEACmRicm91dGVfMDEADXVjX21lbWJlcl8wMDAAGAMH BwMPDw8PDwEBBw8BDwECAQ8PDw8PBxg8ADwAPAAsAZYAlgDAADwAeAA8ADwAHgDg9z4= PdvuVRlY6Q8AhwAAAAYDAAAAAIsPAAAAAAEAGP///0DSPgMAAACyyO5VssjuVQEAAAAKMTIzNDU2 Nzg5MgwrMTM4MTAwMDEwMDIJAHRlc3R1c2VyMyBlMTBhZGMzOTQ5YmE1OWFiYmU1NmUwNTdmMjBm ODgzZQGyyO5VAQEABGFiY2QAAAAA '/*!*/; ### DELETE FROM `dbroute_01`.`uc_member_000` ### WHERE ###   @1=3 ###   @2=1441712306 ###   @3=1441712306 ###   @4=1 ###   @5='1234567892' ###   @6='+13810001002' ###   @7=NULL ###   @8='testuser3' ###   @9='e10adc3949ba59abbe56e057f20f883e' ###   @10=NULL ###   @11=1 ###   @12=1441712306 ###   @13=NULL ###   @14=1 ###   @15=NULL ###   @16=NULL ###   @17=1 ###   @18=NULL ###   @19=NULL ###   @20=NULL ###   @21=NULL ###   @22=NULL ###   @23='abcd' ###   @24=0 # at 774 #150908 20:57:33 server id 1042776  end_log_pos 801     Xid = 3451 COMMIT/*!*/; DELIMITER ; # End of log file ROLLBACK /* added by mysqlbinlog */; /*!50003 SET COMPLETION_TYPE=@OLD_COMPLETION_TYPE*/; /*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE=0*/; ``` 在非docker的MariaDB 10，docker的5.6上都是如此，请问是什么问题呢？ prepare to find start position by switch ::1441612069000 基于时间戳定位位点，找不到比这时间小的binlog.  建议. 删除历史位点，重新订阅 
128,fix rename table ddl parser tablename and schemaname error 之前的rename table 正则表达式在匹配 tablename或者schemaname 中以to 打头的表名或者库名的时候会有异常，匹配到空。具体的bad case可以看测试文件 tks 
127,fix not skip filter and read the error data 官方协议中有两个byte的filter字段。这里没有跳过，导致后面的数据读错。 ``` lenenc_str     catalog lenenc_str     schema lenenc_str     table lenenc_str     org_table lenenc_str     name lenenc_str     org_name lenenc_int     length of fixed-length fields [0c] 2              character set 4              column length 1              type 2              flags 1              decimals 2              filler [00] [00]   if command was COM_FIELD_LIST { lenenc_int     length of default-values string[$len]   default values   } ``` 按照官网的说法，filler 字段应该是 00，所以最后以为 string length 如果也是0的话不会报错。 但是我利用这个方法请求`SHOW BINARY LOGS`，发现 filter 返回的是 31。 所以才发现了这个问题。 为什么返回31还不知道，但是这里最后一位的读取肯定读错了。 tks 
126,Canal有计划支持基于GTID去做数据同步么 如果MySQL使用VIP是去HA，当MYSQL发生主从切换的时候，canal会可能会报错，因为和备机上的binlog和position对不上。如果是基于GTID去拉binlog，应该就能解决这个问题。 基于主备复制的mysql，GTID可以解决切换后位点查找的问题，技术上可以实现，具体时间不可保证 嗯，明白。感谢@agapple 同学。代码提交的时候关下这个issue，好让我知道去试用下。 这个进展如何呢？有计划支持么 @agapple 所以就是迄今为止 canal 也没有支持 GTID 是么?  请问现在支持GTID？ 暂时没有，期待有完成的同学贡献一个PR 很久以前我完成了这个功能，代码略糙，没法PR 我在1.0.25的基础上增加了GTID的支持，需要帮助review。https://github.com/alibaba/canal/pull/618 ## 改动说明 1. 增加instance级配置`gtidon`，指示该instance是否启用GTID模式。 2. driver包增加GTID相关类：`GTIDSet`，`UUIDSet`是GTID表示，`BinlogDumpGTIDCommandPacket`是`COM_BINLOG_DUMP_GTID`命令。 3. 补全dbsync包下的`GtidLogEvent`事件的解析。 4. protocol包下，增加了`EntryType`枚举项`GTIDLOG`，是binlog流中的GTIDLOG事件；`Header`增加了字段`gtid`，这里后面单独段落详细说明。 5. parse包下，`ErosaConnection`增加根据GTID同步的dump方法。 6. parse包下，`AbstractEventParser.start内`，如果instance使用GTID模式，则发送`COM_BINLOG_DUMP_GTID`命令给mysql。 ## 关于`CanalEntry.Header`增加`gtid`字段 slave通过文件位点的方式同步binlog时，每个事件头里都携带了filename position信息，后续存储最新同步位点只要取客户端ack过的最后一条事件的filename position即可。 对应的，GTID模式则需要记录消费过的[GTIDSet](https://dev.mysql.com/doc/refman/5.7/en/replication-gtids-concepts.html)。 一个事务提交后，slave收到如下一连串事件，`GTID_LOG`，`TRANSACTIONBEGIN`，`ROWDATA`，`TRANSACTIONEND`，其中只有`GTID_LOG`有本次事务的GTID信息。现在通过`com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvertGTID`类，记录合并最后一个GTID，并赋值给后续事件的Header.gtid。 发现没法给某个tag发pr.. 我调整下对master发个pr @agapple com.alibaba.fastsql包2.0.0_preview_228版可以提供一份吗？ @hiwjd 在alpha的二进制包里有一份fastsql的jar gtid的PR已经合并了，非常感谢 @hiwjd 的贡献 https://dev.mysql.com/doc/refman/5.7/en/replication-mode-change-online-enable-gtids.html，开启gtid的方案
125,canal 1.0.20与otter.canal-client 1.0.20使用的protobuf版本不同 一个是2.4.1一个是2.5，导致用canal-client连接 canal时报错： ``` java java.lang.UnsupportedOperationException: This is supposed to be overridden by subclasses.         at com.google.protobuf.GeneratedMessage.getUnknownFields(GeneratedMessage.java:180) ~[protobuf-java-2.5.0.jar:na] ``` 目前canal和otter使用的protobuf版本均为2.4.1.  目前看来是2.4.1生成的class，2.5无法支持，需要测试使用2.5之后，部分老版本用户使用2.4.1版本是否可以正常使用 问题是在maven repository里面，canal.protocol的dependency中protobuf没有指定版本： http://search.maven.org/#artifactdetails%7Ccom.alibaba.otter%7Ccanal.protocol%7C1.0.20%7Cjar ``` xml        <dependency>             <groupId>com.google.protobuf</groupId>             <artifactId>protobuf-java</artifactId>         </dependency> ``` 默认会用2.5.0 https://repo1.maven.org/maven2/com/alibaba/otter/canal/1.0.20/canal-1.0.20.pom，父工程会显示声明protobuf版本为2.4.1 在pom里面更改protobuf版本，然后再本地重新生成protocol文件放到protocol/src/main/java/com/alibaba/otter/canal/protocol/里面，可以解决这个问题。或者直接clone我的代码 https://github.com/chaopengio/canal https://github.com/alibaba/canal/commit/c86fba6092979c4d42734903829843c3d62342ae，有人已经提交了PR，升级到了2.6.1  
124,fixed a hard coded mysql host ip address tks 
123,Update event被识别为INSERT EVENT当表格存储为ndbcluster同时设置有pk的情况下 创建这样的表格，这里要同时有PK和ndbcluster engine的指定。只指定其一，不会触发这种情况。 create table test3col(id int(3)  name varchar(10)  last varchar(10)  primary key(id))engine=ndbcluster;  mysql> insert into test3col values(1 'a' 'b'); mysql> update test3col set last='c' where id=1; SimpleCilent打印出来的log，可以发现update命令被认为是insert类型，同时只显示了主键和更新列 ================> binlog[log-bin.000001:3946366]   name[cudb_user_data test3col]   eventType : CREATE empty count : 1 empty count : 2 empty count : 3 empty count : 4 empty count : 5 empty count : 6 ================> binlog[log-bin.000001:3946753]   name[mysql ndb_apply_status]   eventType : INSERT server_id : 501    update=true epoch : 15301516776701952    update=true log_name :     update=true start_pos : 0    update=true end_pos : 0    update=true ================> binlog[log-bin.000001:3946814]   name[cudb_user_data test3col]   eventType : INSERT id : 1    update=true name : a    update=true last : b    update=true ================> binlog[log-bin.000001:3947108]   name[mysql ndb_apply_status]   eventType : INSERT server_id : 501    update=true epoch : 15301538251538433    update=true log_name :     update=true start_pos : 0    update=true end_pos : 0    update=true ================> binlog[log-bin.000001:3947169]   name[cudb_user_data test3col]   eventType : INSERT id : 1    update=true last : c    update=true 1. 只显示主键和变更列，估计和你mysql版本有关，mysql5.6之后支持minial image模式，只会记录变更列和主键 2. 至于update被认为insert，个人初步判断和ndbcluster有关，以前没测试过 建议你直接看一下show binlog events看一下对应update sql的binlog对象 mysql 5.6  mysql> show binlog events in 'log-bin.000001' from 3960268 limit 100; +----------------+---------+------------+-----------+-------------+-----------------------------------------+ | Log_name       | Pos     | Event_type | Server_id | End_log_pos | Info                                    | +----------------+---------+------------+-----------+-------------+-----------------------------------------+ | log-bin.000001 | 3960268 | Write_rows |       501 |     3960329 | table_id: 34                            | | log-bin.000001 | 3960329 | Write_rows |       501 |     3960368 | table_id: 521 flags: STMT_END_F         | | log-bin.000001 | 3960368 | Query      |       501 |     3960433 | COMMIT                                  | | log-bin.000001 | 3960433 | Query      |       501 |     3960497 | BEGIN                                   | | log-bin.000001 | 3960497 | Table_map  |       501 |     3960560 | table_id: 521 (cudb_user_data.test3col) | | log-bin.000001 | 3960560 | Table_map  |       501 |     3960622 | table_id: 34 (mysql.ndb_apply_status)   | | log-bin.000001 | 3960622 | Write_rows |       501 |     3960683 | table_id: 34                            | | log-bin.000001 | 3960683 | Write_rows |       501 |     3960723 | table_id: 521 flags: STMT_END_F         | | log-bin.000001 | 3960723 | Query      |       501 |     3960788 | COMMIT                                  | +----------------+---------+------------+-----------+-------------+-----------------------------------------+ 9 rows in set (0.00 sec) 其中 | log-bin.000001 | 3960329 | Write_rows |       501 |     3960368 | table_id: 521 flags: STMT_END_F 对应一个insert 语句 | log-bin.000001 | 3960683 | Write_rows |       501 |     3960723 | table_id: 521 flags: STMT_END_F   对应一个update语句。 从这里看不出来insert和update的区别。 而针对一张InnoDB的表，可以看到event_type区分为Write_rows和Update_rows mysql> show binlog events in 'log-bin.000001' from 3960268 limit 100; +----------------+---------+-------------+-----------+-------------+-----------------------------------------+ | Log_name       | Pos     | Event_type  | Server_id | End_log_pos | Info                                    | +----------------+---------+-------------+-----------+-------------+-----------------------------------------+ | log-bin.000001 | 3960268 | Write_rows  |       501 |     3960329 | table_id: 34                            | | log-bin.000001 | 3960329 | Write_rows  |       501 |     3960368 | table_id: 521 flags: STMT_END_F         | | log-bin.000001 | 3960368 | Query       |       501 |     3960433 | COMMIT                                  | | log-bin.000001 | 3960433 | Query       |       501 |     3960497 | BEGIN                                   | | log-bin.000001 | 3960497 | Table_map   |       501 |     3960560 | table_id: 521 (cudb_user_data.test3col) | | log-bin.000001 | 3960560 | Table_map   |       501 |     3960622 | table_id: 34 (mysql.ndb_apply_status)   | | log-bin.000001 | 3960622 | Write_rows  |       501 |     3960683 | table_id: 34                            | | log-bin.000001 | 3960683 | Write_rows  |       501 |     3960723 | table_id: 521 flags: STMT_END_F         | | log-bin.000001 | 3960723 | Query       |       501 |     3960788 | COMMIT                                  | | log-bin.000001 | 3960788 | Query       |       501 |     3960852 | BEGIN                                   | | log-bin.000001 | 3960852 | Table_map   |       501 |     3960915 | table_id: 521 (cudb_user_data.test3col) | | log-bin.000001 | 3960915 | Table_map   |       501 |     3960977 | table_id: 34 (mysql.ndb_apply_status)   | | log-bin.000001 | 3960977 | Write_rows  |       501 |     3961038 | table_id: 34                            | | log-bin.000001 | 3961038 | Write_rows  |       501 |     3961078 | table_id: 521 flags: STMT_END_F         | | log-bin.000001 | 3961078 | Query       |       501 |     3961143 | COMMIT                                  | | log-bin.000001 | 3961143 | Query       |       501 |     3961207 | BEGIN                                   | | log-bin.000001 | 3961207 | Table_map   |       501 |     3961270 | table_id: 521 (cudb_user_data.test3col) | | log-bin.000001 | 3961270 | Table_map   |       501 |     3961332 | table_id: 34 (mysql.ndb_apply_status)   | | log-bin.000001 | 3961332 | Write_rows  |       501 |     3961393 | table_id: 34                            | | log-bin.000001 | 3961393 | Write_rows  |       501 |     3961431 | table_id: 521 flags: STMT_END_F         | | log-bin.000001 | 3961431 | Query       |       501 |     3961496 | COMMIT                                  | | log-bin.000001 | 3961496 | Query       |       501 |     3961574 | BEGIN                                   | | log-bin.000001 | 3961574 | Table_map   |       501 |     3961630 | table_id: 510 (cudb_user_data.test)     | | log-bin.000001 | 3961630 | Write_rows  |       501 |     3961668 | table_id: 510 flags: STMT_END_F         | | log-bin.000001 | 3961668 | Xid         |       501 |     3961695 | COMMIT /\* xid=17139826 _/               | | log-bin.000001 | 3961695 | Query       |       501 |     3961773 | BEGIN                                   | | log-bin.000001 | 3961773 | Table_map   |       501 |     3961829 | table_id: 510 (cudb_user_data.test)     | | log-bin.000001 | 3961829 | Update_rows |       501 |     3961876 | table_id: 510 flags: STMT_END_F         | | log-bin.000001 | 3961876 | Xid         |       501 |     3961903 | COMMIT /_ xid=17139887 */               | +----------------+---------+-------------+-----------+-------------+-----------------------------------------+ 通过binlog的event type无法区分insert和update的情况下，还可以有其他方式区分吗 Write_rows/Update_rows/Delete_rows，binlog针对不同的类型会有严格区分，你这里就是记录为insert类型，估计和你选择的ndb存储有关 
122,修复rename table命令中包含多个表，解析失败问题 简单的修复下 rename table命令中包含多个表的问题。 如 https://github.com/alibaba/canal/issues/79 中提到的： 后续完美解决：使用类似druid/cobar的mysql sql语法解析，提取对应的表. 是比较理想解决方案。 由于canal获取binlog存在延迟，可能拿到binlog的时候通过desc命令获取的表结构实际是和binlog产生时的表结构有差距的。 最好的解决方案是ddl语句都直接触发tablemeta cache的对应变化，而不是clear tablemetacache。 1.只有drop table 会clear tablemeta cache 2.只有收集到table metacache 中没有的表的binlog时才会desc 获取表结构。 3.ddl语句都直接作用在tablemeta cache上 这个才能最大程度的减少错误的发生，这个是个临时的修补方案，解决目前遇到的 rename table命令中包含多个表的问题。 
121,Update README.md tks 
120,增加了一个脚本来自动获取mysql master binlog信息，并自动更新instance配置文件 tks 
119,支持下mysql5.6的noblob/minimal的binlog解析 mysql 5.6开始支持binlog_row_image属性 允许应用控制记录binlog里的before/after的字段内容，以前默认是记录所有字段.  文档:http://dev.mysql.com/doc/refman/5.6/en/replication-options-binary-log.html#sysvar_binlog_row_image.  支持: 1. 解析识别changeColumns 针对noblob/minimal的情况 只返回binlog里有的字段类型 2. MysqlEventParser增加supportBinlogFormats/Images的控制 允许用户定义期望支持的类型，如果db不符合这类型直接报错.  (for otter   只支持row模式 + FULL image) 
118,我在windows启动node 4.2.11 报下面错误 Listening for transport dt_socket at address: 8099 閿欒: 鎵句笉鍒版垨鏃犳硶鍔犺浇涓荤被 澶勪簬鎵撳紑鐘舵€併€ 1. 先解决乱码问题， 2. Listening这日志是正常输出 
117,现在很多时候binlog-format都是mixed 现在很多时候binlog-format都是mixed 以后会支持这类型么 目前canal版本解析支持mixed/statement/row格式 
116,ArrayIndexOutOfBoundsException: 0  see : https://github.com/alibaba/otter/issues/76 
115,Typo javadoc 修正  更新 guava 到18.0 修正了 embeded -> embedded 部分 javadoc 修正 更新 guava 到18.0 主要是主要是 MapMaker 的 computeMake 在18.0是包级别的 目前没有更换实现 从 Map 更改为 LoadingCache 太麻烦 只是将包级别的方法暴露了出来 还是原来的配方 因此 guava 是可以更换为其他版本的 没有用到新特性. 多谢你的pull request  修正了我的一些低级错误，可能code style和format有一些区别，合并代码后回统一进行格式化，保证内部代码风格的统一，请见谅.  👌 
114,是否可以更新下 Guava 版本 canal 里的是 guava r09 现在开发的都不可能用这么旧的版本 所以无法把 canal 服务嵌入到应用中直接使用 能不能给接受新的版本呢? 如果能接受 我可以改好 然后 pr 目前我的分支已经可以用更新版本的 guava 了 ```         <dependency>             <groupId>com.github.wenerme.canal</groupId>             <artifactId>canal.server</artifactId>             <version>596b060</version>         </dependency>         <repository>             <id>jitpack.io</id>             <url>https://jitpack.io</url>         </repository> ``` Done at 6a0733bbab7b600c0d41afa8a421e9c443ddecbc 
113,add missing api parameter doc com.alibaba.otter.canal.client.CanalConnectors方法修改后没改javadoc，读代码的时候造成了一些疑惑. tks 
112,mysql5.6时间毫秒精度支持 支持下mysql5.6毫秒精度的解析.  ## 测试case： CREATE TABLE `t1` (   `id` int(11) NOT NULL AUTO_INCREMENT   `time0` time DEFAULT NULL   `time1` time(1) DEFAULT NULL   `time2` time(2) DEFAULT NULL   `time3` time(3) DEFAULT NULL   `time4` time(4) DEFAULT NULL   `time5` time(5) DEFAULT NULL   `time6` time(6) DEFAULT NULL   `timestamp0` timestamp NOT NULL DEFAULT '0000-00-00 00:00:00'   `timestamp1` timestamp(1) NOT NULL DEFAULT '0000-00-00 00:00:00.0'   `timestamp2` timestamp(2) NOT NULL DEFAULT '0000-00-00 00:00:00.00'   `timestamp3` timestamp(3) NOT NULL DEFAULT '0000-00-00 00:00:00.000'   `timestamp4` timestamp(4) NOT NULL DEFAULT '0000-00-00 00:00:00.0000'   `timestamp5` timestamp(5) NOT NULL DEFAULT '0000-00-00 00:00:00.00000'   `timestamp6` timestamp(6) NOT NULL DEFAULT '0000-00-00 00:00:00.000000'   `datetime0` datetime DEFAULT NULL   `datetime1` datetime(1) DEFAULT NULL   `datetime2` datetime(2) DEFAULT NULL   `datetime3` datetime(3) DEFAULT NULL   `datetime4` datetime(4) DEFAULT NULL   `datetime5` datetime(5) DEFAULT NULL   `datetime6` datetime(6) DEFAULT NULL   PRIMARY KEY (`id`) ## ) ENGINE=InnoDB AUTO_INCREMENT=7 DEFAULT CHARSET=utf8 测试数据: 1. insert into t1 values(null '00:00:00.0' '00:00:00.1' '00:00:00.02' '00:00:00.003' '00:00:00.1004' '00:00:00.1005' '00:00:00.101016' '2015-03-30 16:42:39.0' '2015-03-30 16:42:39.0' '2015-03-30 16:42:39.01' '2015-03-30 16:42:39.032' '2015-03-30 16:42:39.1023' '2015-03-30 16:42:39.10132' '2015-03-30 16:42:39.121232' '2015-03-30 16:42:39.0' '2015-03-30 16:42:39.0' '2015-03-30 16:42:39.01' '2015-03-30 16:42:39.032' '2015-03-30 16:42:39.1023' '2015-03-30 16:42:39.10132' '2015-03-30 16:42:39.121232'); 2. insert into t1 values(null '16:42:39.0' '16:42:39.1' '16:42:39.02' '16:42:39.003' '16:42:39.1004' '16:42:39.1005' '16:42:39.101016' '2015-03-30 16:42:39.0' '2015-03-30 16:42:39.0' '2015-03-30 16:42:39.01' '2015-03-30 16:42:39.032' '2015-03-30 16:42:39.1023' '2015-03-30 16:42:39.10132' '2015-03-30 16:42:39.121232' '2015-03-30 16:42:39.0' '2015-03-30 16:42:39.0' '2015-03-30 16:42:39.01' '2015-03-30 16:42:39.032' '2015-03-30 16:42:39.1023' '2015-03-30 16:42:39.10132' '2015-03-30 16:42:39.121232') 3. insert into t1 values(null '-16:42:39.0' '-16:42:39.1' '-16:42:39.02' '-16:42:39.003' '16:42:39.1004' '-16:42:39.1005' '-16:42:39.101016' '2015-03-30 16:42:39.0' '2015-03-30 16:42:39.0' '2015-03-30 16:42:39.01' '2015-03-30 16:42:39.032' '2015-03-30 16:42:39.1023' '2015-03-30 16:42:39.10132' '2015-03-30 16:42:39.121232' '2015-03-30 16:42:39.0' '2015-03-30 16:42:39.0' '2015-03-30 16:42:39.01' '2015-03-30 16:42:39.032' '2015-03-30 16:42:39.1023' '2015-03-30 16:42:39.10132' '2015-03-30 16:42:39.121232'); 
111,是否考虑合并模块 目前模块数量太多了 目前是15个模块吧 部分模块内只有很少的内容 不便于开发和阅读 经常都是在模块之间跳来跳去. 暂时不考虑合并，倒是可以考虑打包时出一个类似spring-all的包 
110,FileLogFetcher's fetch() method should fix up position and origin even i... ...f we weren't successful at fetching anything from the file 
109, Could not find first log file name in binary log index file 2015-01-15 10:40:01.756 [destination = example   address = /127.0.0.1:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: Received error packet: errno = 1236  sqlstate = HY000 errmsg = Could not find first log file name in binary log index file         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:116)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209)         at java.lang.Thread.run(Thread.java:662) 我在data目录下看到了mysql-bin.index和mysql-bin.000001 可是报错说找不到index file，我用的是ubuntu的系统，不知道是不是和系统有关系，我在windows下测试没有问题的。 这个应该和你的mysql又关吧，指定一个canal起始位点，如果有问题，可以通过命令验证下mysql binlog show binlog events in 'mysql-bin.000001' from xxx limit 10; 已经解决了 是linux文件权限问题 @NewsGitHub 请问是哪个文件的权限问题，我这边也碰到相同的问题了 @NewsGitHub  亲，给了日志-R 777的权限貌似不行的呢 @changsong  删除canal下面的meta.dat，然后重启一下就行。 这问题的解决没描述清楚啊，我指定了位置点还是不行 我也是卡在这里。在mysql端发现canal这个用户一直在执行Binlog Dump 这个怎么能和权限有关系呢？如果canal读的不是本机呢？
108,新版本编译代码提示内存溢出 parse events has an errorjava.lang.OutOfMemoryError: Java heap space     at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57) ~[na:1.7.0_67]     at java.nio.ByteBuffer.allocate(ByteBuffer.java:331) ~[na:1.7.0_67]     at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.readNextPacket(SimpleCanalConnector.java:364) ~[classes/:na]     at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.doConnect(SimpleCanalConnector.java:129) ~[classes/:na]     at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.connect(SimpleCanalConnector.java:97) ~[classes/:na]     at com.alibaba.otter.canal.example.AbstractCanalClientTest.process(AbstractCanalClientTest.java:110) ~[classes/:na]     at com.alibaba.otter.canal.example.AbstractCanalClientTest$2.run(AbstractCanalClientTest.java:80) ~[classes/:na]     at java.lang.Thread.run(Thread.java:745) ~[na:1.7.0_67] ## stop the canal client## canal client is down. 运行的是mvn？尝试调整下mvn的jvm参数 
107,canal 解析mysql异常 2015-01-07 17:37:51.436 [destination = example   address = /192.168.0.20:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /192.168.0.20:3306 has an error  retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. java.lang.IllegalArgumentException: bad format  x exceed: 2121395706  1000000000 这个是什么问题引起的，mysql是slave，并且binlog不是在默认目录下面 请提供一下mysql版本信息和canal版本信息 mysql5.6.21 canal1.0.20一，在slave库上测试报：2015-01-14 10:19:05.121 [main] WARN  org.springframework.beans.TypeConverterDelegate - PropertyEditor [com.sun.beans.editors.EnumEditor] found through deprecated global PropertyEditorManager fallback - consider using a more isolated form of registration  e.g. on the BeanWrapper/BeanFactory!2015-01-14 10:19:05.231 [main] INFO  c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-example2015-01-14 10:19:05.365 [main] INFO  c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start successful....2015-01-14 10:19:05.366 [destination = example   address = /192.168.0.20:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position by switch ::14207065360002015-01-14 10:19:05.770 [destination = example   address = /192.168.0.20:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - ERROR ## parse this event has an error   last position : [EntryPosition[included=false journalName=mysql-bin.000092 position=59469 timestamp=1421036672000]]com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed.java.lang.IllegalArgumentException: bad format  x exceed: 2016149504  1000000000        at com.taobao.tddl.dbsync.binlog.LogBuffer.getDecimal0(LogBuffer.java:1635) ~[canal.parse.dbsync-1.0.19.jar:na]        at com.taobao.tddl.dbsync.binlog.LogBuffer.getDecimal(LogBuffer.java:1561) ~[canal.parse.dbsync-1.0.19.jar:na]        at com.taobao.tddl.dbsync.binlog.event.RowsLogBuffer.fetchValue(RowsLogBuffer.java:335) ~[canal.parse.dbsync-1.0.19.jar:na]        at com.taobao.tddl.dbsync.binlog.event.RowsLogBuffer.nextValue(RowsLogBuffer.java:95) ~[canal.parse.dbsync-1.0.19.jar:na]        at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseOneRow(LogEventConvert.java:449) ~[canal.parse-1.0.19.jar:na]        at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:364) ~[canal.parse-1.0.19.jar:na]        at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:109) ~[canal.parse-1.0.19.jar:na]        at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:62) ~[canal.parse-1.0.19.jar:na]        at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:323) ~[canal.parse-1.0.19.jar:na]        at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:176) ~[canal.parse-1.0.19.jar:na]        at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:124) [canal.parse-1.0.19.jar:na]        at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) [canal.parse-1.0.19.jar:na]         at java.lang.Thread.run(Thread.java:744) [na:1.7.0_51]二，在master测试报：015-01-14 10:10:28.992 [main] WARN  org.springframework.beans.TypeConverterDelegate - PropertyEditor [com.sun.beans.editors.EnumEditor] found through deprecated global PropertyEditorManager fallback - consider using a more isolated form of registration  e.g. on the BeanWrapper/BeanFactory!2015-01-14 10:10:29.118 [main] INFO  c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-example2015-01-14 10:10:29.278 [main] INFO  c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start successful....2015-01-14 10:10:29.279 [destination = example   address = /192.168.0.21:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position by switch ::14207065360002015-01-14 10:10:29.371 [destination = example   address = /192.168.0.21:3306   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - Didn't find the corresponding binlog files from mysql-bin.000001 to mysql-bin.000002 2015-01-14 10:10:29.376 [destination = example   address = /192.168.0.21:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /192.168.0.21:3306 has an error  retrying. caused bycom.alibaba.otter.canal.parse.exception.CanalParseException: can't find start position for example 李迅（ruson） 上海-hadoop工程师，ELEME IncEmail：xun.li@ele.me | Mobile:18201873319http://ele.me 饿了么                             原始邮件                             发件人: agapplenotifications@github.com收件人: alibaba/canalcanal@noreply.github.com抄送: rusongitxun.li@ele.me发送时间: 2015年1月13日(周二) 18:41主题: Re: [canal] canal 解析mysql异常 (#107)请提供一下mysql版本信息和canal版本信息 —Reply to this email directly or view it on GitHub. mysql-bin.000092 position=59469 timestamp=1421036672000，方便提供一下对应的binlog文件给我不？本地测试了对应的mysql 5.6.22版本的decimal类型，解析是正常的 这是我们线上库，不方便提供binlog，我们库是slave库，是不是这个引起的？李迅（ruson） 上海-hadoop工程师，ELEME IncEmail：xun.li@ele.me | Mobile:18201873319http://ele.me 饿了么                             原始邮件                             发件人: agapplenotifications@github.com收件人: alibaba/canalcanal@noreply.github.com抄送: rusongitxun.li@ele.me发送时间: 2015年1月14日(周三) 15:17主题: Re: [canal] canal 解析mysql异常 (#107)mysql-bin.000092 position=59469 timestamp=1421036672000，方便提供一下对应的binlog文件给我不？本地测试了对应的mysql 5.6.22版本的decimal类型，解析是正常的 —Reply to this email directly or view it on GitHub. 不确定，我试过5.6.22版本是正常的。可以试着在你们测试库中重现一下问题，到时候发我binlog即可 初步怀疑是这类问题，先关闭.  see : https://github.com/alibaba/canal/issues/119 
106,CanalController.java异常处理不规范 **问题描述** 工程引入xerces.jar、xercesImpl.jar时，读取配置文件会出问题，不能正常运行。但是因为CanalController.java（Line:286 ）缺少对异常情况的处理，异常被吞，导致不能输出异常信息，增加排查问题的难度。 **异常信息** ``` org.springframework.beans.factory.BeanDefinitionStoreException: Parser configuration exception parsing XML from class path resource [spring/file-instance.xml]; nested exception is javax.xml.parsers.ParserConfigurationException: Unable to validate using XSD: Your JAXP provider [org.apache.xerces.jaxp.DocumentBuilderFactoryImpl@2e1551b0] does not support XML Schema. Are you running on Java 1.4 with Apache Crimson? Upgrade to Apache Xerces (or Java 1.5) for full XSD support. ``` **建议** 增加try catch，并记录异常日志。 ``` try { // 设置当前正在加载的通道，加载spring查找文件时会用到该变量     System.setProperty(CanalConstants.CANAL_DESTINATION_PROPERTY  destination);     instanceGenerator.setBeanFactory(getBeanFactory(config.getSpringXml()));     instance = instanceGenerator.generate(destination); }catch(Exception e){     if(logger.isErrorEnabled()){         logger.error("generator instance failed." e);     } }finally {     System.setProperty(CanalConstants.CANAL_DESTINATION_PROPERTY  ""); } ``` 已修复 
105,mysql5.6开启checksum后 基于时间查找位点会找到错误的位置 问题描述：在mysql5.6之后版本中，因为引入了checksum机制，会在正常event的末尾加上4字节的checksum，而在做基于时间查找的seek方法中，decoder解析未识别FORMAT格式的event事件，导致没有判断出开启了checksum，所以导致正常的事务头的BEGIN，多了个4个字节，而判断是否为事务头采用了严格的字符串匹配，就因为多了4个字节，导致event类型出错。 影响范围：基于mysql5.6 + 开启了checksum + 涉及时间查找(比如主备切换和基于时间戳启动) 修改: 1.  增加format类型解析 ```  LogDecoder decoder = new LogDecoder();         decoder.handle(LogEvent.ROTATE_EVENT);         decoder.handle(LogEvent.FORMAT_DESCRIPTION_EVENT);         decoder.handle(LogEvent.QUERY_EVENT);         decoder.handle(LogEvent.XID_EVENT);         LogContext context = new LogContext();         while (fetcher.fetch()) {             LogEvent event = null;             event = decoder.decode(fetcher  context);             if (event == null) {                 throw new CanalParseException("parse failed");             }             if (!func.sink(event)) {                 break;             }         } ``` 1. 基于时间查找，增加事务begin/end的多重判断. 13bf7c85a037a9e24d26c8f5c0aa007441fd9ae4 1.  基于时间查找，增加事务begin/end的多重判断. 基于1.0.19版本做canal instance ha  被这个issues 坑了一把，没能找到正确的position 丢了数据; 而且日志里没有异常，后来对比 switch time 和  ha切换后的 binlog timestamp  才发现的。 https://github.com/alibaba/canal/commit/13bf7c85a037a9e24d26c8f5c0aa007441fd9ae4  建议低版本的尽快修复上述patch
104,修复了重复订阅没有真正修改过滤器的问题 我自己只用default-instance.xml里面的配置，group模式的没改 
103,parse解析自动切换失败 原因在于haController里边的eventParser没有赋值。 问题不够明确，麻烦描述下更多的重现该问题的条件 
102,修正插件的声明 tks 
101,设置filterTransactionEntry后，导致MetaManager中client得cursor 得不到更新 在EvenSink中，https://github.com/alibaba/canal/blob/master/sink/src/main/java/com/alibaba/otter/canal/sink/entry/EntryEventSink.java#L77 此处代码会在filterTransactionEntry设置为true的时候过滤到TRANSACTION_BEGIN和END的event。 而在getWithoutAck ->  getEvents -> com/alibaba/otter/canal/store/memory/MemoryEventStoreWithBuffer.java 中，的 doGet 方法中可以看到，设置LogPositionRange 的ack属性是根据这个range中是否存在TRANSACTION_BEGIN 和END的event来进行设置的。 如果设置了filterTransactionEntry 那么这里返回给client的message对应的positionRange的ack为null。 在client确认的时候，服务端执行ack方法： https://github.com/alibaba/canal/blob/master/server/src/main/java/com/alibaba/otter/canal/server/embeded/CanalServerWithEmbeded.java#L348 这里会发现如果ack为null，则不更新meta信息 看了一下代码，虽然你强制更新了cursor，但是没解决positionRanges.getAck()出现null值，如果null值还是会忽略本次更新。这里应该是返回最后一条的数据位点.  ps.  这样的更新还是会有一些问题，因为你位点记录的可能是事务中的第一条更新，下次重启后，重新获取binlog会出现找不到TableId的情况 哦，filterTransaction这个选项当初设计是为了什么情形的？ 发自ChinaXing iPhone > 在 2014年10月10日，18:32，agapple notifications@github.com 写道： >  > 看了一下代码，虽然你强制更新了cursor，但是没解决positionRanges.getAck()出现null值，如果null值还是会忽略本次更新。这里应该是返回最后一条的数据位点. >  > ps. 这样的更新还是会有一些问题，因为你位点记录的可能是事务中的第一条更新，下次重启后，重新获取binlog会出现找不到TableId的情况 >  > — > Reply to this email directly or view it on GitHub. 这个暂时应该没用上，之前主要是考虑group-instance.xml模式，多个库数据合并时忽略事物begin/end，后来做了一个版本可以整个事务一起做合并，这块代码就相当于没用上过，可以忽略。 事务的begin/end，我有一定的机制会忽略空的事务 你有忽略事务begin/end的需求？ 
100,设置filterTransactionEntry后，导致MetaManager中client得cursor 得不到更新 在EvenSink中，https://github.com/alibaba/canal/blob/master/sink/src/main/java/com/alibaba/otter/canal/sink/entry/EntryEventSink.java#L77 此处代码会在filterTransactionEntry设置为true的时候过滤到TRANSACTION_BEGIN和END的event。 而在getWithoutAck ->  getEvents -> com/alibaba/otter/canal/store/memory/MemoryEventStoreWithBuffer.java 中，的 doGet 方法中可以看到，设置LogPositionRange 的ack属性是根据这个range中是否存在TRANSACTION_BEGIN 和END的event来进行设置的。 如果设置了filterTransactionEntry 那么这里返回给client的message对应的positionRange的ack为null。 在client确认的时候，服务端执行ack方法： https://github.com/alibaba/canal/blob/master/server/src/main/java/com/alibaba/otter/canal/server/embeded/CanalServerWithEmbeded.java#L348 这里会发现如果ack为null，则不更新meta信息 
99,重启canal后某些情况下parseEvent失败无法继续 异常如下： 2014-09-23 12:49:09.635 [destination = db2   address = /10.15.2.116:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:db2[com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.TableIdNotFoundException: not found tableId:71 此异常会一直不断重复，导致binlog的消费停留在此LogPosition不往下继续。 触发场景： 停掉canal后启动. 异常情况原因推断是因为重启后遇到的binlog event没有TABLE_MAP_EVENT开头，而恰好直接出现后续的event，导致logContext的table字段未设置 通过查询异常的出现点在： ``` com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert private Entry parseRowsEvent(RowsLogEvent event) {         try {             TableMapLogEvent table = event.getTable();             if (table == null) {                 // tableId对应的记录不存在                 throw new TableIdNotFoundException("not found tableId:" + event.getTableId());             }  ... ``` event的table字段设置： ``` com.taobao.tddl.dbsync.binlog.event.RowsLogEvent public final void fillTable(LogContext context)     {         table = context.getTable(tableId);         // end of statement check:         if ((flags & RowsLogEvent.STMT_END_F) != 0)         {             // Now is safe to clear ignored map (clear_tables will also             // delete original table map events stored in the map).             context.clearAllTables();         }     } ``` ``` com.taobao.tddl.dbsync.binlog.LogContext public final void putTable(TableMapLogEvent mapEvent)     {         mapOfTable.put(Long.valueOf(mapEvent.getTableId())  mapEvent);     } ``` ``` com.taobao.tddl.dbsync.binlog.LogDecoder.decode 方法  case LogEvent.TABLE_MAP_EVENT:             {                 TableMapLogEvent mapEvent = new TableMapLogEvent(header                         buffer  descriptionEvent);                 /* updating position in context */                 logPosition.position = header.getLogPos();                 context.putTable(mapEvent);                 return mapEvent;             } ``` 出现TableIdNotFoundException，主要是你binlog的起始位点不是一个事务开头或者结尾，只能重置下位点 哦。是通过show bin log events ，然后找一个附近的位点？ 2014-09-30 10:11 GMT+08:00 agapple notifications@github.com: > 出现TableIdNotFoundException，主要是你binlog的起始位点不是一个事务开头或者结尾，只能重置下位点 >  > — > Reply to this email directly or view it on GitHub > https://github.com/alibaba/canal/issues/99#issuecomment-57258051. ##  --- Best Regards ! 陈云星  Tel : 1866122992 是的 
98,fix bug in test OS with 64bit (follow symbole link files) 修正探测OS 是否为64bit的bug: 1. 当$JAVA 为符号链接文件时 无法识别 2. `$JAVA_HOME/bin/java  /=  $JAVA` 
97,group 配置实现 你好，我看了group-instance.xml 发现两个eventParser配置了同一个destination，而且下面的master 和standby 只有一个数据库，请问是如何实现N：1 的？ 多谢！ 找到了master 和 standby 配置了两个库。 我这边还有一些疑问： 1. group-instance.xml 中logPosition 使用的时Memory的，这样在故障恢复的时候，position 会变成binlog 的最后位置（show master status），这样的话，会造成一部分bin log 事件丢失。 2. 基于zk 的logPosition 持久化实现。无法使用在group-instance.xml中，因为parser1 和 parser2 都是同一个destination值，在zk中会取到同一份数据。 3. 关于N:1模式多个db，数据顺序性是没法保证的吧。这个有没有什么问题在某些情况下？ group模式的位点记录是基于timestamp的，每个destination只有一份，然后不同的parser基于时间戳做重新定位，重复数据相比于单库模式要多一些 
96,表黑名单默认关闭 目前表黑名单定义针对spring模式，虽然配置了一个空值，针对AviaterRegexFilter空值意味着全匹配，所以所有的数据全部被当作黑名单过滤了 AviaterRegexFilter曾加个属性，特殊处理空值 
95,canal解析将mysql set类型转为unsigned long类型 mysql中set的数据表示是通过bit来描述.   举个例子：比如set('a' 'b 'c ' 'd')，插入一条'a' 'c'的记录，mysql用bit表示就是1010，转化为10进制就是5，注意是litten-endian. 
94,canal/example/src/main/bin/startup.sh ALIBABA_JAVA="/usr/alibaba/java/bin/java" TAOBAO_JAVA="/opt/taobao/java/bin/java" ...   elif [ -f $ALIBABA_JAVA ] ; then     JAVA=$TAOBAO_JAVA ...   elif [ -f $TAOBAO_JAVA ] ; then     JAVA=$TAOBAO_JAVA ... 看到脚本问题了，应该是这样 ... elif [ -f $ALIBABA_JAVA ] ; then JAVA=$ALIBABA_JAVA ... elif [ -f $TAOBAO_JAVA ] ; then JAVA=$TAOBAO_JAVA ... 
93,支持表黑名单定义 <=1.0.17版本，订阅表目前只支持正向匹配，而目前正则表达式对于需要单独排除某张表时不是很方便，希望可以有白名单+黑名单的定义.  策略： 1.  如果只有白名单，那就是只进行正向匹配 2.  如果只有黑名单，那就是只排除黑名单匹配 3.  如果黑+白名单，先做白名单匹配，再做黑名单排除 不配置黑名单的情况下，白名单不工作。 看issue #96 ，已经修复 
92,如果出现解析异常，允许跳过 针对出现列不一致，表不存在等情况，跳过解析异常，并记录对应的数据日志，供人肉处理，默认为false，配合otter使用 错误数据可以通过Alarm的方式报告，比如otter manager 
91,SimpleCanalConnector在竞争的时候会抛should connect first的Exception _Repro_: 1. 使用default-instance模式（有Zookeeper支持）的Canal Server； 2. Setup第一个ClusterCanalClient，调用connect的时候，成功连接； 3. Setup第二个ClusterCanalClient，调用connect的时候，会抛Exception: com.alibaba.otter.canal.protocol.exception.CanalClientException: should connect first         at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.waitClientRunning(SimpleCanalConnector.java:418)         at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.subscribe(SimpleCanalConnector.java:186)         at com.alibaba.otter.canal.client.impl.ClusterCanalConnector.subscribe(ClusterCanalConnector.java:100) _Expected_: 第二个ClusterCanalClient在connect的时候应该被block住，直至其它的client disconnect。 推测原因（见下面的gist修改） https://gist.github.com/caojia/10960073/revisions 你的理解是对的，估计修改另一个bug考虑不全，代码我做了下修改，多谢 谢谢！在connect函数里面调用waitClientRunning的时候是不是还应该传个false的参数? 另外，你们会把这个修改push到maven上么？ 验证通过后会提交到maven仓库 
90, Master挂掉后的恢复 Hi jianghang， 你好！请问如果在Canal的架构中，如果Mysql的Master挂掉后，新的Slave被Promote成新的Master，Canal能够找到binlog在原Master的位置所对应的新的Master的位置么？是否能保证Canal中的Change Event是Exactly Once的？ 谢谢！ 不保证Exactly Once，会有重复数据 
89,客户端不停重连服务端，服务端启动后，客户端连接线程卡死。 当只有一个canal server的时候，客户端不停的重连服务端，当服务端启动后，客户端连接线程会卡死。 问题出现在SimpleCanalConnector的initClientRunningMonitor的方法。    runningMonitor.setListener(new ClientRunningListener() { ```             public InetSocketAddress processActiveEnter() {                 InetSocketAddress address = doConnect();                 mutex.set(true);                 if (rollbackOnConnect) {                     rollback();                 }                 return address;             }             public void processActiveExit() {                 mutex.set(false);                 doDisconnnect();             }         }); ``` 当doConnect出现异常退出后，mutex.set(true)未被执行，抛出的异常会被外层调用函数catch住，这样导致其他函数会卡死在mutex这个锁上。 https://github.com/alibaba/canal/issues/77 和这个issue相似，使用1.0.16+的版本 暂时关闭，有问题再reopen 
88,canal deployer 1.0.14版本，在client端接受到的entry都是TRANSACTION_BEGIN和TRANSACTION_END 具体现象是这样的，在canal的server端启动的一小段时间内接受到的数据是正常的，但是在过大约10分钟之后，客户端接受到的entry的entryType就全部为TRANSACTION_BEGIN和TRANSACTION_END了。 曾经怀疑是我写的客户端有问题，但是把客户端换成示例客户端还是一样的效果。 已经尝试将server端的日志级别设置为debug，但是没有得到什么有价值的调试信息。 麻烦提供一下解决此问题的思路，或者我的server端配置有问题，可能是哪儿的问题，谢谢！！！ 提供server端的instance.properties配置和客户端代码 这个问题的原因找到了。 在instance.properties中配置的数据库名称是小写的，但是mysql上的数据库是大写的，同时配置了大小写敏感，所以大多数的是空的。 但是：：不解的是为什么会消费一段时间，然后就不行了。如果是因为大小写的原因，那应该是一条都没有，这样才合理。 建议：在canal.properties中增加大小写敏感的选项 instance.properties中的表达式，在canal编译为正则表达式时开启了大小写不敏感参数(Perl5Compiler.CASE_INSENSITIVE_MASK)，所以你这个问题推论可能有误 是的。 客户端配置订阅规则的时候配置的问题。subscribe的规则写错了，导致所有的内容都进不来。 客户端的subscribe优先于instance.properties中的配置 同样的问题 `connector.subscribe("actionlog\\d{8}");`我想订阅所有actionlog打头接数字的表，结果没有ROWDATA了，是正则错了，还是哪里错了？ 请教一下想订阅指定的库或者表，这个正则要怎么写？
87,group模式开启了HeartBeat线程，出现数据阻塞 otter issue : https://github.com/alibaba/otter/issues/46 
86,canal中针对group情况不支持非HeartBeat的HA方式 支持下otter issue : https://github.com/alibaba/otter/issues/45 
85,无法设置max_packet_size，可能导致无法同步binlog max_packet_size默认是16m，如果mysql packet大于16m，则canal无法同步binlog。 http://dev.mysql.com/doc/refman/5.5/en/server-system-variables.html#sysvar_max_allowed_packet 需要通过设置session变量来解决 看了下binlog的packet，目前默认是读取3字节做为length 也就是最多只能标示16MB的大小，超过16MB拆分为多个packet，暂时不清楚max_allowed_packet调整后，mysql server端是否会调整netlen的字节数来解决大包的传输.  <pre> // Fetching the first packet(may a multi-packet). int netlen = getUint24(PACKET_LEN_OFFSET); int netnum = getUint8(PACKET_SEQ_OFFSET); </pre> 其实发出来的单个包代码里面用宏写死的，最大就是16M，netlen非压缩包默认就是解析前三个字节作为长度。mysql server端不会根据max_allowed_packet调节这个东西的。 长度控制这一块有好几个层级。max_allowed_packet影响不到最内层的发包大小限制。 MYSQL  读取日志的函数handle_slave_io里面的 read_event会调用my_net_read，而这个函数默认会有一个组包的过程。 所以MYSQL正常是可以拼出长度大于16M的log event的。canal这一块处理可能没有做。 附： ulong my_net_read(NET *net) {   size_t len  complen;   MYSQL_NET_READ_START(); #ifdef HAVE_COMPRESS   if (!net->compress)   { #endif     len= net_read_packet(net  &complen);     if (len == MAX_PACKET_LENGTH)     {       /\* First packet of a multi-packet.  Concatenate the packets _/       ulong save_pos = net->where_b;       size_t total_length= 0;       do       {         net->where_b += len;         total_length += len;         len= net_read_packet(net  &complen);       } while (len == MAX_PACKET_LENGTH);       if (len != packet_error)         len+= total_length;       net->where_b = save_pos;     }     net->read_pos = net->buff + net->where_b;     if (len != packet_error)       net->read_pos[len]=0;     /_ Safeguard for mysql_use_result */     MYSQL_NET_READ_DONE(0  len);     return len; #ifdef HAVE_COMPRESS   } 多些反馈mysql的实现，mysql通过多packet模式来解决大包问题，只需要客户端处理的时候能对packet进行重新组包即可.   目前canal处理逻辑： // Fetching the first packet(may a multi-packet). int netlen = getUint24(PACKET_LEN_OFFSET); int netnum = getUint8(PACKET_SEQ_OFFSET); if (!fetch0(NET_HEADER_SIZE  netlen)) { return false;  } while (netlen == MAX_PACKET_LENGTH) // 针对16MB的packet，继续读取下一个packet {      netlen = getUint24(PACKET_LEN_OFFSET);      netnum = getUint8(PACKET_SEQ_OFFSET);      if (!fetch0(limit  netlen))      { return false; } } 
84,支持mariadb5.5.34/10.0.7版本的binlog解析 支持下maraidb 5/10版本的binlog解析 变化内容： https://github.com/alibaba/canal/wiki/BinlogChange%28MariaDB5%2610%29 
82,Canal会导致ZooKeeper的磁盘IO负载过高。 在测试中我们发现Canal会导致ZooKeeper的IO负载极高，iotop下可以看到ZooKeeper持续以400KB/s的速度向磁盘写入事务数据，因此磁盘IO负载居高不下。我们目前并没有提供独立的服务器来运行ZooKeeper，因此我们希望能够通过降低ZooKeeper的write/update压力，从而减少它的磁盘IO占用率。 按我的理解，Canal应该是每次客户端ACK时都会将最新的binlog偏移量更新入ZooKeeper，这就导致ZooKeeper的事务很多，磁盘IO占用率很大。 我修改了canal.properties中的canal.zookeeper.flush.period配置为5000  希望Canal能每5秒钟更新一次ZooKeeper。但是配置之后，发现它根本不起作用，ZooKeeper的写压力仍然巨大。 之后我监视ZooKeeper的日志发现，绝大部分的写操作并非来自Canal服务器，而是来自于Canal客户端。 为什么Canal客户端会直接高频率地向ZooKeeper服务器写入binlog偏移量？ binlog偏移量的更新或写入不是应该由Canal服务器统一调度吗？ ZooKeeper所在服务器磁盘IO负载情况： > # iostat -xdm 1 >  > Device:         rrqm/s   wrqm/s   r/s   w/s    rMB/s    wMB/s avgrq-sz avgqu-sz   await  svctm  %util > sda             228.00     2.00 442.00 15.00    39.32     0.06   176.47    11.21   24.79   2.19 100.00 > sda1            228.00     2.00 442.00 15.00    39.32     0.06   176.47    11.21   24.79   2.19 100.00 > sda2              0.00     0.00  0.00  0.00     0.00     0.00     0.00     0.00    0.00   0.00   0.00 ZooKeeper的磁盘IO占用情况： > # iotop -p 10411(zookeeper的pid) >  > 10411 be/4 datacent    0.00 B/s  475.99 K/s 52.38 % 48.88 % java -Dzook...oo.cfg 找到问题所在了，不是Canal的问题。 Canal客户端同时也向hbase写入数据，导致zookeeper IO过高是hbase而不是canal。 
81,Canal服务器重启后，SlaveID被强制更新为-1 运行环境：     Zookeeper集群     单Canal服务器（测试）     单Canal客户端（测试）     2个MySQL实例（不断有新binlog） 问题：     某次Canal关闭时stop操作无响应，无奈之下采用kill -9 pid强制关闭canal进程。 ``` 下次启动Canal时抛出“Canal节点已存在”异常，之后再次重启Canal时异常莫名其妙的消失了，应该是因为Zookeeper检测到Canal离线，自动删除了。 一番周折之后，成功启动后的Canal不再读取binlog，反而打印warn日志：prepare to find start position just last position，此时mysql中binlog是持续存在的。 我手动查看zookeeper中存储的canal数据时发现： [zookeeper] get /otter/canal/destinations/{名称}/1001/cursor {"@type":"com.alibaba.otter.canal.protocol.position.LogPosition" "identity":{"slaveId":-1 "sourceAddress":{"address":"..." "port":3310}} "postion":{"included":false "journalName":"localhost-bin.000008" "position":689389260 "timestamp":1388062800000}} 请注意"slaveId":-1，我采用的是Canal默认配置slaveId=1234  现在变为-1真是莫名其妙。我怀疑是Canal出现异常后，自动更改为-1的，于是我手动将此salveID从-1重新改为1234  但是之后再次启动Canal发现此slaveID又被强制改变为-1. 由于slaveid被改变为-1  之后canal就无法读取到binlog数据。 ``` 以下是canal其中一个mysql实例的启动日志（已开启debug级别），希望大家能帮我解决这个问题： 2013-12-27 09:51:57.745 [canal-instance-scan-0] DEBUG o.s.beans.factory.xml.PluggableSchemaResolver - Found XML schema [http://www.springframework.org/schema/beans/spring-beans-2.0.xsd] in classpath: org/springframework/beans/factory/xml/spring-beans-2.0.xsd 2013-12-27 09:51:57.860 [canal-instance-scan-0] DEBUG o.s.b.factory.xml.DefaultBeanDefinitionDocumentReader - Loading bean definitions 2013-12-27 09:51:57.898 [canal-instance-scan-0] DEBUG o.s.beans.factory.xml.BeanDefinitionParserDelegate - Neither XML 'id' nor 'name' specified - using generated bean name [com.alibaba.otter.canal.instance.spring.support.PropertyPlaceholderConfigurer#0] 2013-12-27 09:51:57.903 [canal-instance-scan-0] DEBUG o.s.beans.factory.xml.BeanDefinitionParserDelegate - Neither XML 'id' nor 'name' specified - using generated bean name [org.springframework.beans.factory.config.CustomEditorConfigurer#0] 2013-12-27 09:51:57.907 [canal-instance-scan-0] DEBUG o.s.beans.factory.xml.BeanDefinitionParserDelegate - Neither XML 'id' nor 'name' specified - using generated bean name [com.alibaba.otter.canal.meta.ZooKeeperMetaManager#291792b7] 2013-12-27 09:51:57.911 [canal-instance-scan-0] DEBUG o.s.beans.factory.xml.BeanDefinitionParserDelegate - Neither XML 'id' nor 'name' specified - using generated bean name [com.alibaba.otter.canal.parse.ha.HeartBeatHAController#4aa14174] 2013-12-27 09:51:57.913 [canal-instance-scan-0] DEBUG o.s.beans.factory.xml.BeanDefinitionParserDelegate - Neither XML 'id' nor 'name' specified - using generated bean name [com.alibaba.otter.canal.filter.aviater.AviaterRegexFilter#55d580a8] 2013-12-27 09:51:57.915 [canal-instance-scan-0] DEBUG o.s.beans.factory.xml.BeanDefinitionParserDelegate - Neither XML 'id' nor 'name' specified - using generated bean name [com.alibaba.otter.canal.parse.index.MemoryLogPositionManager#33f1c19e] 2013-12-27 09:51:57.915 [canal-instance-scan-0] DEBUG o.s.beans.factory.xml.BeanDefinitionParserDelegate - Neither XML 'id' nor 'name' specified - using generated bean name [com.alibaba.otter.canal.parse.index.MetaLogPositionManager#44de86b6] 2013-12-27 09:51:57.915 [canal-instance-scan-0] DEBUG o.s.beans.factory.xml.BeanDefinitionParserDelegate - Neither XML 'id' nor 'name' specified - using generated bean name [com.alibaba.otter.canal.parse.index.FailbackLogPositionManager#6d5e3a0c] 2013-12-27 09:51:57.916 [canal-instance-scan-0] DEBUG o.s.beans.factory.xml.BeanDefinitionParserDelegate - Neither XML 'id' nor 'name' specified - using generated bean name [com.alibaba.otter.canal.parse.support.AuthenticationInfo#453521ec] 2013-12-27 09:51:57.917 [canal-instance-scan-0] DEBUG o.s.beans.factory.xml.BeanDefinitionParserDelegate - Neither XML 'id' nor 'name' specified - using generated bean name [com.alibaba.otter.canal.parse.support.AuthenticationInfo#23463073] 2013-12-27 09:51:57.918 [canal-instance-scan-0] DEBUG o.s.beans.factory.xml.BeanDefinitionParserDelegate - Neither XML 'id' nor 'name' specified - using generated bean name [com.alibaba.otter.canal.protocol.position.EntryPosition#5516e01c] 2013-12-27 09:51:57.919 [canal-instance-scan-0] DEBUG o.s.beans.factory.xml.BeanDefinitionParserDelegate - Neither XML 'id' nor 'name' specified - using generated bean name [com.alibaba.otter.canal.protocol.position.EntryPosition#50d379e8] 2013-12-27 09:51:57.920 [canal-instance-scan-0] DEBUG o.s.beans.factory.xml.XmlBeanDefinitionReader - Loaded 10 bean definitions from location pattern [classpath:spring/default-instance.xml] 2013-12-27 09:51:57.920 [canal-instance-scan-0] INFO  o.s.context.support.ClassPathXmlApplicationContext - Bean factory for application context [org.springframework.context.support.ClassPathXmlApplicationContext@344d638a]: org.springframework.beans.factory.support.DefaultListableBeanFactory@6be897cc 2013-12-27 09:51:57.920 [canal-instance-scan-0] DEBUG o.s.context.support.ClassPathXmlApplicationContext - 10 beans defined in org.springframework.context.support.ClassPathXmlApplicationContext@344d638a: display name [org.springframework.context.support.ClassPathXmlApplicationContext@344d638a]; startup date [Fri Dec 27 09:51:57 CST 2013]; root of context hierarchy 2013-12-27 09:51:57.980 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating shared instance of singleton bean 'com.alibaba.otter.canal.instance.spring.support.PropertyPlaceholderConfigurer#0' 2013-12-27 09:51:57.981 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating instance of bean 'com.alibaba.otter.canal.instance.spring.support.PropertyPlaceholderConfigurer#0' 2013-12-27 09:51:58.021 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Eagerly caching bean 'com.alibaba.otter.canal.instance.spring.support.PropertyPlaceholderConfigurer#0' to allow for resolving potential circular references 2013-12-27 09:51:58.066 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Invoking afterPropertiesSet() on bean with name 'com.alibaba.otter.canal.instance.spring.support.PropertyPlaceholderConfigurer#0' 2013-12-27 09:51:58.067 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Finished creating instance of bean 'com.alibaba.otter.canal.instance.spring.support.PropertyPlaceholderConfigurer#0' 2013-12-27 09:51:58.069 [canal-instance-scan-0] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2013-12-27 09:51:58.070 [canal-instance-scan-0] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [war_wist_ios_cn_h17_ww2/instance.properties] 2013-12-27 09:51:58.138 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating shared instance of singleton bean 'org.springframework.beans.factory.config.CustomEditorConfigurer#0' 2013-12-27 09:51:58.138 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating instance of bean 'org.springframework.beans.factory.config.CustomEditorConfigurer#0' 2013-12-27 09:51:58.139 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Eagerly caching bean 'org.springframework.beans.factory.config.CustomEditorConfigurer#0' to allow for resolving potential circular references 2013-12-27 09:51:58.143 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating shared instance of singleton bean 'socketAddressEditor' 2013-12-27 09:51:58.144 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating instance of bean 'socketAddressEditor' 2013-12-27 09:51:58.144 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Eagerly caching bean 'socketAddressEditor' to allow for resolving potential circular references 2013-12-27 09:51:58.157 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Finished creating instance of bean 'socketAddressEditor' 2013-12-27 09:51:58.169 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Finished creating instance of bean 'org.springframework.beans.factory.config.CustomEditorConfigurer#0' 2013-12-27 09:51:58.174 [canal-instance-scan-0] DEBUG o.s.context.support.ClassPathXmlApplicationContext - Unable to locate MessageSource with name 'messageSource': using default [org.springframework.context.support.DelegatingMessageSource@d170baf] 2013-12-27 09:51:58.177 [canal-instance-scan-0] DEBUG o.s.context.support.ClassPathXmlApplicationContext - Unable to locate ApplicationEventMulticaster with name 'applicationEventMulticaster': using default [org.springframework.context.event.SimpleApplicationEventMulticaster@5ee5c122] 2013-12-27 09:51:58.180 [canal-instance-scan-0] INFO  o.s.beans.factory.support.DefaultListableBeanFactory - Pre-instantiating singletons in org.springframework.beans.factory.support.DefaultListableBeanFactory@6be897cc: defining beans [com.alibaba.otter.canal.instance.spring.support.PropertyPlaceholderConfigurer#0 socketAddressEditor org.springframework.beans.factory.config.CustomEditorConfigurer#0 instance alarmHandler zkClientx metaManager eventStore eventSink eventParser]; root of factory hierarchy 2013-12-27 09:51:58.180 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Returning cached instance of singleton bean 'com.alibaba.otter.canal.instance.spring.support.PropertyPlaceholderConfigurer#0' 2013-12-27 09:51:58.180 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Returning cached instance of singleton bean 'socketAddressEditor' 2013-12-27 09:51:58.180 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Returning cached instance of singleton bean 'org.springframework.beans.factory.config.CustomEditorConfigurer#0' 2013-12-27 09:51:58.180 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating shared instance of singleton bean 'instance' 2013-12-27 09:51:58.181 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating instance of bean 'instance' 2013-12-27 09:51:58.183 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Eagerly caching bean 'instance' to allow for resolving potential circular references 2013-12-27 09:51:58.215 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating shared instance of singleton bean 'eventParser' 2013-12-27 09:51:58.215 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating instance of bean 'eventParser' 2013-12-27 09:51:58.225 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Eagerly caching bean 'eventParser' to allow for resolving potential circular references 2013-12-27 09:51:58.246 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating shared instance of singleton bean 'eventSink' 2013-12-27 09:51:58.246 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating instance of bean 'eventSink' 2013-12-27 09:51:58.249 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Eagerly caching bean 'eventSink' to allow for resolving potential circular references 2013-12-27 09:51:58.260 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating shared instance of singleton bean 'eventStore' 2013-12-27 09:51:58.260 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating instance of bean 'eventStore' 2013-12-27 09:51:58.264 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Eagerly caching bean 'eventStore' to allow for resolving potential circular references 2013-12-27 09:51:58.283 [canal-instance-scan-0] DEBUG org.springframework.beans.BeanUtils - No property editor [com.alibaba.otter.canal.store.model.BatchModeEditor] found for type com.alibaba.otter.canal.store.model.BatchMode according to 'Editor' suffix convention 2013-12-27 09:51:58.285 [canal-instance-scan-0] WARN  org.springframework.beans.TypeConverterDelegate - PropertyEditor [com.sun.beans.editors.EnumEditor] found through deprecated global PropertyEditorManager fallback - consider using a more isolated form of registration  e.g. on the BeanWrapper/BeanFactory! 2013-12-27 09:51:58.286 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Finished creating instance of bean 'eventStore' 2013-12-27 09:51:58.286 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Finished creating instance of bean 'eventSink' 2013-12-27 09:51:58.287 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Added autowiring by name from bean name 'eventParser' via property 'eventSink' to bean named 'eventSink' 2013-12-27 09:51:58.288 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating instance of bean 'com.alibaba.otter.canal.parse.ha.HeartBeatHAController#4aa14174' 2013-12-27 09:51:58.296 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Finished creating instance of bean 'com.alibaba.otter.canal.parse.ha.HeartBeatHAController#4aa14174' 2013-12-27 09:51:58.296 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating shared instance of singleton bean 'alarmHandler' 2013-12-27 09:51:58.297 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating instance of bean 'alarmHandler' 2013-12-27 09:51:58.297 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Eagerly caching bean 'alarmHandler' to allow for resolving potential circular references 2013-12-27 09:51:58.302 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Finished creating instance of bean 'alarmHandler' 2013-12-27 09:51:58.303 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating instance of bean 'com.alibaba.otter.canal.filter.aviater.AviaterRegexFilter#55d580a8' 2013-12-27 09:51:58.466 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Finished creating instance of bean 'com.alibaba.otter.canal.filter.aviater.AviaterRegexFilter#55d580a8' 2013-12-27 09:51:58.467 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating instance of bean 'com.alibaba.otter.canal.parse.index.FailbackLogPositionManager#6d5e3a0c' 2013-12-27 09:51:58.476 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating instance of bean 'com.alibaba.otter.canal.parse.index.MemoryLogPositionManager#33f1c19e' 2013-12-27 09:51:58.483 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Finished creating instance of bean 'com.alibaba.otter.canal.parse.index.MemoryLogPositionManager#33f1c19e' 2013-12-27 09:51:58.484 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating instance of bean 'com.alibaba.otter.canal.parse.index.MetaLogPositionManager#44de86b6' 2013-12-27 09:51:58.491 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating shared instance of singleton bean 'metaManager' 2013-12-27 09:51:58.491 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating instance of bean 'metaManager' 2013-12-27 09:51:58.493 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Eagerly caching bean 'metaManager' to allow for resolving potential circular references 2013-12-27 09:51:58.504 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating instance of bean 'com.alibaba.otter.canal.meta.ZooKeeperMetaManager#291792b7' 2013-12-27 09:51:58.511 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating shared instance of singleton bean 'zkClientx' 2013-12-27 09:51:58.511 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating instance of bean 'zkClientx' 2013-12-27 09:51:58.512 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Eagerly caching bean 'zkClientx' to allow for resolving potential circular references 2013-12-27 09:51:58.523 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Invoking afterPropertiesSet() on bean with name 'zkClientx' 2013-12-27 09:51:58.523 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Finished creating instance of bean 'zkClientx' 2013-12-27 09:51:58.525 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Finished creating instance of bean 'com.alibaba.otter.canal.meta.ZooKeeperMetaManager#291792b7' 2013-12-27 09:51:58.525 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Finished creating instance of bean 'metaManager' 2013-12-27 09:51:58.526 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Finished creating instance of bean 'com.alibaba.otter.canal.parse.index.MetaLogPositionManager#44de86b6' 2013-12-27 09:51:58.526 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Finished creating instance of bean 'com.alibaba.otter.canal.parse.index.FailbackLogPositionManager#6d5e3a0c' 2013-12-27 09:51:58.526 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating instance of bean 'com.alibaba.otter.canal.parse.support.AuthenticationInfo#453521ec' 2013-12-27 09:51:58.531 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Finished creating instance of bean 'com.alibaba.otter.canal.parse.support.AuthenticationInfo#453521ec' 2013-12-27 09:51:58.531 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating instance of bean 'com.alibaba.otter.canal.parse.support.AuthenticationInfo#23463073' 2013-12-27 09:51:58.532 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Finished creating instance of bean 'com.alibaba.otter.canal.parse.support.AuthenticationInfo#23463073' 2013-12-27 09:51:58.532 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating instance of bean 'com.alibaba.otter.canal.protocol.position.EntryPosition#5516e01c' 2013-12-27 09:51:58.541 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Finished creating instance of bean 'com.alibaba.otter.canal.protocol.position.EntryPosition#5516e01c' 2013-12-27 09:51:58.541 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating instance of bean 'com.alibaba.otter.canal.protocol.position.EntryPosition#50d379e8' 2013-12-27 09:51:58.542 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Finished creating instance of bean 'com.alibaba.otter.canal.protocol.position.EntryPosition#50d379e8' 2013-12-27 09:51:58.543 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Finished creating instance of bean 'eventParser' 2013-12-27 09:51:58.543 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Returning cached instance of singleton bean 'eventSink' 2013-12-27 09:51:58.543 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Returning cached instance of singleton bean 'eventStore' 2013-12-27 09:51:58.543 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Returning cached instance of singleton bean 'metaManager' 2013-12-27 09:51:58.544 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Returning cached instance of singleton bean 'alarmHandler' 2013-12-27 09:51:58.544 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Finished creating instance of bean 'instance' 2013-12-27 09:51:58.544 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Returning cached instance of singleton bean 'alarmHandler' 2013-12-27 09:51:58.544 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Returning cached instance of singleton bean 'zkClientx' 2013-12-27 09:51:58.545 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Returning cached instance of singleton bean 'metaManager' 2013-12-27 09:51:58.545 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Returning cached instance of singleton bean 'eventStore' 2013-12-27 09:51:58.545 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Returning cached instance of singleton bean 'eventSink' 2013-12-27 09:51:58.545 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Returning cached instance of singleton bean 'eventParser' 2013-12-27 09:51:58.546 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Returning cached instance of singleton bean 'instance' 2013-12-27 09:51:58.546 [canal-instance-scan-0] INFO  c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-war_wist_ios_cn_h17_ww2  2013-12-27 09:51:58.577 [destination = war_wist_ios_cn_h17_ww2   address = /118.26.235.88:3310   EventParser] INFO  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - start heart beat....  2013-12-27 09:51:58.578 [destination = war_wist_ios_cn_h17_ww2   address = /118.26.235.88:3310   EventParser] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - connect MysqlConnection to /118.26.235.88:3310... 2013-12-27 09:51:58.596 [canal-instance-scan-0] INFO  c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start successful.... 2013-12-27 09:51:58.596 [canal-instance-scan-0] INFO  c.a.otter.canal.server.embeded.CanalServerWithEmbeded - start CanalInstances[war_wist_ios_cn_h17_ww2] successfully 2013-12-27 09:52:09.329 [destination = war_wist_ios_cn_h17_ww2   address = /118.26.235.88:3310   EventParser] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - handshake initialization packet received  prepare the client authentication packet to send 2013-12-27 09:52:09.338 [destination = war_wist_ios_cn_h17_ww2   address = /118.26.235.88:3310   EventParser] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - client authentication packet is sent out. 2013-12-27 09:52:09.355 [destination = war_wist_ios_cn_h17_ww2   address = /118.26.235.88:3310   EventParser] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - connect MysqlConnection to /118.26.235.88:3310... 2013-12-27 09:52:19.509 [destination = war_wist_ios_cn_h17_ww2   address = /118.26.235.88:3310   EventParser] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - handshake initialization packet received  prepare the client authentication packet to send 2013-12-27 09:52:19.510 [destination = war_wist_ios_cn_h17_ww2   address = /118.26.235.88:3310   EventParser] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - client authentication packet is sent out. 2013-12-27 09:52:19.541 [destination = war_wist_ios_cn_h17_ww2   address = /118.26.235.88:3310   EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position 2013-12-27 09:52:19.580 [destination = war_wist_ios_cn_h17_ww2   address = /118.26.235.88:3310   EventParser] INFO  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=localhost-bin.000008 position=679706051 timestamp=1388060804000] 2013-12-27 09:52:19.580 [destination = war_wist_ios_cn_h17_ww2   address = /118.26.235.88:3310   EventParser] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - disConnect MysqlConnection to /118.26.235.88:3310... 2013-12-27 09:52:19.580 [destination = war_wist_ios_cn_h17_ww2   address = /118.26.235.88:3310   EventParser] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - connect MysqlConnection to /118.26.235.88:3310... 2013-12-27 09:52:29.239 [destination = war_wist_ios_cn_h17_ww2   address = /118.26.235.88:3310   EventParser] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - handshake initialization packet received  prepare the client authentication packet to send 2013-12-27 09:52:29.240 [destination = war_wist_ios_cn_h17_ww2   address = /118.26.235.88:3310   EventParser] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - client authentication packet is sent out. 2013-12-27 09:52:29.244 [destination = war_wist_ios_cn_h17_ww2   address = /118.26.235.88:3310   EventParser] DEBUG c.a.otter.canal.parse.driver.mysql.MysqlUpdateExecutor - read update result... 2013-12-27 09:52:29.748 [destination = war_wist_ios_cn_h17_ww2   address = /118.26.235.88:3310   EventParser] DEBUG c.a.otter.canal.parse.driver.mysql.MysqlUpdateExecutor - read update result... 2013-12-27 09:52:29.830 [destination = war_wist_ios_cn_h17_ww2   address = /118.26.235.88:3310   EventParser] DEBUG c.a.otter.canal.parse.driver.mysql.MysqlUpdateExecutor - read update result... 2013-12-27 09:52:29.831 [destination = war_wist_ios_cn_h17_ww2   address = /118.26.235.88:3310   EventParser] DEBUG c.a.otter.canal.parse.driver.mysql.MysqlUpdateExecutor - read update result... 2013-12-27 09:52:29.957 [destination = war_wist_ios_cn_h17_ww2   address = /118.26.235.88:3310   EventParser] DEBUG c.a.otter.canal.parse.driver.mysql.MysqlUpdateExecutor - read update result... 2013-12-27 09:52:30.053 [destination = war_wist_ios_cn_h17_ww2   address = /118.26.235.88:3310   EventParser] INFO  c.a.otter.canal.parse.inbound.mysql.MysqlConnection - COM_BINLOG_DUMP with position:BinlogDumpCommandPacket[binlogPosition=679706051 slaveServerId=1234 binlogFileName=localhost-bin.000008 command=18] 2013-12-27 09:52:30.346 [destination = war_wist_ios_cn_h17_ww2   address = /118.26.235.88:3310   EventParser] INFO  com.taobao.tddl.dbsync.binlog.LogEvent - common_header_len= 19  number_of_event_types= 27 2013-12-27 09:52:50.072 [New I/O server worker #1-2] INFO  c.a.otter.canal.server.embeded.CanalServerWithEmbeded - rollback successfully  clientId:1001 2013-12-27 09:52:50.073 [New I/O server worker #1-2] INFO  c.a.otter.canal.server.embeded.CanalServerWithEmbeded - subscribe successfully  use last cursor position:ClientIdentity[destination=war_wist_ios_cn_h17_ww2 clientId=1001 filter=._.._]  2013-12-27 09:52:50.115 [New I/O server worker #1-2] INFO  c.a.otter.canal.server.embeded.CanalServerWithEmbeded - rollback successfully  clientId:1001 2013-12-27 09:52:50.118 [New I/O server worker #1-2] INFO  c.a.otter.canal.server.embeded.CanalServerWithEmbeded - getWithoutAck successfully  clientId:1001 batchSize:1000  real size is 619 and result is [batchId:1   position:PositionRange[start=LogPosition[identity=LogIdentity[sourceAddress=/118.26.235.88:3310 slaveId=-1] postion=EntryPosition[included=false journalName=localhost-bin.000008 position=679706051 timestamp=1388060804000]] ack=LogPosition[identity=LogIdentity[sourceAddress=/118.26.235.88:3310 slaveId=-1] postion=EntryPosition[included=false journalName=localhost-bin.000008 position=689389260 timestamp=1388062800000]] end=LogPosition[identity=LogIdentity[sourceAddress=/118.26.235.88:3310 slaveId=-1] postion=EntryPosition[included=false journalName=localhost-bin.000008 position=689389260 timestamp=1388062800000]]]] 2013-12-27 09:52:50.240 [New I/O server worker #1-2] INFO  c.a.otter.canal.server.embeded.CanalServerWithEmbeded - ack successfully  clientId:1001 batchId:1 position:PositionRange[start=LogPosition[identity=LogIdentity[sourceAddress=/118.26.235.88:3310 slaveId=-1] postion=EntryPosition[included=false journalName=localhost-bin.000008 position=679706051 timestamp=1388060804000]] ack=LogPosition[identity=LogIdentity[sourceAddress=/118.26.235.88:3310 slaveId=-1] postion=EntryPosition[included=false journalName=localhost-bin.000008 position=689389260 timestamp=1388062800000]] end=LogPosition[identity=LogIdentity[sourceAddress=/118.26.235.88:3310 slaveId=-1] postion=EntryPosition[included=false journalName=localhost-bin.000008 position=689389260 timestamp=1388062800000]]] 2013-12-27 09:52:50.248 [New I/O server worker #1-2] DEBUG c.a.otter.canal.server.embeded.CanalServerWithEmbeded - getWithoutAck successfully  clientId:1001 batchSize:1000 but result is null 2013-12-27 09:52:51.291 [New I/O server worker #1-2] DEBUG c.a.otter.canal.server.embeded.CanalServerWithEmbeded - getWithoutAck successfully  clientId:1001 batchSize:1000 but result is null 2013-12-27 09:52:52.335 [New I/O server worker #1-2] DEBUG c.a.otter.canal.server.embeded.CanalServerWithEmbeded - getWithoutAck successfully  clientId:1001 batchSize:1000 but result is null 2013-12-27 09:52:53.380 [New I/O server worker #1-2] INFO  c.a.otter.canal.server.embeded.CanalServerWithEmbeded - getWithoutAck successfully  clientId:1001 batchSize:1000  real size is 327 and result is [batchId:2   position:PositionRange[start=LogPosition[identity=LogIdentity[sourceAddress=/118.26.235.88:3310 slaveId=-1] postion=EntryPosition[included=false journalName=localhost-bin.000008 position=696177601 timestamp=1388062803000]] ack=LogPosition[identity=LogIdentity[sourceAddress=/118.26.235.88:3310 slaveId=-1] postion=EntryPosition[included=false journalName=localhost-bin.000008 position=696612594 timestamp=1388062803000]] end=LogPosition[identity=LogIdentity[sourceAddress=/118.26.235.88:3310 slaveId=-1] postion=EntryPosition[included=false journalName=localhost-bin.000008 position=696612594 timestamp=1388062803000]]]] PS：我采用的Canal版本为1.0.15。 解释几个问题： 1.  prepare to find start position just last position.    这里是正常的，当前启动是从上一次的位点继续，这里的last position理解为最后一次的消费位点 2.  kill -9强制关闭后，抛出“Canal节点已存在”异常后自动删除，这个理解是对的，是zookeeper server有个sessionTimeout时间，超过这个时间后才会自动删除节点，正常退出时canal会自己删除，所以不会有问题 3.  slaveId = -1，这个是目前已知的，代码里本身就没有把正确的slaveId写到zookeeper中，每次都会从canal.properties中读取slaveId，这个不影响 非常感谢agapple的解释～ 我想借此机会再咨询一个Canal导致zookeeper的IO负载过高的问题。 在测试中我们发现Canal会导致ZooKeeper的IO负载相当的高，iotop下可以看到ZooKeeper持续以400KB/s的速度向磁盘写入事务数据，导致磁盘IO负载居高不下。我们目前并没有提供独立的服务器来运行ZooKeeper，因此我们希望能够通过降低ZooKeeper的write/update压力，从而减少它的磁盘IO占用率。 按我的理解，Canal应该是每次客户端ACK时都会将最新的binlog偏移量更新入ZooKeeper，这就导致ZooKeeper的事务很多，磁盘IO占用率很大。 请问有没有什么办法让Canal每隔一段时间再更新一次binlog偏移量，我们希望通过这种办法降低ZooKeeper的写压力。配置中的canal.zookeeper.flush.period好像不起作用，将它配置为5000后，ZooKeeper的写压力仍然巨大。 @agapple 哈喽，麻烦您帮我看看楼上的问题哈。 忽略楼上吧，导致ZooKeeper的IO负载过高不是Canal的问题。 
80,优化下should start first异常 比如出现如下异常： <pre>  failed to subscribe with reason: something goes wrong with channel:[id: 0x01039fc0  /192.168.22.153:15686 => /192.168.22.19:11111]  exception=com.alibaba.otter.canal.server.exception.CanalServerException: destination:example should start first </pre> 大致原因：canal server发生一次切换，比如在上一个节点刚好在执行getWithoutAck()，切到下一个节点后，如果继续执行getWithoutAck()，因为未进行subscribe()调用，会出现server未初始化，调用getWithoutAck()就会出现should start first. 优化方式：缓存上一次的filter条件，在下一次自动切换时，顺序调用disconnnect/connect/subscribe(filter)，保证新的canal server完成初始化后，再调用getWithoutAck()获取数据 
79,多表ddl的sql tableName解析失败 <pre> DROP /*!40005 TEMPORARY */ TABLE IF EXISTS `temp_bond_keys` `temp_bond_key_id`  RENAME TABLE A TO B   C TO D . </pre> 目前已知的这两种ddl会存在多表操作，目前暂不支持多表解析.   后续完美解决：使用类似druid/cobar的mysql sql语法解析，提取对应的表.  ddl不能正常解析出表名，存在的影响： 1.  ddl不能正常同步.  2.  ddl语句不会发送给canal client，因为不满足对应的过滤条件，但不会阻塞整个解析 先临时方案解决，后续统一修改为SQL解析器进行优化 
78,default-instance.xml模式下，客户端订阅的filter条件在canal server切换后会丢失filter  ZooKeeperMetaManager的listAllSubscribeInfo方法中只读取了clientId信息，并没有读取对应的filter，导致canal client提交的信息在canal server下一次启动时并没有载入，导致切换后客户端提交的filter并未生效.   ps . MemoryMetaManager/FileMetaManager模式下正常.  <pre>   String path = ZookeeperPathUtils.getDestinationPath(destination);         List<String> childs = null;         try {             childs = zkClientx.getChildren(path);         } catch (ZkNoNodeException e) {             // ignore         }         if (CollectionUtils.isEmpty(childs)) {             return new ArrayList<ClientIdentity>();         }         List<Short> clientIds = new ArrayList<Short>();         for (String child : childs) {             if (StringUtils.isNumeric(child)) {                 clientIds.add(ZookeeperPathUtils.getClientId(child));             }         }         Collections.sort(clientIds); // 进行一个排序         List<ClientIdentity> clientIdentities = Lists.newArrayList();         for (Short clientId : clientIds) {             clientIdentities.add(new ClientIdentity(destination  clientId));         }         return clientIdentities; } </pre> 
77,canal server宕掉后，canal client有可能切不到备用server上 canal server 通过ZK做集群，当一个server宕掉后，如果ZK上临时节点没有消失，则canal client有可能会一直阻塞。 Caused by: java.net.ConnectException: Connection refused: connect     at sun.nio.ch.Net.connect(Native Method) ~[na:1.6.0_45]     at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:532) ~[na:1.6.0_45]     at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.doConnect(SimpleCanalConnector.java:117) [canal.client-1.0.9.jar:na]     ... 14 common frames omitted [2013-12-13 21:40:47  INFO com.alibaba.otter.canal.client.impl.ClusterCanalConnector:167] [shared-pool-thread-1] restart the connector for next round retry. 不再拦截异常，connect失败后会直接抛出异常，如果是cluster模式，会自动调用restart就可以得到重试。如果是simple模式，需要手工调用disconnect后进行connect.  server running的也存在这个风险，只不过因为没有外部依赖，出现异常的可能性小，唯一出错的就是和zk之间的网络链接，所以一旦临时节点创建后，基本都是本地类初始化的动作，基本不会出错.  
76,canal心跳检查出现ArrayIndexOutOfBoundsException <pre> Exception in thread "destination = transfer1   address = /192.168.237.26:3306   MysqlHeartBeatTimeTask" java.lang.ArrayIndexOutOfBoundsException: 1 at com.alibaba.otter.canal.parse.driver.mysql.utils.ByteHelper.readBinaryCodedLengthBytes(ByteHelper.java:83) at com.alibaba.otter.canal.parse.driver.mysql.packets.server.OKPacket.fromBytes(OKPacket.java:44) at com.alibaba.otter.canal.parse.driver.mysql.MysqlUpdateExecutor.update(MysqlUpdateExecutor.java:53) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.update(MysqlConnection.java:73) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser$MysqlHeartBeatTimeTask.run(MysqlEventParser.java:208) at java.util.TimerThread.mainLoop(Timer.java:512) at java.util.TimerThread.run(Timer.java:462) </pre> 对应的心跳检查sql为; show master status; 主要原因为：mysql的协议区分为query/update两类，对应的数据包解析不一样，这个问题就是错误的将show master status使用了update协议进行解析，导致出错，使用query协议即可 
75,heartbeat心跳检查程序出现Timer already cancelled. <pre> 2013-11-26 15:33:28.895 [destination = inter_ttsdb1   address = /192.168.24.130:3306   EventParser] WARN  c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - Received EOF packet from server  apparent master disconnected. 2013-11-26 15:33:28.895 [destination = inter_ttsdb1   address = /192.168.24.130:3306   EventParser] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - disConnect MysqlConnection to /192.168.24.130:3306... 2013-11-26 15:33:28.896 [destination = inter_ttsdb1   address = /192.168.24.130:3306   EventParser] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - disConnect MysqlConnection to /192.168.24.130:3306... 2013-11-26 15:33:42.819 [destination = inter_ttsdb1   address = /192.168.24.130:3306   EventParser] INFO  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - start heart beat....  2013-11-26 15:33:42.825 [destination = inter_ttsdb1   address = /192.168.24.130:3306   EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /192.168.24.130:3306 has an error  retrying. caused by  java.lang.IllegalStateException: Timer already cancelled.     at java.util.Timer.sched(Timer.java:354) ~[na:1.6.0_20]     at java.util.Timer.schedule(Timer.java:222) ~[na:1.6.0_20]     at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.startHeartbeat(MysqlEventParser.java:153) ~[canal.parse-1.0.7.jar:na]     at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:83) ~[canal.parse-1.0.7.jar:na]     at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:143) ~[canal.parse-1.0.7.jar:na]     at java.lang.Thread.run(Thread.java:619) [na:1.6.0_20] 2013-11-26 15:33:42.826 [destination = inter_ttsdb1   address = /192.168.24.130:3306   EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:inter_ttsdb1[java.lang.IllegalStateException: Timer already cancelled.     at java.util.Timer.sched(Timer.java:354)     at java.util.Timer.schedule(Timer.java:222)     at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.startHeartbeat(MysqlEventParser.java:153)     at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:83)     at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:143)     at java.lang.Thread.run(Thread.java:619) ] 2013-11-26 15:33:42.826 [destination = inter_ttsdb1   address = /192.168.24.130:3306   EventParser] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - the channel /192.168.24.130:3306 is not connected 2013-11-26 15:33:42.826 [destination = inter_ttsdb1   address = /192.168.24.130:3306   EventParser] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - the channel /192.168.24.130:3306 is not connected </pre> 异常问题分析： 1.  首先是出现mysql异常关闭，导致binlog dump和meta的mysql链接都出现断开 2.  MysqlHeartBeatTimeTask在下一次timer调度中，会出现异常，目前try...catch的异常只包含了SocketTimeoutException/IOException，如果出现其他，比如RuntimeException，就会导致Timer退出，并设置为cancel状态，触发本issue的bug.  
74,mysql create table语句解析错误 <pre> public static final String TABLE_PATTERN        = "^(IF\\s*NOT\\s*EXIST\\s*)?(IF\\s*EXIST\\s*)?(`?.+?`?[;\\(\\s]+?)?.*$"; // 采用非贪婪模式 </pre> 在mysql语法里正确为EXISTS 
73,EntryProtocol增加mysql type类型，用于客户自定义处理 <pre> /**每个字段的数据结构**/ message Column {     /**字段下标**/     optional int32      index           =       1;          /**字段java中类型**/     optional int32      sqlType         =       2;          /**字段名称(忽略大小写)，在mysql中是没有的**/     optional string     name            =       3;          /**是否是主键**/     optional bool       isKey           =       4;          /**如果EventType=UPDATE 用于标识这个字段值是否有修改**/     optional bool       updated         =       5;          /** 标识是否为空  **/     optional bool       isNull          =       6 [default = false];          /**预留扩展**/     repeated Pair       props           =       7;            /** 字段值 timestamp Datetime是一个时间格式的文本 **/     optional string     value           =       8;          /** 对应数据对象原始长度 **/     optional int32      length          =       9;          /**字段mysql类型**/     optional string     mysqlType       =       10; } </pre> 字段信息：包含sqlType和mysqlType sqlType是经过canal转换处理的，比如unsigned int会被转化为Long，unsigned long会被转换为BigDecimal. mysqlType是原始mysql中的字段文本描述，可以通过desc xxx来获取  输出结果： <pre> ID : 38    type=int(10) unsigned ADDRESS : hello    type=varchar(32) EVENT_DATA : 2013-11-12 11:43:06    type=datetime    update=true CHAR_VALUES :     type=char(1) NUMBER_VALUES :     type=decimal(19 8) FIOAT_VALUES :     type=float(19 8) DOUBLE_VALUES :     type=double(19 8) TINYINT_VALUES : -128    type=tinyint(3) TINYINT_UN_VALUES : 255    type=tinyint(3) unsigned SMALLINT_VALUES : 30000    type=smallint(5) SMALLINT_UN_VALUES : 65535    type=smallint(5) unsigned MEDIUMINT_VALUES : 8077215    type=mediumint(7) MEDIUMINT_UN_VALUES : 16777215    type=mediumint(7) unsigned INT_VALUES : 2094967296    type=int(10) INT_UN_VALUES : 4294967295    type=int(10) unsigned BIGINT_VALUES : 9223372036854775807    type=bigint(19) BIGINT_UN_VALUES : 9223372036854775808    type=bigint(19) unsigned DATETIME_VALUES : 0000-00-00 00:00:00    type=datetime TIMESTAMP_VALUES : 0000-00-00 00:00:00    type=timestamp DATE_VALUES : 0000-00-00    type=date TIME_VALUES : 00:00:00    type=time YEAR4_VALUES : 0000    type=year(4) YEAR2_VALUES : 0000    type=year(4) BLOB_VALUE : hello    type=blob TINY_BLOB_VALUE :     type=tinyblob MEDIA_BLOB_VALUE :     type=mediumblob LONG_BLOB_VALUE :     type=longblob TEXT_VALUE : ä¸­æ–‡1    type=text TINY_TEXT_VALUE :     type=tinytext MEDIA_TEXT_VALUE :     type=mediumtext LONG_TEXT_VALUE :     type=longtext </pre> 
72,mysql5.6日期字段无法同步 版本：otter-4.2.3 os：CentOS release 6.3 mysql：5.6.10-log MySQL Community Server 字段类型：  create_time         | datetime     | YES  |     | NULL    |       | update_time         | datetime     | YES  |     | NULL    |       | 配置了一个单向同步channel，源表里有create_time、update_time字段，启动channel同步就会出现挂起，查看日志发现报错，提示时间格式不对，相应的错误日志如下： pid:3 nid:1 exception:setl:com.alibaba.otter.node.etl.load.exception.LoadException: java.util.concurrent.ExecutionException: com.alibaba.otter.node.etl.load.exception.LoadException: com.alibaba.otter.node.etl.load.exception.LoadException: com.alibaba.otter.node.etl.load.exception.LoadException: org.springframework.dao.DataIntegrityViolationException: PreparedStatementCallback; SQL [update tb set  `create_time` = ?   `update_time` = ?  where ]; Data truncation: Incorrect datetime value: '1383277380000' for column 'create_time' at row 1; nested exception is com.mysql.jdbc.MysqlDataTruncation: Data truncation: Incorrect datetime value: '1383277380000' for column 'create_time' at row 1     at org.springframework.jdbc.support.SQLStateSQLExceptionTranslator.doTranslate(SQLStateSQLExceptionTranslator.java:101)     at org.springframework.jdbc.support.AbstractFallbackSQLExceptionTranslator.translate(AbstractFallbackSQLExceptionTranslator.java:72)     at org.springframework.jdbc.support.AbstractFallbackSQLExceptionTranslator.translate(AbstractFallbackSQLExceptionTranslator.java:80)     at org.springframework.jdbc.support.AbstractFallbackSQLExceptionTranslator.translate(AbstractFallbackSQLExceptionTranslator.java:80)     at org.springframework.jdbc.core.JdbcTemplate.execute(JdbcTemplate.java:603)     at org.springframework.jdbc.core.JdbcTemplate.update(JdbcTemplate.java:812)     at org.springframework.jdbc.core.JdbcTemplate.update(JdbcTemplate.java:868)     at com.alibaba.otter.node.etl.load.loader.db.DbLoadAction$DbLoadWorker$2.doInTransaction(DbLoadAction.java:607)     at org.springframework.transaction.support.TransactionTemplate.execute(TransactionTemplate.java:130)     at com.alibaba.otter.node.etl.load.loader.db.DbLoadAction$DbLoadWorker.doCall(DbLoadAction.java:599)     at com.alibaba.otter.node.etl.load.loader.db.DbLoadAction$DbLoadWorker.call(DbLoadAction.java:527)     at com.alibaba.otter.node.etl.load.loader.db.DbLoadAction.doTwoPhase(DbLoadAction.java:449)     at com.alibaba.otter.node.etl.load.loader.db.DbLoadAction.doLoad(DbLoadAction.java:274)     at com.alibaba.otter.node.etl.load.loader.db.DbLoadAction.load(DbLoadAction.java:160)     at com.alibaba.otter.node.etl.load.loader.db.DbLoadAction$$FastClassByCGLIB$$d932a4cb.invoke()     at net.sf.cglib.proxy.MethodProxy.invoke(MethodProxy.java:191)     at org.springframework.aop.framework.Cglib2AopProxy$DynamicAdvisedInterceptor.intercept(Cglib2AopProxy.java:618)     at com.alibaba.otter.node.etl.load.loader.db.DbLoadAction$$EnhancerByCGLIB$$80fd23c2.load()     at com.alibaba.otter.node.etl.load.loader.db.DataBatchLoader$2.call(DataBatchLoader.java:198)     at com.alibaba.otter.node.etl.load.loader.db.DataBatchLoader$2.call(DataBatchLoader.java:189)     at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)     at java.util.concurrent.FutureTask.run(FutureTask.java:166)     at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)     at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)     at java.util.concurrent.FutureTask.run(FutureTask.java:166)     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)     at java.lang.Thread.run(Thread.java:724) Caused by: com.mysql.jdbc.MysqlDataTruncation: Data truncation: Incorrect datetime value: '1383277380000' for column 'create_time' at row 1     at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3560)     at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3494)     at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:1960)     at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2114)     at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2696)     at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2105)     at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:2398)     at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:2316)     at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:2301)     at org.apache.commons.dbcp.DelegatingPreparedStatement.executeUpdate(DelegatingPreparedStatement.java:105)     at org.apache.commons.dbcp.DelegatingPreparedStatement.executeUpdate(DelegatingPreparedStatement.java:105)     at org.springframework.jdbc.core.JdbcTemplate$2.doInPreparedStatement(JdbcTemplate.java:818)     at org.springframework.jdbc.core.JdbcTemplate$2.doInPreparedStatement(JdbcTemplate.java:1)     at org.springframework.jdbc.core.JdbcTemplate.execute(JdbcTemplate.java:587)     ... 23 more :----------------- - PairId: 5   TableId: 1   EventType : U   Time : 1383277861000  ## \- Consistency : M   Mode :  ---Pks     EventColumn[index=0 columnType=12 columnName=sub_serial columnValue=426931043 isNull=false isKey=true isUpdate=true] ---oldPks ---Columns     EventColumn[index=7 columnType=93 columnName=create_time columnValue=1383277380000 isNull=false isKey=false isUpdate=true]    中间省略 EventColumn[index=8 columnType=93 columnName=update_time columnValue=1383277864000 isNull=false isKey=false isUpdate=true]     EventColumn[index=9 columnType=4 columnName=status columnValue=1 isNull=false isKey=false isUpdate=true] 我把同步表中的时间相关的字段去掉再开启同步，该问题不会出现。 参见 otter issue : https://github.com/alibaba/otter/issues/30 
71,增加filterQueryDdl功能 之前支持filterQueryDcl filterQueryDml，部分用户期望只关乎对应的row数据，这里也加上filterQueryDdl控制，允许忽略ddl事件.  
70,mysql bit类型支持 测试的表： <pre> CREATE TABLE `test_bit_all` (   id int(10) unsigned NOT NULL AUTO_INCREMENT   bit1 bit(1)   bit2 bit(9)   bit3 bit(17)   bit4 bit(25)   bit5 bit(33)   bit6 bit(41)   bit7 bit(49)   bit8 bit(57)   bit9 bit(64)   PRIMARY KEY (`id`)   )  ENGINE=InnoDB DEFAULT CHARSET=gbk; </pre> 构造数据： insert into test_bit_all(id bit1 bit2 bit3 bit4 bit5 bit6 bit7 bit8 bit9) values(null 18446744073709551615 18446744073709551615 18446744073709551615 18446744073709551615 18446744073709551615 18446744073709551615 18446744073709551615 18446744073709551615 18446744073709551615) ; 输出结果： EventColumn[index=1 columnType=-7 columnName=bit1 columnValue=1 isNull=false isKey=false isUpdate=true] EventColumn[index=2 columnType=-7 columnName=bit2 columnValue=511 isNull=false isKey=false isUpdate=true] EventColumn[index=3 columnType=-7 columnName=bit3 columnValue=131071 isNull=false isKey=false isUpdate=true] EventColumn[index=4 columnType=-7 columnName=bit4 columnValue=33554431 isNull=false isKey=false isUpdate=true] EventColumn[index=5 columnType=-7 columnName=bit5 columnValue=8589934591 isNull=false isKey=false isUpdate=true] EventColumn[index=6 columnType=-7 columnName=bit6 columnValue=2199023255551 isNull=false isKey=false isUpdate=true] EventColumn[index=7 columnType=-7 columnName=bit7 columnValue=562949953421311 isNull=false isKey=false isUpdate=true] EventColumn[index=8 columnType=-7 columnName=bit8 columnValue=144115188075855871 isNull=false isKey=false isUpdate=true] EventColumn[index=9 columnType=-7 columnName=bit9 columnValue=18446744073709551615 isNull=false isKey=false isUpdate=true] 
69,mysql登录失败 <pre> Caused by: java.io.IOException: connect /192.168.1.206:3306 failure:java.lang.ArrayIndexOutOfBoundsException: 1         at com.alibaba.otter.canal.parse.driver.mysql.utils.ByteHelper.readUnsignedShortLittleEndian(ByteHelper.java:56)         at com.alibaba.otter.canal.parse.driver.mysql.packets.server.ErrorPacket.fromBytes(ErrorPacket.java:35)         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:170)         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:66)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:51)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:92)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:143)         at java.lang.Thread.run(Thread.java:662)         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:69)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:51)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:92)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:143)         at java.lang.Thread.run(Thread.java:662) </pre> mysql version :  5.5.30-log mysql环境：    old_passwords=1 mysql的用户密码：UPDATE mysql.user SET Password = OLD_PASSWORD('ottermysql') WHERE user = 'ottermysql'.   在使用正常的auth411登录认证时会返回-2，需要使用auth311进行登录验证.  
68,canal监控配置文件变化时考虑下与meta.dat文件输出 测试过程中，移除了一个instance配置，发现通道并未停止，而是继续运行着，检查了下conf目录，发现又重新生成了该目录，该目录下存在一个meta.dat文件.  导致自动停止失败.  解决： 忽略空的文件，每个instance必须包含instance.properties才算一个正常的，否则需要进行关闭 
67,canal增加heartbeat事件 支持下otter需求： https://github.com/alibaba/otter/issues/25 目前心跳信息只在parser和sink模块中间进行传输，目前store暂无需求，所以暂不输出到store中，直接在sink中被消费和过滤 
66,mysql binary类型处理 otter issue : https://github.com/alibaba/otter/issues/22 针对同步binary类型，binlog里记录为String，导致后续在处理时按照字符串进行处理，出现错误.  
65,schema为纯数字时，desc 123.table会包语法错误，导致获取表结构异常 <pre> mysql> desc 123.pre_common_addon ; ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '123.pre_common_addon' at line 1 </pre> 需要对schema / table name做一下转义符 
64,canal client example异常处理 1.  canal server集群，所有的节点全挂了，对应的canal client example需要处理no alive canal server的异常.  2.  ClusterCanalConnector.connect()，当重试retryTimes次后，如果还建立不了链接，增加异常反馈，而不应该只是记录日志.  
63,zkClient使用0.2版本后，出现NoSuchMethodError ![zkclient](https://f.cloud.github.com/assets/834743/1148993/adb7ffce-1ed2-11e3-85d7-135e9c1d3490.jpg) zkClient使用0.2版本后，出现NoSuchMethodError zkclient在0.2版本，修改了方法： https://github.com/sgroschupf/zkclient/commit/e72d8fd682907004d0b1488e47d8fe55d405b751 然后在zkclinet 0.3版本，修复了方法不兼容的问题： https://github.com/sgroschupf/zkclient/commit/26f4d50fe8b3f5f6a4aa3747f254162c6a6c410b 
62,ddl语句解析异常 针对如下ddl语句： <pre> CREATE TABLE `cm_settle_incash` (    `batch_id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT '批次'    `counter_acct_code` bigint(20) DEFAULT NULL COMMENT '柜台科目号'    `channel_sum_amount` decimal(18 2) NOT NULL COMMENT '总金额'    `channel_sum_cost` decimal(18 2) NOT NULL DEFAULT '0.00' COMMENT '总成本'    `bank_sum_amount` decimal(18 2) NOT NULL DEFAULT '0.00' COMMENT '总来款'    `begin_date` date DEFAULT NULL COMMENT '开始日期'    `end_date` date DEFAULT NULL COMMENT '结束日期'    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间'    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '更新时间'    `cdk_amount` decimal(18 2) DEFAULT '0.00' COMMENT '长短款金额'    `cdk_type` tinyint(4) DEFAULT NULL COMMENT '长短款类型 1长款2短款'    `deal_id` bigint(20) DEFAULT NULL COMMENT '交易id'    `cdk_deal_id` bigint(20) DEFAULT NULL COMMENT '长短款id'    PRIMARY KEY (`batch_id`)  ) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8; </pre> 通过SimpleDdlParser，无法解析正确的schema/table name，导致解析出错.  主要原因： 1.  SimpleDdlParser是通过正则进行表名提取，比如会处理`schema`.`table`的情况，如果对应整个ddl中有个.号，会导致schema的匹配出错 
61,canal支持下otter ddl同步的需求 otter issue : https://github.com/alibaba/otter/issues/12 ddl同步的无法支持幂等性处理，需要尽可能保证ddl/dml是相互独立，尽可能是单条的ddl进行串行执行，不考虑batch.   解决：  1. canal server增加参数，canal.instance.get.ddl.isolation，代表是否孤立的返回单条ddl语句.  
60,SimpleDdlParser process query failed.  出现类似异常： <pre> pid:1 nid:1 exception:canal:6.21->6.20:com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: SimpleDdlParser process query failed. pls submit issue with this queryString: CREATE table `bak591`.`j_order_log_back_201309` like j_order_log Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: SimpleDdlParser process query failed. pls submit issue with this queryString: CREATE table `bak591`.`j_order_log_back_201309` like j_order_log </pre> 登录数据库，在bak库下，执行ddl语句，跨库建表，导致解析异常.  
59,canal server 高可用连接数问题  client server连接了两个数据库。默认是数据库A，standby 数据库B。当数据库A挂掉的时候，canal与数据库B的连接数暴增，导致连接拒绝。 是否有开启心跳SQL ？  对应canal版本是多少？  暂时关闭问题，怀疑和早期的canal版本心跳sql bug导致出现执行心跳sql的链接不释放，已修复 
58,不能按特定规则指定DDL订阅 比如订阅DML   pop.\*      对于DDL而言，_._的数据都会被订阅  
57,特殊数据类型解析 特殊数据： 1. year类型，默认值为0000 2. date类型，默认值为0000-00-00 00:00:00 canal <= 1.0.8版本，目前的测试： 插入数据： +----+---------------------+---------------------+-------------+-------------+--------------+--------------+ | ID | DATETIME_VALUES     | TIMESTAMP_VALUES    | DATE_VALUES | TIME_VALUES | YEAR4_VALUES | YEAR2_VALUES | +----+---------------------+---------------------+-------------+-------------+--------------+--------------+ |  3 | 0000-00-00 00:00:00 | 0000-00-00 00:00:00 | 0000-00-00  | 00:00:00    |         0000 |         2000 | +----+---------------------+---------------------+-------------+-------------+--------------+--------------+ 实际输出： 3 0002-11-30 00:00:00.0 1970-01-01 08:00:00.0 0002-11-30 00:00:00 1900 2000 测试过程中例子： +----+---------------------+---------------------+-------------+-------------+--------------+--------------+ | ID | DATETIME_VALUES     | TIMESTAMP_VALUES    | DATE_VALUES | TIME_VALUES | YEAR4_VALUES | YEAR2_VALUES | +----+---------------------+---------------------+-------------+-------------+--------------+--------------+ | 26 | 9999-12-31 23:59:59 | 2038-01-18 23:59:59 | 9999-12-31  | 838:59:59   |         2155 |         2155 | | 27 | 0000-00-00 00:00:00 | 0000-00-00 00:00:00 | 0000-00-00  | -838:59:59  |         0000 |         0000 | +----+---------------------+---------------------+-------------+-------------+--------------+--------------+ 实际测试输出： --- 26 9999-12-31 23:59:59 2038-01-18 23:59:59 9999-12-31 838:59:59 2155 2155 --- 27 0000-00-00 00:00:00 0000-00-00 00:00:00 0000-00-00 -838:59:59 0000 0000 --- it's fixed.   可以看一下，mysql的时间类型：http://dev.mysql.com/doc/refman/5.0/en/date-and-time-types.html 
56,canal server在开启heartbeat，遇到解析异常后会出现mysql链接暴涨 canal server在开启heartbeat，遇到解析异常后会出现mysql链接暴涨.  用户在使用canal解析mysql 5.6.13版本时，解析时出现如下异常： <pre> Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: java.lang.IllegalArgumentException: !! Unknown BLOB packlen = 0         at com.taobao.tddl.dbsync.binlog.event.RowsLogBuffer.fetchValue(RowsLogBuffer.java:752)         at com.taobao.tddl.dbsync.binlog.event.RowsLogBuffer.nextValue(RowsLogBuffer.java:97)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseOneRow(LogEventConvert.java:294)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:249)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:102)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:60)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:297)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:161)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:120)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:189) </pre> 运行一段时间后，mysql链接逐步持续增长，最终超过最大连接数 问题原因分析： MysqlEventParser总共有3个socket链接到mysql上  1.  binlog dump链接  (start/stop中都会关闭，start解析异常时会触发preDump/afterDump调用) 2.  table meta查询链接 (preDump/afterDump/stop中都会关闭) 3.  heartbeat 执行链接  (preDump/stop中都会关闭). 所以刚好遇上解析异常时，重复触发preDump/afterDump调用，而heartbeat没有在afterDump中关闭链接，导致被重复创建.   原本heartbeat的设计思路时，只启动一次，当出现switch时才会进行重新链接的过程.  需要避免重复调用prefDump触发创建多次heartbeat链接的问题   
55,client在server不可用时，出现cpu100% 用户反馈：在linux上，ClusterCanalConnector这种方式里面只有一个地址的时候 如果这个宕机了 重连的速度非常快 cpu100%.   大致客户端代码： while(true) {     client.connect()     client.getWithoutAck()/client.ack(); } windows下cpu不会100%，linux环境下明显.   
54,缺少thred id讯息？ 当做程序事物日志来用的时候缺少前后台关联性，需要加入binlog本身已有的thredid。希望能够在下个版本里加入。 
53,能不能增加 mysql 和oracle 方面的手册？ 能不能增加手册 或者详细的文档？谢谢！ 看一下wiki吧，应该有你想要的内容。 wiki地址：https://github.com/alibaba/canal/wiki 
52,canal client支持getWithout timeout超时控制的接口调用 RT.   解决： CanalConnector新增方法 1.  Message getWithoutAck(int batchSize  Long timeout  TimeUnit unit) 2.  Message get(int batchSize  Long timeout  TimeUnit unit)  
51,mysql地址配置出错，增加异常描述信息 Caused by: org.springframework.beans.factory.BeanCreationException: Error creatng bean with name 'com.alibaba.otter.canal.parse.support.AuthenticationInfo#1f5ea4a' defined in class path resource [spring/file-instance.xml]: Initialization of bean failed; nested exception is java.lang.ArrayIndexOutOfBoundsException: 1 at org.springframework.beans.factory.support.AbstractAutowireCapableBeaFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:480) ~[spring-2.5..jar:2.5.6] at org.springframework.beans.factory.support.AbstractAutowireCapableBeaFactory$1.run(AbstractAutowireCapableBeanFactory.java:409) ~[spring-2.5.6.jar:25.6] at java.security.AccessController.doPrivileged(Native Method) ~[na:1.6._17] 解决：  识别出异常配置，抛异常时记录原始的配置信息.    throw new RuntimeException("address[" + text + "] is illegal  eg.127.0.0.1:3306"); 
50,canal中输出表名全为小写 canal 1.0.6版本之前，解析的schema   tablename 全部转化为了小写。 解决：需要保留下binlog中记录的内容，不做大小写转化  需要在my.cnf中设置lower_case_table_names=0，保证mysql大小写铭感.  不然针对mysql存储表名全为小写 
49,canal server在windows下启动失败，conf目录下修改配置一直不生效 canal deployer在打包的时候，将conf也打入到了jar中，而在一些特定的环境下，classloader会优先加载了jar包中的conf配置文件，从而导致在打包目录下的配置一直未生效.   也就是说：conf下和canal.deployer-xxxx.jar中都有canal.properties和instance.properties 解决办法： 1.  在canal.deployer-xxx.jar打包时去除conf目录.  fixed issue 49    9515b83 
48,mysql主备切换采用虚ip切换，canal的failover机制 采用虚ip进行mysql主备管理时，暴露的ip只有1个，目前canal识别主备切换是根据ip和对应meta信息中的ip地址发生不一致时，进行一次切换操作，基于时间戳重新定位binlog . 所以，当使用虚ip，后端发生了主备切换，前端无法感知，一直会出现对应binlog不存在的错误，一直重试，从而导致canal整个不可用 解决：在出现binlog不存在的错误时，尝试按照时间戳重新定位binlog .  什么情况下binlog会出现不存在： 1. 虚ip　mysql主备切换 2. canal解析延迟过久，对应binlog已经被删除.  针对情况2，需要避免。解决办法：当按照时间戳进行重新定位binlog时，如果当前所有binlog的时间戳都晚于查找的时间戳，那应该挂起，不能直接使用第一个binlog进行解析处理 binlog不存在的错误类型，比较多.   不存在的binlog :  mysql> show binlog events in 'mysql-bin.000273' from 75489806 limit 1; ERROR 1220 (HY000): Error when executing command SHOW BINLOG EVENTS: Could not find target log 存在的binlog，错误的offest:  mysql> show binlog events in 'mysql-bin.000374' from 75489806 limit 1; ERROR 1220 (HY000): Error when executing command SHOW BINLOG EVENTS: Wrong offset or I/O error --- binlog dump指令： 不存在的binlog :  : Received error packet: errno = 1236  sqlstate = HY000 errmsg = Client requested master to start replication from position > file size; the first event 'mysql-bin.000011' at 12313  the last event read from './mysql-bin.000011' at 4  the last byte read from './mysql-bin.000011' at 4. 存在的binlog，错误的offest: Received error packet: errno = 1236  sqlstate = HY000 errmsg = binlog truncated in the middle of event; consider out of disk space on master; the first event 'mysql-bin.000011' at 123  the last event read from './mysql-bin.000011' at 123  the last byte read from './mysql-bin.000011' at 142. 情况暂时还没不能全面枚举，暂时hold一下issue 
47,canal server启动时针对canal.ip配置为空时，默认监听为AddressUtils.getHostIp()的地址，导致多IP地址不可访问 canal server启动时针对canal.ip配置为空时，默认监听为AddressUtils.getHostIp()的地址，导致多IP地址不可访问.    解决：  1.  如果配置ip地址为空时，默认socket的监听为 :${canal.port}，不指定ip绑定，接收任何该机器任何ip过来的请求.  2.  同时会通过AddressUtils.getHostIp()选择一个ip，暴露到zk中，由客户端进行访问.  
46,lll 
45,合并原作者更新 
44,关注表的条件变更，对应生效时间问题 目前关注表的正则，可以通过两种方式进行设置 1.  conf下的instance.properties中，可以设置table.regex，在第一次启动的时候直接载入使用 2.  某个client进行subscribe(filter)，可以提交一个filter 如果方式2中，提交了filter，目前的策略是直接覆盖了方式1中的条件，如果canal server选择了file/zookeeper持久化了meta，即使下次server重启，对应的关注表条件也会是方式2提交的数据 生效时间问题： 方式1：修改instance.properties，需要重启instance(如果开启scan，自动会重启)，instance重启会回退一部分binlog，比如客户端未ack的数据，或者消费了一半的事务，还有就是未及时刷出内存位点数据到持久化介质.  所以，对应的生效时间会对之前的部分数据进行作用 方式2：关闭client，修改订阅条件后，会是同步即时生效，subscribe调用成功后，直接替换了canal server中老版本的filter. 
43,canal重启后会丢失client提交的filter条件，被instance.properties配置的filter给覆盖 canal server在重启后，没有重新读取一次客户端subscribe()提交的filter记录，导致启动时重新载入了instance.properties，client filter条件被丢弃 解决：canal instance重新启动时，优先读取一次MetaManager中记录的client filter信息 
42,canal自动重连时会出现destination:idbbond-1 should start first信息 <pre title="code"> 2013-06-05 18:08:40.960 [main] INFO c.alibaba.otter.canal.client.impl.ClusterCanalConnector - restart the connector for next round retry. 2013-06-05 18:08:40.975 [main] WARN c.alibaba.otter.canal.client.impl.ClusterCanalConnector - something goes wrong when getWithoutAck data from server:/192.168.1.134:11111 com.alibaba.otter.canal.protocol.exception.CanalClientException: something goes wrong with reason: something goes wrong with channel:[id: 0x3622e177  /192.168.1.134:50993 => /192.168.1.134:11111]  exception=com.alibaba.otter.canal.server.exception.CanalServerException: destination:idbbond-1 should start first at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:241) at com.alibaba.otter.canal.client.impl.ClusterCanalConnector.getWithoutAck(ClusterCanalConnector.java:135) at SimpleCanalClientExample.main(SimpleCanalClientExample.java:35) </pre> 问题说明： 1. canal server在启动时，是先暴露了端口，再启动对应需要启动的instance，因为启动instance需要一点时间，所以有一定概率会撞上instance未启动完成.  
41,mysql 5.6新增两种type事件MYSQL_TYPE_TIMESTAMP2/MYSQL_TYPE_DATETIME2 需要注意，新的时间类型基于数字进行压缩存储，而且采用了big-endian模式，和其他binlog对象解析有所不同 
39,HA模式下mysql master-standby无法自动切换 canal 主 canal config ## detecing config canal.instance.detecting.enable = true canal.instance.detecting.sql = insert into retl.xdual values(1 now()) on duplicate key update x=now() canal.instance.detecting.interval.time = 3 canal.instance.detecting.retry.threshold = 3 canal.instance.detecting.heartbeatHaEnable = true #  example log  2013-05-14 10:23:29.509 [destination = example   address = /192.168.196.82:3306   EventParser] WARN  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - connect failed!java.net.ConnectException: Connection refused         at sun.nio.ch.Net.connect(Native Method)         at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:525)         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:65)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:51)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:141)         at java.lang.Thread.run(Thread.java:679) canal 从  ## detecing config canal.instance.detecting.enable = true canal.instance.detecting.sql = insert into retl.xdual values(1 now()) on duplicate key update x=now() canal.instance.detecting.interval.time = 3 canal.instance.detecting.retry.threshold = 3 canal.instance.detecting.heartbeatHaEnable = true 
38,scan定时扫描需要忽略canal meta/paser.dat文件 默认配置中，canal选择的是file-intance.xml机制，会将相关数据定时刷新到文件. 而文件的存储默认是和instance的配置文件在一起.   比如 conf/example/meta.dat.  所以在scan定时扫描的时候，就会发现instance下的文件有所变化，导致需要进行一次reload.  解决： scan扫描时，忽略.dat的文件变更 
37,DDL语句不能及时输出 ddl语句会在Transaction Buffer中被缓存中，只有在下一次出现Transaction Begin/End的消息时，才会输出到store中，client才可见 解决：ddl语句在binlog协议中，前后不会有begin/commit事件，需要特殊处理，进入Transaction Buffer后，也立马flush到store中 影响版本 <= 1.0.3 
36,eclipse中直接启动canal server eclipse直接运行CanalLanuncher会启动失败，对应依赖的canal.properties和spring/xxx-instance.xml无法找到 原因： 1.  canal server启动时为了简化路径依赖，都使用了相对路径或者classpath:进行描述资源依赖，所以基于eclipse启动后，导致查找资源失败 解决： 1.  将conf/目录下的配置，加入到eclipse的classpath中 eclipse直接运行CanalLauncher即可，默认会找到deployer工程下的src/main/resources中的资源文件. 比如: 1. logback.xml 2. canal.properties，这里面依赖了spring/xxxx-instance.xml为classpath路径 3. spring/xxxx-instance.xml，这里面依赖了xxxx/instance.properties为classpath路径 直接运行后，对应的logs输出为canal/logs(工程根目录的logs目录) 
35,支持binlog_format为statement mixed模式的解析输出 binlog_format模式有statement  row  mixed三种模式，主要的区别在于insert/update/delete记录是否采取sql方式，还是详细的记录变更.  期望： 1. canal也可以支持非row模式的数据解析，解析对应的sql.  (非row模式下，不会提供before/after columns信息) 
34,canal新增基于file记录位点信息，不依赖zookeeper持久化canal server状态 计划支持非zookeeper模式，也可以有持久化的功能，关闭canal server后，下一次启动时也能继续上一次的位置进行消费数据，不会丢失数据 注意： 基于file持久化的模式，存在一个问题，就是无法实现集群failover的功能，因为file属于本机，failover切换到另一机器后无法找到file，会造成无法找到上一次解析位置.  
33,新建个example module <name>canal driver module for otter ${project.version}</name> modulename  太长了 目前example module没有纳入canal trunk管理，可以将其移入，方便学习 
32,client基于zookeeper地址断开一次后，立马重新启动，偶儿会出现NPE异常 问题描述： 1.  client基于zookeeper地址获取canal server的工作节点 2.  client第一次起来工作后，被非正常退出，比如kill.   然后立马又重新启动 3.  client会出现NullPointException异常 原因分析：  1.  client第一次启动后，会在zookeeper记录一个client running节点，非正常退出后，running节点不会立马小时 2.  当client下一次立马启动后，发现running节点存在，并不会立马创建socket，当执行后面的get数据操作时，发现socket为null，就出现了NPE问题 影响的版本 <= 1.0.3.  解决方案： 1.  发现running节点存在，并不会立马创建socket，但同时阻塞后续的get数据的操作，直到socket被建立  
31,mysql YEAR类型binlog输出使用short替换Date 问题： mysql YEAR类型在canal1.0.3版本中使用了java.sql.Date对象来标示，无法准确的表示一个年份信息，原本一个年份2013，输出结果会是：2013-01-01 if (cal == null)    cal = Calendar.getInstance(); cal.clear(); cal.set(Calendar.YEAR  i32 + 1900); value = new java.sql.Date(cal.getTimeInMillis()); 解决：  1.  直接使用数字存储表示，protobuf输出结果为2013 
30,mac下startup.sh启动脚本报错 问题： bin_abs_path=$(readlink -f $(dirname $0)) mac下readlink -f不能正常工作 解决：  case "`uname`" in     Linux)                 bin_abs_path=$(readlink -f $(dirname $0))                 ;;           *)                   bin_abs_path=`cd $(dirname $0); pwd`                 ;;  
29,canal解析DDL操作出现异常，导致整个解析挂起 问题描述:  1.  启动canal server/client 2.  执行ddl操作，比如create，alter   delete，create表的操作.  特定的case触发DDL表名解析错误 3.  canal在解析binlog后，发现binlog column信息和当前tablemeta cache中的数据不一致，抛出异常，进行重试.  存在的问题:  就是在第三步出现异常后，没有更新对应的cache数据，导致下一次解析时一直使用上一次错误的tablemeta，导致一直出错，一直在重试.  
28,canal HA模式cluster集群列表数据更新问题 每个instance下面都会有个cluster目录，代表服务这个instance的canal server的机器列表： <pre> [zk: localhost:2181(CONNECTED) 15] ls /otter/canal/destinations/example/cluster  [10.20.144.22:11111] </pre> 1.0.2版本的cluster列表只会在running节点抢占成功后才会生成running节点，和原本cluseter的定位想矛盾 解决： canal server启动对应的instance时，不管有没有抢占到running节点，都应该创建cluster节点，表明自己是可以work for this instance，如果此时发生running节点删除，客户端就可以从cluster列表中找到一个节点，进行链接，lazy的方式促使它进行启动。 (ps. 当然running节点删除，canal server的另一台机器自己也能感应到，并启动instance) 
27,canal server running节点判断是否为本机操作优化 canal server在zookeeper的节点：  <pre class="java" name="code"> [zk: localhost:2181(CONNECTED) 15] get /otter/canal/destinations/example/running   {"active":true "address":"10.20.144.51:11111" "cid":1} </pre> 目前一台机器，判断当前是否为工作节点，是通过cid是否和本机的id相同来判断，但目前cid这信息在启动canal server没有严格校验是否有重复，导致两台机器都认为是自己本机，对于后续的命令控制操作有风险 解决：将判断是否本机的操作，修改为基于address是否相同来判断，ip+port可以唯一定义一个jvm 
26, 集群模式下，客户端与instance断开连接，zookeeper running节点 自动被删除了 问题：   集群模式下，客户端与instance断开连接，zookeeper running节点 自动被删除了。并且查看canal.log 无任何异常信息输出。 请问这是目前的bug么，还是说zookeeper配置问题 ？ 我所期望的：    客户端断开链接后，running节点依然存在，下次客户端连接上来，可以继续使用之前的信息，包括cursor信息。 ps:貌似看到类似问题是1.0.3解决。现在是啥版本。。。 可以看一下这个issue : https://github.com/alibaba/canal/issues/22 主要是canal 1.0.2版本下，当一个instance只有一个客户端链接，而且这个客户端链接断开时，会自动停止。下一次客户端启动时，会自动从所有的instance的cluster机器列表中随机选择一台，重新建立链接，这时instance会重新启动.  目前是否关闭在canal 1.0.3版本会做成可配置的模式，允许用户定义.  (PS. 1.0.3目前还未正式发布，清明过后会release) canal.properties中新增参数： #as far as possible to stop canal instance where client disconnect canal.stopInstanceAsPossible = true / false  # false代表客户端断开时，不主动关闭instance，即你锁期望的保留running节点 还有一点要说明，你选择了zookeeper的集群模式配置启动了canal server，它会定时记录cursor信息到zookeeper中，默认的刷新频率可见 canal.zookeeper.flush.period这参数，默认为1秒。  所以下一次重新链接时，重新启动instance，会继续从最后一次提交到zookeeper中的cursor位置进行启动，会有一定的重复数据.  目前canal 1.0.2还有一个已知bug : #23 ，会有个NPE问题，导致重新链接时会出现启动失败。 所以目前canal 1.0.2使用了集群部署instance，发生切换或者客户端主动断开链接，会导致instance环境不可用，如果想使用集群部署，还是等一下canal 1.0.3版本，如有问题，可进群讨论(wiki首页有群号信息)或者继续在issue上留言.  多谢！ 
25,windows bat脚本启动失败 出错提示：  'conf_dir' 不是内部或外部命令，也不是可运行的程序 或批处理文件。 'canal_conf' 不是内部或外部命令，也不是可运行的程序 或批处理文件。 'logback_configurationFile' 不是内部或外部命令，也不是可运行的程序 或批处理文件。 Listening for transport dt_socket at address: 9099 Exception in thread "main" java.lang.NoClassDefFoundError: com/alibaba/otter/can al/deployer/CanalLauncher Caused by: java.lang.ClassNotFoundException: com.alibaba.otter.canal.deployer.Ca nalLauncher         at java.net.URLClassLoader$1.run(URLClassLoader.java:202)         at java.security.AccessController.doPrivileged(Native Method)         at java.net.URLClassLoader.findClass(URLClassLoader.java:190)         at java.lang.ClassLoader.loadClass(ClassLoader.java:307)         at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)         at java.lang.ClassLoader.loadClass(ClassLoader.java:248) Could not find the main class: com.alibaba.otter.canal.deployer.CanalLauncher. Program will exit. windows环境测试通过.  环境： windows xp系统  java版本： java version "1.6.0_18" Java(TM) SE Runtime Environment (build 1.6.0_18-b07) Java HotSpot(TM) Client VM (build 16.0-b13  mixed mode  sharing) 
24,windows在与mysql建立链接时，出现unexpected blocking io behavior异常 异常栈： <code> 0:42:02.725 [main] WARN  c.a.o.c.p.d.mysql.MysqlConnector - connect failed!java.io.IOException: unexpected blocking io behavior  at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.write(PacketManager.java:54)  at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:159)  at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:66)  at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnectorTest.testQuery(MysqlConnectorTest.java:20)  </code> 对应代码： <code> public static void write(SocketChannel ch  ByteBuffer[] srcs) throws IOException {          long total = 0;          for (ByteBuffer buffer : srcs) {              total += buffer.remaining();              System.out.println(total);           }          long size = ch.write(srcs);          if (size != total) {              throw new IOException("unexpected blocking io behavior");          }      }  </code> 找了windows环境测试未重现该问题 ，暂时close  
23,canal client在第一次建立连接时，如果canal server所有都配置了lazy模式，client启动失败 client启动时需要做一个check :  1.  如果当前instance已经有running的canal server，直接选择该节点，进行链接 2.  如果没有running节点，再从instance/cluster节点中，随机选择一个节点，进行链接 我就是遇到#26 这个问题。我想知道是谁删除了running 节点。 删除running节点的代码：  1. https://github.com/alibaba/canal/blob/master/server/src/main/java/com/alibaba/otter/canal/server/netty/handler/SessionHandler.java    stopCanalInstanceIfNecessary方法，发现client断开时，尝试关闭instance.  2. https://github.com/alibaba/canal/blob/master/common/src/main/java/com/alibaba/otter/canal/common/zookeeper/running/ServerRunningMonitor.java ，对应的stop方式，会删除running节点 
22,canal server在处理客户端主动关闭连接时，是否需要关闭instance 在canal 1.0.2版本以及以前的版本，在设计的时候，canal server针对instance管理一直都是lazy模式处理.  当canal client断开链接时，canal server会主动关闭instance，等下一次canal client链接后再重新创建.  先前的考虑： 1. canal server主动关闭instance，主要是考虑是想将canal server作成无状态，下一次canal client可以随机选择一台机器获取数据，新的canal server会重新启动instance，整个HA模型会比较简单.  改进： 1.  canal server在client退出时不主动关闭instance资源 2.  下一次canal client重新链接后，需要通过Cluster模式(通过zookeeper获取到HA模式中正在运行的canal server的机器信息)，然后与其重新建立链接即可 一个前提，客户端disconnect/connect间隔时间不会很长，尽可能减少资源消耗.  
21,canal MemoryStore支持按内存大小定义 canal v1.0.2版本支持的MemoryEventStoreWithBuffer，是通过定义bufferSize来控制内存使用，但遇到大文本字段时，单纯按照记录数来控制就很容易爆内存.  所以，需要支持基于内存大小管理的EventStore，同时支持按内存大小获取数据，方便client进行内存控制.  MemoryEventStoreWithBuffer.setBufferMemSize() 
20,支持mysql 5.6吗？ 支持mysql 5.6吗？ 目前暂不支持mysql5.6，主要是mysql 5.6协议上出现不兼容处理，修改了LogEvent事件type类型，需要升级canal版本解决 计划在v1.0.3支持mysql 5.6协议，目前canal版本release暂定为2周一个周期 感谢agapple的回复，等待您们的新版本 我找了半天也没有找到5.6 binlog的格式文档。你知道在哪里吗？ http://dev.mysql.com/doc/internals/en/binary-log.html ，这个链接是你列出来的，里面的内容应该是5.5版本以下的。 http://dev.mysql.com/doc/refman/5.6/en/binary-log.html 已经支持，commit记录： https://github.com/alibaba/canal/commit/286a8f82916d99d3464ac544e2267397290e7880 mysql5.6协议变化文档：https://github.com/alibaba/canal/wiki/BinlogChange%28mysql5.6%29 
19,mysql metaConnection链接泄漏 运行一段时间后，与mysql之间的数据库链接达到了几千条，且全部都是处于ESTABLISHED状态。 进行jmap dump内存对象，却无法找到SocketChannelImpl的相关实例，说明是被full gc回收了.  
18,mysql text中文字符出现乱码 mysql table中字段类型为text时，数据库编码和表编码均为utf-8，canal配置的解析编码为utf-8，解析出来的数据记录为乱码.   原因分析： 1. mysql binlog中将text/blob类型都记录为LogEvent.MYSQL_TYPE_BLOB 2. canal识别到BLOB信息，无法区分是text还是blob，都按照iso-8859-1进行编码，导致问题的产生 解决： 1.  拿到binlog后，针对BLOB类型，需要反查下table meta信息，获取真实的字段类型，区分出text，然后按照编码进行解析.  
17,mysql varchar类型处理'\000'字符问题 线上测试遇到一个问题： a. 业务执行sql插入了一条记录，其中一个字段为：'210012\000\000\000' b. otter中美同步，更新了这条记录，将字段更新为： '210012'  (去除了\000) c. canal再一次解析时，发现before和after值相同，没有任何字段发生变更，导致otter同步sql执行失败。 原因分析： 1. dbsync在解析'210012\000\000\000'，等价于'210012'，自动忽略了'\000'请求 代码： for (; (found < end) && buf[found] != '\0'; found++) 说明： \0为c-style风格的字符串结束符，至于业务执行怎么插入了\000，目前暂未知原因 
16,column字段变更信息丢失 在LogEventConvert中处理时column，信息不正确。  1. isUpdate所有都为true，正确的应该是：根据before和after字进行判断.  2. sqlType所有都为0，正确应该是：int类型对应于的java.sql.Types 1. 针对isNull字段，没有添加到before/after变更字段列表 
15,修改canal.properties里的canal.address为canal.ip 修改canal.properties里的canal.address为canal.ip 跟com.alibaba.otter.canal.deployer.CanalConstants.CANAL_IP常量对应上 
14,cannal.properties里的配制选项是否轻微错误? 在deployer工程里的canal.properties里的 canal.address=  是否应该改为 canal.ip= 因为在 CanalConstants 这个类里是如下定义的: public static final String CANAL_IP                          = ROOT + "." + "ip"; 眼神真敏锐啊！ 确实是这样的，代码中有这样的逻辑： ``` java ip = getProperty(properties  CanalConstants.CANAL_IP); if (StringUtils.isEmpty(ip)) {   ip = AddressUtils.getHostIp(); } ``` 所以一直没有别发现。 @wenzhihong2003  要不你来改下，然后做一次 pull request？ #15  多谢：） 
13,mysql instance支持group模式 支持多个mysql parse数据合并为一个store输出，进行消费。 典型的业务：数据按水平拆分为16个库后，合并为逻辑的一个canal destination进行消费，客户端不用关心后续16个库的链接情况 需要考虑： 1. 强一致 (group内所有的parse都正常工作，才运行客户端消费数据) 2. 弱一致 (group内只要有一个parse正常工作，就允许客户端消费数据) 目前的解决，只支持强一致，要求所有的parse都正常工作，才允许客户端消费数据。针对弱一致性，建议使用多通道部署的方式来解决 
12,1.0.0版本代码提交 1.0.0版本代码提交 
11,mysql driver在获取table meta时，针对表不存在出现阻塞 mysql driver在处理返回的数据包没有考虑异常情况 fixed at v1.0.1 
10,mysql特定的ddl操作(删除字段，rename table等)，导致解析挂起 目前已知会导致挂起的ddl操作： 1. 字段删除   2. rename table  (导致desc table时找不到对应的meta信息，无法补全Entry信息) 会导致信息错乱的ddl操作： 1. 首先删除一个字段，然后再新加一个字段(非添加到末尾) 原因： - mysql binlog中的tablemap LogEvent，没有包含对应的column name信息，只是按照当前table中column的顺序，列出了字段类型 - 回退到ddl操作前的binlog重新开始解析，此时数据库中desc table所获取的信息就会和table map中不一致(无法进行match操作)，会导致信息混乱 避免的操作： 1.  业务操作避免带删除性质的ddl，比如字段删除，rename table   drop table 2.  业务操作避免添加字段时调整顺序，只能添加到末尾.  如果真不可避免出现出现上述操作中的情况，只要不回退canal解析位置，就可以正常往下解析，不会出现数据错乱.  不过需要考虑：当canal解析延迟比较大，此时进行带删除性质的ddl操作，待解析的binlog中还存在对应ddl的表数据，那还是会出现问题。（尽可能选择合适的时间进行ddl操作） 需要使用上避免，不是bug 最好能加入此种操作方式下，让canal能再次重新正常解析的完整步骤！ 如果遇到ddl带删除性质的操作，正常的话只要保证这时client和server不要有重启，那到问题不大，如果真遇上了，那只能重置位点。  重置位点方法：  1. 删除zookeeper的cursor节点信息  2. 修改instance.properties配置到一个新的起始位点  3. 重启启动server，然后启动client.  重置位点，可能会导致这一段时间的数据丢失，只能通过人肉的方式去补这个数据了，解析出binlog，去掉一些ddl操作的信息，然后构造sql  现在我也在考虑一个问题，是否可以考虑引入一个strict模式，如果非strict模式，针对这些ddl问题的数据，直接忽略，记录日志的方式来解决. 出问题的时候手工切换为非strict，尽可能少的跳过数据，避免大量数据丢失 
9,canal新增DevGuide 新增DevGuide，指导用户如何开发canal的组件，主要是介绍canal的类结构和模型 devguide在哪里？ 已完成DevGuide，https://github.com/alibaba/canal/wiki/DevGuide 
8,canal新增AdminGuide 需要新增AdminGuide，指导用户如何正确配置canal的相关参数，以及运维的基本操作 https://github.com/alibaba/canal/wiki/AdminGuide 
7,table meta是否主键信息判断错误 Entry数据中isKey的数据返回错误，全部返回为true 
6,dbsync binlog解析增加testcase用例 增加testcase用例，引导别人使用 
5,抽取mysql packet协议包为独立工程，方便重用 mysql packet介绍：模拟mysql登录，select/update语句与mysql进行通讯 fix issue 5     1. 新增了driver project 提供了MysqlConnector MysqlQueryExector MysqlUpdateExector     2. 修改原先的parse的包依赖，重构MysqlConnection，使用新的MysqlConnector 
3,mysql 编码参数去除charsetNumber，根据connectionCharset设置的编码即可 v1.0.0的版本中，设定编码需要同时设置charsetNumber和connectionCharset，charsetNumber主要是和mysql进行handshark时，指定character_set_results的返回编码。connectionCharset指定为客户端的解析编码 
2,instance增加自动扫描机制 扫描conf目录下的子目录，排除spring以外，将目录名做为instance destination，自动加载该instance，默认读取global lazy 
1,修复memory-instance.xml下，不配置zkserver，可以正常启动和关闭 fixed at version 1.0.1 
