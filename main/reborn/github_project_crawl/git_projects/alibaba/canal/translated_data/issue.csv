issue_id,issue_content
1086,Application yml configuration package settings default template
1084,fix #1083
1083,Flat Send RocketMQ appears to send empty packets. Kafka should have the same problem. [https://github.com/alibaba/canal/blob/master/server/src/main/java/com/alibaba/otter/canal/rocketmq/CanalRocketMQProducer.java](url) Code The value returned in the array of arrays at line 96 may be empty but there is no judgment below to send a null message to mq. Kafka should also have the same problem. Another question, please ask God to help answer FlatMessage This object Attribute data length Under what circumstances is greater than 1 There is a dml operation data Size will be greater than 1
1082,Cana kafak1 1 0 version build kafka consumer client how to parse the ROWDATA type storeValue 2018-10-31 15:49:04.318 [Thread-1] INFO c.a.o.canal.client.running.kafka.CanalKafkaClientExample - Message[id=12 entries=[header { version: 1 logfileName: "mysql-bin.000002" logfileOffset: 1206761 serverId: 1 serverenCode: "UTF-8" executeTime: 1540972144000 sourceType: MYSQL schemaName: "" tableName: "" eventLength: 79 } entryType: TRANSACTIONBEGIN storeValue: " _" header { version: 1 logfileName: "mysql-bin.000002" logfileOffset: 1206909 serverId: 1 serverenCode: "UTF-8" executeTime: 1540972144000 sourceType: MYSQL schemaName: "smart_meter" tableName: "sys_region_new" eventLength: 46 eventType: INSERT props { key: "rowsCount" value: "1" } } entryType: ROWDATA storeValue: "\b\212\001\020\001P\000b\217\001\022\036\b\000\020\004\032\002id \001(\0010\000B\00530026R\aint(11)\022\037\b\001\020\f\032\004name \000(\0010\000B\000R\vvarchar(20)\022!\b\002\020\004\032\tparent_id \000(\0010\000B\0012R\aint(11)\022)\b\003\020\371\377\377\377\377\377\377\377\377\001\032\005level \000(\0010\000B\0011R\ntinyint(1)" header { version: 1 logfileName: "mysql-bin.000002" logfileOffset: 1206955 serverId: 1 serverenCode: "UTF-8" executeTime: 1540972144000 sourceType: MYSQL schemaName: "" tableName: "" eventLength: 31 } entryType: TRANSACTIONEND storeValue: "\022\0049961" ] raw=true rawEntries=[]] I want to know how to resolve storeValue I have been tossing for a long time. Can refer to MessageUtil.parse4Dml
1081,10 1 22 MariaDB version database journalName garbled Hello, I am using 10 1 22 MariaDB and 1 1 1 version canal after booting the following error 2018-11-01 09:40:42.663 [destination = cloud address = /192.168.1.21:3306 EventParser] WARN c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - prepare to find start position just last position {"identity":{"slaveId":-1 "sourceAddress":{"address":"192.168.1.21" "port":3306}} "postion":{"gtid":"" "included":false **"journalName":"mysql-bin.000607Æ\u009E´U"** "position":91598031 "serverId":1 "timestamp":1541028888000}} 2018-11-01 09:40:42.674 [destination = cloud address = /192.168.1.21:3306 EventParser] WARN c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - find start position : EntryPosition[included=false journalName=mysql-bin.000607Æ´U position=91598031 serverId=1 gtid= timestamp=1541028888000] 2018-11-01 09:40:42.690 [destination = cloud address = /192.168.1.21:3306 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:102) ~[canal.parse-1.1.1.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:216) [canal.parse-1.1.1.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:252) [canal.parse-1.1.1.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-11-01 09:40:42.691 [destination = cloud address = /192.168.1.21:3306 EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address /192.168.1.21:3306 has an error retrying. caused by java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:102) ~[canal.parse-1.1.1.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:216) ~[canal.parse-1.1.1.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:252) ~[canal.parse-1.1.1.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-11-01 09:40:42.692 [destination = cloud address = /192.168.1.21:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:cloud[java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:102) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:216) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:252) at java.lang.Thread.run(Thread.java:748) ] Seeing that there is garbled journalName, how can I solve it? Local has not been heavy about the existing debug ability, you can try to see the resolution process of RotateLogEvent I changed the EntryPosition class Added an analytical method ``` public EntryPosition(String journalName Long position Long timestamp){ super(timestamp); this.journalName = filterLogFileName(journalName); this.position = position; } public EntryPosition(String journalName Long position Long timestamp Long serverId){ this(journalName position timestamp); this.serverId = serverId; } public String filterLogFileName(String logfileName) { Pattern p = Pattern.compile("(mysql-bin\\.[0-9]{6})."); Matcher m = p.matcher(logfileName); while(m.find()){ if(m.groupCount() >= 1) { return m.group(1); } } return logfileName; } public String getJournalName() { return filterLogFileName(journalName); } public void setJournalName(String journalName) { this.journalName = filterLogFileName(journalName); } ``` Then it’s so good This reform is a hard-coded process of saving the country without finding the root cause. @haopangxu Can try my change main reason Mariadb is not quite consistent with mysql in handling checksum behavior. For the rotate_event binlog object mariab will execute checksum and mysql will not need special judgment.
1079,Application yml is not packaged into jar Application yml is not packaged into jar Solve the merge error in the application yml and config in the jar
1078,canal-server Will a reboot in the backlog event discard the backlog in memory? ## problem canal-server In cooperation ZooKeeperLogPositionManager is read after restarting binlog The locus is server Last time parse Position still client ack s position according to https://github.com/alibaba/canal/wiki/Introduction#eventparser%E8%AE%BE%E8%AE%A1 Description of EventParser recorded binlog The last time is parse Does this behavior at the site of the site lead to server Drop event on reboot ## phenomenon This question was raised because it was encountered in practice. binlog The problem of missing events is as follows in canal-server There is an event backlog about 20 M restarts when the set memory limit is not reached canal server canal client zk 使用 docker-compose Deploy the server client with zk Single instance server Use mirroring as canal/canal-server:v1.1.0），canal Some events will be discarded ## binlog Lost reason guess according to https://github.com/alibaba/canal/wiki/Introduction#eventparser%E8%AE%BE%E8%AE%A1 Description of EventParser Record only parse Arrived binlog The reason for the site guess is server The terminal restarts after restarting the event from the last time parse The consumption of the site is caused by the memory parse、sink、store But not yet get of binlog Lost ## client Code client based on commit node 82f8a9f - fixed issue #483 show slave hosts Modify use com.alibaba.otter.canal.client.ClientLauncher With custom CanalOuterAdapter Make a purchase in order to record the received binlog Site pair com.alibaba.otter.canal.client.adapter.loader.CanalAdapterWorker The following modifications were made ```java while (running) { try { // if (switcher != null) { // switcher.get(); // } logger.info("=============> Start to connect destination: {} <=============" this.canalDestination); connector.connect(); logger.info("=============> Start to subscribe destination: {} <=============" this.canalDestination); connector.subscribe(); logger.info("=============> Subscribe destination: {} succeed <=============" this.canalDestination); while (running) { // try { // if (switcher != null) { // switcher.get(); // } // } catch (TimeoutException e) { // break; // } // Server configuration canal instance network soTimeout default 30s) // The server does not interact with the server within the scope. The socket connection will be closed. // Below is the added part Message message = connector.getWithoutAck(BATCH_SIZE); // Get the specified amount of data for (CanalEntry.Entry entry : message.getEntries()) { if (!entry.getEntryType().equals(CanalEntry.EntryType.ROWDATA)) { continue; } logger.info("receive binlog event file {} offset {}" entry.getHeader().getLogfileName() entry.getHeader().getLogfileOffset()); } ``` an examination client Log found received binlog Discontinuous range canal Totally discarded binlog The same range because it is client Printed at the entrance binlog Site so I think client Not received binlog Event server Discarded this part of the incident ## Configuration instance.xml versus canal/deployer/src/main/resources/spring/default-instance.xml of diff as follows ```xml 112 120c112 113 < <bean class="com.alibaba.otter.canal.parse.index.FailbackLogPositionManager"> < <constructor-arg> < <bean class="com.alibaba.otter.canal.parse.index.MemoryLogPositionManager" /> < </constructor-arg> < <constructor-arg> < <bean class="com.alibaba.otter.canal.parse.index.MetaLogPositionManager"> < <constructor-arg ref="metaManager"/> < </bean> < </constructor-arg> --- > <bean class="com.alibaba.otter.canal.parse.index.ZooKeeperLogPositionManager"> > <constructor-arg index="0" ref="zkClientx"/> 129c122 < <bean class="com.alibaba.otter.canal.parse.support.AuthenticationInfo" init-method="initPwd"> --- > <bean class="com.alibaba.otter.canal.parse.support.AuthenticationInfo"> 133 134d125 < <property name="pwdPublicKey" value="${canal.instance.pwdPublicKey:retl}" /> < <property name="enableDruid" value="${canal.instance.enableDruid:false}" /> 139c130 < <bean class="com.alibaba.otter.canal.parse.support.AuthenticationInfo" init-method="initPwd"> --- > <bean class="com.alibaba.otter.canal.parse.support.AuthenticationInfo"> 143 144d133 < <property name="pwdPublicKey" value="${canal.instance.pwdPublicKey:retl}" /> < <property name="enableDruid" value="${canal.instance.enableDruid:false}" /> ``` ## Please ask the gods to be confused, is my configuration wrong? server Really lose backlog of events after reboot The storage of the canal server event is based on the memory implementation of the EventStore Losing is inevitable, look at the wiki. The wiki is written very clearly. Restart reading the last client The ack&#39;s memory data will be lost but will continue to be dump based on the last successful consumption. Binlog So there is no message loss on the architecture as long as the correct ack site You are a configuration problem. You must refer to the file default instance xml in the com alibaba otter canal parse index FailbackLogPositionManager configuration. It will read MetaLogPositionManager when restarting. Ack&#39;s locus > You are a configuration problem. You must refer to the file default instance xml in the com alibaba otter canal parse index FailbackLogPositionManager configuration. It will read MetaLogPositionManager when restarting. Ack&#39;s locus Thank God for guiding the problem has been solved
1076,About rds oss binlog Offline read problem hi Hello I configured it in the instance configuration file. #rds oss binlog canal.instance.rds.accesskey= canal.instance.rds.secretkey= canal.instance.rds.instanceId= Restart canal The cluster still can&#39;t get the binlog of oss What is the reason for this? Look at the instance log and there is no information about oss. This is the error message 2018-10-30 16:33:20.395 [destination = instance address = address EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:instance[java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:102) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:153) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:262) at java.lang.Thread.run(Thread.java:745) Canal instance rds accesskey canal instance rds secretkey operation ID to the application rds instanceId instance id is also no problem oss Binlog&#39;s backtracking currently only supports timestamp-based positioning. It will determine the current mysql. If binlog has the data of the timestamp, it will automatically download binlog from oss and then parse it locally until local mysql Time can be connected in the binlog ps. If you give a wrong binlog site, you are not sure whether to give the site error or the binlog is deleted. You can then optimize the RDS case. @agapple Thank you for your reply. Now there is still a problem that I canal The normal consumption binlog of the clients has been running for a while, but I haven’t updated the location where the instance was consumed in zk. What is the reason? The HA of the canal is made with three nodes. As shown ![2121](https://user-images.githubusercontent.com/12511065/47762190-91bdca00-dcf6-11e8-8aaa-aec578ff67d1.jpg) Yesterday afternoon, I saw this, there is no update position. canal What is the update strategy for the ha site? I see the documentation saying so. **canal After receiving the ack of the client, the server will record the last site submitted by the client. But my site has not been updated 1. You can&#39;t open canal instance filter transaction entry true on your server side. 2. Check the meta log corresponding to the server node of the current run. If you have an ack, the information will be recorded. @agapple 1. I set canal instance filter transaction entry true for each node. 2 did not see the running node has a corresponding meta log ... Well, there is a problem with my cluster setup. 1. Canal instance filter transaction entry true This set of questions filters the header and tail without updates. Since the bit record is recorded, the new end v1 1 1 is compatible with this parameter. Filter does not filter all but keeps a transaction event for a few seconds. 2. rds The problem with the binlog site has been compatible. The normal site is deleted. As long as the timestamp is in the site, the oss will be downloaded automatically. binlog @agapple Thanks again Zk site update can already be caused by the problem of canal instance filter transaction entry true > oss Binlog&#39;s backtracking currently only supports timestamp-based positioning. It will determine the current mysql. If binlog has the data of the timestamp, it will automatically download binlog from oss and then parse it locally until local mysql Time can be connected in the binlog > > ps. If you give a wrong binlog site, you are not sure whether to give the site error or the binlog is deleted. You can then optimize the RDS case. What do you say about RDS optimization? I still can&#39;t determine if it has gone to oss to download the binlog I want. Optimization I have submitted the code and can re-package based on the master
1075,Kafka canal instance filter transaction entry problem The kafka mode is set after the canal instance filter transaction entry true in the canal properties file. Masked TransactionBegin event but can also receive Transactionend event Is the use of flatMessage false can try flat case is true
1071,Canal1 1 1 running client KafkaClientRunningTest error In the 1 1 0 version is Message message = connector.getWithoutAck(3L TimeUnit.SECONDS); In the 1 1 1 version is List Message messages = connector.getList(3L TimeUnit.SECONDS); There is no problem running the 1 1 0 version. You can see that the message packet runs 1 1 1 and the error is reported at this line of code. Is it still not a stable version? Set flatMessage in the configuration mq yml file in deploy to false KafkaClientRunningTest for native message reception Configure mq yml file to set flatMessage to false KafkaClientRunningTest to native message connector.getWithoutAck(batchSize 5L TimeUnit.SECONDS); Execution can&#39;t run in 2018-10-30 09:45:54，"rewerma" <notifications@github.com> Write Set flatMessage in the configuration mq yml file in deploy to false KafkaClientRunningTest for native message reception — You are receiving this because you authored the thread. Reply to this email directly view it on GitHub or mute the thread. 报com alibaba otter canal protocol exception CanalClientException  mq not support this method connector.getWithoutAck(batchSize 5L TimeUnit SECONDS This is not a method provided by kafkaConnector Test class do not modify There is no problem with using this code. ``` executor.submit(new Runnable() { @Override public void run() { connector.connect(); connector.subscribe(); while (running) { try { List<Message> messages = connector.getList(3L TimeUnit.SECONDS); if (messages != null) { System.out.println(messages); } connector.ack(); } catch (WakeupException e) { // ignore } } connector.unsubscribe(); connector.disconnect(); } }); ``` Use the above test class code flatMessage set to false after running, no response to change the catch block to Exception After seeing the error message is `Caused by: com.alibaba.otter.canal.protocol.exception.CanalClientException: deserializer failed at com.alibaba.otter.canal.client.CanalMessageDeserializer.deserializer(CanalMessageDeserializer.java:52) at com.alibaba.otter.canal.client.CanalMessageDeserializer.deserializer(CanalMessageDeserializer.java:14) at com.alibaba.otter.canal.client.kafka.MessageDeserializer.deserialize(MessageDeserializer.java:24) at com.alibaba.otter.canal.client.kafka.MessageDeserializer.deserialize(MessageDeserializer.java:1) at org.apache.kafka.common.serialization.ExtendedDeserializer$Wrapper.deserialize(ExtendedDeserializer.java:65) at org.apache.kafka.common.serialization.ExtendedDeserializer$Wrapper.deserialize(ExtendedDeserializer.java:55) at org.apache.kafka.clients.consumer.internals.Fetcher.parseRecord(Fetcher.java:967) at org.apache.kafka.clients.consumer.internals.Fetcher.access$3300(Fetcher.java:93) at org.apache.kafka.clients.consumer.internals.Fetcher$PartitionRecords.fetchRecords(Fetcher.java:1144) at org.apache.kafka.clients.consumer.internals.Fetcher$PartitionRecords.access$1400(Fetcher.java:993) at org.apache.kafka.clients.consumer.internals.Fetcher.fetchRecords(Fetcher.java:527) at org.apache.kafka.clients.consumer.internals.Fetcher.fetchedRecords(Fetcher.java:488) at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1155) at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1115) at com.alibaba.otter.canal.client.kafka.KafkaCanalConnector.getListWithoutAck(KafkaCanalConnector.java:173) at com.alibaba.otter.canal.client.kafka.KafkaCanalConnector.getList(KafkaCanalConnector.java:159) at com.alibaba.otter.canal.client.running.kafka.KafkaClientRunningTest$1.run(KafkaClientRunningTest.java:44) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) Caused by: com.google.protobuf.InvalidProtocolBufferException: Protocol message end-group tag did not match expected tag. at com.google.protobuf.InvalidProtocolBufferException.invalidEndTag(InvalidProtocolBufferException.java:110) at com.google.protobuf.CodedInputStream$ArrayDecoder.checkLastTagWas(CodedInputStream.java:660) at com.google.protobuf.CodedInputStream$ArrayDecoder.readGroup(CodedInputStream.java:869) at com.google.protobuf.UnknownFieldSet$Builder.mergeFieldFrom(UnknownFieldSet.java:541)` groupId another The old faltMessage you are consuming Ok, thank you master
1069,pass useDruidDdlFilter parameter to LogEventConvert Description of tsdb function changes 1. Tsdb should only care about ddl Dml and dcl should be skipped image #450 Such problems will not appear again, of course fastsql should also support dcl 2. Regardless of whether ddl filters tsdb functions, it should ensure correctness. 3. ddl The history write is placed behind the filter to ensure that there is no permission problem when the table structure is compared. [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=1069) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=1069) before we can accept your contribution.<br/><hr/>**wuwo** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=1069) it.</sub> [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=1069) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=1069) before we can accept your contribution.<br/><hr/>**wuwo** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=1069) it.</sub> Such a change will have a risk if fastsql Parse is parsing normal DDL statements when there is an exception DdlResult returns the default is Query, you will ignore it here, even a little log is not If the ddl parsed exception plus the ddl parsed exception is ignored and then directly return Query Ddl exception plus effect, I think there is no difference between it and before. If the druidFilter can be parsed normally, the unsubscribed table has been filtered. I rolled back the changes about tsdb but currently as you said if the normal ddl parsing failed Will return directly to Query. If dcl filtering is enabled, this ddl change will still lose the normal ddl resolution. If ddl filtering is enabled. Ddl changes will also be lost I still think that the correctness of the tsdb function should not be tied to the filterQueryDdl filterQueryDml filterQueryDcl. It is estimated that many users are not aware of this problem. I think about other elegant adjustments. tks
1066,Ddl is lost during binlog synchronization Mysql version 5.6.26 Canalserver version 1.0.24 In the process of real-time synchronization binlog processing dml, tens of seconds sent a ddl statement and found that the client did not get this ddl statement. What is the reason? canal.instance.get.ddl.isolation Is it related to this configuration? I use the default value of false. Close the issue, I found it myself. @yxzhang666 > Close the issue, I found it myself. @yxzhang666 The logic of our DBA add table field is inconsistent with our processing. We pass the CanalEntry EventType ALTER Get ddl And deal with the ddl statement dba is to add the field after the table name is replaced by the clone table and then the previous table drop and then change the name of the table before the clone table name. The new table name we did not pay attention to, so did not get the ddl strictly speaking Canal The server did not lose the binlog We didn’t get a false alarm Canal instance get ddl isolation This isolation we want to open there is no suggestion Is this not the practice of pt schema change online and ghost? Canal instance get ddl isolation true If it is open it will be separated from other DML multiple batches A batch a ddl No difference between the essence can be used for data synchronization > A batch of a ddl is not the essence of the difference can be used for data synchronization Thank you, I&#39;ve now understood > Is this not the practice of pt schema change online and ghost? Don&#39;t know much about this piece
1065,fix: fix NPE When using CanalServerWithManager, do not pass the masterPosition in GTID mode. NPE is generated when the masterPosition is not passed in the GTID mode when using the CanalServerWithManager. [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=1065) <br/>All committers have signed the CLA. tks Your CanalServerWithManager docked your own console @agapple We are a platform based on canal for incremental and full data synchronization. I made a set of consoles myself. > I made a set of consoles myself. I haven&#39;t done the page yet, but I have done core functions.
1064,1 1 0 version of the golang client Error message Fatal error: proto: Messages: illegal tag 0 (wire type 0)exit status 1 Can you give me a picture of how you called it? you can go [canal-go-issue](https://github.com/CanalClient/canal-go/issues) Issue Looked at this error occurred when the get operation proto decoding and there is no operation at the time I am testing the pass here. Is it still wrong? Occasionally there is not always there and there should be no data at the time when the database is not operating. Has been resolved
1063,RocketMQ partition sending No commit RocketMQ partition sending No commit [https://github.com/alibaba/canal/blob/master/server/src/main/java/com/alibaba/otter/canal/rocketmq/CanalRocketMQProducer.java](url)
1061,Abnormal full Gc problem We are using the 1 10 version to do a simple secondary development to achieve data transmission to kafka multi-partition through the pinpoint monitoring tool to find that the server will trigger full when the heap memory is used to 1 5G Gc but the default setting of the maximum available memory is 3G consumer does not have this problem start start sh content has not been modified for specific reasons are being investigated, do other people have this problem
1060,AdminGuide document not updated 1. operating system a. Pure java development windows linux can support b. Jdk recommends using a stable version of 1 6 25 or higher. Currently Alibaba uses this version basically. 2. Mysql request a. Currently canal supports mysql 5 5 version below mysql5 6 does not support mysql4 x version has not been rigorously tested theoretically compatible Fixed
1059,CanalConnector interface removes the stopRunning method CanalConnector interface removes the stopRunning method The stopRunning method is only called internally
1058,Doubts about the Docker pattern 1、docker stop Container id can&#39;t stop the container 2 Using docker rm -f Remove the container but use the image to start the container again. It shows that it has just been removed. It already exists. Sometimes there is still 11111 port occupied. You need to modify the container name in the startup script to start the container. Thank you for explaining Look for the basic operation of docker online. close
1057,kafka canal-adapter Distributed switch bug Solve the problem that the distributed synchronous switch is off synchronously Restart kafka Sync after the canal adapter Missing data bug
1056,group The thread pool is not initialized.
1055,Code sorting
1054,[ISSUE1027]Add flat message support to rocketmq Added support for rocketmq queues after hashing so that it can distribute Flat message. tks
1053,Whether can support transaction grouping In the database operation, the multi-table operation with transaction nature can not group or package this part of data into one packet when the data is output or group it according to the transaction id. The data transaction has a begin end for parcel service and can be combined based on this block. > The data transaction has a begin end for parcel service and can be combined based on this block. Whether can support python docking to synchronize data to kafka The new 1 1 1 version will support direct delivery of data to kafka and can be consumed by the python client corresponding to kafka. > The new 1 1 1 version will support direct delivery of data to kafka and can be consumed by the python client corresponding to kafka. Thank you, I can directly configure the data from kafka through python. Yes > Yes I saw the commit record in the 1 1 1 version on GitHub and wrote to remove kafka The server side TRANSACTIONBEGIN and TRANSACTIONEND judge this version to handle the transaction operation, then by what way to determine the transaction id?
1052,Canal start normal client can not get binlog # Mysql version 5.7 # canal 1.1.1 # logs/example/example.log ``` 2018-10-28 09:20:28.615 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2018-10-28 09:20:28.630 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties] 2018-10-28 09:20:28.860 [main] WARN o.s.beans.GenericTypeAwarePropertyDescriptor - Invalid JavaBean property 'connectionCharset' being accessed! Ambiguous write methods found next to actually used [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.lang.String)]: [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.nio.charset.Charset)] 2018-10-28 09:20:28.967 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2018-10-28 09:20:28.967 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties] 2018-10-28 09:20:29.289 [main] ERROR com.alibaba.druid.pool.DruidDataSource - testWhileIdle is true validationQuery not set 2018-10-28 09:20:29.736 [main] INFO c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-example 2018-10-28 09:20:29.916 [main] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - subscribe filter change to .*\..* 2018-10-28 09:20:29.916 [main] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - start successful.... 2018-10-28 09:20:29.918 [destination = example address = /192.168.255.129:3306 EventParser] WARN c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - prepare to find start position just show master status 2018-10-28 09:20:59.976 [New I/O server worker #1-1] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - subscribe filter change to .*\..* 2018-10-28 09:23:20.777 [destination = example address = /192.168.255.129:3306 EventParser] WARN c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - find start position : EntryPosition[included=false journalName=mysql-bin.000009 position=383 serverId=1 gtid=<null> timestamp=1540689592000] 2018-10-28 09:25:09.385 [New I/O server worker #1-2] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - subscribe filter change to .*\..* ~ ``` Canal canal log error ``` 2018-10-28 12:13:15.534 [New I/O server worker #1-2] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x26e93058 /192.168.255.1:65515 => /192.168.255.129:11111] exception=java.io.IOException: Connection reset by peer at sun.nio.ch.FileDispatcherImpl.read0(Native Method) at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39) at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223) at sun.nio.ch.IOUtil.read(IOUtil.java:192) at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380) at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:322) at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) ``` Connection reset by peer Estimated free link is closed > Connection reset by peer Estimated free link is closed How to solve please advise solved
1051,Binlog change client is not responding Mysql canal use docker mode to deploy both startups are normal mysql Binlog file mapping in the host var lib mysql directory operation database client side no response always print empty Whether count is a problem with the binlog file location Mysql version is 5 7 The same problem Check canal Corresponding log in the server I am using stand-alone mode and need to set the parallel parameter to false but the error log only shows a null pointer error message is not friendly
1050,Modify the hbase configuration item name
1049,1 1 1 version configuration kafka What is the groupId in AbstractKafkaTest?
1047,tablemeta Tsdb data increases expiration cleanup ability TableMetaTSDB介绍 https github com alibaba canal wiki TableMetaTSDB Thinking about the problem tablemeta will do checkpoint 24 hours by default to generate a snapshot data persistence. As the running time gets longer and longer, the snapshot data will continue to expand. It is necessary to add an expire strategy to periodically clean up the stale data. Design ideas 1. Add two parameters canal instance tsdb snapshot interval / canal.instance.tsdb.snapshot.expire 2. Periodically clean the snapshot data that exceeds the expire time within the interval of the interval. Note that the first snapshot of the initial init will be retained here. The first binlog_timestamp 1 Avoid all snapshot data being cleaned up
1046,Column type is tinyint 1 Unsigned but the column value obtained by canal is incorrect when the actual data value is greater than 127 Column type is tinyint 1 Unsigned but the actual data value is greater than 127. Between 128 and 255 canal is processed according to boolean and directly converted to string value becomes negative. In LogEventConvert java, line 674 is recommended to be modified as follows. // if (isSingleBit && javaType == Types.TINYINT) { // javaType = Types.BIT; // } if (buffer.isNull()) { columnBuilder.setIsNull(true); } else { final Serializable value = buffer.getValue(); if (isSingleBit && javaType == Types.TINYINT && ((Number) value).intValue() >= 0) { javaType = Types.BIT; } //... https://github.com/alibaba/canal/blob/master/parse/src/main/java/com/alibaba/otter/canal/parse/inbound/mysql/dbsync/LogEventConvert.java Convenient to provide SQL for testing
1045,Modify the package configuration tks
1044,canal Filtered expression Expression is too large. Caused by: com.alibaba.otter.canal.filter.exception.CanalFilterException: org.apache.oro.text.regex.MalformedPatternException: Expression is too large. Caused by: org.apache.oro.text.regex.MalformedPatternException: Expression is too large. at org.apache.oro.text.regex.Perl5Compiler.compile(Unknown Source) at org.apache.oro.text.regex.Perl5Compiler.compile(Unknown Source) at com.alibaba.otter.canal.filter.PatternUtils$1.apply(PatternUtils.java:30) at com.alibaba.otter.canal.filter.PatternUtils$1.apply(PatternUtils.java:25) at com.google.common.collect.ComputingConcurrentHashMap$ComputingValueReference.compute(ComputingConcurrentHashMap.java:356) at com.google.common.collect.ComputingConcurrentHashMap$ComputingSegment.compute(ComputingConcurrentHashMap.java:182) at com.google.common.collect.ComputingConcurrentHashMap$ComputingSegment.getOrCompute(ComputingConcurrentHashMap.java:151) at com.google.common.collect.ComputingConcurrentHashMap.getOrCompute(ComputingConcurrentHashMap.java:67) at com.google.common.collect.MapMaker$ComputingMapAdapter.get(MapMaker.java:885) at com.alibaba.otter.canal.filter.PatternUtils.getPattern(PatternUtils.java:41) at com.alibaba.otter.canal.filter.aviater.RegexFunction.call(RegexFunction.java:24) at Script_1540529734651_0.execute0(Unknown Source) at com.googlecode.aviator.ClassExpression.execute(ClassExpression.java:53) at com.alibaba.otter.canal.filter.aviater.AviaterRegexFilter.filter(AviaterRegexFilter.java:74) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEventForTableMeta(LogEventConvert.java:456) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$SimpleParserStage.onEvent(MysqlMultiStageCoprocessor.java:247) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$SimpleParserStage.onEvent(MysqlMultiStageCoprocessor.java:222) at com.lmax.disruptor.BatchEventProcessor.processEvents(BatchEventProcessor.java:168) at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:125) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) This question is very strange. I will have no problem after I execute it later. How long is your expression? Wait, let me revisit it carefully. I am debugging it. canal Expression calculation module pattern="^pub_operation_log2.tb_operation_log_429$|^pub_operation_log2.tb_operation_log_428$|^pub_operation_log0.tb_operation_log_201$|^pub_operation_log2.tb_operation_log_427$|^pub_operation_log0.tb_operation_log_200$|^pub_operation_log2.tb_operation_log_426$|^pub_operation_log2.tb_operation_log_425$|^pub_operation_log2.tb_operation_log_424$|^pub_operation_log0.tb_operation_log_205$|^pub_operation_log2.tb_operation_log_423$|^pub_operation_log0.tb_operation_log_204$|^pub_operation_log2.tb_operation_log_422$|^pub_operation_log0.tb_operation_log_203$|^pub_operation_log2.tb_operation_log_421$|^pub_operation_log0.tb_operation_log_202$|^pub_operation_log2.tb_operation_log_420$|^pub_operation_log0.tb_operation_log_209$|^pub_operation_log0.tb_operation_log_208$|^pub_operation_log0.tb_operation_log_207$|^pub_operation_log0.tb_operation_log_206$|^pub_operation_log2.tb_operation_log_419$|^pub_operation_log2.tb_operation_log_418$|^pub_operation_log2.tb_operation_log_417$|^pub_operation_log2.tb_operation_log_41 。。。。。。" target= "tts_operation_log0.tb_operation_log_244" I&#39;m here otter In the configuration, the sub-library is also used. pub_operation_log[0-4] | tb_operation_log_[0-499] I saw it really is The table name is too long 2500 个 pub_operation_log2 tb_operation_log_417  Similar to this expression I am running a demo I can&#39;t run anymore. idea Directly say Error 109 60) java: Constant string is too long When using sub-database sub-tables, you need to pay attention to the following strings can not be too long You are a sub-library You can use regular matching to match
1043,Canal can&#39;t parse mysql5 6 binlog for God help Operating system Centos6 5 Database version mysql5 6 First of all, I added these three in the etc my cnf file of mysql. [mysqld] server-id=1 log-bin=mysql-bin binlog-format=ROW After the configuration is finished, look at the picture ![image](https://user-images.githubusercontent.com/32409300/47547957-f4dfe300-d929-11e8-9cfb-c6bac9679f58.png) Added a canal account to my mysql Open all permissions Repl_slave_priv Y ![image](https://user-images.githubusercontent.com/32409300/47548081-6750c300-d92a-11e8-942b-9b5bba436b97.png) Monitor canal&#39;s log tail -f logs/example/example.log Start to start canal The log content is as follows ![image](https://user-images.githubusercontent.com/32409300/47548180-bdbe0180-d92a-11e8-9a64-7608dccd144f.png) My canal configuration item ![image](https://user-images.githubusercontent.com/32409300/47548240-05dd2400-d92b-11e8-8dc2-d7a1bced6de3.png) This parameter is not in the screenshot canal instance filter regex Executing sql to view the files in the binlog log is subject to change. ![image](https://user-images.githubusercontent.com/32409300/47548333-5785ae80-d92b-11e8-9d0f-5ed4882955b9.png) In the above figure, the position of the binlog is a bit problematic. The correct posture is usr bin mysqlbinlog. -v mysql-bin.000003 ![image](https://user-images.githubusercontent.com/32409300/47548446-a7647580-d92b-11e8-9bd1-69c495fb00b7.png) Test is my database name user is my table name After I executed sql Canal log has not changed The canal client can connect to the canal server normally, but the canal server can&#39;t parse the mysql binlog. If the description is not clear, please correct me. Example project is to print to another directory carefully looking for The big god that prints to another directory means that if the binlog log is generated, it is not visible in the log file example example log. @agapple ![image](https://user-images.githubusercontent.com/32409300/47554077-1dbca400-d93b-11e8-88ce-4cf6082b9646.png)
1042,canal adapter readme.md
1041,Canal example 1 1 0 The startup bat script under the tar gz package prompts that the command syntax is incorrect. I am currently testing in windows7 environment. Grammar error message content or submit a repaired PR to me ![2018-10-29_134009](https://user-images.githubusercontent.com/12081348/47631335-a710e800-db80-11e8-8825-06467c7045da.png) Very low-level questions basic bat usage you first google
1040,When the canal cluster is hung up, the zk node will be deleted. Will the data be repeated after another sever? When the canal cluster is hung up, the zk node will be deleted. Will the data be repeated after another sever? Data will be repeated > Data will be repeated Repeat is normal @agapple Can&#39;t avoid data not repeating more wiki
1039,Specification note tks
1038,Canal Send to Kafka Topic Implementing Partition at the same time as the message Partition Canal 1.1.0 Version messages are sent to Kafka while supporting specified messages by Partition Key Distribute to different partitions? Https github com alibaba canal wiki Canal Kafka RocketMQ QuickStart Upcoming 1 1 1 supports such hashing
1037,Improve the adapter launcher and hbase adaper related functions Perfect adapter launcher Perfect hbase adaper Add hbase Etl function Add client sync switch Ettl sync lock and other functions tks
1036,Clients can&#39;t connect after Docker deploys canal First canal has been deployed in docker to start the service port 11112 Then docker has a host IP with Virtual IP native computer access is via virtual IP 192 168 102 111 6633 Then the client uses the 192 168 102 111 6633 connection canal to start the service, the client does not receive the information and the canal server always outputs the ca otter canal server netty handler SessionHandler - message receives in session handler... How to solve the IP problem? Follow Docker The script in QuickStart is started as host mode and port mapping is done.
1034,How will canal The CanalEntry received by the client Parsed into executable sql Statement Master Mysql A Executed in sql then Slave canal Received CanalEntry How the object parses the object become Executable sql Then sync to the database mysql B has cost a lot of effort to assemble sql I found that if I had to assemble it, I wouldn’t have a different situation. Is there any convenient way? 1.1.0 In the version DDL operations can get sql directly Can the DML operation get sql directly? In the canal example project, see the sample code synchronized to the DB https github com alibaba canal tree master example src main java com alibaba otter canal example db
1032,v1.1.1-alpha 1 The version does not add a configurable row parameter. The default is true. v1.1.1-alpha 1 The version does not add a configurable row parameter. The default is true. There is still an Enable parsing performance issue with CanalServerWithEmbedded ![Uploading image.png…]() ![image](https://user-images.githubusercontent.com/34024756/47475509-474bd180-d84e-11e8-88f7-bc45e2df4483.png) ![image](https://user-images.githubusercontent.com/34024756/47475517-4f0b7600-d84e-11e8-8266-425e7162b102.png)
1031,Added adapter launcher project Add adapter launcher Springboot project
1030,Start up always does not move or error log has this 2018-10-24 11:08:47.331 [destination = canal-car address = rm-m5e5o46l2f13j7an8.mysql.rds.aliyuncs.com/172.31.19.222:3306 EventParser] WARN c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - prepare to find start position just show master status 2018-10-24 11:08:52.651 [destination = canal-car address = rm-m5e5o46l2f13j7an8.mysql.rds.aliyuncs.com/172.31.19.222:3306 EventParser] WARN c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - find start position : EntryPosition[included=false journalName=mysql- bin.000570 position=1244577 serverId=1797136470 gtid=<null> timestamp=1540350517000] This does not affect
1029,V1 1 0 Update the t_canal_table table on mysql about 30w records of the same field capacity 755B Number of fields 79 canal parsing bin Log error column size is not match for table: database_canal.t_canal_table 79 vs 78 Update statement sql = "UPDATE t_canal_table SET update_time= " + "'" + str(datetime.datetime.now()) + "'" Exception information Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: column size is not match for table:glocalme_css_tcanal.t_css_vsim 79 vs 78 ] 2018-10-10 11:37:10.242 [destination = canalEsInstance address = /192.168.0.135:3306 EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address 192.168.0.135/192.168.0.135:3306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Table exists in the ddl column delete operation needs to open the TSDB to support @agapple This operation does not perform DDL delete operation
1028,Fix Kafka FlatMessage Model Null Value set to "" canal Use kafka model And flatMessage in the case of The database null value is set to The phenomenon is as follows ![image](https://user-images.githubusercontent.com/7855069/47401555-6d04a800-d774-11e8-99a6-5b89b0338123.png) description Is null Case kafka message body Value It is recommended to return null Otherwise varchar Type null cannot be distinguished ![image](https://user-images.githubusercontent.com/7855069/47401616-a9d09f00-d774-11e8-9305-e2ba1c94d932.png) [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=1028) <br/>All committers have signed the CLA. [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=1028) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=1028) before we can accept your contribution.<br/><hr/>**titeng.jiang** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=1028) it.</sub>
1027,RocketMQ Increase support for FlatMessage In order to improve rocketmq Binlog distribution performance needs to increase data fragmentation support If the fragmentation data consistency is increased, it is not guaranteed. Whether this fragmentation should be based on the choices used to make the table level open for the primary key will not be modified or just to monitor the event itself does not pay attention to the order should be more suitable At present, we want to make a topic for each mysql database instance. Different database instances have different topics. Then Tag uses the library name table name to distinguish the table for the individual needs. The specified queue is sent. If the library has such a table, then The entire library is sent to a queue. The corresponding configuration needs to be transferred to the expmle for configuration. Code submitted
1026,canal varchar Null processing Use canal kafka to receive the message that the column value is null is set to int date Type received Can be understood as null by default For varchar How to distinguish text types Null and A single column object has a getisNull method to determine whether the value corresponding to this column is null. CanalEntry.Column This class LS Correct Answer
1025,destination:user_cards[com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.TableIdNotFoundException: not found tableId:21005 Environment Ubuntu 16.04 jdk：1.8 Canal environment a canal Server two instances Canal version canal kafka 1 1 0 The cause of the problem 1 delete the otter on zk znode Purpose to reset the binlog of the instance File and Position Detailed log 2018-10-23 16:05:48.880 [pool-6-thread-2] ERROR com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - process error! java.lang.NullPointerException: null at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.calculateSize(MemoryEventStoreWithBuffer.java:555) ~[canal.store-1.1.0.jar:na] at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.doGet(MemoryEventStoreWithBuffer.java:322) ~[canal.store-1.1.0.jar:na] at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.tryGet(MemoryEventStoreWithBuffer.java:261) ~[canal.store-1.1.0.jar:na] at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getEvents(CanalServerWithEmbedded.java:478) ~[canal.server-1.1.0.jar:na] at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:310) ~[canal.server-1.1.0.jar:na] at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:273) ~[canal.server-1.1.0.jar:na] at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.worker(CanalKafkaStarter.java:121) [canal.kafka-1.1.0.jar:na] at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.access$000(CanalKafkaStarter.java:26) [canal.kafka-1.1.0.jar:na] at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter$1.run(CanalKafkaStarter.java:70) [canal.kafka-1.1.0.jar:na] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_181] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_181] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_181] 2018-10-23 16:05:48.881 [pool-6-thread-2] INFO com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - ## the canal consumer user_cards is running now ...... 2018-10-23 16:05:48.881 [pool-6-thread-2] ERROR com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - process error! java.lang.NullPointerException: null at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.calculateSize(MemoryEventStoreWithBuffer.java:555) ~[canal.store-1.1.0.jar:na] at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.doGet(MemoryEventStoreWithBuffer.java:322) ~[canal.store-1.1.0.jar:na] at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.tryGet(MemoryEventStoreWithBuffer.java:261) ~[canal.store-1.1.0.jar:na] at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getEvents(CanalServerWithEmbedded.java:478) ~[canal.server-1.1.0.jar:na] at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:310) ~[canal.server-1.1.0.jar:na] at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:273) ~[canal.server-1.1.0.jar:na] at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.worker(CanalKafkaStarter.java:121) [canal.kafka-1.1.0.jar:na] at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.access$000(CanalKafkaStarter.java:26) [canal.kafka-1.1.0.jar:na] at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter$1.run(CanalKafkaStarter.java:70) [canal.kafka-1.1.0.jar:na] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_181] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_181] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_181] I violently deleted the znode in zk. When I restart the canal, he will reload the instance and canal. Server configuration? Stop canal Server then delete zknode and finally restart this to validate the location definition in the instance ps. Why do I need to stop the server and then delete because the server will refresh zknode if it does not stop, so deleting zknode in advance does not make any sense. Btw I stopped canal Delete the znode after the server and then from the upstream database show master Status takes out the binlog File and Position restart canal Server no problem canal Server and instance are not reported But once the upstream library has an update operation canal Server will always report this error instance without error 。 Use the latest v1 1 1 version Canal deployer deploys kafka synchronization Known issues fixed ps. The new version has merged the canal kafka project into the canal deployer Mainly modify the canal serverMode kafka in canal properties Thank you for this question 2018-10-23 22:02:58.642 [pool-4-thread-1] ERROR com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - process error! java.lang.NullPointerException: null at com.alibaba.otter.canal.store.helper.CanalEventUtils.createPosition(CanalEventUtils.java:69) ~[canal.store-1.1.0.jar:na] at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:375) ~[canal.store-1.1.0.jar:na] at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:36) ~[canal.store-1.1.0.jar:na] at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:307) ~[canal.server-1.1.0.jar:na] at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:273) ~[canal.server-1.1.0.jar:na] at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.worker(CanalKafkaStarter.java:121) [canal.kafka-1.1.0.jar:na] at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.access$000(CanalKafkaStarter.java:26) [canal.kafka-1.1.0.jar:na] at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter$1.run(CanalKafkaStarter.java:70) [canal.kafka-1.1.0.jar:na] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_131] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_131] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131]
1024,canal How to support the DRDS database [destination = example1 address = *************************:3306 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 1047 sqlstate = HY000 errmsg = [dc26f11c5801000][172.16.22.194:3306][zbtsteam_test]Unknown command @daoshunli Canal is a binlog that simulates mysql The dump protocol DRDS does not currently have direct support for the binlog protocol. It can directly subscribe to the RDS under the DRDS to directly subscribe.
1023,c.a.o.canal.parse.inbound.mysql.dbsync.LogEventConvert - table parser error WARN c.a.o.canal.parse.inbound.mysql.dbsync.LogEventConvert - table parser error : header{} Suddenly encountered such an abnormality today, this table synchronization is unsuccessful. I was still normal today and suddenly found out that this abnormality is going on. It is estimated that the corresponding table in the binlog is deleted. The reset site is skipped and the TSDB can be opened to avoid the TSDB.
1022,FlatMessage is directly produced to kafka and then kafka consumer end analysis efficiency is not higher, it feels that probuf is more efficient than json analysis. Object serialization will have many kinds of information message body is about 3 times larger than json The throughput dropped significantly
1021,1.0.24 canal Parsing mysql in hv mode Timestamp problem Parsing mysql in a single machine timestamp Data return is correct Instance mysql time 2018-10-19 19:09:09 Single machine parsing over canal gets 2018-10-19 19:09:09 Hv mode The parsing gets 2018 10 19 07:02:09 The difference is 12 hours Stand-alone and hv are the same server Hv three server time is CTC time zone Time is also correct. Do you know what the problem is? Hv is 啥canal is based on the data in the binlog can first look at the original binlog data through mysqlbinlog Wrong HA mode Yesterday, the mailbox has not been verified, I did not reply yesterday. Because of the production environment problem, I re-installed the canal environment HA mode in the test environment to resolve the timestamp type.
1020,canal-kafka Data synchronization to kafka after kafka Topic garbled ![default](https://user-images.githubusercontent.com/6134206/47210704-34587d80-d3c6-11e8-805c-0c7ed3772058.png) Data can be synchronized to kafka after canal startup But the synchronized data is garbled The canal version is 1 1 0 local view topic garbled for single node deployment The default transmission of kafka is the serialized Message bytecode. Not visible characters Need to deserialize to a Message with the kafka module in the client Good String and Message will be supported in the next version. You can change the code of the main branch of the kafka related code in the canal server package. Available First-arrival arrival Just wait for the new version to come out. String and Message will be supported in the next version. You can change the code of the main branch of the kafka related code in the canal server package. Available First-arrival arrival Just wait for the new version to come out. I use python client consumption kafka hope to support clear text send json data guide kafka convenient consumption without considering deserialization
1019,canal 1.1.0 Entry resolution performance issues Here entry toByteString is based on what we consider from canal 1 0 24 upgrade to 1 1 0 found that performance is much worse I saw that this place has already got the Entry object. It should be cached so that there is no need to do a parse again in the business. ``` public Event(LogIdentity logIdentity CanalEntry.Entry entry){ this.logIdentity = logIdentity; this.entryType = entry.getEntryType(); this.executeTime = entry.getHeader().getExecuteTime(); this.journalName = entry.getHeader().getLogfileName(); this.position = entry.getHeader().getLogfileOffset(); this.serverId = entry.getHeader().getServerId(); this.gtid = entry.getHeader().getGtid(); this.eventType = entry.getHeader().getEventType(); // build raw this.rawEntry = entry.toByteString(); this.rawLength = rawEntry.size(); if (entryType == EntryType.ROWDATA) { List<CanalEntry.Pair> props = entry.getHeader().getPropsList(); if (props != null) { for (CanalEntry.Pair p : props) { if ("rowsCount".equals(p.getKey())) { rowsCount = Integer.parseInt(p.getValue()); break; } } } } } ``` Similarly, there is already a RowChange but it has been serialized into bytes. ``` RowChange rowChange = rowChangeBuider.build(); if (tableError) { Entry entry = createEntry(header EntryType.ROWDATA ByteString.EMPTY); logger.warn("table parser error : {}storeValue: {}" entry.toString() rowChange.toString()); return null; } else { Entry entry = createEntry(header EntryType.ROWDATA rowChange.toByteString()); return entry; } ``` We are using CanalServerWithEmbedded Entry toByteString is mainly for the deployment mode of the server client to avoid the client serialization period is too long to cause the client Not going on tps Consider adding a configuration that allows the entry of an entry object, which introduces memory overhead.
1017,RDS configuration properties for the 1 1 0 version of the canal Support for the use of manager? canalInstanceWithManager This form of configuration Is it still not supported by the 3 configurations newly added by RDS? Consider submitting a PR to me already fixed
1016,Encountered once a day should execute connector.connect() first What is the cause of the exception? Otter encounters the following exception every day. The stack is connected and the interrupt needs to be rejoined. However, when this error is encountered, sometimes the synchronization is quickly restored. Sometimes the synchronization will be interrupted for about 10 minutes. What is the reason? pid:4 nid:3 exception:canal:hd_canal_236:com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4832) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:160) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:759) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:428) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:114) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:66) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:337) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:184) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:145) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) at java.lang.Thread.run(Thread.java:724) Caused by: java.io.IOException: should execute connector.connect() first at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.(MysqlQueryExecutor.java:30) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:87) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMetaByDB(TableMetaCache.java:80) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:30) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:55) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:50) at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) at com.google.common.cache.LocalCache.get(LocalCache.java:3937) at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941) at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824) at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830) The corresponding otter version is 啥 It is recommended to upgrade the latest version already processed
1015,Seek to guide the transaction number tranactionID obtained by canal and the transaction number transactionID in mysql are inconsistent. 1 canal&#39;s transactionID acquisition method TransactionEnd txEnd = TransactionEnd.parseFrom(entry.getStoreValue()); String txID = txEnd.getTransactionId(); Result 46 2. Query results in mysql Trx id counter 88363 How to find a query method in your mysql is obtained from the mysqlbinlog command Thank you for your reply, I am sorry that the method of the previous query is wrong. The transactionID obtained by canal has no problem. Current query method sudo mysqlbinlog --no-defaults /usr/local/mysql/data/mysql-bin.000149 Just MySQL&#39;s DDL statement is non-transactional. Can&#39;t roll back the DLL statement. How does canal handle ddl? Canal only parses the content recorded in the DDL binlog in the binlog. The transaction has been successfully submitted.
1014,Repair through show master Status Get multiple GTIDs When the carriage return causes the UUID Parsing failed bug as title tks
1013,canal.kafka Received garbled code with bin kafka console consumer sh command After inserting a record in the mysql database, use the command bin kafka console consumer sh --zookeeper 192.168.206.128:2181 --topic canal1 From beginning see the consumer output garbled `*: mysql-bin.000027*UTF-80BJP229 *7 mysql-bin.000027*UTF-80BJPF mysql-bin.000027*UTF-80Bcan_dbJ user_testP+Xb _-+_C-+++1Pb_id (0B6Ri++(11) +a+e (0BdafdR +a_cha_(40)age (0B15Ri++(11): +y_-+-bi+.000027 *UTF-80 8BJP261 *7 +y_-+-bi+.000027 *UTF-80 8BJPF  +y_-+-bi+.000027 *UTF-80 8Bca+_dbJ +_e__+e_+P*Xb _-+_C-+++1Pb^id (0B7Ri++(11) +a+e (0BeeeR +a_cha_(40)age (0B11Ri++(11): +y_-+-bi+.000027 *UTF-80 8BJP262` Then read the code in the kafka client and changed the corresponding configuration parameters of AbstractKafkaTest java. `public static String topic = "canal1"; public static Integer partition = 0; public static String groupId = ""; public static String servers = "192.168.206.128:6667"; public static String zkServers = "192.168.206.128:2181";` After inserting a new record into mysql but running KafkaClientRunningTest java did not receive the packet, have you ever encountered this situation? canal kafka The mode default is that the serialized byte of the Send Message object is not a visible string. Client receives deserialization To send a visible string Please use flatMessage mode Will send json format data @brightsong I have encountered the same problem here. Is your problem solved? > After inserting a record in the mysql database, use the command bin kafka console consumer sh --zookeeper 192.168.206.128:2181 --topic canal1 From beginning see the consumer output garbled > `�*��: ���mysql-bin.000027�*UTF-80BJP����229 �*�7 ���mysql-bin.000027�*UTF-80BJPF���� �����mysql-bin.000027�*UTF-80B�can_dbJ user_testP+X�b _-+_C-+++��1������Pb_�����id �(�0B�6Ri++(11)��� ��+a+e (�0B�dafdR +a_cha_(40)�����age (�0B�15Ri++(11)�: ���+y_-+-bi+.000027�� �*UTF-80 8�BJP����261 �*��7 ���+y_-+-bi+.000027�� �*UTF-80 8�BJPF���� ��� ���+y_-+-bi+.000027�� �*UTF-80 8�B�ca+_dbJ +_e__+e_+P*X�b _-+_C-+++��1������Pb^�����id �(�0B�7Ri++(11)��� ��+a+e (�0B�eeeR +a_cha_(40)�����age (�0B�11Ri++(11)�: ���+y_-+-bi+.000027�� �*UTF-80 8�BJP����262` > Then read the code in the kafka client and changed the corresponding configuration parameters of AbstractKafkaTest java. > `public static String topic = "canal1"; public static Integer partition = 0; public static String groupId = ""; public static String servers = "192.168.206.128:6667"; public static String zkServers = "192.168.206.128:2181";` > After inserting a new record into mysql but running KafkaClientRunningTest java did not receive the packet, have you ever encountered this situation? I encountered the same problem as you did when changing the receiving format to ConsumerRecords String String> Can receive the output but the native ConsumerRecords String Message> No news, is your problem solved? > canal kafka The mode default is that the serialized byte of the Send Message object is not a visible string. Client receives deserialization > To send a visible string Please use flatMessage mode Will send json format data Which configuration item is corresponding to which configuration file? Solved the problem of changing the virtual machine cpu to a larger point, if you can not modify the canal instance parser parallel parameter in 2018-10-22 11:06:28，"jkl0898" <notifications@github.com> Write @brightsong I have encountered the same problem here. Is your problem solved? After inserting a record in the mysql database, use the command bin kafka console consumer sh --zookeeper 192.168.206.128:2181 --topic canal1 From beginning see the consumer output garbled �*��: ���mysql-bin.000027�*UTF-80BJP����229 �*�7 ���mysql-bin.000027�*UTF-80BJPF���� �����mysql-bin.000027�*UTF-80B�can_dbJ user_testP+X�b _-+_C-+++��1������Pb_�����id �(�0B�6Ri++(11)��� ��+a+e (�0B�dafdR +a_cha_(40)�����age (�0B�15Ri++(11)�: ���+y_-+-bi+.000027�� �*UTF-80 8�BJP����261 �*��7 ���+y_-+-bi+.000027�� �*UTF-80 8�BJPF���� ��� ���+y_-+-bi+.000027�� �*UTF-80 8�B�ca+_dbJ +_e__+e_+P*X�b _-+_C-+++��1������Pb^�����id �(�0B�7Ri++(11)��� ��+a+e (�0B�eeeR +a_cha_(40)�����age (�0B�11Ri++(11)�: ���+y_-+-bi+.000027�� �*UTF-80 8�BJP����262 Then read the code in the kafka client and changed the corresponding configuration parameters of AbstractKafkaTest java. public static String topic = "canal1"; public static Integer partition = 0; public static String groupId = ""; public static String servers = "192.168.206.128:6667"; public static String zkServers = "192.168.206.128:2181"; After inserting a new record into mysql but running KafkaClientRunningTest java did not receive the packet, have you ever encountered this situation? I encountered the same problem as you did when changing the receiving format to ConsumerRecords String String> Can receive the output but the native ConsumerRecords String Message> No news, is your problem solved? — You are receiving this because you were mentioned. Reply to this email directly view it on GitHub or mute the thread. Sorry to read wrong, it has not been resolved yet. in 2018-10-22 11:06:28，"jkl0898" <notifications@github.com> Write @brightsong I have encountered the same problem here. Is your problem solved? After inserting a record in the mysql database, use the command bin kafka console consumer sh --zookeeper 192.168.206.128:2181 --topic canal1 From beginning see the consumer output garbled �*��: ���mysql-bin.000027�*UTF-80BJP����229 �*�7 ���mysql-bin.000027�*UTF-80BJPF���� �����mysql-bin.000027�*UTF-80B�can_dbJ user_testP+X�b _-+_C-+++��1������Pb_�����id �(�0B�6Ri++(11)��� ��+a+e (�0B�dafdR +a_cha_(40)�����age (�0B�15Ri++(11)�: ���+y_-+-bi+.000027�� �*UTF-80 8�BJP����261 �*��7 ���+y_-+-bi+.000027�� �*UTF-80 8�BJPF���� ��� ���+y_-+-bi+.000027�� �*UTF-80 8�B�ca+_dbJ +_e__+e_+P*X�b _-+_C-+++��1������Pb^�����id �(�0B�7Ri++(11)��� ��+a+e (�0B�eeeR +a_cha_(40)�����age (�0B�11Ri++(11)�: ���+y_-+-bi+.000027�� �*UTF-80 8�BJP����262 Then read the code in the kafka client and changed the corresponding configuration parameters of AbstractKafkaTest java. public static String topic = "canal1"; public static Integer partition = 0; public static String groupId = ""; public static String servers = "192.168.206.128:6667"; public static String zkServers = "192.168.206.128:2181"; After inserting a new record into mysql but running KafkaClientRunningTest java did not receive the packet, have you ever encountered this situation? I encountered the same problem as you did when changing the receiving format to ConsumerRecords String String> Can receive the output but the native ConsumerRecords String Message> No news, is your problem solved? — You are receiving this because you were mentioned. Reply to this email directly view it on GitHub or mute the thread. > canal kafka The mode default is that the serialized byte of the Send Message object is not a visible string. Client receives deserialization > To send a visible string Please use flatMessage mode Will send json format data Where can I configure it? I also did not configure my content to be parsed after receiving the Message object and then put the content in json and send it to another kafka. in 2018-10-23 17:12:59，"liugaozy" <notifications@github.com> Write canal kafka The mode default is that the serialized byte of the Send Message object is not a visible string. Client receives deserialization To send a visible string Please use flatMessage mode Will send json format data Where can I configure it? — You are receiving this because you were mentioned. Reply to this email directly view it on GitHub or mute the thread.
1012,Canal can get LSN Log in mysql sequence Number Binlog does not have this Thank you so much
1011,Kafka mode client Offset does not advance Kafka mode client Offset does not advance, go back and forth, consume a few pieces of data, no error
1010,The latest 1 1 1 SNAPSHOT enable gtid mode will still report errno = 1236 1 1 1 SNAPSHOT version enables gtid mode source database gtid_purged has value after startup error can not be synchronized ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = The slave is connecting using CHANGE MASTER TO MASTER_AUTO_POSITION = 1 but the master has purged binary logs containing GTIDs that the slave requires. at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:102) Holding this exception area google mysql configuration problem > Holding this exception area google mysql configuration problem Mysql is configured gtid mode gtid_purged also has value online search for this error get the solution is to stop slave sync and then set gtid_purged on the slave But how do you set gtid_purged for a canal that simulates a slave? https://dev.mysql.com/doc/refman/5.6/en/replication-mode-change-online-enable-gtids.html Refer to this operation to set mysql server
1009,meta.dat not update [canal 1.1.0] Canal server and client consumes data normally but the timestamp and content of meta.dat not change which will cause an exception 'Could not find first log file name in binary log index file' when restart canal server. Pls check the configurations in attachment. [instance.log](https://github.com/alibaba/canal/files/2486428/instance.log) [canal.log](https://github.com/alibaba/canal/files/2486429/canal.log) Could not find first log file name in binary log index file The binlog on the mysql host has been deleted. I got more information with adding log in canal server. something unexpected occurs the updateCursor does not execute. ![image](https://user-images.githubusercontent.com/9636488/47079731-b9ae2780-d238-11e8-8fe6-41fedf6ac8df.png) finally I compile a new canal server with source code and deploy the meta.dat was updated normally.
1008,For mysql Enum type Canal returns int by default ie 1 2 3 How to return the value in enum For mysql Enum type Canal defaults to return int ie 1 2 3 Now you need to return the specific value of the enum type field. How to achieve it? // *********** My previous practice 1. Modify the mysqlToJavaType method of the rowslogbuffer of the parser module to return Types CHAR for enum 2. Modify the fetchValue method of rowslogbuffer. For the enum type, modify the string type to return the string but report the error. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: java.lang.IllegalArgumentException: limit excceed: 63 at com.taobao.tddl.dbsync.binlog.LogBuffer.getFullString(LogBuffer.java:1122) <img width="1125" alt="enum1" src="https://user-images.githubusercontent.com/28953872/47005872-44beed00-d167-11e8-83a9-bb56a2cae283.png"> <img width="1121" alt="enum2" src="https://user-images.githubusercontent.com/28953872/47005873-45578380-d167-11e8-8512-1b0eee9b05cc.png"> <img width="981" alt="enum3" src="https://user-images.githubusercontent.com/28953872/47005875-45578380-d167-11e8-84a4-b942af2f2816.png"> Ask the god how to modify it? Do not modify in the binlog parsing can get the table structure in the periphery based on the int table to find the specific text to replace such as LogEventConvert The big god you said to find the specific text to replace is not based on the enum subscript index from the fieldMeta getColumnType parsing the corresponding content as shown below fieldMeta.getColumnType() is: enum('100' '200' '300' '400' '500' '600') >>>>>> columnType.startsWith("enum") >>>>>> String.valueOf(value): 3 From 100 '200' '300' '400' '500' Parsing the third element 300 in 600 But in case the elements in the enum contain special characters such as commas Or how to do single quotes Still have other ways Ask for advice @agapple Actually test the various cases of enumerating special characters Good thank you guide
1007,restart canal-server canal stuck at at sun.nio.ch.Net.poll(Native Method) Server and client works normally restart server client will disconnect and try to connect and subscribe while I found client thread stuck. Do someone have got this or come up with some ideas about this. Callstack and information as follow: 2018-10-16 10:01:41 ERROR [CustomCanalClient] Main Loop outter Exception.com.alibaba.otter.canal.protocol.exception.CanalClientException: java.io.IOException: end of stream when reading header 2018-10-16 10:01:41 WARN [HttpConnectionPool] object == null 2018-10-16 10:01:41 TRACEBACK [CustomCanalClient]com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:317) com.alibaba.otter.canal.custom.CustomCanalClient.process(CustomCanalClient.java:106) com.alibaba.otter.canal.custom.CustomCanalClient$1.run(CustomCanalClient.java:67) java.lang.Thread.run(Thread.java:748) 2018-10-16 10:01:43 ERROR [CustomCanalClient] Main Loop outter Exception.com.alibaba.otter.canal.protocol.exception.CanalClientException: java.net.ConnectException: Connection refused 2018-10-16 10:01:43 WARN [HttpConnectionPool] object == null 2018-10-16 10:01:43 TRACEBACK [CustomCanalClient]com.alibaba.otter.canal.client.impl.SimpleCanalConnector.doConnect(SimpleCanalConnector.java:190) com.alibaba.otter.canal.client.impl.SimpleCanalConnector.connect(SimpleCanalConnector.java:114) com.alibaba.otter.canal.custom.CustomCanalClient.process(CustomCanalClient.java:101) com.alibaba.otter.canal.custom.CustomCanalClient$1.run(CustomCanalClient.java:67) java.lang.Thread.run(Thread.java:748) java.lang.Thread.State: RUNNABLE at sun.nio.ch.Net.poll(Native Method) at sun.nio.ch.SocketChannelImpl.poll(SocketChannelImpl.java:954) - locked <0x0000000094054d90> (a java.lang.Object) at sun.nio.ch.SocketAdaptor$SocketInputStream.read(SocketAdaptor.java:204) - locked <0x0000000094054d80> (a java.lang.Object) at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103) - locked <0x0000000094054e98> (a sun.nio.ch.SocketAdaptor$SocketInputStream) at java.nio.channels.Channels$ReadableByteChannelImpl.read(Channels.java:385) - locked <0x0000000094054f08> (a java.lang.Object) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.read(SimpleCanalConnector.java:429) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.readNextPacket(SimpleCanalConnector.java:419) - locked <0x00000000c038e900> (a java.lang.Object) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.readNextPacket(SimpleCanalConnector.java:403) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.receiveMessages(SimpleCanalConnector.java:322) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:315) at com.alibaba.otter.canal.custom.CustomCanalClient.process(CustomCanalClient.java:106) at com.alibaba.otter.canal.custom.CustomCanalClient$1.run(CustomCanalClient.java:67) at java.lang.Thread.run(Thread.java:748) client example : https://github.com/alibaba/canal/blob/master/example/src/main/java/com/alibaba/otter/canal/example/SimpleCanalClientTest.java [jstack_log.log](https://github.com/alibaba/canal/files/2481661/jstack_log.log) I add a shell script 'test.sh' to start canal.example.SimpleCanalClientTest class but the same result comes out. ![image](https://user-images.githubusercontent.com/834743/46998952-b3e01580-d156-11e8-89a9-5f964f6e4eb9.png) if no binlog message you can add sleep time . ex : Thread.sleep(1000) Thank you
1006,support golang canal client address link as title tks
1005,binlog-rows-query-log-events=1 Queryevent event in less library name row data less GTID 5 6 database contains configuration binlog-rows-query-log-events=1 The queryevent event in the library name row data less GTID I saw binlog binlog really do not have these two canal help to fill these two properties? header { version: 1 logfileName: "mysql-bin.000945" logfileOffset: 430773470 serverId: 1346306 serverenCode: "UTF-8" executeTime: 1539165635000 sourceType: MYSQL schemaName: "" Less library name tableName: "product_sc_id" eventLength: 374 eventType: QUERY gtid: "816fb525-8e3c-11e7-b7b0-525400730122:1-212612172 73aa9f66-8e18-11e7-b6c5-525400730120:1-358980 b064380d-0559-11e6-b5be-005056996b87:1-1593948705" } entryType: ROWDATA storeValue: "xxx" header Less gtid version: 1 logfileName: "mysql-bin.000945" logfileOffset: 430773918 serverId: 1346306 serverenCode: "UTF-8" executeTime: 1539165635000 sourceType: MYSQL schemaName: "gome_jiaoyu" tableName: "product_sc_id" eventLength: 126 eventType: UPDATE props { key: "rowsCount" value: "1" } } entryType: ROWDATA storeValue: "xxx" binlog-rows-query-log-events The schema can&#39;t get the binlog. Adding gtid to ROWDATA has been added but needs to understand a semantic of gtid. If a row in the transaction records gtid as a bit, it will cause the remaining records of the current transaction to be discarded after being sent to mysql.
1004,Support Mysql8? Have you tested support for Mysql8? Currently not supported mysql8 What is the highest version of mysql supported? Read more wiki
1003,Canal writes kafka&#39;s logic without considering kafka&#39;s message Size limit https://github.com/alibaba/canal/blob/e4b6385dcc439fcf495c81b0c0f89da858dd377b/server/src/main/java/com/alibaba/otter/canal/kafka/CanalKafkaProducer.java#L104-L113 If the size of the record that needs to be sent exceeds the kafka limit, the default is 1MB. Is it impossible to continue processing? I am dealing with it myself. canal batchSize configuration should not exceed 1M ``` servers: localhost:9092 #for rocketmq: means the nameserver retries: 0 batchSize: 16384 lingerMs: 1 bufferMemory: 33554432 # Canal&#39;s batch size Default 50K Due to the maximum message body limitation of kafka, please do not exceed 1M 900K or less. canalBatchSize: 50 ``` @rewerma Hello, still have some doubts, need to ask What is the meaning of canalBatchSize? The granularity of control is the transaction level or event level or Row level or Field level if a field exceeds 1M How to deal with it? canalBatchSize is the batch size of the send that pulls the canal pull Please refer to canal for details. Configuration instructions If there is a single field greater than 1M, you can temporarily adjust the upper limit of the message body limit of kafka.
1002,1. AbstractEventParser Should stop the multiStageCoprocess every time at the end of the while instead of rese 1. AbstractEventParser The multiStageCoprocess should be stopped at the end of the while instead of reset every time. Reset will recreate two thread pools in mysqlMultiStageCoprocessor If there is an error in the dump Will create an infinite number of thread pools and do not stop without terminal Resulting that system resources are occupied Note that the while loop will create a new mysqlMultiStageCoprocessor every time. 2. In the MySqlMultiStageComprocessor, the thread pool needs to determine whether it has been shut down at stop. In the actual situation, there is a case where the thread pool shutdown is true and the terminal is always false. [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=1002) <br/>All committers have signed the CLA. tks
1001,use docker deploy canal-server Not available docker-restart command - phenomenon When used docker Deployment official canal-server Mirror canal canal server v1 1 0 and call after successful startup ```shell docker restart ``` There will be the following error message ```log mv: cannot stat `/home/admin/canal-server/conf/example': No such file or directory ``` - the reason an examination docker Startup script discovery https://github.com/alibaba/canal/blob/master/docker/image/admin/app.sh of 84-92 Line thrown error ```shell destination=`perl -le 'print $ENV{"canal.destinations"}'` if [[ "$destination" =~ ' ' ]]; then echo "multi destination:$destination is not support" exit 1; else if [ "$destination" != "" ] && [ "$destination" != "example" ] ; then mv /home/admin/canal-server/conf/example /home/admin/canal-server/conf/$destination fi fi ``` The behavior here is if it is specified each time it is started destination And destination Value is not Example will put ```path /home/admin/canal-server/conf/example ``` Rename the directory to the specified destination value Since I specified destination And the value is not Example so this behavior leads to execution ```shell docker restart ``` When there is no inside the container example Folder so container can&#39;t start - problem How do you solve the problem by yourself, how do you mirror yourself or have other postures? You can write one Docker compose yml mapping can refer to the docker compose yml I wrote https github com CanalSharp CanalSharp blob 692672b5e7ecec9c91019cd254a1e73adfea7289 docker docker compose yml The mv operation does the judgment to determine the existence of the directory. > You can write one Docker compose yml mapping can refer to the docker compose yml I wrote https github com CanalSharp CanalSharp blob 692672b5e7ecec9c91019cd254a1e73adfea7289 docker docker compose yml @WithLin I tried this method but reported other errors when I restarted. ```log server_1 | mv: cannot move `/home/admin/canal-server/conf/example' to `/home/admin/canal-server/conf/uc': Device or resource busy ```
1000,Parsing site records can&#39;t be started when using FileMixedLogPositionManager Version 1 1 0 mysql：5.6.36 All configurations are only modified in two places. The ip port of the mysql configured in the example is another spring file instance xml. as follows <!-- Parsing site record --> <property name="logPositionManager"> <bean class="com.alibaba.otter.canal.parse.index.FailbackLogPositionManager"> <constructor-arg> <bean class="com.alibaba.otter.canal.parse.index.FileMixedLogPositionManager" > <constructor-arg index="0" value="/tmp/parse.dat" /> <constructor-arg index="1" value="2000" /> <constructor-arg index="2" ref="memoryLogPositionManager"/> </bean> </constructor-arg> <constructor-arg> <bean class="com.alibaba.otter.canal.parse.index.MetaLogPositionManager"> <constructor-arg ref="metaManager"/> </bean> </constructor-arg> </bean> </property> memoryLogPositionManager definition `<bean id="memoryLogPositionManager" class="com.alibaba.otter.canal.parse.index.MemoryLogPositionManager" />` The error is as follows 2018-10-12 15:46:57.245 [destination = example address = /127.0.0.1:3307 EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address /127.0.0.1:3307 has an error retrying. caused by java.lang.NullPointerException: null at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPositionInternal(MysqlEventParser.java:480) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPosition(MysqlEventParser.java:362) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:182) ~[canal.parse-1.1.0.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171] 2018-10-12 15:46:57.258 [destination = example address = /127.0.0.1:3307 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.lang.NullPointerException at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPositionInternal(MysqlEventParser.java:480) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPosition(MysqlEventParser.java:362) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:182) at java.lang.Thread.run(Thread.java:748) ] The original configuration of spring keeps the original configuration and starts everything normally. <!-- Parsing site record --> <property name="logPositionManager"> <bean class="com.alibaba.otter.canal.parse.index.FailbackLogPositionManager"> <constructor-arg> <bean class="com.alibaba.otter.canal.parse.index.MemoryLogPositionManager" /> </constructor-arg> <constructor-arg> <bean class="com.alibaba.otter.canal.parse.index.MetaLogPositionManager"> <constructor-arg ref="metaManager"/> </bean> </constructor-arg> </bean> </property> Com alibaba otter canal parse index FileMixedLogPositionManager here requires dataDir to be a root path to find a path when looking for a dat file dataDir + /$destination$/ + Parse dat your path is constructed incorrectly @agapple thx
999,ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.lang.IllegalArgumentException Reference Canal Kafka The instructions in QuickStart are configured with canal to kafka data synchronization start canal kafka post-report `2018-10-12 15:26:32.189 [destination = example address = /192.168.206.136:3306 EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address /192.168.206.136:3306 has an error retrying. caused by java.lang.IllegalArgumentException: null at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1310) ~[na:1.7.0_79] at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1233) ~[na:1.7.0_79] at java.util.concurrent.Executors.newFixedThreadPool(Executors.java:114) ~[na:1.7.0_79] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor.start(MysqlMultiStageCoprocessor.java:84) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:238) ~[canal.parse-1.1.0.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.7.0_79] 2018-10-12 15:26:32.189 [destination = example address = /192.168.206.136:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.lang.IllegalArgumentException at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1310) at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1233) at java.util.concurrent.Executors.newFixedThreadPool(Executors.java:114) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor.start(MysqlMultiStageCoprocessor.java:84) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:238) at java.lang.Thread.run(Thread.java:745) ]` Have you encountered this problem? Use environment stand-alone zookeeper 3 4 12 kafka_2 10 0 10 0 1 canal kafka 1 1 0 ； I also encountered the same problem, have you solved it? This issue has been resolved with the server side profile canal properties The number of parallel threads in the middle can be turned off and the singleton can be run in a single instance.
998,RowChange resolves Chinese garbled CanalEntry.RowChange rowChange = CanalEntry.RowChange.parseFrom(canalEntry.getStoreValue()); The data obtained is Chinese garbled and the ddl sql contains rn The database and canal configuration are utf8, why not go back to the big answer ![image](https://user-images.githubusercontent.com/26699764/46853678-60f21f80-ce31-11e8-8201-c7e87c996bfe.png) ![image](https://user-images.githubusercontent.com/26699764/46853718-77987680-ce31-11e8-8f17-e21e953c837a.png) Sorry toString to kill the light to see the information printed toString is not directly obtained value stupid
996,RDS deployment in the quarry tower is invalid Problem Description I wget Project code Compiled successfully and Changed ```conf/example/instance.properties``` content Error after running bin startup sh ``` 2018-10-11 20:12:49.383 [Thread-5] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - stop CannalInstance for null-example 2018-10-11 20:12:49.391 [Thread-5] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - stop successful.... 2018-10-11 20:12:57.090 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2018-10-11 20:12:57.095 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties] 2018-10-11 20:12:57.289 [main] WARN o.s.beans.GenericTypeAwarePropertyDescriptor - Invalid JavaBean property 'connectionCharset' being accessed! Ambiguous write methods found next to actually used [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.nio.charset.Charset)]: [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.lang.String)] 2018-10-11 20:12:57.343 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2018-10-11 20:12:57.343 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties] 2018-10-11 20:12:57.553 [main] ERROR com.alibaba.druid.pool.DruidDataSource - testWhileIdle is true validationQuery not set 2018-10-11 20:12:57.865 [main] INFO c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-example 2018-10-11 20:12:57.881 [main] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - start successful.... 2018-10-11 20:12:57.918 [destination = example address = xxxxxxxx:3333 EventParser] WARN c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - prepare to find start position just show master status 2018-10-11 20:12:59.013 [destination = example address = xxxxxxxx:3333 EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address xxxxxxxx:3333 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: ErrorPacket [errorNumber=1142 fieldCount=-1 message=SHOW command denied to user 'xxxxxx' for table 'slow_log' sqlState=42000 sqlStateMarker=#] with command: show create table `mysql`.`event`;show create table `mysql`.`func`;show create table `mysql`.`general_log`;show create table `mysql`.`help_category`;show create table `mysql`.`help_keyword`;show create table `mysql`.`help_relation`;show create table `mysql`.`help_topic`;show create table `mysql`.`proc`;show create table `mysql`.`slow_log`;show create table `mysql`.`time_zone`;show create table `mysql`.`time_zone_leap_second`;show create table `mysql`.`time_zone_name`;show create table `mysql`.`time_zone_transition`;show create table `mysql`.`time_zone_transition_type`; Caused by: java.io.IOException: ErrorPacket [errorNumber=1142 fieldCount=-1 message=SHOW command denied to user 'xxxxxxx' for table 'slow_log' sqlState=42000 sqlStateMarker=#] with command: show create table `mysql`.`event`;show create table `mysql`.`func`;show create table `mysql`.`general_log`;show create table `mysql`.`help_category`;show create table `mysql`.`help_keyword`;show create table `mysql`.`help_relation`;show create table `mysql`.`help_topic`;show create table `mysql`.`proc`;show create table `mysql`.`slow_log`;show create table `mysql`.`time_zone`;show create table `mysql`.`time_zone_leap_second`;show create table `mysql`.`time_zone_name`;show create table `mysql`.`time_zone_transition`;show create table `mysql`.`time_zone_transition_type`; at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.queryMulti(MysqlQueryExecutor.java:109) ~[canal.parse.driver-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.queryMulti(MysqlConnection.java:107) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:175) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:129) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:91) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:188) ~[canal.parse-1.1.0.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171 ``` Remove the mysql library subscription
995,How can deal with the data of the source table delete Such as title Delete in the binlog is only one before Image&#39;s event and insert update have no difference. What do you mean by processing? > Delete in the binlog is only one before Image&#39;s event and insert update have no difference. What do you mean by processing? Ok, actually, my expression is unclear. It should be said that canal can resolve the binlog and can definitely get the delete statement. My problem should be to extract the data from the canal after obtaining the delete data. How does the hive repository handle the deleted data? note 1 hive does not enable transactions, cannot delete operations This is what you hive to deal with and what does it have to do with canal? You try apache orc > This is what you hive to deal with and what does it have to do with canal? You try apache orc Ok, I will try thank you.
994,Whether can support multiple instances pointing to the same database Just the corresponding whitelist is different Tried to set up multiple instances The database address of the instance properties is the same Just a whitelist pointing to a different table is mainly for a table corresponding to a kafka topic But the error is instantiated when the data source is initialized. Can be set But there is no update for the position of the cursor on zookeeper.
993,Specify xxx instance xml for a specific instance Excuse me If there are now two instances `conf/instance1/instance.properties` `conf/instance2/instance.properties ` So how do you specify conf spring for each of them? Different xml Configuration There should be no way to open only two canal @littleneko Open two instances in one server Specify a configuration for an instance Give the answer in the application extension https://github.com/alibaba/canal/wiki/DevGuide#%E5%BA%94%E7%94%A8%E6%89%A9%E5%B1%95 1. In conf Create a new instance property file in `new-instance/instance.properties` 2. New xml Configuration file `spring/custom-instance.xml` 3. canal.properties Medium specification `canal.instance.new-instance.spring.xml = classpath:spring/custom-instance.xml` in case canal.properties No automatic scan canal auto scan = true Turn on automatic scanning Need to be in canal.properties Medium specification canal.destinations= example new-instance --------------------------------------- The channel name here Is the name of the instance what is this else But canal instance destination What is the role of this placeholder? how to use Thank you, Da Yu @yudianer This method can&#39;t specify two different xml xml is configured in canal properties The channel name is new instance. Each instance needs to be created in the canal conf dir default conf directory in the canal properties configuration in the new directory to write the instance properties configuration file Canal instance destination is actually the directory of each instance 在 https github com alibaba canal blob master deployer src main java com alibaba otter canal deployer CanalController java 中初始化  ```java // This variable is used when setting the currently loaded channel to load the spring lookup file. System.setProperty(CanalConstants.CANAL_DESTINATION_PROPERTY destination); ``` @littleneko I feel the meaning of the trick. :smile: ![image](https://user-images.githubusercontent.com/12033023/46842989-50c34b80-ce03-11e8-8224-bf0a5aa593e0.png) > // This variable is used when setting the currently loaded channel to load the spring lookup file. System.setProperty(CanalConstants.CANAL_DESTINATION_PROPERTY destination); The sentence you said reminded me. `canal.instance.destination` Not inclined to user settings But when loading the configuration file Set to each instance name Then on properties Placeholders in the configuration file are replaced Thank you
992,Use CanalKafkaClientExample storeValue is garbled 2018-10-10 17:25:19.304 [Thread-2] INFO c.a.o.canal.client.running.kafka.CanalKafkaClientExample - Message[id=16 entries=[header { version: 1 logfileName: "mysql-bin.000156" logfileOffset: 298158292 serverId: 1 serverenCode: "UTF-8" executeTime: 1539163519000 sourceType: MYSQL schemaName: "payment" tableName: "tmp_canal_test" eventLength: 69 eventType: UPDATE props { key: "rowsCount" value: "1" } } entryType: ROWDATA storeValue: "\b\242\006\020\002P\000b\341\001\n\033\b\000\020\004\032\002id \001(\0000\000B\00245R\aint(11)\n\'\b\001\020\f\032\busername \000(\0000\000B\003111R\fvarchar(255)\n*\b\002\020\f\032\bpassword \000(\0000\000B\006123123R\fvarchar(255)\022\033\b\000\020\004\032\002id \001(\0000\000B\00245R\aint(11)\022\'\b\001\020\f\032\busername \000(\0000\000B\003111R\fvarchar(255)\022\'\b\002\020\f\032\bpassword \000(\0010\000B\003123R\fvarchar(255)" ] raw=true rawEntries=[]] How can I handle it? storeValue is com alibaba otter canal protocol RowChange type needs further analysis. See the definition in EntryProtocol proto. LS Correct Answer
991,Canal achieves HA And send a message to the kafka cluster The active/standby switch causes the message to be sent repeatedly. After canal achieve HA The position of binlog is recorded in zookeeper But after the main app is paused The backup will re-deliver the binlog that has not been processed through it for a while before to Kafka. In HA mode Stop A Switch to B Restart A Then stop the position of the cursor on the zk of the B operation by A is not updated. I don&#39;t know if it is integrated with Kafka or other reasons. Known issues are recommended to upgrade 1 1 1 new version
990,increase Target database password encryption and decryption using druid mode for encryption and decryption and setting canal instance enableDruid true, please make increase Target database password encryption and decryption using druid mode for encryption and decryption and setting canal instance enableDruid true, please make [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=990) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=990) before we can accept your contribution.<br/><hr/>**shichengming** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=990) it.</sub> [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=990) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=990) before we can accept your contribution.<br/><hr/>**shichengming** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=990) it.</sub> tks
989,Modify canal The code of the dbsync module is not valid. The installation and deployment of the canal has been completed and is running normally during the process of customizing the canal. Now need to code the dbsync module of canal com alibaba otter canal parse inbound mysql dbsync package below the class LogEventConvert method parseOneRow rewrite download canal Source and in intellij Opened in idea but never worked, don&#39;t know why Which god can help me with your thank you <img width="1232" alt="dbsync" src="https://user-images.githubusercontent.com/28953872/46657304-89380f00-cbe2-11e8-973f-3c13658b8e04.png"> @agapple @lcybo Ask God to have time to give directions Can the debug breakpoint not come into effect? Setting breakpoints in the idea has not been done here. Is it necessary to change the lib canal parse in the canal installation package? dbsync 1 0 24 jar The breakpoint can go to other places. First make sure that the debug in IDEA can go to the code block you added. The test is no problem. Put the project into a jar and deploy it. thanks for the reply Sorry to write the wrong before should be modified parser module is not dbsync module LogEventConvert code does not take effect In addition, I tried to set breakpoint debugging in other code of the parser module but did not run to the breakpoint. ********************** The specific situation is such that I am customizing the canal to get the mysql incremental data and after conversion to get a custom json String contains 1 modified record json format as follows insert 1 record { "table":"mysql.mysql.all_types" "transactionID":"29" "scn":"29.0" "ts":"1539333745000" "data":[{"opType":"i" "after":{"smallint1":"111" "timestamp1":"2018-10-12 16:42:25.000000" "float1":"345.102" "inta":"108" "tinyint1":"11" "varchar1":"a" "decimal1":"-1235666.666668899987000000000000000000" "double1":"123.134577501986" "datetime1":"2018-10-12 16:42:25.000000"}}]} But the nativeCanalClientTest java of the native canal 1 0 24 client-side example module uses double and decimal data obtained by column getvalue. with “mysql The precision of the query data inserted in the shell and the number of decimal places precision Inconsistent scale In order to solve this data consistency problem, I tried to rewrite the parse module&#39;s code com alibaba otter canal parse inbound mysql dbsync package below the class LogEventConvert method parseOneRow but it does not work, so I am in intellij Breakpoint debugging is turned on in idea but the program display will not be executed to parseOneRow I think the reason may be that I am in intellij Modified canal in idea Parser module canal Source code but canal server That is, the canal homepage download Deployer binary and decompressed to get lib bin conf and logs folder where lib contains various jars responsible for parsing mysql binary files so the real need to change is canal server The jar below the lib So I need to be in intellij After modifying the parser source code in the idea, you need to re-create the new parser. Jar package and replace canal with this new jar package server Parser below lib jar。 Guess to be verified Test results show that need to be in intellij After modifying the parser source code in the idea, you need to re-create the new parser. Jar package and replace canal with this new jar package server Parser below lib The jar modification has taken effect Thank you @theonesmx
988,Canal instance filter regex does not take effect Tried two mysql servers with the same configuration one takes effect and the other does not take effect Already checked bin The log format is ROW format Using KafkaCanalConnector No parameters in the subscribe method May I ask what is the reason Server 1 failed ![image](https://user-images.githubusercontent.com/24933564/46660266-44fc3d00-cbe9-11e8-93f3-2f7218564f10.png) ![image](https://user-images.githubusercontent.com/24933564/46660815-79bcc400-cbea-11e8-9085-ee90f1eb5933.png) Server 2 Effective ![image](https://user-images.githubusercontent.com/24933564/46660299-534a5900-cbe9-11e8-9aca-ae68e98b27ed.png) ![image](https://user-images.githubusercontent.com/24933564/46660833-82ad9580-cbea-11e8-80d8-9ce49dbbf46b.png) Will the differences in the configuration of the two server binlogs affect? Take a look at the FAQ in the wiki. There are several common cases where the filter is not in effect.
987,Parse create After the schema log is called, the constant throwing exception > Question 1 Scene start canal execute create Schema statement passed Message getWithoutAck(int batchSize gets the log data and throws the following exception when the method is called again next time If canal parsing create The script that parses the other statements crud before the schema log does not have such an error. ### Exception information stack 2018-10-09 10:19:20.755 [main] ERROR syncLogger - Incremental synchronization exception com alibaba otter canal protocol exception CanalClientException something goes wrong with reason: something goes wrong with channel:[id: 0xcf561b9e /192.168.1.179:50803 => /192.168.1.179:2017] exception=java.lang.NullPointerException at com.alibaba.otter.canal.store.helper.CanalEventUtils.createPosition(CanalEventUtils.java:69) at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:375) at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:36) at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:307) at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:273) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:124) at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88) at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791) at org.jboss.netty.channel.SimpleChannelHandler.messageReceived(SimpleChannelHandler.java:142) at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:36) at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:294) at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70) at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296) at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:536) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:435) at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70) at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255) at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88) at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:109) at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312) at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90) at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178) at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.receiveMessages(SimpleCanalConnector.java:344) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:315) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:287) > Question 2 Also create The event corresponding to the schema statement Type is Query isDdl false doesn&#39;t feel right ![image](https://user-images.githubusercontent.com/22972651/46643311-2167d100-cbae-11e8-8695-a9a2559f92b3.png) 1. Upgrade 1 1 11 version 2. create schema The syntax does not recognize the ddl syntax. It mainly recognizes the create alter drop. Table and other operations
986,idle timeout exceeds close channel to save server resources Canal1 1 1 version error 2018-10-08 18:09:07.144 [Hashed wheel timer #1] WARN c.a.o.c.server.netty.handler.ClientAuthenticationHandler - channel:[id: 0x2d3c7336 /10.8.32.241:54621 => /10.8.32.241:11111] idle timeout exceeds close channel to save server resources... 2018-10-08 18:09:13.279 [New I/O server worker #1-14] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x4863b38b /10.8.32.241:54622 :> /10.8.32.241:11111] exception=java.nio.channels.ClosedChannelException at org.jboss.netty.channel.socket.nio.NioWorker.cleanUpWriteBuffer(NioWorker.java:629) at org.jboss.netty.channel.socket.nio.NioWorker.close(NioWorker.java:605) at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:356) at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) Client has not been with canal for a long time The server has an interaction that causes the default timeout of 1 hour.
985,The server is configured with multiple instances. How to recover after an instance is reported incorrectly? The server listens to multiple libraries and has a library. Because of network problems, it has been interrupted for a while and has not been reported for timeout. If you want to reconnect, can you only restart the server or you can only restart the wrong instance? ![default](https://user-images.githubusercontent.com/22339074/46599888-0e57f100-cb1b-11e8-81e4-38203ac9e231.PNG) It is recommended to test the latest 1 1 1 version directly.
984,Ask a question about the canal code data structure RowChange getRowDatasList in rowChange may Will return more than one if it will Under what circumstances will Batch situation
983,1 1 0 Whether to support MariaDB 10 3 6 Support mariadb 5 x and 10 x
982,How can the client get it? TableMeta Thank you If you want to get it on the client side Primary key or unique key in the table But I don&#39;t know where to start. Now we use canal Is 1 0 22 It is said that after 1 0 26 canal increase tsdb to store table structure information in real time, how to obtain the client? Direct connect to druid or in the batch pull from the returned message or other objects can be obtained Please let me know thank you all. The primary key of the table already has the identifier isKey in the returned protobuf object. If you want to get a unique key, you need to get the table name to check the database table structure. Thank you, I&#39;ve now understood
981,Rocketmq client package scope in client module Provide external use alone tks
980,table meta error ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - pls submit github issue show create table ddl:CREATE TABLE `partner_organization_data` ( `id` int(11) NOT NULL AUTO_INCREMENT `organizationName` varchar(250) DEFAULT NULL `organizationId` varchar(250) NOT NULL `organizationLogo` varchar(500) DEFAULT NULL COMMENT Institutional logo `organizationUrl` varchar(500) DEFAULT NULL COMMENT Institution address `isDelete` tinyint(4) NOT NULL DEFAULT '0' COMMENT Whether to delete 0 is to delete 1 is not to delete `isOnline` tinyint(4) NOT NULL DEFAULT '0' COMMENT Whether on line 0 is not on line 1 is on line `organizationLogoh5_2` varchar(500) DEFAULT NULL COMMENT 'h5 Second logo `organizationLogoh5` varchar(500) DEFAULT NULL COMMENT H5 agency logo `organizationUrlh5` varchar(500) DEFAULT NULL COMMENT H5 agency url PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=11 DEFAULT CHARSET=utf8 compare failed . db : TableMeta [schema=operation table=partner_organization_data fileds= FieldMeta [columnName=id columnType=int(11) nullable=false key=true defaultValue=null extra=null unique=false] FieldMeta [columnName=organizationName columnType=varchar(250) nullable=true key=false defaultValue=null extra=null unique=false] FieldMeta [columnName=organizationId columnType=varchar(250) nullable=false key=false defaultValue=null extra=null unique=false] FieldMeta [columnName=organizationLogo columnType=varchar(500) nullable=true key=false defaultValue=null extra=null unique=false] FieldMeta [columnName=organizationUrl columnType=varchar(500) nullable=true key=false defaultValue=null extra=null unique=false] FieldMeta [columnName=isDelete columnType=tinyint(4) nullable=false key=false defaultValue=0 extra=null unique=false] FieldMeta [columnName=isOnline columnType=tinyint(4) nullable=false key=false defaultValue=0 extra=null unique=false] FieldMeta [columnName=organizationLogoh5_2 columnType=varchar(500) nullable=true key=false defaultValue=null extra=null unique=false] FieldMeta [columnName=organizationLogoh5 columnType=varchar(500) nullable=true key=false defaultValue=null extra=null unique=false] FieldMeta [columnName=organizationUrlh5 columnType=varchar(500) nullable=true key=false defaultValue=null extra=null unique=false] ] mem : TableMeta [schema=operation table=partner_organization_data fileds= FieldMeta [columnName=id columnType=int(11) nullable=false key=true defaultValue=null extra=null unique=false] FieldMeta [columnName=organizationName columnType=varchar(250) nullable=true key=false defaultValue=null extra=null unique=false] FieldMeta [columnName=organizationId columnType=varchar(250) nullable=false key=false defaultValue=null extra=null unique=false] FieldMeta [columnName=organizationLogo columnType=varchar(500) nullable=true key=false defaultValue=null extra=null unique=false] FieldMeta [columnName=organizationUrl columnType=varchar(500) nullable=true key=false defaultValue=null extra=null unique=false] FieldMeta [columnName=isDelete columnType=tinyint(4) nullable=false key=false defaultValue=0 extra=null unique=false] FieldMeta [columnName=isOnline columnType=tinyint(4) nullable=false key=false defaultValue=0 extra=null unique=false] FieldMeta [columnName=organizationLogoh5_2 columnType=varchar(500) nullable=true key=false defaultValue=null extra=null unique=false] FieldMeta [columnName=organizationLogoh5_2 columnType=varchar(500) nullable=true key=false defaultValue=null extra=null unique=false] FieldMeta [columnName=organizationLogoh5 columnType=varchar(500) nullable=true key=false defaultValue=null extra=null unique=false] FieldMeta [columnName=organizationUrlh5 columnType=varchar(500) nullable=true key=false defaultValue=null extra=null unique=false] ] 2018-09-30 15:53:41.988 [[scheduler-table-meta-snapshot]] ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - compare failed check log The above is the error log Added a data column two days ago organizationLogoh5_2 Two organizationLogoh5_2 appeared in the error mem that happened today. 1. The corresponding version is 2. You can find the DDL corresponding to the operation on the table from the local h2 file. I will reproduce it locally. The memory table structure is not subject to weight. The main reason is that meta_history will perform deduplication based on the serverId bit. If it is normal, the active and standby devices will retain the serverId when copying, but the corresponding sites will be different. Meta_history will have the main The respective DDLs of the standby database will have duplicate memory columns if they are backtracked again. After the National Day, support the idempotent processing of the tsdb memory table structure to avoid the backtracking of the active and standby switching. Master version 2 days ago a canal instance The instance configuration is connected to the main library. Standby not set select * from PUBLIC.meta_snapshot The following is the result of the query Only one 1 2018-09-29 15:53:47.086000000 2018-09-29 15:53:47.086000000 partner 0 0 -1 -2 {"operation":"CREATE TABLE `application` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT \n\t`name` varchar(100) NOT NULL DEFAULT '' \n\t`type` varchar(50) NOT NULL DEFAULT '' \n\tPRIMARY KEY (`id`) \n\tUNIQUE Application noun index (`name`)\n) ENGINE = InnoDB AUTO_INCREMENT = 3 CHARSET = utf8; \nCREATE TABLE `op_account_history` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT \n\t`hexun_user_id` int(11) NOT NULL \n\t`hexun_user_nick` varchar(255) NOT NULL \n\t`add_time` datetime NOT NULL \n\t`type` varchar(255) NOT NULL \n\t`content` varchar(255) NOT NULL \n\t`success` int(1) NOT NULL \n\t`params` text \n\t`access_type` varchar(45) DEFAULT NULL \n\t`access_log_param` text \n\t`access_log_detail` text \n\t`ip` varchar(45) DEFAULT NULL \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 217066 CHARSET = utf8 COMMENT Administrator account access record \nCREATE TABLE `op_account_url_permission` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT COMMENT Self-increasing permission id\n' \n\t`permission_url` varchar(255) NOT NULL COMMENT Permissions are controlled according to the url \n\t`permission_desc` varchar(255) DEFAULT NULL COMMENT Description of permissions such as audit management \n\t`allowed_roles` varchar(512) NOT NULL COMMENT Allowed role ID \n\t`log_message_format` varchar(6000) DEFAULT NULL \n\t`access_type` varchar(45) DEFAULT NULL \n\t`permission_api_id` varchar(128) DEFAULT NULL \n\tPRIMARY KEY (`id`) \n\tUNIQUE `id_UNIQUE` (`id`) \n\tUNIQUE `permission_url_UNIQUE` (`permission_url`)\n) ENGINE = InnoDB AUTO_INCREMENT = 353 CHARSET = utf8 COMMENT The role that the role can exercise \nCREATE TABLE `op_account_user` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT \n\t`hexun_user_id` int(11) NOT NULL \n\t`hexun_user_name` varchar(255) NOT NULL \n\t`hexun_user_nick` varchar(255) NOT NULL \n\t`email_notification` varchar(255) NOT NULL \n\t`user_role_id` int(11) NOT NULL \n\t`create_time` datetime NOT NULL \n\t`creator_user_id` int(11) NOT NULL \n\t`creator_user_name` varchar(255) NOT NULL \n\t`creator_user_nick` varchar(255) NOT NULL \n\t`true_name` varchar(255) DEFAULT NULL \n\tPRIMARY KEY (`id`) \n\tUNIQUE `id_UNIQUE` (`id`) \n\tUNIQUE `hexun_user_id_UNIQUE` (`hexun_user_id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 85 CHARSET = utf8 COMMENT Operational background administrator table \nCREATE TABLE `op_account_user_role` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT \n\t`role_id` tinyint(4) NOT NULL \n\t`role_name` varchar(255) NOT NULL \n\t`role_desc` text \n\tPRIMARY KEY (`id`) \n\tUNIQUE `id_UNIQUE` (`id`) \n\tUNIQUE `roleName_UNIQUE` (`role_name`) \n\tUNIQUE `roleId_UNIQUE` (`role_id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 26 CHARSET = utf8; \nCREATE TABLE `op_check_flow_right` (\n\t`id` tinyint(8) NOT NULL \n\t`check_level` tinyint(4) DEFAULT NULL \n\t`role_id` varchar(16) DEFAULT NULL \n\t`modular_id` tinyint(4) DEFAULT NULL \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB CHARSET = utf8; \nCREATE TABLE `op_check_partner_eventflow` (\n\t`id` int(8) NOT NULL AUTO_INCREMENT \n\t`partner_id` int(8) DEFAULT NULL \n\t`check_level` tinyint(2) DEFAULT '1' COMMENT Divided into Level 1 and Level 2 Audit Level 3 is dismissed \n\t`check_status` tinyint(2) DEFAULT '1' COMMENT 1 pending review 2 approved \n\t`check_time` datetime DEFAULT NULL \n\t`check_reason` varchar(512) DEFAULT NULL \n\t`partner_submit_time` datetime DEFAULT NULL \n\t`adviser_status` tinyint(2) DEFAULT '0' COMMENT Investigate identity review 0 live broadcast audit 1 investment review 2 investment review completed 3 vote to downgrade to live broadcasters \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 7034 CHARSET = utf8; \nCREATE TABLE `op_check_partner_log` (\n\t`id` int(8) NOT NULL AUTO_INCREMENT \n\t`operateName` varchar(32) DEFAULT '' COMMENT Operator name \n\t`operateType` varchar(32) DEFAULT '' COMMENT Operator role \n\t`partnerId` int(8) DEFAULT NULL COMMENT Event ID \n\t`operateTime` datetime DEFAULT NULL COMMENT Operating time \n\t`operateContent` varchar(256) DEFAULT '' COMMENT Operational content \n\t`checkType` varchar(64) DEFAULT '' COMMENT Operation type application rejected for review \n\t`checkUserId` int(11) DEFAULT NULL \n\t`reason` mediumtext \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 2534 CHARSET = utf8; \nCREATE TABLE `op_check_partner_situation_detail` (\n\t`id` int(8) NOT NULL AUTO_INCREMENT \n\t`check_user_id` int(11) DEFAULT NULL \n\t`partner_id` int(11) DEFAULT NULL \n\t`check_time` datetime DEFAULT NULL \n\t`check_status` tinyint(2) DEFAULT NULL \n\t`check_level` tinyint(2) DEFAULT NULL \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 104 CHARSET = utf8; \nCREATE TABLE `op_menu_management` (\n\t`id` int(8) NOT NULL AUTO_INCREMENT \n\t`parent_id` int(8) DEFAULT '0' COMMENT Menu parent id \n\t`menu_name` varchar(64) NOT NULL COMMENT Menu name \n\t`menu_flag` varchar(32) NOT NULL COMMENT Menu identifier \n\t`menu_link_address` varchar(128) DEFAULT NULL COMMENT Secondary menu link address \n\t`menu_permission` varchar(32) DEFAULT NULL COMMENT Menu permissions \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 104 CHARSET = utf8; \nCREATE TABLE `op_operate_log` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT \n\t`operaterId` int(11) DEFAULT NULL \n\t`operaterName` varchar(64) DEFAULT NULL \n\t`model` varchar(64) DEFAULT NULL \n\t`userId` int(11) DEFAULT NULL \n\t`operateContent` varchar(512) DEFAULT NULL \n\t`timeStamp` datetime DEFAULT NULL \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 1949 CHARSET = utf8; \nCREATE TABLE `op_partner_comment_statistics` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT \n\t`dataDate` datetime(4) DEFAULT NULL COMMENT date \n\t`articleCommentCount` int(4) DEFAULT '0' COMMENT Article comments \n\t`videoCommentCount` int(4) DEFAULT '0' COMMENT Live video commentary \n\t`systemClassCommentCount` int(4) DEFAULT '0' COMMENT System class comments \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 422 CHARSET = utf8; \nCREATE TABLE `op_partner_data_statistics` (\n\t`id` int(8) NOT NULL AUTO_INCREMENT \n\t`dataDate` datetime DEFAULT NULL \n\t`partnerId` int(11) DEFAULT NULL \n\t`caiquanCount` int(4) DEFAULT '0' COMMENT Number of financial circles \n\t`articleCount` int(4) DEFAULT '0' COMMENT Total number of articles \n\t`articleMessageCount` int(4) DEFAULT '0' COMMENT Article number of comments \n\t`videoDemandClassCount` int(4) DEFAULT '0' COMMENT Video on demand lesson \n\t`liveClassCount` int(4) DEFAULT '0' COMMENT Video live lessons \n\t`caiquanReplyCount` int(4) DEFAULT '0' COMMENT Financial replies \n\t`newAttentionCount` int(4) DEFAULT '0' COMMENT Add attention \n\t`cancelAttentionCount` int(4) DEFAULT '0' COMMENT Unfollow the number \n\t`increaseAttentionCount` int(4) DEFAULT '0' COMMENT Net increase in attention \n\t`totalAttentionCount` int(4) DEFAULT '0' COMMENT Cumulative number of followers \n\t`caiboCount` int(4) DEFAULT '0' COMMENT Number of financial broadcasts \n\t`sales` int(8) DEFAULT '0' COMMENT Sales \n\t`aSharesCount` int(4) DEFAULT '0' COMMENT A-share radar number \n\t`tuguReplyCount` int(4) DEFAULT '0' COMMENT Answers \n\t`remarkCount` int(4) DEFAULT '0' COMMENT Total number of teachers in the live room \n\t`studentSpeakCount` int(4) DEFAULT '0' COMMENT Total number of participants in the live room \n\t`openReplyCount` int(4) DEFAULT '0' COMMENT The total number of public replies from the teacher in the live room \n\t`studentSecretAskCount` int(4) DEFAULT '0' COMMENT The total number of questions asked by the students \n\t`secretReplyCount` int(4) DEFAULT '0' COMMENT Teacher private letter reply total \n\t`freeStudentSpeaks` int(4) DEFAULT '0' COMMENT Students speak for free \n\t`nonFreeStudentSpeaks` int(4) DEFAULT '0' COMMENT Student speaking fee \n\t`freeStudentAsks` int(4) DEFAULT '0' COMMENT Student private letter asking for free \n\t`nonFreeStudentAsks` int(4) DEFAULT '0' COMMENT Student private letter questioning fee \n\t`freeTeacherSpeaks` int(4) DEFAULT '0' COMMENT Teacher speaks free of charge \n\t`nonFreeTeacherSpeaks` int(4) DEFAULT '0' COMMENT Teacher public speaking fee \n\t`freeTeacherRepliesPublic` int(4) DEFAULT '0' COMMENT Teacher public reply free \n\t`nonFreeTeacherRepliesPublic` int(4) DEFAULT '0' COMMENT Teacher publicly responded to the charge \n\t`freeTeacherRepliesPrivate` int(4) DEFAULT '0' COMMENT Teacher private reply free \n\t`nonFreeTeacherRepliesPrivate` int(4) DEFAULT '0' COMMENT Teacher private reply fee \n\t`freeArticleCount` int(4) DEFAULT '0' COMMENT Free articles \n\t`nonfreeArticleCount` int(4) DEFAULT '0' COMMENT Number of articles charged \n\t`onlineTime` int(4) DEFAULT '0' COMMENT Teacher live room online time \n\t`freeStudentCount` int(4) DEFAULT '0' COMMENT Free live room online number \n\t`nonFreeStudentCount` int(4) DEFAULT '0' COMMENT Charged live room online number \n\t`freeTeacherSecretSpeaks` int(4) DEFAULT '0' COMMENT Free viewing room with privacy views \n\t`nonFreeTeacherSecretSpeaks` int(4) DEFAULT '0' COMMENT Privacy view number of live broadcast room no data \n\t`totalTeacherSecretSpeaks` int(4) DEFAULT '0' COMMENT Teacher privacy view all \n\tPRIMARY KEY (`id`) \n\tUNIQUE `index_data_id` USING BTREE (`dataDate` `partnerId`) \n\tKEY `index_date` USING BTREE (`dataDate`)\n) ENGINE = InnoDB AUTO_INCREMENT = 1412753 CHARSET = utf8; \nCREATE TABLE `op_partner_data_statistics_log` (\n\t`id` int(8) NOT NULL AUTO_INCREMENT \n\t`partnerId` int(11) DEFAULT NULL \n\t`dataDate` datetime DEFAULT NULL \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 943575 CHARSET = utf8; \nCREATE TABLE `op_product_data` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT \n\t`productId` int(11) DEFAULT NULL \n\t`productName` varchar(100) DEFAULT NULL \n\t`partnerId` int(11) DEFAULT NULL \n\t`checkType` varchar(20) DEFAULT NULL COMMENT Audit type \n\t`productType` varchar(30) DEFAULT NULL \n\t`createTime` datetime DEFAULT NULL \n\t`bussinessProId` varchar(50) DEFAULT NULL \n\t`bussinessCheck` varchar(10) DEFAULT NULL \n\t`bussinessInfo` varchar(1000) DEFAULT NULL \n\t`bussinessLog` varchar(1000) DEFAULT NULL \n\t`modifiTime` datetime DEFAULT NULL \n\t`isClose` int(8) DEFAULT '0' \n\t`verifyCode` varchar(100) DEFAULT NULL \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 137 CHARSET = utf8; \nCREATE TABLE `op_req_external_service_log` (\n\t`id` int(8) NOT NULL AUTO_INCREMENT \n\t`partner_id` int(11) DEFAULT NULL COMMENT Partner id \n\t`service_name` varchar(64) DEFAULT NULL COMMENT Request interface name \n\t`req_param` text COMMENT Request parameter \n\t`req_time` datetime DEFAULT NULL COMMENT Request time \n\t`resp_content` text COMMENT Interface return content \n\t`resp_status` tinyint(2) DEFAULT NULL COMMENT Whether the request was successful 0 successful 1 failure \n\t`manual_sync_status` tinyint(2) DEFAULT '0' COMMENT '1 This record representing failure has been manually synchronized \n\tPRIMARY KEY (`id`) \n\tKEY `index_partner_id` USING BTREE (`partner_id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 1484 CHARSET = utf8; \nCREATE TABLE `op_violation_forbidden_words` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT \n\t`forbidden_word` varchar(255) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL DEFAULT '' \n\t`add_time` datetime NOT NULL \n\t`creator_user_id` int(11) NOT NULL \n\t`creator_user_nick` varchar(255) NOT NULL \n\tPRIMARY KEY (`id`) \n\tUNIQUE `id_UNIQUE` (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 503987 CHARSET = utf8 COMMENT Sensitive word \nCREATE TABLE `partner_advertising_data` (\n\t`id` int(8) NOT NULL AUTO_INCREMENT \n\t`partnerId` int(11) DEFAULT NULL COMMENT Teacher id \n\t`content` text COMMENT Advertising content \n\t`content_h` text COMMENT Extended field \n\t`createTime` datetime DEFAULT NULL COMMENT Creation time \n\t`createUserId` int(11) DEFAULT NULL COMMENT Create User ID \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 5224 CHARSET = utf8; \nCREATE TABLE `partner_circle_data` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT \n\t`partnerId` int(11) NOT NULL \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 3 CHARSET = utf8; \nCREATE TABLE `partner_config` (\n\t`id` int(4) NOT NULL AUTO_INCREMENT \n\t`partnerId` bigint(20) NOT NULL COMMENT Partner id \n\t`isCombined` tinyint(4) DEFAULT '0' COMMENT Whether to hide 1 is a combination account 0 is not a combination account \n\t`isHide` tinyint(4) NOT NULL DEFAULT '0' COMMENT Whether to hide 1 is hidden 0 is not hidden \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 12 CHARSET = utf8; \nCREATE TABLE `partner_data` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT COMMENT Self-added id \n\t`partnerId` int(11) NOT NULL COMMENT User id \n\t`truename` varchar(25) DEFAULT NULL COMMENT actual name \n\t`sex` varchar(5) DEFAULT NULL COMMENT gender \n\t`province` varchar(25) DEFAULT NULL COMMENT province \n\t`city` varchar(25) DEFAULT NULL COMMENT city \n\t`qq` varchar(25) DEFAULT NULL COMMENT Qq number \n\t`blog` varchar(250) DEFAULT NULL COMMENT Blog \n\t`weibo` varchar(250) DEFAULT NULL COMMENT Maiho \n\t`orgname` varchar(100) DEFAULT NULL COMMENT name of association \n\t`intro` text COMMENT Introduction \n\t`noticeType` varchar(10) DEFAULT NULL COMMENT Label type \n\t`noticeInfo` text COMMENT content \n\t`level` varchar(32) DEFAULT NULL COMMENT content \n\t`modifitime` datetime DEFAULT NULL COMMENT Last Modified \n\t`createtime` datetime DEFAULT NULL COMMENT Creation time \n\t`cooperationState` tinyint(2) DEFAULT '0' COMMENT 0 not cooperation 1 cooperation 2 suspension 3 pull black \n\t`aplanstate` tinyint(2) DEFAULT '0' COMMENT '0 Deactivate 1 open \n\t`zhibostate` tinyint(2) DEFAULT '1' COMMENT Text live service 0 Deactivate 1 open \n\t`busDepartment` varchar(500) DEFAULT NULL \n\t`jobCode` varchar(250) DEFAULT NULL \n\t`adviserCooperationState` tinyint(2) DEFAULT '0' COMMENT 0 not cooperation 1 cooperation 2 suspension 3 pull black \n\t`openWeChatPlatform` tinyint(2) DEFAULT '0' COMMENT 0 has not opened WeChat platform 1 to open WeChat platform \n\t`adviserHide` tinyint(2) DEFAULT '0' COMMENT 1 hidden \n\t`openEntrust` tinyint(2) DEFAULT '0' COMMENT 0 is no permission 1 is authorized \n\tPRIMARY KEY (`id`) \n\tUNIQUE `partnerId` (`partnerId`) \n\tKEY `level` USING BTREE (`level`)\n) ENGINE = InnoDB AUTO_INCREMENT = 7171 CHARSET = utf8; \nCREATE TABLE `partner_navigation` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT \n\t`partnerId` int(11) NOT NULL COMMENT Partner id \n\t`navigation` varchar(250) DEFAULT NULL \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 882 CHARSET = utf8; \nCREATE TABLE `partner_notice` (\n\t`id` int(8) NOT NULL AUTO_INCREMENT \n\t`title` varchar(64) DEFAULT NULL \n\t`content` varchar(512) DEFAULT NULL \n\t`createTime` datetime DEFAULT NULL \n\t`createUserName` varchar(64) DEFAULT NULL \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 68 CHARSET = utf8; \nCREATE TABLE `partner_notice_number` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT \n\t`partnerId` int(11) DEFAULT NULL \n\t`readNumber` int(11) DEFAULT NULL \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 464 CHARSET = utf8; \nCREATE TABLE `partner_organization` (\n\t`id` int(4) NOT NULL AUTO_INCREMENT \n\t`partnerId` int(8) DEFAULT NULL COMMENT Partner id \n\t`organization` varchar(25) DEFAULT NULL \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 95 CHARSET = utf8; \nCREATE TABLE `partner_organization_data` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT \n\t`organizationName` varchar(250) DEFAULT NULL \n\t`organizationId` varchar(250) NOT NULL \n\t`organizationLogo` varchar(500) DEFAULT NULL COMMENT Institutional logo \n\t`organizationUrl` varchar(500) DEFAULT NULL COMMENT Institution address \n\t`isDelete` tinyint(4) NOT NULL DEFAULT '0' COMMENT Whether to delete 0 is to delete 1 is not to delete \n\t`isOnline` tinyint(4) NOT NULL DEFAULT '0' COMMENT Whether on line 0 is not on line 1 is on line \n\t`organizationLogoh5_2` varchar(500) DEFAULT NULL COMMENT 'h5 Second logo \n\t`organizationLogoh5` varchar(500) DEFAULT NULL COMMENT H5 agency logo \n\t`organizationUrlh5` varchar(500) DEFAULT NULL COMMENT H5 agency url \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 11 CHARSET = utf8; \nCREATE TABLE `partner_recommend` (\n\t`partner_id` int(11) NOT NULL COMMENT Teacher id \n\t`partner_order` int(11) NOT NULL COMMENT Sort \n\t`open_recommend` tinyint(2) DEFAULT NULL COMMENT Whether to open the recommendation \n\tPRIMARY KEY (`partner_id`) \n\tUNIQUE `index_partner_id` USING BTREE (`partner_id`)\n) ENGINE = InnoDB CHARSET = utf8; \nCREATE TABLE `partner_recommend_copy` (\n\t`partner_id` int(11) NOT NULL COMMENT Teacher id \n\t`partner_order` int(11) NOT NULL COMMENT Sort \n\t`open_recommend` tinyint(2) DEFAULT NULL COMMENT Whether to open the recommendation \n\tPRIMARY KEY (`partner_id`) \n\tUNIQUE `index_partner_id` USING BTREE (`partner_id`)\n) ENGINE = InnoDB CHARSET = utf8; \nCREATE TABLE `partner_service_data` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT \n\t`partnerId` int(11) NOT NULL COMMENT Partner id \n\t`serviceStatus` tinyint(4) NOT NULL COMMENT Whether to display the service button \n\t`aplanStatus` tinyint(4) NOT NULL COMMENT Whether to show the A plan \n\t`articleStatus` tinyint(4) NOT NULL COMMENT Whether to display articles \n\t`circleStatus` tinyint(4) NOT NULL COMMENT Whether to show circles \n\t`classesStatus` tinyint(4) NOT NULL COMMENT Whether to display the course \n\t`blogStatus` tinyint(4) DEFAULT NULL COMMENT Whether to display the blog \n\t`aplanUrl` varchar(500) DEFAULT NULL COMMENT a plan link \n\t`articleUrl` varchar(500) DEFAULT NULL COMMENT Article url \n\t`circleUrl` varchar(500) DEFAULT NULL COMMENT Circle url \n\t`classUrl` varchar(500) DEFAULT NULL COMMENT Course url \n\t`blogUrl` varchar(500) DEFAULT NULL COMMENT Blog url \n\tPRIMARY KEY (`id`) \n\tKEY `partnerId` USING BTREE (`partnerId`)\n) ENGINE = InnoDB AUTO_INCREMENT = 15 CHARSET = utf8; \nCREATE TABLE `partner_signelectric` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT COMMENT Self-incrementing primary key \n\t`partnerId` int(11) NOT NULL COMMENT Partner id \n\t`partnerName` varchar(50) DEFAULT NULL COMMENT Partner name \n\t`trueName` varchar(20) DEFAULT NULL COMMENT actual name \n\t`idCard` varchar(50) DEFAULT NULL COMMENT identity number \n\t`address` varchar(200) DEFAULT NULL COMMENT address \n\t`createtime` datetime DEFAULT NULL COMMENT Creation time \n\t`modifitime` datetime DEFAULT NULL COMMENT Last Modified \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 1279 CHARSET = utf8; \nCREATE TABLE `partner_tag_category` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT COMMENT Self-incrementing primary key \n\t`categoryId` int(11) NOT NULL DEFAULT '0' COMMENT Column id \n\t`parentId` int(11) DEFAULT NULL COMMENT Parent column id \n\t`categoryName` varchar(50) DEFAULT NULL COMMENT program name \n\t`categoryType` int(11) DEFAULT NULL COMMENT Column type \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 16 CHARSET = utf8; \nCREATE TABLE `partner_tag_data` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT COMMENT Self-incrementing primary key \n\t`tagId` int(11) NOT NULL DEFAULT '0' COMMENT Subject id \n\t`tagName` varchar(50) DEFAULT NULL COMMENT Subject name \n\t`tagType` int(11) DEFAULT NULL COMMENT Type of target \n\t`categoryId` int(11) DEFAULT NULL COMMENT Subject column \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 49 CHARSET = utf8; \nCREATE TABLE `partner_user_tag` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT COMMENT Self-incrementing primary key \n\t`partnerId` int(11) DEFAULT NULL COMMENT Partner id \n\t`tag` varchar(200) DEFAULT NULL COMMENT Subject \n\t`tagType` int(4) DEFAULT NULL COMMENT Types of \n\t`createtime` datetime DEFAULT NULL COMMENT time \n\t`userDefined` tinyint(1) DEFAULT NULL \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 64455 CHARSET = utf8; \nCREATE TABLE `statistics_px-operation-app` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT \n\t`timestamp` bigint(1) NOT NULL DEFAULT '0' COMMENT Timestamp \n\t`serviceInterface` varchar(255) NOT NULL DEFAULT '' COMMENT Interface name \n\t`method` varchar(255) NOT NULL DEFAULT '' COMMENT Method name \n\t`type` varchar(10) DEFAULT NULL COMMENT Currently called application type \n\t`tps` float(11 2) NOT NULL DEFAULT '0.00' COMMENT TPS value \n\t`kbps` float(11 2) DEFAULT NULL COMMENT flow \n\t`host` varchar(50) DEFAULT NULL COMMENT Ip address \n\t`elapsed` int(11) DEFAULT NULL COMMENT time consuming \n\t`concurrent` int(11) DEFAULT NULL COMMENT Concurrent number \n\t`input` int(11) DEFAULT NULL COMMENT input value \n\t`output` int(11) DEFAULT NULL COMMENT Output size \n\t`successCount` int(11) DEFAULT NULL COMMENT Number of successes \n\t`failureCount` int(11) DEFAULT NULL COMMENT number of failures \n\t`remoteAddress` varchar(50) DEFAULT NULL COMMENT Remote address \n\t`remoteType` varchar(20) DEFAULT NULL COMMENT Remote application type \n\tPRIMARY KEY (`id`) \n\tKEY `time-index` (`timestamp`) \n\tKEY `method-index` (`method`) \n\tKEY `service-index` (`serviceInterface`)\n) ENGINE = InnoDB AUTO_INCREMENT = 168704 CHARSET = utf8; \nCREATE TABLE `statistics_zhibo-app` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT \n\t`timestamp` bigint(1) NOT NULL DEFAULT '0' COMMENT Timestamp \n\t`serviceInterface` varchar(255) NOT NULL DEFAULT '' COMMENT Interface name \n\t`method` varchar(255) NOT NULL DEFAULT '' COMMENT Method name \n\t`type` varchar(10) DEFAULT NULL COMMENT Currently called application type \n\t`tps` float(11 2) NOT NULL DEFAULT '0.00' COMMENT TPS value \n\t`kbps` float(11 2) DEFAULT NULL COMMENT flow \n\t`host` varchar(50) DEFAULT NULL COMMENT Ip address \n\t`elapsed` int(11) DEFAULT NULL COMMENT time consuming \n\t`concurrent` int(11) DEFAULT NULL COMMENT Concurrent number \n\t`input` int(11) DEFAULT NULL COMMENT input value \n\t`output` int(11) DEFAULT NULL COMMENT Output size \n\t`successCount` int(11) DEFAULT NULL COMMENT Number of successes \n\t`failureCount` int(11) DEFAULT NULL COMMENT number of failures \n\t`remoteAddress` varchar(50) DEFAULT NULL COMMENT Remote address \n\t`remoteType` varchar(20) DEFAULT NULL COMMENT Remote application type \n\tPRIMARY KEY (`id`) \n\tKEY `time-index` (`timestamp`) \n\tKEY `method-index` (`method`) \n\tKEY `service-index` (`serviceInterface`)\n) ENGINE = InnoDB AUTO_INCREMENT = 1657961 CHARSET = utf8; \n"}
979,reset last position when apply snapshot to database The meaning of reset in applySnapshotToDB is ![image](https://user-images.githubusercontent.com/8461826/46252914-816dc180-c4a2-11e8-8eef-39f8a859b2aa.png) If not reset Even without ddl changes The position here still points to the last ddl The position in the applySnapshotToDB is passed to the location where the last DDL change is applied to the memory structure. I still don&#39;t understand why I need to reset.
978,Solve kafka Client message cannot be ack bug Solve kafka Client message cannot be ack bug
977,Synchronization site problem canal Version 1 0 24 Source instance RDS Problem canal The server is configured with multiple intancaes after startup. If the rds active/standby switchover or instance migration occurs during the subscription process, after modifying the configuration file, the intances are only reloaded. The modification information will not be synchronized to zk. Then what should I do at this time? Stop canal server 删除zookeeper中 otter canal destinations your_instance 1001 Restart the canal for the entire znode server @1241407808 If it is the corresponding multiple intenses, restart the server will affect other intenses? Of course, other instances will stop. canal Server will put other instances The site is recorded to the next start in zk and will continue according to the corresponding site. Thank you very much.
976,Canal kafka 1 1 0 version binlog does not change when the cpu takes up a core 99 99 cpu (50.00% of 2 core) Use the canal kafka 1 1 0 version of the canal properties to use the default configuration after the startup cpu occupancy is very high binlog no change Server CPU information [cpuinfo.txt](https://github.com/alibaba/canal/files/2430253/cpuinfo.txt) Thread call information is ![123](https://user-images.githubusercontent.com/16751056/46240746-a862bf80-c3de-11e8-9707-8a6bb6771a6e.png) Flame map ![image](https://user-images.githubusercontent.com/16751056/46240795-5bcbb400-c3df-11e8-9e51-874821bc76fa.png) Is it because the machine configuration is low or other reasons? Please follow the latest version #904
975,Data is always syncing but the amount of data is wrong ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x127a1b89 /5.39.221.52:60106 => My canal ip I’m going to report this mistake every few seconds. Is someone stealing my canal? This ip seems to be Dutch 5.39.221.52:60106 Don&#39;t expose yourself to the public website. -_-#
973,[ISSUE 800 provides RocketMQ native access canal Rocketmq Connector docking canal but currently only supports single-partition ordering to the upper canal The message can be split according to the business field hash, and then the multi-partition is ordered, but the change to the canal is larger. Subsequent follow-ups indicate that there is a problem with multi-partition order processing but there is an association between the tables. [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=973) <br/>All committers have signed the CLA. [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=973) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=973) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=973) it.</sub> The client&#39;s canal client yml configuration format is a bit problematic should be changed to ``` mqTopics: - mqMode: kafka topic: example groups: - groupId: example_g1 outAdapters: - name: logger ``` rocketMQ can consider adding support for FlatMessage to do data hash fragmentation
972,canal1.1.0 Stop the consumption of ClosedChannelException after running for a while in ZK mode Continue to consume after restarting the client but the location is not updated. The following is the error log that appears on the canal server. 2018-09-26 17:14:32.336 [New I/O server worker #1-16] ERROR c.a.otter.canal.server.netty.handler.Ses sionHandler - something goes wrong with channel:[id: 0x4b49367f /172.16.40.202:46512 :> /172.16.40. 200:11111] exception=java.nio.channels.ClosedChannelException at org.jboss.netty.channel.socket.nio.NioWorker.cleanUpWriteBuffer(NioWorker.java:629) at org.jboss.netty.channel.socket.nio.NioWorker.close(NioWorker.java:605) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioSe rverSocketPipelineSink.java:119) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketP ipelineSink.java:76) at org.jboss.netty.channel.Channels.close(Channels.java:720) at org.jboss.netty.channel.AbstractChannel.close(AbstractChannel.java:208) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.exceptionCaught(SessionHandle r.java:272) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwar eChannelHandler.java:48) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.exceptionCaught(ReplayingDecoder.ja va:462) at org.jboss.netty.channel.Channels.fireExceptionCaught(Channels.java:432) at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:332) at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) 2018-09-26 17:15:30.046 [New I/O server worker #1-1] ERROR c.a.otter.canal.server.netty.handler.Sess ionHandler - something goes wrong with channel:[id: 0x7ac76894 /172.16.40.202:51872 => /172.16.40.2 00:11111] exception=java.io.IOException: Connection reset by peer at sun.nio.ch.FileDispatcherImpl.read0(Native Method) at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39) at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223) at sun.nio.ch.IOUtil.read(IOUtil.java:192) at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380) at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:322) at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Restart the server site and start updating.
971,Question: canal v1.1.0 Why is there a comparison process like DatabaseTableMeta compareTableMetaDbAndMemory? Such as the title In order to ensure the correctness of the table structure maintained based on the memory tsdb, compare it with the current database before doing the snapshot. In the case that the table structure maintained by the memory tsdb is correct, the following two cases will still cause the DatabaseTableMeta compareTableMetaDbAndMemory to return false. 1. The consumption of canal has a relatively large delay. At this time, there is a difference between the table structure in memory and the real table structure of mysql. 2. Regression site repeated consumption binlog consumption is very slow, not catching up to the latest site within 24 hours The snapshot will be started every 24 hours. If the comparison fails in one time, it can be recorded after the next comparison. 3Q I know the design intent of this piece.
970,fix bug #968 bug Reproduce 1. Enable parallel parsing 2. Normal connection source database - Number of print threads 3. Close the source database - Number of print threads By frequently printing the current number of threads, the number of threads is constantly increasing until OOM give it a like
969,Fix link error as title tks
968,Parallel parsing of the database has not been able to connect to cause OOM exceptions version v1.1.1-alpha 1 heapdump: ![image](https://user-images.githubusercontent.com/33280738/46004767-435d5000-c0e6-11e8-942d-1b283879a799.png)
967,Canal Running for a while Server error [New I/O server worker #1-2] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x5c5493e4 /10.152.0.121:18964 => /10.193. 16.48:11111] exception=java.io.IOException: Connection reset by peer canal version 1.1.0 See other issues Said to be idleTimeout This idle timeout problem I looked at the error. It seems that I was given an error after 1 hour. I want to ask if I can set this idleTimeout. What time is the parameter? Thank you. SimpleCanalConnector.idleTimeout
966,Source code questions about MemoryEventStoreWithBuffer checkFreeSlotAt implementation The put operation of MemoryEventStoreWithBuffer will call checkFreeSlotAt to check if there is any space source as follows `private boolean checkFreeSlotAt(final long sequence) { final long wrapPoint = sequence - bufferSize; final long minPoint = getMinimumGetOrAck(); if (wrapPoint > minPoint) { // Just catching up with the round return false; } else { //... } }` I want to know why the getMinimumGetOrAck method is called to get the smaller value of getSequence or ackSequence. ackSequence should not always be less than or equal to getSequence. It is not possible to compare directly with ackSequence. Yes ack < get < Put basically satisfies this relationship
965,Tsdb persistent storage scheme can use mysql to monitor the data and change the data to h2. The version is 1 1 0 H2 configuration in canal properties #table meta tsdb info canal.instance.tsdb.enable=true canal.instance.tsdb.dir=${canal.file.data.dir:../conf}/${canal.instance.destination:} canal.instance.tsdb.url=jdbc:h2:${canal.instance.tsdb.dir}/h2;CACHE_SIZE=1000;MODE=MYSQL; canal.instance.tsdb.dbUsername=canal canal.instance.tsdb.dbPassword=canal instance.properties H2 configuration #table meta tsdb info Recording table structure by time series canal.instance.tsdb.enable=true Save the table structure data in h2 Embedded database canal.instance.tsdb.url=jdbc:h2:${canal.instance.tsdb.dir}/h2;CACHE_SIZE=1000;MODE=MYSQL; canal.instance.tsdb.dbUsername=canal canal.instance.tsdb.dbPassword=canal Open h2 Database file found meta_history with meta_snapshot No data in the middle ![h2](https://user-images.githubusercontent.com/10652165/45876725-0cc5c380-bdce-11e8-8d64-dd5814762051.png) The problem is too general attention to the server exception log
964,The field in the table creation statement of the meta_history table of tsdb is different from the field of the query statement in sqlmap_history, which causes an error when changing the table structure. The version used is 1 1 0 conf\spring\tsdb\sql\create_table.sql The table construction statement for creating the meta_history table is as follows ``` CREATE TABLE IF NOT EXISTS `meta_history` ( `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT COMMENT Primary key `gmt_create` datetime NOT NULL COMMENT Creation time `gmt_modified` datetime NOT NULL COMMENT Change the time `destination` varchar(128) DEFAULT NULL COMMENT Channel name `binlog_file` varchar(64) DEFAULT NULL COMMENT Binlog file name `binlog_offest` bigint(20) DEFAULT NULL COMMENT Binlog offset `binlog_master_id` varchar(64) DEFAULT NULL COMMENT Binlog node id `binlog_timestamp` bigint(20) DEFAULT NULL COMMENT Timestamp of the binlog application `use_schema` varchar(1024) DEFAULT NULL COMMENT The corresponding schema when executing sql `schema` varchar(1024) DEFAULT NULL COMMENT Corresponding schema `table` varchar(1024) DEFAULT NULL COMMENT Corresponding table `sql` longtext DEFAULT NULL COMMENT Executing sql `type` varchar(256) DEFAULT NULL COMMENT Sql type `extra` text DEFAULT NULL COMMENT Additional extended information PRIMARY KEY (`id`) UNIQUE KEY binlog_file_offest(`destination` `binlog_master_id` `binlog_file` `binlog_offest`) KEY `destination` (`destination`) KEY `destination_timestamp` (`destination` `binlog_timestamp`) KEY `gmt_modified` (`gmt_modified`) ) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8 COMMENT table structure change schedule ``` conf\spring\tsdb\sql-map\sqlmap_history.xml The fields in the query are as follows ```xml <sql id="allVOColumns"> <![CDATA[ a.id as id a.gmt_create as gmtCreate a.gmt_modified as gmtModified a.destination as destination a.binlog_file as binlogFile a.binlog_offest as binlogOffest a.binlog_master_id as binlogMasterId a.binlog_timestamp as binlogTimestamp a.use_schema as useSchema a.sql_schema as sqlSchema a.sql_table as sqlTable a.sql_text as sqlText a.sql_type as sqlType a.extra as extra ]]> </sql> ``` Inconsistent field shema -> sql_shema table ->sql_table sql ->sql_text type ->sql_type The latest backbone has been fixed
963,Canal ClientIdentity Destination with clientId What is the meaning? Please help explain Read more wiki
962,canal How does the client customize the data type of the field content obtained by the client? canal The content of the field obtained by the client is defaulted to the string type. Please advise me how to customize the data type of the field content obtained by the client. See more wikis to convert strings to specific types based on the type of business.
961,canal The precision of the float type field obtained by the client is inconsistent with that in mysql. phenomenon 1. Field type of source table in mysql float(36 6) 2. The value of this field queried in the shell 345.678894 Query after insert 3. The value obtained in the canal after the insert operation triggers the canal 345 6789 Get method client side column getValue problem canal The precision of the float type field obtained by the client is inconsistent with that in mysql. I have tried to rewrite Com alibaba otter canal common utils CanalToStringStyle appendDetail method is as follows <img width="1230" alt="mysql float" src="https://user-images.githubusercontent.com/28953872/45869265-98355980-bdba-11e8-900c-fd9bebe6fcf0.png"> But the result of the actual method rewriting does not work. How can God correct this problem? good luck Tell me about the way to reproduce the mysql version of the canal version of the table statement test SQL and so on The reason why the fix has not taken effect before it has taken effect is that I am not in intellij Rewrite the modified source code in the idea and replace the installed canal The repair method of the corresponding jar under the lib in the server is inconsistent. The parseOneRow method of the logeventconvert under the parser module case Types REAL converts float to double and then processes It is your own change of code that leads to phenomenon Field type of source table in mysql float(36 6) The value of this field queried in the shell 345.678894 Query after insert The value obtained in the canal after the insert operation triggers the canal 345 6789 Get method client side column getValue problem canal The precision of the float type field obtained by the client is inconsistent with that in mysql. This inconsistency in the accuracy problem is not due to changes to the source code. ******************************************** Mysql version Server version: 5.7.22-log MySQL Community Server (GPL) Canal version 1 0 24 Table statement CREATE TABLE `all_types` ( `varchar1` varchar(20) DEFAULT NULL `inta` int(10) unsigned NOT NULL AUTO_INCREMENT `tinyint1` tinyint(4) DEFAULT NULL `smallint1` smallint(6) DEFAULT NULL `bigint1` bigint(20) DEFAULT NULL `decimal1` decimal(65 30) DEFAULT NULL `double1` double DEFAULT NULL `date1` date DEFAULT NULL `time1` time DEFAULT NULL `year1` year(4) DEFAULT NULL `datetime1` datetime DEFAULT NULL `timestamp1` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP `float1` float(36 6) DEFAULT NULL PRIMARY KEY (`inta`) ) ENGINE=InnoDB AUTO_INCREMENT=111118 DEFAULT CHARSET=latin1 Test SQL insert into all_types values('a' 111112 11 111 1111111 -1235666.6666688999870 123.1345775019860 sysdate() sysdate() '1999' sysdate() sysdate() 345.67890110); After testing, it may be because it was done in the parseOneRow method of logeventconvert case Types.REAL: // The object is number type direct valueof columnBuilder.setValue(String.valueOf(value)); break; String valueOf value will cause loss of precision if value is float This is not necessarily a good change. 1. Mysql comes with the mysqlbinlog tool parsing 345 67890110 directly into 345 679 canal parsing to 345 6789 if the strong to double is 345 67889404296875 2. Inserted by yourself is 345 67890110 The actual mysql deposit is 345 678894 3. Mysql for the description of the float on the storage itself has inaccurate behavior https dev mysql com doc refman 5 7 en problems with float html consider the accuracy error of 0 0001 I don’t know if it’s handled correctly. Float first double mysql stored as 345 678894 double is 345 67889404296875 data type is float 36 6 Therefore scale is 6 double decimal point scale 1 bit 345 6788940 double the decimal point scale bit When the decimal point scales 1 digit 5 Then the decimal place of the scale 1 is not changed, so the 345 6788940 takes 6 is 345 678894 After several tests, it is found that the processed results are consistent with the results stored in mysql. There is no reliable case to rely on. mysql&#39;s own binlog parsing code is also directly float object processing. Insert statement directly into the 345 6789 mysql select results is also 345 678894 Yeah, this is really bad. The type of float itself determines Float is not recommended in mysql
960,Is there any tool to put Message Change to SQL statement I need to put Message Convert to a statement executable from the library Canal Whether the relevant interface is provided Look at the example in the client
959,Kafka message order problem Kafka can only guarantee partition Order of messages How to deal with a certain record of a table needs to guarantee a strict order canalDestinations Not for kafka Key what special treatment Default all send play partition 0 Configured pk The hash will be sent to the corresponding partition #958
958,Kafka production side increased by pk Hash to the corresponding partition function Kafka production side increased by pk Hash to the corresponding partition function does not set the partition hash or the default state is all sent to the partition 0 The server side kafka adds the following configuration ```` canalDestinations: - canalDestination: example topic: expample Corresponding number of topic partitions partitionsNum: 3 partitionHash: Library name table name Unique primary key Specify a unique primary key in the library table to use its value as a hash distribution partition mytest.person: id ```` This feature is only available in flatMessage mode. It is recommended to optimize the hash field definition of the table to facilitate configuration problems in a large number of tables. 1. Add a wildcard match table to support default keywords like pk 2. Missing wildcards schema.table : columngA Match Can&#39;t wait
957,Some related issues of canal kafka Currently using canal kafka to achieve a set of data increments to elasticsearch projects have some questions 1. When the client hangs up the incremental data during this time, after the client restarts, the incremental data will not notify the client that there will be data loss problem? 2. Is there any monitoring solution for cannal similar to jmxtrans InfluxDb Grafana? Monitoring for kafka Look at the documentation. The answers you want are all monitored. There is direct docking for prometheus.
956,Is there a roadmap for canal in the coming year? Can pay attention to issue and wiki
955,canal V1 1 0 will open GTID will appear CanalParseException: Java util ConcurrentModificationException Configure canal instance gtidon true as follows # enable gtid use true/false canal.instance.gtidon=true Caused by: java.util.ConcurrentModificationException at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:901) at java.util.ArrayList$Itr.next(ArrayList.java:851) at com.alibaba.otter.canal.parse.driver.mysql.packets.UUIDSet.toString(UUIDSet.java:125) at com.alibaba.otter.canal.parse.driver.mysql.packets.MysqlGTIDSet.toString(MysqlGTIDSet.java:111) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.createHeader(LogEventConvert.java:849) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.createHeader(LogEventConvert.java:823) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.buildQueryEntry(LogEventConvert.java:807) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsQueryEvent(LogEventConvert.java:394) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:137) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$DmlParserStage.onEvent(MysqlMultiStageCoprocessor.java:298) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$DmlParserStage.onEvent(MysqlMultiStageCoprocessor.java:288) at com.lmax.disruptor.WorkProcessor.run(WorkProcessor.java:143) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) Use the latest 1 1 1 alpha version
954,Canal1 1 1 encountered several problems on RDS Using RDS Mysql5 6 high-availability version 2 RDS have applied for a high-right account to configure an error according to the regular instance properties 2018-09-19 11:30:38.912 [destination = xxx address = xxx/xxx:3306 EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump addressxxx/xxx:3306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: ErrorPacket [errorNumber=1142 fieldCount=-1 message=SHOW VIEW command denied to user 'canal'@'xxx' for table 'db_view' sqlState=42000 sqlStateMarker=#] It seems that the high-rights account does not have the permissions of the db_view table of the Mysql library. Also does not have this permission but starts normally Consulted the next Ali cloud engineer said that the problem of canal instance filter regex needs to eliminate the mysql library listener and then start normal but the client does not receive data The second problem is to delete the node on the server side but every time you start the error, you can find destination:{} Tracking debugging with source code found that the node that was originally deleted is too strange. I deleted the entire deployer package and redeployed it or started the deleted node. I am using a stand-alone deployment without using zk. Honestly, I don’t understand your question too much. > Honestly, I don’t understand your question too much. The background is such that deploying a single machine on the ECS The server side then connected 4 RDS The first problem is that we can not access some of the table high-privileged accounts below mysql is not OK then this seems to only remove the mysql by configuring the filter The monitoring of the schema at that time does not receive the binlog data should be the problem of the filter regular configuration error. The second problem is in the deploy The server&#39;s conf folder deleted an RDS configuration but after rebooting canal The server is still loading this deleted configuration is very strange. I will delete the deploy project and redeploy it or will load it. This is what I saw and then only restart ECS. It will be fine after restarting. So I suspect that it is cached under conf. Node The cache should be no one in the conf directory > The cache should be no one in the conf directory In addition, if the log data of the server mode in stand-alone mode will not burst if it is not consumed, these days, ECS is inexplicably full of memory optimization. Is there any good suggestion? Or change zk See more wikis will not explode memory
953,Word spelling correction MemoryEventStoreWithBuffer Inside INIT_SQEUENCE Change to INIT_SEQUENCE [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=953) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=953) before we can accept your contribution.<br/><hr/>**zhikuodai** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=953) it.</sub> tks
952,canal 1.1.0 Do not print data rowdata information ![default](https://user-images.githubusercontent.com/16894071/45675324-b3595c80-bb61-11e8-9d31-103f0386ba22.png) Class information reference https://github.com/alibaba/canal/blob/master/example/src/main/java/com/alibaba/otter/canal/example/AbstractCanalClientTest.java written ` protected void printEntry(List<Entry> entrys) { for (Entry entry : entrys) { long executeTime = entry.getHeader().getExecuteTime(); long delayTime = System.currentTimeMillis() - executeTime; Date date = new Date(entry.getHeader().getExecuteTime()); SimpleDateFormat simpleDateFormat = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss"); if (entry.getEntryType() == EntryType.TRANSACTIONBEGIN || entry.getEntryType() == EntryType.TRANSACTIONEND) { if (entry.getEntryType() == EntryType.TRANSACTIONBEGIN) { TransactionBegin begin = null; try { begin = TransactionBegin.parseFrom(entry.getStoreValue()); } catch (InvalidProtocolBufferException e) { throw new RuntimeException("parse event has an error data:" + entry.toString() e); } // Print transaction header information execution thread id transaction time consuming logger.info(transaction_format new Object[] { entry.getHeader().getLogfileName() String.valueOf(entry.getHeader().getLogfileOffset()) String.valueOf(entry.getHeader().getExecuteTime()) simpleDateFormat.format(date) entry.getHeader().getGtid() String.valueOf(delayTime) }); logger.info(" BEGIN ----> Thread id: {}" begin.getThreadId()); printXAInfo(begin.getPropsList()); } else if (entry.getEntryType() == EntryType.TRANSACTIONEND) { TransactionEnd end = null; try { end = TransactionEnd.parseFrom(entry.getStoreValue()); } catch (InvalidProtocolBufferException e) { throw new RuntimeException("parse event has an error data:" + entry.toString() e); } // Print transaction commit information transaction id logger.info("----------------\n"); logger.info(" END ----> transaction id: {}" end.getTransactionId()); printXAInfo(end.getPropsList()); logger.info(transaction_format new Object[] { entry.getHeader().getLogfileName() String.valueOf(entry.getHeader().getLogfileOffset()) String.valueOf(entry.getHeader().getExecuteTime()) simpleDateFormat.format(date) entry.getHeader().getGtid() String.valueOf(delayTime) }); } continue; } if (entry.getEntryType() == EntryType.ROWDATA) { RowChange rowChage = null; try { rowChage = RowChange.parseFrom(entry.getStoreValue()); } catch (Exception e) { throw new RuntimeException("parse event has an error data:" + entry.toString() e); } EventType eventType = rowChage.getEventType(); String tableName = entry.getHeader().getTableName(); for (CanalEntry.RowData rowData : rowChage.getRowDatasList()) { Map map = new HashMap<>(); map.put("tableName" tableName); map.put("eventType" eventType); map.put("eventDate" org.apache.http.client.utils.DateUtils.formatDate(new Date() "yyyy-MM-dd")); if (eventType == CanalEntry.EventType.DELETE) { printColumn(map rowData.getBeforeColumnsList()); Send delete command } else if (eventType == CanalEntry.EventType.INSERT) { Send insert command printColumn(map rowData.getAfterColumnsList()); } else { Send modification command printColumn(map rowData.getAfterColumnsList()); } } if (eventType == EventType.QUERY || rowChage.getIsDdl()) { logger.info(" sql ----> " + rowChage.getSql() + SEP); continue; } // printXAInfo(rowChage.getPropsList()); } } } ` Check filter conditions
951,When there is no data on the source side of the V1 1 0 version, the socket2 5s of the dump log causes the timeout to throw an exception and causes reconnection. Start Canal If there is continuous data, no error, when there is no data, 2 5s, SocketTimeoutException will cause reconnection. The error is located in BioSocketChannel read BioSocketChannel java 123 Look at the SO_TIMEOUT 1000 in the code BioSocketChannel or 1s. If the read is less than the data 1s, it will throw a SocketTimeoutException. The 2 5s added to the input will throw an exception up and cause the reconnection. The first accumulated timeout is 25s Secondly, the canal opens the master. Heartbeat will get a heartbeat packet reset timeout every 15s when there is no data by default. You can start checking the system from here. @lcybo I described the wrong is 25s is not 2 5s The master is defined in the file DirectLogFetcher hearbeat 15 然后READ_TIMEOUT_MILLISECONDS MASTER_HEARTBEAT_PERIOD_SECONDS + 10) * 1000 is the timeout 25s when read The current phenomenon is that the timeout time of the read 25 method in BioSocketChannel is 25s. If there is no incremental data more than 25s, the SocketTimeoutException is thrown. as you said If the master is not turned on Heartbeat is not 25s will reconnect the sockettimeout exception Turn on the master Heartbeat is the canal instance detection enable in the configuration file. =true 和canal instance detecting heartbeatHaEnable = True, I have set it up now, but it’s not working. canal instance detecting enable和binlog master Heartbeat is not a concept Canal instance detecting enable is fork out another connection by canal Server-initiated heartbeat binlog master Heartbeat is MySQL Part of the dump protocol by MySQL Master initiated directly in dump Transmission canal in connection server No need to configure direct opening in order to solve the semi-join problem https://dev.mysql.com/doc/refman/8.0/en/change-master-to.html Can see MASTER_HEARTBEAT_PERIOD This section checks whether the master side takes effect. @lcybo Thanks to me for introducing this MASTER_HEARTBEAT_PERIOD Is it set on the slave side? canal Server as a slave directly open already should not need to set up on the Canal side You asked me to check if the master is effective. What is meant by the master? Execute CHANGE on the master side MASTER TO MASTER_HEARTBEAT_PERIOD 20, my command execution is successful but still reconnected in 25s My Canal The server is connected to the master Or canal In order to solve the semi-connection problem, it is normal for the 25s to automatically reconnect once. 2018-09-18 17:40:44.165 [destination = example address = /127.0.0.1:3306 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - Socket timeout expired closing connection java.net.SocketTimeoutException: Timeout occurred failed to read 4 bytes in 25000 milliseconds. at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.read(BioSocketChannel.java:123) ~[classes/:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:174) ~[classes/:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:77) ~[classes/:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:213) [classes/:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:248) [classes/:na] at java.lang.Thread.run(Unknown Source) [na:1.7.0_45] @lcybo Thanks, I tried it again. The master connected before is mysql5 1 It does not support MASTER_HEARTBEAT_PERIOD Now it is changed to mysql5 7. So that MASTER_HEARTBEAT_PERIOD does not take effect in 25s. If the connection below 5 6 does not support MASTER_HEARTBEAT_PERIOD, it will be reconnected. Right @theonesmx Well, this feature is 5 5 joined
950,GTID mode synchronously reports gtid_purged error 1 1 0 version enables gtid mode source database gtid_purged has value after startup error can not be synchronized ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = The slave is connecting using CHANGE MASTER TO MASTER_AUTO_POSITION = 1 but the master has purged binary logs containing GTIDs that the slave requires. at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:102) see 902 If you are in a hurry, you can compile and package the main code first and then try it out. Google errmsg
949,Fix simpleCanalConnector bug as title tks
948,Serialization problem Deserialization error after updating version ERROR com.alibaba.otter.canal.client.kafka.MessageDeserializer - Error when deserializing byte[] to message
947,Fix group mode error repair https://github.com/alibaba/canal/issues/943 1. counter Unify to the abstractMySQLEventParser layer 2. Separate statistics for each parser in gourp tks
945,Received a DML statement for a non-subscribed table Mysql 5.7 binlog-rows-query-log-events ON Canal 1.1.0 canal.instance.filter.query.dml false DML statements that receive non-subscription tables are not normal and will not receive non-subscription table data. Setting canal.instance.filter.query.dml = True, currently not going to 5 7 DML The query statement is filtered because the corresponding dbname cannot be obtained. @agapple Ok originally wanted to record the SQL statement can not be filtered, then the binlog rows query log events also turned off tks
944,Is there a problem with the ack of CanalAdapterKafkaWorker? Com alibaba otter canal client adapter loader CanalAdapterKafkaWorker class The following code means that only the poll to null or execution time is more than one minute to execute the ack operation. The normal consumption data will never execute the connector ack. Is there a problem? Why do you want to? Write like this while (running) { try { // switcher.get(); Waiting switch on final Message message = connector.getWithoutAck(100L TimeUnit.MILLISECONDS timeFlag startTime); timeFlag = false; executing.set(true); if (message != null) { ....... // Ack once every interval Prevent switching to another client due to timeout not responding long currentTS = System.currentTimeMillis(); while (executing.get()) { // Less than 1 minute has not consumed ack once keep alive if (System.currentTimeMillis() - currentTS > 60000) { connector.ack(); currentTS = System.currentTimeMillis(); } } } else { connector.ack(); } } catch (CommitFailedException e) { logger.warn(e.getMessage()); } catch (Exception e) { logger.error(e.getMessage() e); TimeUnit.SECONDS.sleep(1L); } } There are indeed bugs in the latest pr will fix the interval here for a while ack is to prevent consumption timeout kafka Consumer will switch to another one @rewerma Okay thank you
943,V1 1 0 version goup mode problem At startup An instance of goup will report an error Other examples will not but consume normally Error is as follows com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.lang.String)]: [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.nio.charset.Charset)] 2018-09-12 14:51:57.847 [main] WARN com.alibaba.otter.canal.prometheus.PrometheusService - Unable to register instance exports for coupon. java.lang.IllegalArgumentException: CanalEventParser must be MysqlEventParser at com.alibaba.otter.canal.prometheus.impl.ParserCollector.register(ParserCollector.java:86) ~[canal.prometheus-1.1.0.jar:na] at com.alibaba.otter.canal.prometheus.CanalInstanceExports.register(CanalInstanceExports.java:65) ~[canal.prometheus-1.1.0.jar:na] at com.alibaba.otter.canal.prometheus.PrometheusService.register(PrometheusService.java:96) ~[canal.prometheus-1.1.0.jar:na] at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.start(CanalServerWithEmbedded.java:109) [canal.server-1.1.0.jar:na] at com.alibaba.otter.canal.deployer.CanalController$2$1.processActiveEnter(CanalController.java:140) [canal.deployer-1.1.0.jar:na] at com.alibaba.otter.canal.common.zookeeper.running.ServerRunningMonitor.processActiveEnter(ServerRunningMonitor.java:244) [canal.common-1.1.0.jar:na] at com.alibaba.otter.canal.common.zookeeper.running.ServerRunningMonitor.initRunning(ServerRunningMonitor.java:149) [canal.common-1.1.0.jar:na] at com.alibaba.otter.canal.common.zookeeper.running.ServerRunningMonitor.start(ServerRunningMonitor.java:103) [canal.common-1.1.0.jar:na] at com.alibaba.otter.canal.deployer.CanalController.start(CanalController.java:438) [canal.deployer-1.1.0.jar:na] at com.alibaba.otter.canal.deployer.CanalLauncher.main(CanalLauncher.java:38) [canal.deployer-1.1.0.jar:na] 2018-09-12 14:51:57.847 [main] INFO c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-coupon 2018-09-12 14:51:57.858 [main] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - start successful.... It seems to be the real use of setConnectionCharset Charset connectionCharset this method But it is set to use setConnectionCharset(String connectionCharset this method but why other modes do not report an error https://github.com/alibaba/canal/pull/947 @shizhengchao If you can, please patch and test it. Thank you very much. > #947 > @shizhengchao If you can, please patch and test it. Thank you very much. @lcybo Well, now I have not reported an error. @shizhengchao thx
942,Add the current gtid related information to the Entry Header 1. Will be the current gtid sequence no And last Committed information is added to Entry trx begin/end rowdata) Header in The client can get the relevant value from the property 2. example Increase the sample of the corresponding gtid related information protected Map<String String> gtidMap = new HashMap<>(); There will be a concurrency when put and get LogDecoder Will be for each event Event instantiation LogHeader Object to logHeader when GTID event arrives The subsequent map put operation, whether serial or parallel parsing, is a get operation on the map in the private header of this event event. My understanding ``` Serial parsing order should be put |-> get-> get -> get Parallel parsing order should be put |->get |-> get |-> get ``` tks
941,Increase flat message for kafka message delivery Adding the FlatMessage class for kafka&#39;s message delivery Message Each Entry will be split into corresponding FlatMessages using json serialization sent to kafka for consumption. Follow-up will be implemented on FlatMessage by pk hash with Partition for splitting needs Increase configuration Server side conf/kafka.yml Add new attribute flatMessage: true True means that the delivery of false messages using the FlatMessage message means that the message is delivered using the native message. Client side canal_client/conf/canal-client.yml Add attribute flatMessage: true True means that the delivery of false messages using the FlatMessage message means that the message is delivered using the native message. Note FlatMessage only delivers native message messages that do not affect tcp for kafka message delivery. ![image](https://user-images.githubusercontent.com/33280738/45535386-e08bcf00-b830-11e8-8349-37378d5470bb.png) I feel that this biggest gap should be One is to submit to Kafka synchronously using asynchronous commit. Otherwise, from the perspective of your package, this difference should not be so big. Have you tried to change it to asynchronous test? Change to asynchronous tps and there is no big difference OK
940,1 1 0 production environment 4 destination points information 2 have not written zk2 not updated Seen from zk 01 and 04 destinaton no cursor directory 02 and 03 have the cursor directory but do not update and restart 2 times. I deploy another canal When the server instance is not reported TableIdNotFoundException point information is all normal Canal canal log error message when starting 2018-09-13 23:46:59.156 [destination = ordercanalprd02 address = /***:3321 EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address /***:3321 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.TableIdNotFoundException: not found tableId:110 Caused by: com.alibaba.otter.canal.parse.exception.TableIdNotFoundException: not found tableId:110 2018-09-13 23:46:59.109 [destination = ordercanalprd01 address = /***:3320 EventParser] WARN c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - prepare to find start position ::1536827400000 **2018-09-13 23:46:59.151 [destination = ordercanalprd03 address = /***:3322 EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address /***:3322 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.TableIdNotFoundException: not found tableId:118 Caused by: com.alibaba.otter.canal.parse.exception.TableIdNotFoundException: not found tableId:118** 2018-09-13 23:46:59.156 [destination = ordercanalprd02 address = /***:3321 EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address /***:3321 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.TableIdNotFoundException: not found tableId:110 Caused by: com.alibaba.otter.canal.parse.exception.TableIdNotFoundException: not found tableId:110 2018-09-13 23:46:59.203 [destination = ordercanalprd03 address = /---:3322 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:ordercanalprd03[com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.TableIdNotFoundException: not found tableId:118 Caused by: com.alibaba.otter.canal.parse.exception.TableIdNotFoundException: not found tableId:118 ] 2018-09-13 23:46:59.204 [destination = ordercanalprd02 address = /---:3321 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:ordercanalprd02[com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.TableIdNotFoundException: not found tableId:110 Caused by: com.alibaba.otter.canal.parse.exception.TableIdNotFoundException: not found tableId:110 ] Configuration information # canal.properties binlog filter config canal.instance.filter.druid.ddl = true canal.instance.filter.query.dcl = true canal.instance.filter.query.dml = true canal.instance.filter.query.ddl = true canal.instance.filter.table.error = true canal.instance.filter.rows = false canal.instance.filter.transaction.entry = false table meta tsdb info **canal.instance.tsdb.enable=false** canal.instance.tsdb.dir=${canal.file.data.dir:../conf}/${canal.instance.destination:} canal.instance.tsdb.url=jdbc:h2:${canal.instance.tsdb.dir}/h2;CACHE_SIZE=1000;MODE=MYSQL; canal.instance.tsdb.dbUsername=canal canal.instance.tsdb.dbPassword=canal canal.instance.global.spring.xml = classpath:spring/default-instance.xml # instance.properties canal.instance.gtidon=false position info canal.instance.master.address=***:3322 canal.instance.master.journal.name= canal.instance.master.position= canal.instance.master.timestamp=1536827400000 canal.instance.master.gtid= rds oss binlog canal.instance.rds.accesskey= canal.instance.rds.secretkey= canal.instance.rds.instanceId= table meta tsdb info **canal.instance.tsdb.enable=false** # Kafka yml configuration information servers: *** retries: 0 batchSize: 16384 lingerMs: 1 bufferMemory: 33554432 Lot size unit of canal The amount of k is recommended to be changed to 1M canalBatchSize: 50 filterTransactionEntry: true canalDestinations: - canalDestination: ordercanalprd01 topic: ordercanalprd01 partition: - canalDestination: ordercanalprd02 topic: ordercanalprd01 partition: - canalDestination: ordercanalprd03 topic: ordercanalprd01 partition: - canalDestination: ordercanalprd04 topic: ordercanalprd01 partition: I checked it again. Estimated that this error is happening canal.destinations= This configuration is directly empty. I put this place on it and then slaveid deploys the other 28 destinations by instance. No problem occurs. I guess it should be a problem caused by the sequence of scanning destinations and other code during startup.
939,Canal1 1 0 The amount of data sent to kafka is inconsistent with the amount generated by mysql My kafka yml is configured like this retries: 0 batchSize: 1024 lingerMs: 0 bufferMemory: 33554432 # Lot size unit of canal The amount of k is recommended to be changed to 1M canalBatchSize: 50 filterTransactionEntry: true canalDestinations: - canalDestination: elample topic: elample partition: Mysql uses a stored procedure to insert 10,000 pieces of data each time can only consume more than 6,000 pieces in kafka batchSize can not be changed too much kafka message body default maximum can not exceed 1M data so canalBatchSize must 1M suggest 500K batchSize can be modified by default @rewerma Is this unit not a byte? 1024 is less than 1M in bytes. I changed to the default and the same situation. retries: 0 batchSize: 16384 lingerMs: 1 bufferMemory: 33554432 # Lot size unit of canal k canalBatchSize: 50 filterTransactionEntry: true I have tried these parameters and they are all the same. I can only synchronize more than 6,600 data at a time.
938,Fix upgrade proto2 to proto3 Fix upgrade proto2 to proto3 And that test is already compatible with previous versions of the client.
937,Canal instance filter transaction entry does not work Mysql version 5.6.26 Canalserver version 1.0.24 Question 1 在canal properties Configuration canal.instance.filter.transaction.entry = true after that In canal client You can still see the entryType Have transactionbegin with transactionend How can I handle it can be removed on the server side? transcation Question two Canal instance filter regex is configured in instance properties after that In canal client You can still see the table that is not in focus. transactionbegin with transactionend It’s just less than paying attention to the watch. Query eventType with Dml eventType Can canalcan only do the synchronization of the binlog of my attention table To reduce the load of the canalserver Question 2 is to upgrade from 1 to 1 1 caused by the failure of the problem 1 to try again ps: 1. Open canal instance filter transaction entry After true, it will lead to failure to ack. Once the synchronization occurs, it may lead to the possibility of losing data or even batch data retransmission. 2. canal server Before the performance optimization of 1 1 0 is done, the performance problem of the version should be greatly alleviated or even solved on 1 1 0. Thanks for answering the current application scenario, it is not allowed to leave ack ; Then try 1 1 version
936,canal1.1.0 Concurrent analysis of multi-threaded GTID update operations Cause java util ConcurrentModificatioinException The specific error is as follows 2018-09-12 18:38:25 776||destination = 1002 address = /*********:3306 EventParser|?|ERROR|c.a.o.c.p.i.mysql.MysqlEventParser - dump address ************:3306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: java.util.ConcurrentModificationException: null at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:901) ~[na:1.8.0_121] at java.util.ArrayList$Itr.next(ArrayList.java:851) ~[na:1.8.0_121] at com.alibaba.otter.canal.parse.driver.mysql.packets.UUIDSet.toString(UUIDSet.java:125) ~[canal.parse.driver-1.1.0-20180912.102450-4.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.packets.MysqlGTIDSet.toString(MysqlGTIDSet.java:111) ~[canal.parse.driver-1.1.0-20180912.102450-4.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.createHeader(LogEventConvert.java:849) ~[canal.parse-1.1.0-20180912.102500-4.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:561) ~[canal.parse-1.1.0-20180912.102500-4.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$DmlParserStage.onEvent(MysqlMultiStageCoprocessor.java:302) ~[canal.parse-1.1.0-20180912.102500-4.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$DmlParserStage.onEvent(MysqlMultiStageCoprocessor.java:288) ~[canal.parse-1.1.0-20180912.102500-4.jar:na] at com.lmax.disruptor.WorkProcessor.run(WorkProcessor.java:143) ~[disruptor-3.4.2.jar:na] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_121] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_121] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_121] 2018-09-12 18:38:25 776||destination = 1002 address = /172.16.2.71:3306 EventParser|?|ERROR|c.a.o.c.common.alarm.LogAlarmHandler - destination:1002[com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: java.util.ConcurrentModificationException at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:901) at java.util.ArrayList$Itr.next(ArrayList.java:851) at com.alibaba.otter.canal.parse.driver.mysql.packets.UUIDSet.toString(UUIDSet.java:125) at com.alibaba.otter.canal.parse.driver.mysql.packets.MysqlGTIDSet.toString(MysqlGTIDSet.java:111) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.createHeader(LogEventConvert.java:849) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:561) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$DmlParserStage.onEvent(MysqlMultiStageCoprocessor.java:302) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$DmlParserStage.onEvent(MysqlMultiStageCoprocessor.java:288) at com.lmax.disruptor.WorkProcessor.run(WorkProcessor.java:143) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) ]st.java:901) at java.util.ArrayList$Itr.next(ArrayList.java:851) at com.alibaba.otter.canal.parse.driver.mysql.packets.UUIDSet.toString(UUIDSet.java:125) at com.alibaba.otter.canal.parse.driver.mysql.packets.MysqlGTIDSet.toString(MysqlGTIDSet.java:111) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.createHeader(LogEventConvert.java:849) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:561) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$DmlParserStage.onEvent(MysqlMu JDK: 1.8.0_21 Configuration information preallell=true parallelthreadSize 4 calculation result Analysis reasons are not necessarily correct 180 lines in LogEventConvert Updated gtid_set In fact, it is updated 34 lines of MysqlGTIDSet sets.get(sid).intervals.addAll(us.intervals) This place updated the modCount of the list In the toString method of UUIDSet Traversing the interval The result triggers the checkForComodification method of 901 rows of ArrayList. Since addAll adds modCount But execptedModCount has not increased Run out of the exception Solution Method 1 preallell false Method 2 in the case of multithreading Is it necessary to lock? https://github.com/alibaba/canal/pull/902 You can try the alpha version of 1 1 1 first. I encountered the same problem analysis reason and mycat lulin is consistent. I use the method of preallell false to test the subsequent consideration of locking.
935,How to specify the data of a table to a partition instead of being scattered into multiple partitions Or how to confirm When Kafka creates a topic, the partition only gives one not to ok. #958
934,How to set a table to a topic specified partition This will be the name of the table as the key of the message. #958
933,canal Modify the instance configuration file to restart the instance. Will this information be synchronized to zk? The version is 1 0 24 will not
932,Can I get a single piece of data? Otherwise, sending to kafka can only be a single partition. I would like to ask 1 10 to send to kafka now can only be a single partition to send multiple partition order can not guarantee that we want to be able to send to different partitions and hash partition according to the key. My approach is to take the entries out and then take the rowchange and traverse the last to get a Rowdata uses the name of the library plus the value of the primary key to spell out the key. By sending this data according to the key, you can ensure that a piece of data will always go to a partition. However, it is very inefficient. Will there be a way to get a single piece of data directly? There is The modified code is as follows public void send(KafkaProperties.Topic topic Message message) throws IOException { Get entries List<ByteString> rawEntries = message.getRawEntries(); for (ByteString rawEntry : rawEntries) { message.addEntry(CanalEntry.Entry.parseFrom(rawEntry)); } Traversing entries for (CanalEntry.Entry entry : message.getEntries()) { String tableName = entry.getHeader().getTableName(); CanalEntry.RowChange rowChage = CanalEntry.RowChange.parseFrom(entry.getStoreValue()); CanalEntry.EventType eventType = rowChage.getEventType(); // Traversing rowchange for (CanalEntry.RowData rowData : rowChage.getRowDatasList()) { String key = ""; List<CanalEntry.Column> list = null; if (eventType == CanalEntry.EventType.DELETE) { list = rowData.getBeforeColumnsList(); } else { list = rowData.getAfterColumnsList(); } Repackage CanalEntry.RowData.Builder rowDataBuilder = CanalEntry.RowData.newBuilder(); for (CanalEntry.Column column : list) { if (column.getIsKey()) { key = tableName + "_" + column.getValue(); } if (eventType == CanalEntry.EventType.DELETE) { rowDataBuilder.addBeforeColumns(column); } else { rowDataBuilder.addAfterColumns(column); } } CanalEntry.RowChange.Builder rowChangeBuilder = CanalEntry.RowChange.newBuilder(); rowChangeBuilder.addRowDatas(rowDataBuilder); CanalEntry.RowChange rowChangeNew = rowChangeBuilder.build(); CanalEntry.Entry.Builder entryBuilder = CanalEntry.Entry.newBuilder(); entryBuilder.setHeader(entry.getHeader()) .setEntryType(entry.getEntryType()) .setStoreValue(rowChangeNew.toByteString()); List<CanalEntry.Entry> listEntry = new ArrayList<>(); listEntry.add(entryBuilder.build()); Message messageNew = new Message(message.getId()); messageNew.setEntries(listEntry); Send hash partition according to rowkey ProducerRecord<String Message> record; record = new ProducerRecord<String Message>(topic.getTopic() key messageNew); producer.send(record); if (logger.isDebugEnabled()) { logger.debug("send message to kafka topic: {} \n {}" topic.getTopic() message.toString()); } } } } For data hashes to different partitions, if you need to do the calculation before building protobuf, you will have the problem of decompression deserialization. Thanks for replying, but looking at the code can only get data by batch. It is not deserialized and can&#39;t be directly split into a single bar. In fact, you can handle canal like this. The data written to kafka as a paste source exists in Kafka and can be used as a sink for pre-processing such as distribution and merge. Because there must be a processor to parse the packet anyway, this part of the source data is always In the form of Message packet, it exists in kafka. No matter how the subsequent data needs to process the data of the paste source, it will serve as the basic data store and serve the different data stream. There are also several scenarios that need to be considered and addressed. 1. Multiple key combination keys 2. Isolation of package data after idempotent processing @wingerx Thanks for providing a layering solution to better decoupling is a good method, but the source layer can only be a single partition pre-processing distribution part can only be single-threaded overall performance is the same as it is now, in fact, the amount is not up to now To the extent that it is not enough, just want to see if there is a better way, so let&#39;s talk to you. @undeadwing I tried to open the concurrent test. I don’t know how much delay is the tps of your current database. @wingerx I originally used the group mode to pick up 5 mysql. It is very difficult to find out that each mysql is connected to a default mode instance. After filtering a single instance, the tps is normally tens to more than 100. There is no delay. Sometimes the whole table will be updated. The field will be stuck for a long time. The tps is also very high. Before using kafka, the data is wrong. The server needs to be rolled back. Then the client can only chase the number of single-threaded chasing. So now I want to use kafka multi-partition after the problem or the whole table update. Multi-threaded consumption is fast Client if the consumption logic assembly data is very slow, the consumption performance is too low, the client can completely record the offset to zk after the multi-threaded consumption transaction is submitted. If kill 9 restarts the consumer consumption output from the offset next time, it is necessary to ensure that the idempotent actually consumes the thread pool. In practice, all are blocked by the hunger state. There is no task waiting for the line to start. You can ignore this scenario. If you need to ensure the order problem, you can initialize the N single-threaded threads after the thread is bound by the client. I am now dealing with 8 pools according to this idea. canal Server backtracking history binlog 25 minutes pull 1100W message consumption 8 nodes each node 50 thread pool takes 40 minutes to consume if it is single-threaded consumption encounters a lot of updates is dumbfounded If you can enter the kafka at the source, you can do the data distribution client without considering the order of concurrency. @yin007008 Thanks for providing ideas before I have thought about thread pool binding. My current practice is to split the data into a single message according to the key sent to kafka multi-partition when sending kafka. The advantage is that the order can be guaranteed when the data is backed up. The end does not need to operate the client side to set the kafka consumption start time. The third is that the consumer can be single instance multi-threaded or multi-instance multi-threaded consumption performance and availability on different machines. The shortcoming is the data splitting and recombination part. Need to deserialize and resequence may become a performance bottleneck. Currently access is normal. The maximum number of concurrent tests is not tested. The biggest problem with multiple partition partitions is the change of pk uk If the business scenario can be circumvented, it is more perfect. ps. Someone has submitted a multi-partition to write to kafka&#39;s next version of the program will be brought #958
931,fix #904: Sleep when empty 1s prevents looping get data too fast 1000ms is a little longer If controlling binlog low latency Can consider 10 100ms tks
930,Canal service exception [New I/O server worker #1-1] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x6b1bba32 /192.10.0.46:57388 => /192.10.16.48:11111] exception=java.io.IOException: Connection reset by peer Consider upgrading the canal version It is estimated that the idle timed out.
929,Message Always return RawEntries How to analyze How can I ask `RawEntries` ByteString in Switch to a normal business object Make it clear that it is first parsed out `Entry` Parse out `RowChange`。 Entry changeEntry = Entry.parseFrom(rawEntry); RowChange.parseFrom(changeEntry.getStoreValue());
928,Consider add filed executeTime at canal-adapter-common class Dml? Consider add filed executeTime? That's sql exactly execute time more useful sometimes. solved on v1.1.1
927,fix #912 Solve guava conflicts Package the guava modification package path into the canal client via the maven plugin tks
926,In version 1 1 0, tsdb error message is enabled by default as follows 2018-09-11 11:10:08.724 [destination = example address = /127.0.0.1:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: apply failed Caused by: org.springframework.jdbc.BadSqlGrammarException: SqlMapClient operation; bad SQL grammar []; nested exception is com.ibatis.common.jdbc.exception.NestedSQLException: --- The error occurred in spring/tsdb/sql-map/sqlmap_history.xml. --- The error occurred while executing query. --- Check the select a.id as id a.gmt_create as gmtCreate a.gmt_modified as gmtModified a.destination as destination a.binlog_file as binlogFile a.binlog_offest as binlogOffest a.binlog_master_id as binlogMasterId a.binlog_timestamp as binlogTimestamp a.use_schema as useSchema a.sql_schema as sqlSchema a.sql_table as sqlTable a.sql_text as sqlText a.sql_type as sqlType a.extra as extra from meta_history a where destination = ? and binlog_timestamp >= ? and binlog_timestamp <= ? order by binlog_timestamp asc id asc . --- Check the SQL Statement (preparation failed). --- Cause: org.h2.jdbc.JdbcSQLException: Table "META_HISTORY" not found; SQL statement: select a.id as id a.gmt_create as gmtCreate a.gmt_modified as gmtModified a.destination as destination a.binlog_file as binlogFile a.binlog_offest as binlogOffest a.binlog_master_id as binlogMasterId a.binlog_timestamp as binlogTimestamp a.use_schema as useSchema a.sql_schema as sqlSchema a.sql_table as sqlTable a.sql_text as sqlText a.sql_type as sqlType a.extra as extra from meta_history a where destination = ? and binlog_timestamp >= ? and binlog_timestamp <= ? order by binlog_timestamp asc id asc [42102-196] at org.springframework.jdbc.support.SQLErrorCodeSQLExceptionTranslator.doTranslate(SQLErrorCodeSQLExceptionTranslator.java:237) at org.springframework.jdbc.support.AbstractFallbackSQLExceptionTranslator.translate(AbstractFallbackSQLExceptionTranslator.java:72) at org.springframework.orm.ibatis.SqlMapClientTemplate.execute(SqlMapClientTemplate.java:206) at org.springframework.orm.ibatis.SqlMapClientTemplate.queryForList(SqlMapClientTemplate.java:296) at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.dao.MetaHistoryDAO.findByTimestamp(MetaHistoryDAO.java:28) at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.applyHistoryOnMemory(DatabaseTableMeta.java:367) at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:123) at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:91) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:188) at java.lang.Thread.run(Thread.java:748) Caused by: com.ibatis.common.jdbc.exception.NestedSQLException: --- The error occurred in spring/tsdb/sql-map/sqlmap_history.xml. --- The error occurred while executing query. --- Check the select a.id as id a.gmt_create as gmtCreate a.gmt_modified as gmtModified a.destination as destination a.binlog_file as binlogFile a.binlog_offest as binlogOffest a.binlog_master_id as binlogMasterId a.binlog_timestamp as binlogTimestamp a.use_schema as useSchema a.sql_schema as sqlSchema a.sql_table as sqlTable a.sql_text as sqlText a.sql_type as sqlType a.extra as extra from meta_history a where destination = ? and binlog_timestamp >= ? and binlog_timestamp <= ? order by binlog_timestamp asc id asc . --- Check the SQL Statement (preparation failed). --- Cause: org.h2.jdbc.JdbcSQLException: Table "META_HISTORY" not found; SQL statement: select a.id as id a.gmt_create as gmtCreate a.gmt_modified as gmtModified a.destination as destination a.binlog_file as binlogFile a.binlog_offest as binlogOffest a.binlog_master_id as binlogMasterId a.binlog_timestamp as binlogTimestamp a.use_schema as useSchema a.sql_schema as sqlSchema a.sql_table as sqlTable a.sql_text as sqlText a.sql_type as sqlType a.extra as extra from meta_history a where destination = ? and binlog_timestamp >= ? and binlog_timestamp <= ? order by binlog_timestamp asc id asc [42102-196] at com.ibatis.sqlmap.engine.mapping.statement.MappedStatement.executeQueryWithCallback(MappedStatement.java:201) at com.ibatis.sqlmap.engine.mapping.statement.MappedStatement.executeQueryForList(MappedStatement.java:139) at com.ibatis.sqlmap.engine.impl.SqlMapExecutorDelegate.queryForList(SqlMapExecutorDelegate.java:567) at com.ibatis.sqlmap.engine.impl.SqlMapExecutorDelegate.queryForList(SqlMapExecutorDelegate.java:541) at com.ibatis.sqlmap.engine.impl.SqlMapSessionImpl.queryForList(SqlMapSessionImpl.java:118) at org.springframework.orm.ibatis.SqlMapClientTemplate$3.doInSqlMapClient(SqlMapClientTemplate.java:298) at org.springframework.orm.ibatis.SqlMapClientTemplate$3.doInSqlMapClient(SqlMapClientTemplate.java:296) at org.springframework.orm.ibatis.SqlMapClientTemplate.execute(SqlMapClientTemplate.java:203) ... 7 more Caused by: org.h2.jdbc.JdbcSQLException: Table "META_HISTORY" not found; SQL statement: select a.id as id a.gmt_create as gmtCreate a.gmt_modified as gmtModified a.destination as destination a.binlog_file as binlogFile a.binlog_offest as binlogOffest a.binlog_master_id as binlogMasterId a.binlog_timestamp as binlogTimestamp a.use_schema as useSchema a.sql_schema as sqlSchema a.sql_table as sqlTable a.sql_text as sqlText a.sql_type as sqlType a.extra as extra from meta_history a where destination = ? and binlog_timestamp >= ? and binlog_timestamp <= ? order by binlog_timestamp asc id asc [42102-196] at org.h2.message.DbException.getJdbcSQLException(DbException.java:345) at org.h2.message.DbException.get(DbException.java:179) at org.h2.message.DbException.get(DbException.java:155) at org.h2.command.Parser.readTableOrView(Parser.java:5552) at org.h2.command.Parser.readTableFilter(Parser.java:1266) at org.h2.command.Parser.parseSelectSimpleFromPart(Parser.java:1946) at org.h2.command.Parser.parseSelectSimple(Parser.java:2095) at org.h2.command.Parser.parseSelectSub(Parser.java:1940) at org.h2.command.Parser.parseSelectUnion(Parser.java:1755) at org.h2.command.Parser.parseSelect(Parser.java:1743) at org.h2.command.Parser.parsePrepared(Parser.java:449) at org.h2.command.Parser.parse(Parser.java:321) at org.h2.command.Parser.parse(Parser.java:293) at org.h2.command.Parser.prepareCommand(Parser.java:258) at org.h2.engine.Session.prepareLocal(Session.java:578) at org.h2.engine.Session.prepareCommand(Session.java:519) at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1204) at org.h2.jdbc.JdbcPreparedStatement.<init>(JdbcPreparedStatement.java:73) at org.h2.jdbc.JdbcConnection.prepareStatement(JdbcConnection.java:288) at com.alibaba.druid.pool.DruidPooledConnection.prepareStatement(DruidPooledConnection.java:349) at com.ibatis.sqlmap.engine.execution.SqlExecutor.prepareStatement(SqlExecutor.java:497) at com.ibatis.sqlmap.engine.execution.SqlExecutor.executeQuery(SqlExecutor.java:175) at com.ibatis.sqlmap.engine.mapping.statement.MappedStatement.sqlExecuteQuery(MappedStatement.java:221) at com.ibatis.sqlmap.engine.mapping.statement.MappedStatement.executeQueryWithCallback(MappedStatement.java:189) Table "META_HISTORY" not found There are some initialized ddl files under conf to check @agapple 在canal 1 1 0 conf spring tsdb sql create_table sql Have this file Try to debug it. The operation for 啥initTable does not take effect initTable in MetaHistoryDAO Okay thank you @agapple
925,How to determine synchronization completion Using the example program example, it will continue to obtain the send dump protocol to get the binlog data of the master and then update it. However, it seems that the program does not end or is in the standby state after the data is synchronized, but continuously repeats the last synchronization data, for example, to the main library. Add 10 records according to the processing flow. The library is increased by 10 according to the general logic. At this time, the client has completed synchronization and should be in standby until the main library has new updates and then synchronize. The sample program shows that it has been synchronized before and after repeated. The data is asked what parameter settings are needed? In addition, the 1 1 0 version of the canal example only outputs the eventype and sql statements for the DDL operation. It seems that there is no corresponding DDL operation for the standby database. I don&#39;t know if the situation is true. Thank you for your answer. 2018-09-10 17:06:07.896 [Thread-2] WARN com.alibaba.otter.canal.example.db.CanalConnectorClient - parse event : QUERY sql: create table xgeom3(id int(20) not null auto_increment location point not null x timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP primary key(id) spatial key sp_index(location)) ENGINE=MyISAM AUTO_INCREMENT=3 DEFAULT CHARSET=utf8 . ignored! 2018-09-10 17:06:07.899 [Thread-2] INFO com.alibaba.otter.canal.example.db.CanalConnectorClient - ===========Transaction begin : =======>>>binlog[mysql-bin.000001:8519] executeTime : 1536567819000 delay : 2548899ms close this issue.
924,update Infinite loop error after the entire table no match ack positionLogPosition The error message is as follows 2018-09-10 12:09:57.669 [pool-7-thread-1] ERROR com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - no match ack positionLogPosition[identity=LogIdentity[sourceAddress=/47.97.169.27:3317 slaveId=-1] postion=EntryPosition[included=false journalName=mysql-bin.000003 position=7080270 serverId=20183317 gtid= timestamp=1536552101000]] sql: update t_order_shop_0024 set modify_time = now() ; # binlog filter config canal.instance.filter.druid.ddl = true canal.instance.filter.query.dcl = true canal.instance.filter.query.dml = true canal.instance.filter.query.ddl = true canal.instance.filter.table.error = true canal.instance.filter.rows = false canal.instance.filter.transaction.entry = true Parsed binlog information * Batch Id: [149136] count : [1] memsize : [8064] Time : 2018-09-10 12:30:11 * Start : [mysql-bin.000003:7080270:1536552101000(2018-09-10 12:01:41)] * End : [mysql-bin.000003:7080270:1536552101000(2018-09-10 12:01:41)] Kafka&#39;s message has been growing to see the log binlog all above is this mysql bin 000003 7080270 1536552101000 The problem is basically identified canal.instance.filter.transaction.entry = true After the filter transaction header is set to true, if there is no point information on the zk, I will give a timestamp 5 days ago. The key point of the problem should be canal instance filter transaction entry = true This setting is true Cannot use canal instance filter transaction entry to true if a site update is required LS Correct Answer
923,How to solve the host access to docker Bind ip Using docker When the mode is canal registered with zookeeper, the ip is the network of the docker container ip 172 17 0 3 For example, the address of the canal server binding in docker is 172 17 0 3 11111 Then my client can&#39;t access it. com.alibaba.otter.canal.protocol.exception.CanalClientException: java.net.ConnectException: Connection timed out: connect at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.doConnect(SimpleCanalConnector.java:171) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.access$000(SimpleCanalConnector.java:48) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector$1.processActiveEnter(SimpleCanalConnector.java:396) at com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor.processActiveEnter(ClientRunningMonitor.java:207) at com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor.initRunning(ClientRunningMonitor.java:118) at com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor.start(ClientRunningMonitor.java:92) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.connect(SimpleCanalConnector.java:93) at com.alibaba.otter.canal.client.impl.ClusterCanalConnector.connect(ClusterCanalConnector.java:63) at com.fcbox.canal.scheduling.SchedulerTask.run(SchedulerTask.java:39) Caused by: java.net.ConnectException: Connection timed out: connect at sun.nio.ch.Net.connect0(Native Method) at sun.nio.ch.Net.connect(Net.java:454) at sun.nio.ch.Net.connect(Net.java:446) at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.doConnect(SimpleCanalConnector.java:132) ... 8 common frames omitted 2018-09-10 11:03:05 454 WARN (ClusterCanalConnector.java:66)- failed to connect to:**### /172.17.0.3:11111** after retry 0 times And canal ip can not be directly written as the host&#39;s ip will report error Can consider docker&#39;s host mode
922,on canal.instance.filter.regex Set problem version 1.0.24 @agapple Problem phenomenon In the same mysql My canal instance filter regex cms rc_ means that only the table starting with rc_ under the cms database is no problem. But when I debug the code, I modify the tables in other databases. such as I modified The table under the test database, my client can still receive the message entryType Only TRANSACTIONBEGIN and TRANSACTIONEND Although these two messages are not what I want, I still hope not to receive the modified spam from other databases. Spam is as follows header { version: 1 logfileName: "mysql-bin.000001" logfileOffset: 6882 serverId: 1 serverenCode: "UTF-8" executeTime: 1536320325000 sourceType: MYSQL schemaName: "" tableName: "" eventLength: 75 } entryType: TRANSACTIONBEGIN storeValue: " \036" I want to express My question 1 canal instance filter regex cms rc_ does achieve my desired results 2 But other database schema messages I have received, add, delete, change, will receive the message entryType Although there are no additions and deletions, only TRANSACTIONBEGIN and TRANSACTIONEND, but I don&#39;t want to receive these two messages. 3 Is there any way to solve it? There are parameters to close Filter out the head and tail of an empty transaction In canal properteis Only one inside is true All others can be changed to false Https github com alibaba canal wiki AdminGuide can be seen here canal.instance.filter.transaction.entry = true This means to filter the head and tail of the transaction. canal.instance.filter.query.dcl = true canal.instance.filter.query.dml = true canal.instance.filter.query.ddl = true canal.instance.filter.table.error = true This configuration is to filter out the default sql statement, that is, after the binlog log is changed to row mode, there will still be a SQL statement with EntryType QUERY. @yin007008 @agapple Thank you
921,canal.instance.filter.transaction.entry After setting true zk Cusor is not updated canal.instance.filter.transaction.entry = true {"@type":"com.alibaba.otter.canal.protocol.position.LogPosition" "identity":{"slaveId":-1 "sourceAddress":{"address":"10.35.165.15" "port":3306}} "postion":{"gtid":"" "included":false "journalName":"mysql-bin.000023" "**position**":43893 "serverId":1 "timestamp":1536312619000}} Canal instance filter transaction entry Will cause the zk state not to be updated
920,canan kafka Data filtering problem Configured whitelist canal.instance.filter.regex=easy4ip.civil_user_phone Still able to receive information from other tables See the print below ``` table:sys_user logfile:mysql-bin.000023 entry:ROWDATA eventType:QUERY sql:UPDATE `sys_user` SET `LOGIN_DATE`='2018-09-08 15:36:02' WHERE (`ID`='21') ``` See FAQ
919,Fix upgrade protobuf version upgrade protobuf version [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=919) <br/>All committers have signed the CLA. [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=919) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=919) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=919) it.</sub> Upgraded protobuf After 3 6 1 Use the old version of protobuf Can the client of 2 6 1 read the data? It is estimated that this upgrade is not compatible. I can read the data. I used the 1 1 0 cliet test to connect. tks
918,Whether the new and old versions are compatible After v1 1 0 comes out, I plan to upgrade the old version 1 0 26 snapshot. Because of the high real-time requirements, I can&#39;t stop thinking about it and I want to upgrade smoothly. 1 The new version is configured with the same config and the old version is co-registered on the same zk 2 Turn off the old version of the node to let the new version preemptive processing rights 3 Then a new version of the server Is there a question in the first step when the new and old versions are compatible? The document says it is fully compatible
917,canal.instance.dbPassword = Database password supports encryption Can be considered to expand according to their own needs propertiesResolver is overwritten
916,Zookeeper cluster build canal startup error - Session 0x0 for server null unexpected error closing socket connection and attempting reconnect java.net.ConnectException: Connection refused at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[na:1.8.0_181] at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) ~[na:1.8.0_181] at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350) ~[zookeeper-3.4.5.jar:3.4.5-1392090] at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1068) ~[zookeeper-3.4.5.jar:3.4.5-1392090] `canal.properties:` canal.id= 1 canal.zkServers=10.153.1.208:2888 10.153.0.183:2888 10.153.0.38:2888 canal.instance.global.spring.xml = classpath:spring/default-instance.xml Is the port number of my canal connection zookeeper connection is wrong
915,About mysql active and standby switching Two questions hope to teach 1 canal set the alternate library main library after the machine cuts to the standby library, the main library restores the standby library, can the machine back to the main library? 2 canal support vip without paying attention to physical node switching 1. Will not take the initiative to the main library 2. Support rds Vip mode @agapple tks After switching to the standby database, the 60s will be rolled back. How to solve the problem? How to solve the problem? If the primary key is modified, for example, the source operation 1、insert id=1; 2、update id=2 where id=1; The source target database is a data id 2. At this time, the main library switches to the standby database to roll back the 60s log and redo the steps 1 and 2. 1 Success 2 fails. The primary key conflicts. The target library becomes id 1 and id. 2 two data data are inconsistent and source 1. Insert can consider using merge sql 2. Update encountered a primary key conflict split into delete before pk + merge insert new pk
914,canal.instance.dbPassword = Database password supports encrypted password This password supports encrypted passwords Currently not supported Consider submitting a PR to me
913,V1 1 0 abnormal scenario test causes operating system not allocate Memory leak Simulate the contents of the MySQL binlong file to drop the part of the data in the binlog file. The canal is abnormal when the send dump is parsed into this binlog file for a long time. Operating system log Fatal error : pthread_create() failed Eventually the operating system cannot allocate memory Finally only restart the system @agapple You check the memory parameters required by jvm and the memory of your machine. @agapple Machine memory 16G Canal uses default memory @agapple Canal will continue to create threads up to 30,000 when the system can not allocate memory. Canal will throw an exception java lang OutOfMemoryError unable to create new native The thread eventually causes the entire system to be unavailable. The jvm parameters are created with the default values ​​of a large number of threads as follows MultiStageCoprocessor-other-example-0 .... MultiStageCoprocessor-other-example-8 MultiStageCoprocessor-Parse-example-0 .... MultiStageCoprocessor-Parse-example-8 The latest master has been fixed https://github.com/alibaba/canal/pull/1002
912,1 1 0 canal and springboot 2 0 4 integration anomaly guava 18 Version is too low springboot Integrated guava 20 ` java.lang.NoSuchMethodError: com.google.common.collect.MapMaker.makeComputingMap(Lcom/google/common/base/Function;)Ljava/util/concurrent/ConcurrentMap; at com.google.common.collect.MigrateMap.makeComputingMap(MigrateMap.java:17) ~[canal.common-1.1.0.jar:na] at com.alibaba.otter.canal.common.zookeeper.ZkClientx.<clinit>(ZkClientx.java:26) ~[canal.common-1.1.0.jar:na] at com.alibaba.otter.canal.client.kafka.KafkaCanalConnector.<init>(KafkaCanalConnector.java:52) ~[classes/:na] at com.alibaba.otter.canal.client.kafka.KafkaCanalConnectors.newKafkaConnector(KafkaCanalConnectors.java:47) ~[classes/:na] at com.louxun.search.service.SyncDataToESJob.syncHouseDataToES(SyncDataToESJob.java:41) ~[classes/:na] at com.louxun.search.listener.StartupListener.onApplicationEvent(StartupListener.java:18) ~[classes/:na] at com.louxun.search.listener.StartupListener.onApplicationEvent(StartupListener.java:10) ~[classes/:na] at org.springframework.context.event.SimpleApplicationEventMulticaster.doInvokeListener(SimpleApplicationEventMulticaster.java:172) ~[spring-context-5.0.8.RELEASE.jar:5.0.8.RELEASE] at org.springframework.context.event.SimpleApplicationEventMulticaster.invokeListener(SimpleApplicationEventMulticaster.java:165) ~[spring-context-5.0.8.RELEASE.jar:5.0.8.RELEASE] at org.springframework.context.event.SimpleApplicationEventMulticaster.multicastEvent(SimpleApplicationEventMulticaster.java:139) ~[spring-context-5.0.8.RELEASE.jar:5.0.8.RELEASE] at org.springframework.context.support.AbstractApplicationContext.publishEvent(AbstractApplicationContext.java:400) ~[spring-context-5.0.8.RELEASE.jar:5.0.8.RELEASE] at org.springframework.context.support.AbstractApplicationContext.publishEvent(AbstractApplicationContext.java:354) ~[spring-context-5.0.8.RELEASE.jar:5.0.8.RELEASE] at org.springframework.context.support.AbstractApplicationContext.finishRefresh(AbstractApplicationContext.java:888) ~[spring-context-5.0.8.RELEASE.jar:5.0.8.RELEASE] at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.finishRefresh(ServletWebServerApplicationContext.java:161) ~[spring-boot-2.0.4.RELEASE.jar:2.0.4.RELEASE] at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:553) ~[spring-context-5.0.8.RELEASE.jar:5.0.8.RELEASE] at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:140) ~[spring-boot-2.0.4.RELEASE.jar:2.0.4.RELEASE] at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:762) [spring-boot-2.0.4.RELEASE.jar:2.0.4.RELEASE] at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:398) [spring-boot-2.0.4.RELEASE.jar:2.0.4.RELEASE] at org.springframework.boot.SpringApplication.run(SpringApplication.java:330) [spring-boot-2.0.4.RELEASE.jar:2.0.4.RELEASE] at org.springframework.boot.SpringApplication.run(SpringApplication.java:1258) [spring-boot-2.0.4.RELEASE.jar:2.0.4.RELEASE] at org.springframework.boot.SpringApplication.run(SpringApplication.java:1246) [spring-boot-2.0.4.RELEASE.jar:2.0.4.RELEASE] at com.louxun.search.SearchApplication.main(SearchApplication.java:10) [classes/:na] ` Exclusion seems to solve I modified the original code to solve ` package com.alibaba.otter.canal.common.zookeeper; import java.util.Map; import com.google.common.cache.CacheBuilder; import com.google.common.cache.CacheLoader; import com.google.common.cache.LoadingCache; import org.I0Itec.zkclient.IZkConnection; import org.I0Itec.zkclient.ZkClient; import org.I0Itec.zkclient.exception.ZkException; import org.I0Itec.zkclient.exception.ZkInterruptedException; import org.I0Itec.zkclient.exception.ZkNoNodeException; import org.I0Itec.zkclient.exception.ZkNodeExistsException; import org.I0Itec.zkclient.serialize.ZkSerializer; import org.apache.zookeeper.CreateMode; import com.google.common.base.Function; import com.google.common.collect.MigrateMap; /** * Use a custom ZooKeeperx for zk connection * * @author jianghang 2012-7-10 02 31 15 PM * @version 1.0.0 */ public class ZkClientx extends ZkClient { /* BUG Cache once for zkclient to avoid using multiple zk inside a jvm connection private static Map<String ZkClientx> clients = MigrateMap.makeComputingMap(new Function<String ZkClientx>() { public ZkClientx apply(String servers) { return new ZkClientx(servers); } }); */ use guava New way Replace the old old one with the springboot2 0 integration problem private static LoadingCache<String ZkClientx> clients = CacheBuilder.newBuilder().build( new CacheLoader<String ZkClientx>() { public ZkClientx load(String servers) { return new ZkClientx(servers); } }); // Other original code omitted without modification } ` I am talking about you in the pom. Where to use guava One of the old dependencies plus the exclusion tag to see if it can be solved No, ah canal depends on the old version of guava 18 with the new guava 20. The outdated method in one of the classes has been removed. I want to unify the new version. So I am now modifying the guava method in which the source code in the canal will expire. Replace with new way Canal uses the spring version for 3 2 6 and spring boot2 0 4 depends on the spring version for 5 x The difference in the version is relatively cautious If you have a successful replacement, you can submit a PR to me. @panjianping Brother, which version of canal you used later? And which version of springboot @gezhiwei8899 I used canal 1 1 0 with springboot-2.0.4 Modified canal Source code ZkClientx.java I also introduced dubbo, I can’t get up. @panjianping @panjianping Can you re-canal a method? Avoid modifying the source code @gezhiwei8899 The main problem now is that the high version of spring boot uses guava 20 and the current canal ZkClientx java relies on the method in guava 18 but this method has been removed in guava 20, so you don&#39;t want to modify the original code, you need to use spring. The 3 x version I probably thought of was the solution. Directly in your pom display dependent guava 18 is not it? I just solved this. <dependency> <groupId>com.google.guava</groupId> <artifactId>guava</artifactId> <version>18.0</version> </dependency>
911,The latest canal kafka 1 1 0 tar deployment can not start debugging source code is also the same exception to pay attention 2018-09-04 16:10:09.223 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2018-09-04 16:10:09.226 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties] 2018-09-04 16:10:09.400 [main] WARN o.s.beans.GenericTypeAwarePropertyDescriptor - Invalid JavaBean property 'connectionCharset' being accessed! Ambiguous write methods found next to actually use d [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.lang.String)]: [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlE ventParser.setConnectionCharset(java.nio.charset.Charset)] 2018-09-04 16:10:09.435 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2018-09-04 16:10:09.435 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties] 2018-09-04 16:10:09.667 [main] ERROR com.alibaba.druid.pool.DruidDataSource - testWhileIdle is true validationQuery not set 2018-09-04 16:10:09.891 [main] INFO c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-example 2018-09-04 16:10:09.905 [main] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - start successful.... 2018-09-04 16:10:09.948 [destination = example address = /192.168.200.30:3306 EventParser] WARN c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - prepare to find start position just las t position {"identity":{"slaveId":-1 "sourceAddress":{"address":"mySlave" "port":3306}} "postion":{"gtid":"" "included":false "journalName":"mysql-bin.000029" "position":132895982 "serverId":2 "timestamp":1 536044055000}} 2018-09-04 16:10:09.993 [Thread-7] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - stop CannalInstance for null-example 2018-09-04 16:10:09.996 [destination = example address = /192.168.200.30:3306 EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - parse events has an error com.alibaba.otter.canal.parse.exception.CanalParseException: dump address /192.168.200.30:3306 has an error retrying. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: should execute connector.connect() first Caused by: java.io.IOException: should execute connector.connect() first at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.<init>(MysqlQueryExecutor.java:30) ~[canal.parse.driver-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.queryMulti(MysqlConnection.java:106) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:175) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:129) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:91) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:188) ~[canal.parse-1.1.0.jar:na] at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_171] 2018-09-04 16:10:09.997 [Thread-7] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - stop successful.... Can&#39;t retry in our environment, you try to debug and see what MysqlConnection is disconnected under what conditions.
910,How can import some data of mysql into redis cluster There are already 6 redis clusters. Canal client In case the data is not updated How can I import the data of some existing tables in mysql into the redis cluster? Thank you In case the data is not updated This premise does not meet the working principle of canal may not meet your needs. Can you see if yugong or dataX can be satisfied? Thank you, I&#39;ve now understood
909,V1 1 0 How can deal with cloud server IP problem size network IP problem in Zookeeper Canal registers the intranet IP of the cloud server in Zookeeper such as 192 168 4 x and the corresponding large network server address is other IP such as 10 100 xx As a result, the canal and the Zookeeper client must be deployed to the same cloud server in a small network segment. @agapple This special network is not in the scope of design considerations. @agapple Can you plan to support Zuul and have similar situations for network segment filtering? Each canal Server own human flesh setting corresponding ip You can consider submitting a pr to me.
908,Meta bat problem Why do I start the service is not the generated meta bat file but the generated h2 mv db file has no big god to explain H2 mv db file will have canal instance global spring xml by default = Classpath spring default instance xml is written to zk on canal instance global spring xml = Classpath spring file instance xml This is the local meta bat LS Correct Answer
907,Error in example 2018-09-04 11:39:04.244 [WrapperSimpleAppMain] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2018-09-04 11:39:04.244 [WrapperSimpleAppMain] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties] 2018-09-04 11:39:04.400 [WrapperSimpleAppMain] WARN o.s.beans.GenericTypeAwarePropertyDescriptor - Invalid JavaBean property 'connectionCharset' being accessed! Ambiguous write methods found next to actually used [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.nio.charset.Charset)]: [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.lang.String)] 2018-09-04 11:39:04.400 [WrapperSimpleAppMain] INFO c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-example 2018-09-04 11:39:04.416 [WrapperSimpleAppMain] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - start successful.... 2018-09-04 11:39:04.416 [canal-instance-scan-0] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - stop CannalInstance for null-example 2018-09-04 11:39:04.432 [destination = example address = /127.0.0.1:3308 EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - parse events has an error com.alibaba.otter.canal.parse.exception.CanalParseException: dump address /127.0.0.1:3308 has an error retrying. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: java.net.SocketException: Socket Closed Caused by: java.net.SocketException: Socket Closed at java.net.SocketInputStream.socketRead0(Native Method) ~[na:1.8.0_77] at java.net.SocketInputStream.socketRead(SocketInputStream.java:116) ~[na:1.8.0_77] at java.net.SocketInputStream.read(SocketInputStream.java:170) ~[na:1.8.0_77] at java.net.SocketInputStream.read(SocketInputStream.java:141) ~[na:1.8.0_77] at java.io.BufferedInputStream.fill(BufferedInputStream.java:246) ~[na:1.8.0_77] at java.io.BufferedInputStream.read1(BufferedInputStream.java:286) ~[na:1.8.0_77] at java.io.BufferedInputStream.read(BufferedInputStream.java:345) ~[na:1.8.0_77] at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.read(BioSocketChannel.java:52) ~[canal.parse.driver-1.1.0.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.readHeader(PacketManager.java:12) ~[canal.parse.driver-1.1.0.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.readNextPacket(MysqlQueryExecutor.java:159) ~[canal.parse.driver-1.1.0.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.query(MysqlQueryExecutor.java:56) ~[canal.parse.driver-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:102) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.loadBinlogFormat(MysqlConnection.java:433) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.getBinlogFormat(MysqlConnection.java:582) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:95) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) ~[canal.parse-1.1.0.jar:na] at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_77] 2018-09-04 11:39:04.432 [canal-instance-scan-0] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - stop successful.... Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: java.net.SocketException: Socket Closed Caused by: java.net.SocketException: Socket Closed Feeling active off
906,1 1 0 version startup exception Unable to detect database changes [destination = example address = /10.19.1.80:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: EOF encountered. Caused by: java.io.IOException: EOF encountered. at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.read(BioSocketChannel.java:56) at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.readHeader(PacketManager.java:12) at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.readNextPacket(MysqlQueryExecutor.java:159) at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.query(MysqlQueryExecutor.java:56) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:102) at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:148) at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:129) at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:91) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:188) at java.lang.Thread.run(Thread.java:748) ] Caused by: java.io.IOException: EOF encountered. Check the database
905,Whether to support multi-subscriber mode Does canal support multi-subscriber mode to maintain different mark for each subscriber? with ack? Does not support the need for multiple subscribers to write data to MQ
904,1.1.0 Version canal server 8 destinations after startup Cpu has been 800 dead loop new Events object 1.1.0 Version canal server After starting, the CPU has been 100 or even 160. This is a problem with this version. Canal instance global spring xml for configuration = classpath:spring/default-instance.xml Using kafka integration Regardless of whether there is binlog down canal server The CPU consumption of the process is basically 100. top -Hp Pid to see which thread accounts for cpu high and then use jstack to see which class and method is causing Now deployed to the pressure test environment, without any binlog download, deploy 4 cloud 8 core machines, 8 destination cpu800, solve the problem. "pool-12-thread-1" #29 prio=5 os_prio=0 tid=0x00007fdee4afa000 nid=0x2394 runnable [0x00007fde6cce4000] java.lang.Thread.State: RUNNABLE at com.alibaba.otter.canal.store.model.Events.<init>(Events.java:23) at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.doGet(MemoryEventStoreWithBuffer.java:280) at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.tryGet(MemoryEventStoreWithBuffer.java:261) at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getEvents(CanalServerWithEmbedded.java:478) at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:310) - locked <0x0000000740492388> (a com.alibaba.otter.canal.instance.spring.CanalInstanceWithSpring) at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:273) at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.worker(CanalKafkaStarter.java:121) at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.access$000(CanalKafkaStarter.java:26) at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter$1.run(CanalKafkaStarter.java:70) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) From the thread dump, it is to enter the infinite loop new Events object From jstat 500M 15 minutes in the eden area YGC 500 times At 10 o&#39;clock last night, I started to the current YGC64000 times. FGC only started initialization because STW&#39;s 2 records means that there is no YGC in operation. # binlog filter config canal.instance.filter.druid.ddl = true canal.instance.filter.query.dcl = true canal.instance.filter.query.dml = true canal.instance.filter.query.ddl = true canal.instance.filter.table.error = true canal.instance.filter.rows = false canal.instance.filter.transaction.entry = false canal.instance.global.spring.xml = classpath:spring/default-instance.xml The thread stack information when the binlog is pulled normally is as follows # "pool-11-thread-3" #73 prio=5 os_prio=0 tid=0x00007f7ae8d88800 nid=0x3e5e runnable [0x00007f7a70927000] java.lang.Thread.State: RUNNABLE at com.alibaba.otter.canal.store.helper.CanalEventUtils.createPosition(CanalEventUtils.java:76) at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:375) at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:36) at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:307) - locked <0x00000007406361f0> (a com.alibaba.otter.canal.instance.spring.CanalInstanceWithSpring) at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:273) at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.worker(CanalKafkaStarter.java:121) at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.access$000(CanalKafkaStarter.java:26) at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter$1.run(CanalKafkaStarter.java:70) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) # "pool-11-thread-2" #72 prio=5 os_prio=0 tid=0x00007f7ae8d86800 nid=0x3e5d runnable [0x00007f7a70968000] java.lang.Thread.State: RUNNABLE at com.google.common.collect.MapMakerInternalMap$Segment.getEntry(MapMakerInternalMap.java:2402) at com.google.common.collect.ComputingConcurrentHashMap$ComputingSegment.getOrCompute(ComputingConcurrentHashMap.java:81) at com.google.common.collect.ComputingConcurrentHashMap.getOrCompute(ComputingConcurrentHashMap.java:67) at com.google.common.collect.MapMaker$ComputingMapAdapter.get(MapMaker.java:885) at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:296) at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:273) at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.worker(CanalKafkaStarter.java:121) at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.access$000(CanalKafkaStarter.java:26) at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter$1.run(CanalKafkaStarter.java:70) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) # "pool-11-thread-1" #71 prio=5 os_prio=0 tid=0x00007f7ae8d85800 nid=0x3e5c runnable [0x00007f7a709a9000] java.lang.Thread.State: RUNNABLE at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:317) - locked <0x000000074004a4d0> (a com.alibaba.otter.canal.instance.spring.CanalInstanceWithSpring) at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:273) at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.worker(CanalKafkaStarter.java:121) at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.access$000(CanalKafkaStarter.java:26) at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter$1.run(CanalKafkaStarter.java:70) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) @rewerma Let me see this question. CanalKafkaStarter worker estimates that sleep time is not set It is equivalent to an infinite loop running against the empty result. Can optimize Modified commit is a bit strange, I don&#39;t have sleep here, I have been wirelessly looping to take data, no problem, the CPU is normal. I am using a 1.8 jdk G1 collector here.
903,Could not find first log file name in binary log index file Error message 2018-09-03 17:06:05.707 [main] INFO c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-example 2018-09-03 17:06:05.709 [main] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - start successful.... 2018-09-03 17:06:05.781 [destination = example address = xxxxx:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position {"identity":{"slaveId":-1 "sourceAddress":{"address":"xxxx" "port":3306}} "postion":{"included":false "journalName":"mysql-bin.002736" "position":30948432 "serverId":2118896143 "timestamp":1535632015000}} 2018-09-03 17:06:05.886 [destination = example address = xxxx:3306 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) ~[canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122) [canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) [canal.parse-1.0.24.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.7.0_141] 2018-09-03 17:06:05.887 [destination = example address = xxxx:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address xxxxx:3306 has an error retrying. caused by java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) ~[canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122) ~[canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) ~[canal.parse-1.0.24.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.7.0_141] 2018-09-03 17:06:05.893 [destination = example address = xxxx:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) at java.lang.Thread.run(Thread.java:748) ] The problem should be that the binlog file in the meta dat can&#39;t keep up with the show. master Log file in STATUS The file in the meta dat is mysql bin 002736 {"clientDatas":[{"clientIdentity":{"clientId":1001 "destination":"example" "filter":""} "cursor":{"identity":{"slaveId":-1 "sourceAddress":{"address":"":"xxxx" "p" "port":3306}} "postion":{"included":false "journalName":"mysql-bin.002736" "position":30948432 "serverId":2118896143 "timestamp":1535632015000}}}] "destination":"example"} show master STATUS latest file mysql-bin.002752 24891270 The previous practice is to delete the meta dat file directly and start syncing again, but the data will be lost. After running for a while, the problem will occur again. How do you solve the problem? mysql Master will sweep based on certain strategies Binlog according to size or time if the log in the slow meta dat is cleaned up Consider this situation to improve overall throughput. 2018-09-04 09:35:33.645 [destination = example address = xxx:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: column size is not match for table:`skstandard`.`tbl_20180824090548lyqbbcqqjdt68u5m` 103 vs 15 Should be due to changes in the table structure, the service has been stuck here. Is there any good way to jump over? How to deal with the situation of this table structure change? I am manually modifying the position skip in the meta dat. Upgrade the new version https://github.com/alibaba/canal/wiki/TableMetaTSDB Oh, you are this column. size is not match for How is the table problem solved? Hope to enlighten me The local version is 1 1 0 but the column will appear. size is not match for Table Daxie enlightenment solution
902,fix bug #890: Parallel analysis + Gtid Did not initialize the gtidSet in the LogContext tks
901,Using the kafka client consumption binlog appears garbled *B> 0mysql-bin.000102�0 UTF 80踬 8B I also encountered the same problem Kafka sends a message packet that is not visible. The client needs to deserialize the package after receiving it. client kafka
900,kafka Server-side code adjustment
899,After the data is consumed for a while, the canal client cannot pull new data to determine that the db is constantly updated. ![image](https://user-images.githubusercontent.com/5965173/44947782-0e424280-ae45-11e8-973a-b2c102d7d704.png) Use the latest version 1 10 Check if there is an error on the server side. ![image](https://user-images.githubusercontent.com/5965173/45035342-57d09e80-b08c-11e8-9bf8-bba18de5ae43.png) After an exception occurs on the server side, the client automatically resubscribes the information and then the client I can&#39;t read the data. Consumption stopped after another abnormality ![image](https://user-images.githubusercontent.com/5965173/45036036-fad5e800-b08d-11e8-8b33-a956e293438d.png) Mysql version 5.6.28 getWithoutAck The above problem occurs with a mode that does not require confirmation. Use get Ack&#39;s model is no problem. First consider upgrading the canal version
898,Canal kafka output garbled CentOS Linux release 7.3.1611 (Core) 3.10.0-514.el7.x86_64 JDK：jdk-8u161-linux-x64.tar.gz zookeeper：zookeeper-3.4.13.tar.gz kafka：kafka_2.11-2.0.0.tgz canal.kafka：canal.kafka-1.1.0.tar.gz MySQL：5.7.22-log Follow https github com alibaba canal wiki Canal Kafka QuickStart configuration Kafka is all garbled I need to adjust there. bin/kafka-console-consumer.sh --bootstrap-server 192.168.10.110:9092 --topic example *D mysql-binlog.000012*UTF-80BJP401663 *? mysql-binlog.000012*UTF-80BJPH  mysql-binlog.000012*UTF-80BtestJtP+Xb _-+_C-+++1Pb? id (0B1Ri++(11)  +e_+ (0B1R +a_cha_(255)D +y_-+-bi++-g.000012 *UTF-80 8BJP401877 *? +y_-+-bi++-g.000012 *UTF-80 8BJPI > +y_-+-bi++-g.000012 *UTF-80 8Beca_dJ+_c+a___ca_d__e-+e_+_+-gPTXb _-+_C-+++1Pb  Kafka data delivery is the data packet. After receiving the data, it must be unpacked into the corresponding message. Refer to the kafka implementation in the client. I saw the AbstractKafkaTest java in the kafka client. After changing the corresponding configuration parameters, insert the new record into mysql and use the command bin kafka console consumer sh --zookeeper 192.168.206.128:2181 --topic canal1 From beginning, you can see that the packet being delivered is garbled but running KafkaClientRunningTest java does not receive the packet.
897,Canal 1 1 0 source canal kafka entry start has been reported abnormal 2018-08-31 09:05:51.141 [destination = example address = /192.168.200.42:3306 EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address /192.168.200.42:3306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: java.net.SocketException: Socket closed Caused by: java.net.SocketException: Socket closed at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:116) ~[na:1.8.0_20] at java.net.SocketOutputStream.write(SocketOutputStream.java:141) ~[na:1.8.0_20] at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.write(BioSocketChannel.java:36) ~[classes/:na] at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.writeBody0(PacketManager.java:42) ~[classes/:na] at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.writeBody(PacketManager.java:35) ~[classes/:na] at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.query(MysqlQueryExecutor.java:55) ~[classes/:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:102) ~[classes/:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.loadBinlogFormat(MysqlConnection.java:433) ~[classes/:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.getBinlogFormat(MysqlConnection.java:582) ~[classes/:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:95) ~[classes/:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) ~[classes/:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_20] 2018-08-31 09:05:51.146 [destination = example address = /192.168.200.42:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: java.net.SocketException: Socket closed Caused by: java.net.SocketException: Socket closed at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:116) at java.net.SocketOutputStream.write(SocketOutputStream.java:141) at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.write(BioSocketChannel.java:36) at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.writeBody0(PacketManager.java:42) at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.writeBody(PacketManager.java:35) at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.query(MysqlQueryExecutor.java:55) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:102) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.loadBinlogFormat(MysqlConnection.java:433) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.getBinlogFormat(MysqlConnection.java:582) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:95) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) at java.lang.Thread.run(Thread.java:745) ] Mysql connection is no problem the same mysql configuration with canal entry to start no problem Socket closed Caused by: java.net.SocketException: Socket closed
896,Fix execTime faster than canal current time delay no data points @wingerx No data from the code The situation of points is basically mysql Host and canal server Ntp does not synchronize mysql time faster Now modify it to be 0 in this case. The wiki will also focus on modifying the description. tks
895,canal To kafka Data consistency problem How to ensure the consistency of data if I set multiple partition canal Whether kafka or rocketmq can only be single-segment global order or single-partition order If there is no strong association between the tables, it is possible to hash the single table to the same partition to ensure that the order table is ordered. It seems that I heard that the project author of canal may be implementing multiple partitions and one table and guarantee that the order does not know whether it is true. In theory, I think that if there is an index table sorted by time, the index table is read first and then the real data is read. Take the binglog data on multiple partitions #958
894,fix #893 index name contains "on" keyword [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=894) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=894) before we can accept your Contribution br hr Zhang Xin seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=894) it.</sub> [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=894) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=894) before we can accept your Contribution br hr Zhang Xin seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=894) it.</sub> tks
893,When creating or deleting an index, if the index name contains an on string, then the name of the table will be incorrect. such as CREATE INDEX `schema_new_index_version_s_idx` ON `tb_email_auth` (`status`) Since version contains on, the table name parsed at this time is _s_idx Correction method com alibaba otter canal parse inbound mysql ddl SimpleDdlParser中的 public static final String CREATE_INDEX_PATTERN = "^\\s*CREATE\\s*(UNIQUE)?(FULLTEXT)?(SPATIAL)?\\s*INDEX\\s*(.*?)\\s+ON\\s+(.*?)$"; public static final String DROP_INDEX_PATTERN = "^\\s*DROP\\s*INDEX\\s*(.*?)\\s+ON\\s+(.*?)$"; The s before and after ON must appear 1 time to multiple times in the source code. Change to In fact, simpparser should be removed from deprecate. This can&#39;t support other complicated scenes.
892,kafka client keep Alive interval
891,canal.deployer-1.1.0 Cursor persistence frequency problem When the test 1 client reconnects, the modification time of the meta dat is updated but the data remains unchanged. Test 2 Due to the problem of test 1, the position is not updated, causing the old persistent cursor client to consume at the old position if the server restarts reading at this time. issue Canal version 1 0 26 alpha5，1.1.0 1.0.26 Alpha2 does not exist this problem every time the client consumes update curosr [#882](https://github.com/alibaba/canal/issues/882)
890,Parallel analysis + Gtid Did not initialize the gtidSet in the LogContext Did not initialize the gtidSet in the LogContext public final void putGtid(GtidLogEvent logEvent) { if (logEvent != null) { String gtid = logEvent.getSid().toString() + ":" + logEvent.getGno(); if (gtidSet == null) { gtid = logEvent.getSid().toString() + ":1-" + logEvent.getGno(); gtidSet = MysqlGTIDSet.parse(gtid); } gtidSet.update(gtid); } } The current server&#39;s gtid in the event will override zk Historical gtidSet in the cursor several server start end combination Next time restart the dump error Is it based on the current main code? Based on 1 1 0 release This commit . commit dd8b1ce9551b59b719516615e43193c467214ade (tag: canal-1.1.0) Author: Seven fronts <jianghang.loujh@alibaba-inc.com> Date: Mon Aug 20 13:28:57 2018 +0800 [maven-release-plugin] prepare release canal-1.1.0 Then cherry pick Parallel parsing of gtid Two bugs fix commit 0ca9fa129975ccb4884d8ca9c02c63b066461e6a Author: winger <winger2049@gmail.com> Date: Sat Aug 25 09:17:13 2018 +0800 fix bug: Open gtid in parallel parsing mode Will cause parsing errors commit 186b58396862cbea82781458625608400fe3e5c5 Author: winger <winger2049@gmail.com> Date: Sat Aug 25 03:54:38 2018 +0800 fix bug: Open gtid in parallel parsing mode Will cause parsing errors If the position stored on the previous zk has multiple gtid collections and some have been mysql master Purged off Very easy to reproduce
889,Canal1 1 0 start example log error 2018-08-27 17:22:51.560 [destination = example address = /192.168.155.35:3306 EventParser] WARN c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - prepare to find start position just show master status 2018-08-27 17:22:51.562 [destination = example address = /192.168.155.35:3306 EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address /192.168.100.249:3306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: command : 'show master status' has an error! pls check. you need (at least one of) the SUPER REPLICATION CLIENT privilege(s) for this operation 2018-08-27 17:22:51.562 [destination = example address = /192.168.155.35:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: command : 'show master status' has an error! pls check. you need (at least one of) the SUPER REPLICATION CLIENT privilege(s) for this operation ] Permissions issue To solve the problem From NetEase Mailbox Master On August 28, 2018 17:47，agapple<notifications@github.com> Write Closed #889. — You are receiving this because you authored the thread. Reply to this email directly view it on GitHub or mute the thread. Daxie An abnormal column appears after the canal modify the table structure size is not match table xx Will ddl open or not? Give trouble to enlighten me Take a look at the document TableMetaTSDB
888,connector.subscribe("xxx") Does not work canal.instance.filter.regex=fid_standard_product\\..* I am on the server side to define this entire library in a client connector subscribe fid_standard_product fid_stock_report One of the tables but the other tables in the library will still be synchronized. Why does the connector subscribe not filter? See more FAQ
887,canal Running an ack error Canal run ack error ack error clientId:1001 batchId:261365 is not exist please check。 This should be a server-side instance restart that causes the server-side configuration to be canal.instance.get.ddl.isolation = false ################################################# ######### destinations ############# ################################################# canal.destinations= db9002 # conf root dir canal.conf.dir = ../conf # auto scan instance dir add/remove and start/stop instance canal.auto.scan = false canal.auto.scan.interval = 5 canal.instance.global.mode = spring canal.instance.global.lazy = false Why does the instance restart? How to control the instance of the server does not restart When you encounter an ack exception, you can roll back again and you can see the example project of example.
886,canal
885,About kafka Topic settings Does it support setting a corresponding kafka for each mysql table? topic？ At first glance, the code seems to only support all topics that correspond to a destination setting multiple topic data will be sent to the settings. Currently only supports one destination for one destination.
884,By canal instance filter regex filter And through the java program connector subscribe Is there anything different? In actual use, only canal can monitor a certain number of tables in a library, and how to match the performance will be better. Look at the wiki
883,Canal1 1 0, when listening to the first change, it will be fine after reporting an error. logEventParserProxy - dump address /106.12.14.74:3306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: java.net.SocketException: Connection reset Caused by: java.net.SocketException: Connection reset at java.net.SocketInputStream.read(SocketInputStream.java:210) ~[na:1.8.0_181] at java.net.SocketInputStream.read(SocketInputStream.java:141) ~[na:1.8.0_181] at java.io.BufferedInputStream.fill(BufferedInputStream.java:246) ~[na:1.8.0_181] at java.io.BufferedInputStream.read1(BufferedInputStream.java:286) ~[na:1.8.0_181] at java.io.BufferedInputStream.read(BufferedInputStream.java:345) ~[na:1.8.0_181] at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.read(BioSocketChannel.java:52) ~[canal.parse.driver-1.1.0.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.readHeader(PacketManager.java:12) ~[canal.parse.driver-1.1.0.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.readNextPacket(MysqlQueryExecutor.java:159) ~[canal.parse.driver-1.1.0.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.query(MysqlQueryExecutor.java:56) ~[canal.parse.driver-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:102) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:148) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:129) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:91) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:188) ~[canal.parse-1.1.0.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_181] 2018-08-26 19:08:07.309 [destination = example address = /106.12.14.74:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: java.net.SocketException: Connection reset Caused by: java.net.SocketException: Connection reset network anomaly Will automatically retry
882,1.0.26-alpha 2: canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield canal.instance.tsdb.enable=false canal.instance.gtidon=false Error log 1 ``` 2018-08-25 10:04:02.917 [destination = platform_data address = ip:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTa bleMeta - parse faield : CREATE TABLE `t_bill_details` ( `bill_id` bigint(20) NOT NULL .... KEY `IDX_BD_CITY` (`store_city`) KEY_BLOCK_SIZE=8 KEY `IDX_BD_DETAIL_ID` (`detail_id`) KEY_BLOCK_SIZE=8 KEY `IDX_BD_SD` (`bill_sale_date`) KEY_BLOCK_SIZE=8 KEY `IDX_BD_SSS` (`bill_status` `store_city` `bill_sale_date` `store_id`) KEY_BLOCK_SIZE=8 KEY `IDX_STORE_ID` (`store_id`) KEY_BLOCK_SIZE=8 KEY `IDX_BD_STATUS` (`bill_status` `bill_sale_date`) KEY_BLOCK_SIZE=8 ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 ROW_FORMAT=COMPRESSED KEY_BLOCK_SIZE=4 com.alibaba.fastsql.sql.parser.ParserException: syntax error error in :'id`) KEY_BLOCK_SIZE=8 KEY `IDX_B' expect RPAREN actual IDENTIFIER pos 1834 line 43 column 32 token IDENTIFIER KEY_BLOCK_SIZE at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:305) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] at com.alibaba.fastsql.sql.parser.SQLParser.accept(SQLParser.java:313) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] at com.alibaba.fastsql.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:260) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] at com.alibaba.fastsql.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:239) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:165) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:76) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:469) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:331) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:71) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.parseTableMeta(TableMetaCache.java:101) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMetaByDB(TableMetaCache.java:87) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:32) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:57) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:52) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) [guava-18.0.jar:na] at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) [guava-18.0.jar:na] at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) [guava-18.0.jar:na] at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) [guava-18.0.jar:na] at com.google.common.cache.LocalCache.get(LocalCache.java:3937) [guava-18.0.jar:na] at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941) [guava-18.0.jar:na] at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824) [guava-18.0.jar:na] at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830) [guava-18.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:185) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:793) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:457) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:133) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:68) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:345) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:187) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:154) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_60] ``` Error log 2 ``` 2018-08-25 09:59:04.733 [destination = platform_data address = ip:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHand ler - destination:platform_data[com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.util.concurrent.UncheckedExecutionException: com.alibaba.otter.canal.parse.exception.CanalParseException: fetch failed by table meta:`platform_data`.`tmp_presto_0e13f1bc14574ec4ae4950cade1bc360` Caused by: com.google.common.util.concurrent.UncheckedExecutionException: com.alibaba.otter.canal.parse.exception.CanalParseException: fetch failed by table meta:`platform_data`.`tmp_presto_0e13f1bc145 74ec4ae4950cade1bc360` at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2203) at com.google.common.cache.LocalCache.get(LocalCache.java:3937) at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941) at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824) at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:185) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:793) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:457) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:127) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:68) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:345) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:187) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:154) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) at java.lang.Thread.run(Thread.java:745) Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: fetch failed by table meta:`platform_data`.`tmp_presto_0e13f1bc14574ec4ae4950cade1bc360` Caused by: java.io.IOException: ErrorPacket [errorNumber=1146 fieldCount=-1 message=Table 'platform_data.tmp_presto_0e13f1bc14574ec4ae4950cade1bc360' doesn't exist sqlState=42S02 sqlStateMarker=#] with command: desc `platform_data`.`tmp_presto_0e13f1bc14574ec4ae4950cade1bc360` at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.query(MysqlQueryExecutor.java:61) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:96) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMetaByDB(TableMetaCache.java:89) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:32) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:62) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:52) at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) at com.google.common.cache.LocalCache.get(LocalCache.java:3937) at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941) at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824) at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:185) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:793) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:457) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:127) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:68) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:345) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:187) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:154) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) at java.lang.Thread.run(Thread.java:745) ``` In fact, I only need to monitor dml. If I configure filtering, how do I configure the following? ``` canal.instance.filter.druid.ddl = true canal.instance.filter.query.dcl = true canal.instance.filter.query.dml = false canal.instance.filter.query.ddl = true canal.instance.filter.table.error = true canal.instance.filter.rows = false ``` Canal instance filter rows specifically do what the wiki did not understand Upgrade the latest 1 1 0 to test first
881,fix bug: Open gtid in parallel parsing mode Will cause parsing errors tks
880,aliyun rds Log parsing failed Could not find first log file name in binary log index file Isn&#39;t that full support for rds? Why is this wrong with canal instance rds accesskey? Those parameters are also matched, but it seems to have no effect. This is the binlog is deleted and cleared off. Use the binlog timestamp to locate
879,Why do I specify a table but I can&#39;t see the schema name and table name from kafka? I set this up canal.instance.filter.regex=db_shopdog_test.deli_order But kafka does not resolve schema names and table names. First look at the wiki to confirm whether the filter is in effect.
878,Instance properties filter the table canal instance filter regex Does not work how to configure changes in a specified table in the monitoring database See more FAQs on the wiki I also have this requirement. I don&#39;t know how to do it. I need to monitor three tables in a library. The FAQ in the wiki inside github can&#39;t be opened. > See more FAQs on the wiki I also have this requirement. I don&#39;t know how to do it. I need to monitor three tables in a library. The FAQ in the wiki inside github can&#39;t be opened.
877,Canal how to configure only listen for update status insert delete does not listen Can filter itself according to the CanalEntry EventType Thank you for your guidance. From NetEase Mailbox Master On August 24, 2018 11:28，xiaokangzhao<notifications@github.com> Write Can filter itself according to the CanalEntry EventType — You are receiving this because you authored the thread. Reply to this email directly view it on GitHub or mute the thread.
876,Integration kafka To server with client tks
875,Canal log start canal error 2018-08-24 06:25:04.409 [New I/O server worker #1-1] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x599715d9 /192.168.254.1:57576 => /192.168.254.128:11111] exception=java.io.IOException: Connection reset by peer at sun.nio.ch.FileDispatcherImpl.read0(Native Method) at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39) at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223) at sun.nio.ch.IOUtil.read(IOUtil.java:192) at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380) at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:322) at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) Looking at the big guys Upgrade to the latest 1 1 0
874,How long is the master-slave synchronization delay? Is there any relevant performance test report? For example, when the amount of data is large, the delay is relatively large. See more wiki perfomance and monitoring
873,Creating a thread pool in a single core environment will cause an error as title [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=873) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=873) before we can accept your contribution.<br/><hr/>**Chuanyi Li L** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=873) it.</sub> [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=873) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=873) before we can accept your contribution.<br/><hr/>**Chuanyi Li L** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=873) it.</sub> Tks It’s no wonder that there have been a lot of feedback thread pool creation failures recently. So many small-size virtual machines are used.
872,canal server、client with canal-kafka server、canal-kafka client merge canal server、client with canal-kafka server、canal-kafka client merge
871,Why is it that I am transmitting to kafka and it is all garbled? The configuration is based on the documentation of the mysql character set is utf8 mysql-bin.000002« *UTF-80¨£¯Ԭ8BJP58025 XshellXshellXshellXshell8 mysql-bin.000002 *UTF-80򯰖 8BJPK w mysql-bin.000002 *UTF-80򯰖 8Btest_heJbP'Xb rowsCount1򂎁Pa (0BdR char(200)= mysql-bin.000002® *UTF-80򯰖 8BJP58052 XshellXshellXshellXshell8 mysql-bin.000002 *UTF-80ȹ񱔬8BJPK w mysql-bin.000002 *UTF-80ȹ񱔬8Btest_heJbP'Xb rowsCount1򂎁Pa (0BdR char(200)= mysql-bin.000002± *UTF-80ȹ񱔬8BJP58481 XshellXshellXshellXshell8 mysql-bin.000002 *UTF-80񺷰Ԭ8BJPK w mysql-bin.000002 *UTF-80񺷰Ԭ8Btest_heJbP'Xb rowsCount1򂎁Pa (0BdR char(200)= mysql-bin.000002´ *UTF-80񺷰Ԭ8BJP58486 XshellXshellXshellXshellXshell8 mysql-bin.000002 *UTF-80º񔪸BJPK w mysql-bin.000002 *UTF-80º񔪸Btest_heJbP'Xb rowsCount1򂎁Pa (0BdR char(200)= mysql-bin.000002· *UTF-80º񔪸BJP58491 XshellXshellXshellXshell8 mysql-bin.000002 *UTF-80񼱖 8BJPK w mysql-bin.000002 *UTF-80񼱖 8Btest_heJbP'Xb rowsCount1򂎁Pa (0BdR char(200)= mysql-bin.000002º *UTF-80񼱖 8BJP58498 XshellXshellXshellXshel8* mysql-bin.000002 *UTF-80тÿ°Ԭ8BJPK w mysql-bin.000002 *UTF-80тÿ°Ԭ8Btest_heJbP'Xb rowsCount1򂎁Pa (0BdR char(200)= mysql-bin.000002½ *UTF-80тÿ°Ԭ8BJP58507 XshellXshellXshellXshell* 8 mysql-bin.000002 *UTF-80Ȇ±Ԭ8BJPK w mysql-bin.000002 *UTF-80Ȇ±Ԭ8Btest_heJbP'Xb rowsCount1򂎁Pa (0BdR char(200)= mysql-bin.000002 *UTF-80Ȇ±Ԭ8BJP58520 XshellXshellXshellXshell* 8 mysql-bin.000002 ¡ *UTF-80¸²±Ԭ8BJPK w mysql-bin.000002¢ *UTF-80¸²±Ԭ8Btest_heJbP'X rowsCount1򂎁Pa (0BdR char(200)= mysql-bin.000002â *UTF-80¸²±Ԭ8BJP58525 XshellXshellXshellXshell* 8 mysql-bin.000002££ *UTF-80׍±Ԭ8BJPK w mysql-bin.000002¤ *UTF-80׍±Ԭ8Btest_heJbP'Xb rowsCount1򂎁Pa (0BdR char(200)= mysql-bin.000002Ƥ *UTF-80׍±Ԭ8BJP58538 Passed to kafka, the serialized data client of the message is deserialized to message after receiving it.
870,Canal monitor how multiple mysql databases should be configured Read more wiki
869,canal-kafka-1.0.26-preview-4: CanalKafkaStarter High CPU usage CanalKafkaStarter.worker Calling getWithoutAck in thread does not use timeout parameter ``` "pool-8-thread-1" #21 prio=5 os_prio=0 tid=0x00007facb47b1000 nid=0x5fe runnable [0x00007fac8d16b000] java.lang.Thread.State: RUNNABLE at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.isStart(CanalServerWithEmbedded.java:126) at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.checkStart(CanalServerWithEmbedded.java:484) at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:279) at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:259) at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.worker(CanalKafkaStarter.java:118) at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.access$000(CanalKafkaStarter.java:26) at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter$1.run(CanalKafkaStarter.java:67) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) ``` The latest 1 1 1 has been fixed
868,canal-kafka-1.1.0: Infinite loop CanalKafkaStarter - process error! kafka: n1:9092 n2:9092 zk: n3:2181 n4:2181 n5:2181 canal: n1 Error log ``` 2018-08-23 16:25:08.274 [destination = example address = n1/192.168.4.11:3306 EventParser] WARN c.a.o.c.p.inboun d.mysql.rds.RdsBinlogEventParserProxy - find start position : EntryPosition[included=false journalName=mysql-bin.0000 01 position=3024 serverId=<null> gtid=<null> timestamp=<null>] 2018-08-23 16:25:08.299 [main] INFO com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - ## start the kafka wo rkers. 2018-08-23 16:25:08.300 [main] INFO com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - ## the kafka workers is running now ...... 2018-08-23 16:25:08.301 [pool-4-thread-1] INFO com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - ## start t he canal consumer: example. 2018-08-23 16:25:08.305 [pool-4-thread-1] INFO com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - ## the can al consumer example is running now ...... 2018-08-23 16:25:15.105 [pool-4-thread-1] ERROR com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - process er ror! java.lang.NullPointerException: null at com.alibaba.otter.canal.store.helper.CanalEventUtils.createPosition(CanalEventUtils.java:69) ~[canal.store -1.1.0.jar:na] at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffe r.java:375) ~[canal.store-1.1.0.jar:na] at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffe r.java:36) ~[canal.store-1.1.0.jar:na] at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java :307) ~[canal.server-1.1.0.jar:na] at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java :273) ~[canal.server-1.1.0.jar:na] at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.worker(CanalKafkaStarter.java:121) [canal.kafka-1 .1.0.jar:na] at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.access$000(CanalKafkaStarter.java:26) [canal.kafk a-1.1.0.jar:na] at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter$1.run(CanalKafkaStarter.java:70) [canal.kafka-1.1 .0.jar:na] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_161] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_161] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-08-23 16:25:15.106 [pool-4-thread-1] ERROR com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - process er ror! java.lang.NullPointerException: null at com.alibaba.otter.canal.store.helper.CanalEventUtils.createPosition(CanalEventUtils.java:69) ~[canal.store -1.1.0.jar:na] at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffe : ``` Configuration canal.properties ```################################################# ######### common argument ############# ################################################# canal.id= 1 canal.ip= canal.port=11111 canal.metrics.pull.port=11112 canal.zkServers=n3:2181 n4:2181 n5:2181 # flush data to zk canal.zookeeper.flush.period = 1000 canal.withoutNetty = true # flush meta cursor/parse position to file canal.file.data.dir = ${canal.conf.dir} canal.file.flush.period = 1000 ## memory store RingBuffer size should be Math.pow(2 n) canal.instance.memory.buffer.size = 16384 ## memory store RingBuffer used memory unit size default 1kb canal.instance.memory.buffer.memunit = 1024 ## meory store gets mode used MEMSIZE or ITEMSIZE canal.instance.memory.batch.mode = MEMSIZE ## detecing config canal.instance.detecting.enable = false #canal.instance.detecting.sql = insert into retl.xdual values(1 now()) on duplicate key update x=now() canal.instance.detecting.sql = select 1 canal.instance.detecting.interval.time = 3 canal.instance.detecting.retry.threshold = 3 canal.instance.detecting.heartbeatHaEnable = false # support maximum transaction size more than the size of the transaction will be cut into multiple transactions delivery canal.instance.transaction.size = 1024 # mysql fallback connected to new master should fallback times canal.instance.fallbackIntervalInSeconds = 60 # network config canal.instance.network.receiveBufferSize = 16384 canal.instance.network.sendBufferSize = 16384 canal.instance.network.soTimeout = 30 # binlog filter config canal.instance.filter.druid.ddl = true canal.instance.filter.query.dcl = true canal.instance.filter.query.dml = false canal.instance.filter.query.ddl = true canal.instance.filter.table.error = false canal.instance.filter.rows = false canal.instance.filter.transaction.entry = true # binlog format/image check canal.instance.binlog.format = ROW STATEMENT MIXED canal.instance.binlog.image = FULL MINIMAL NOBLOB # binlog ddl isolation canal.instance.get.ddl.isolation = false # parallel parser config canal.instance.parser.parallel = true ## concurrent thread number default 60% available processors suggest not to exceed Runtime.getRuntime().availableProcessors() #canal.instance.parser.parallelThreadSize = 16 ## disruptor ringbuffer size must be power of 2 canal.instance.parser.parallelBufferSize = 256 # table meta tsdb info canal.instance.tsdb.enable=true canal.instance.tsdb.dir=${canal.file.data.dir:../conf}/${canal.instance.destination:} canal.instance.tsdb.url=jdbc:h2:${canal.instance.tsdb.dir}/h2;CACHE_SIZE=1000;MODE=MYSQL; canal.instance.tsdb.dbUsername=canal canal.instance.tsdb.dbPassword=canal # rds oss binlog account canal.instance.rds.accesskey = canal.instance.rds.secretkey = ################################################# ######### destinations ############# ################################################# canal.destinations= example # conf root dir canal.conf.dir = ../conf # auto scan instance dir add/remove and start/stop instance canal.auto.scan = true canal.auto.scan.interval = 5 canal.instance.tsdb.spring.xml=classpath:spring/tsdb/h2-tsdb.xml #canal.instance.tsdb.spring.xml=classpath:spring/tsdb/mysql-tsdb.xml canal.instance.global.mode = spring canal.instance.global.lazy = false #canal.instance.global.manager.address = 127.0.0.1:1099 #canal.instance.global.spring.xml = classpath:spring/memory-instance.xml #canal.instance.global.spring.xml = classpath:spring/file-instance.xml canal.instance.global.spring.xml = classpath:spring/default-instance.xml ``` kafka.yml ```servers: n1:9092 n2:9092 retries: 0 batchSize: 16384 lingerMs: 1 bufferMemory: 33554432 # Lot size unit of canal The amount of k is recommended to be changed to 1M canalBatchSize: 50 filterTransactionEntry: true canalDestinations: - canalDestination: example topic: example partition: ``` example/instance.properties ```################################################# ## mysql serverId v1.0.26+ will autoGen # canal.instance.mysql.slaveId=0 # enable gtid use true/false canal.instance.gtidon=false # position info canal.instance.master.address=n1:3306 canal.instance.master.journal.name= canal.instance.master.position= canal.instance.master.timestamp= canal.instance.master.gtid= # rds oss binlog canal.instance.rds.accesskey= canal.instance.rds.secretkey= canal.instance.rds.instanceId= # table meta tsdb info canal.instance.tsdb.enable=false #canal.instance.tsdb.url=jdbc:mysql://127.0.0.1:3306/canal_tsdb #canal.instance.tsdb.dbUsername=canal #canal.instance.tsdb.dbPassword=canal #canal.instance.standby.address = #canal.instance.standby.journal.name = #canal.instance.standby.position = #canal.instance.standby.timestamp = #canal.instance.standby.gtid= # username/password canal.instance.dbUsername=root canal.instance.dbPassword=root canal.instance.connectionCharset=UTF-8 # table regex canal.instance.filter.regex=test\\..* # table black regex canal.instance.filter.black.regex= ################################################# ``` First insert into table Successfully parsing binlog and then this problem occurs Use the SimpleCanalClientTest in the official canal example to read the first inserted data can be parsed after the error log is as follows Always changing ports ``` **************************************************** * Batch Id: [1] count : [1] memsize : [55] Time : 2018-08-25 15:56:55 * Start : [mysql-bin.000005:642:1535212613000(2018-08-25 23:56:53)] * End : [mysql-bin.000005:642:1535212613000(2018-08-25 23:56:53)] **************************************************** ----------------> binlog[mysql-bin.000005:642] name[test ar_tmp] eventType : INSERT executeTime : 1535212613000(2018-08-25 23:56:53) gtid : () delay : -28797618 ms id : 1 type=int(11) update=true name : a type=varchar(32) update=true price : 1.1 type=double update=true time : 2018-08-25 23:56:53 type=datetime update=true process error!com.alibaba.otter.canal.protocol.exception.CanalClientException: something goes wrong with reason: something goes wrong with channel:[id: 0x12e25627 /192.168.4.1:9230 => /192.168.4.21:11111] exception=java.lang.NullPointerException at com.alibaba.otter.canal.store.helper.CanalEventUtils.createPosition(CanalEventUtils.java:69) at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:375) at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:36) at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:307) at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:273) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:124) at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:100) at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754) at org.jboss.netty.channel.SimpleChannelHandler.messageReceived(SimpleChannelHandler.java:154) at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:100) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80) at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:444) at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80) at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:540) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261) at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:350) at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.receiveMessages(SimpleCanalConnector.java:344) ~[classes/:na] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:315) ~[classes/:na] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:287) ~[classes/:na] at com.alibaba.otter.canal.example.AbstractCanalClientTest.process(AbstractCanalClientTest.java:110) ~[classes/:na] at com.alibaba.otter.canal.example.AbstractCanalClientTest$2.run(AbstractCanalClientTest.java:76) [classes/:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171] process error!com.alibaba.otter.canal.protocol.exception.CanalClientException: failed to subscribe with reason: something goes wrong with channel:[id: 0x1fda2da3 /192.168.4.1:9232 => /192.168.4.21:11111] exception=java.lang.NullPointerException at com.alibaba.otter.canal.store.helper.CanalEventUtils.createPosition(CanalEventUtils.java:69) at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:375) at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:36) at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.subscribe(CanalServerWithEmbedded.java:157) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:77) at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:100) at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754) at org.jboss.netty.channel.SimpleChannelHandler.messageReceived(SimpleChannelHandler.java:154) at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:100) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80) at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:444) at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80) at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:540) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261) at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:350) at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.subscribe(SimpleCanalConnector.java:241) ~[classes/:na] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.subscribe(SimpleCanalConnector.java:218) ~[classes/:na] at com.alibaba.otter.canal.example.AbstractCanalClientTest.process(AbstractCanalClientTest.java:108) ~[classes/:na] at com.alibaba.otter.canal.example.AbstractCanalClientTest$2.run(AbstractCanalClientTest.java:76) [classes/:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171] process error!com.alibaba.otter.canal.protocol.exception.CanalClientException: failed to subscribe with reason: something goes wrong with channel:[id: 0x1c0b056f /192.168.4.1:9233 => /192.168.4.21:11111] exception=java.lang.NullPointerException at com.alibaba.otter.canal.store.helper.CanalEventUtils.createPosition(CanalEventUtils.java:69) at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:375) at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:36) at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.subscribe(CanalServerWithEmbedded.java:157) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:77) at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:100) at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754) at org.jboss.netty.channel.SimpleChannelHandler.messageReceived(SimpleChannelHandler.java:154) at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:100) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80) at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:444) at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80) at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:540) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261) at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:350) at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.subscribe(SimpleCanalConnector.java:241) ~[classes/:na] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.subscribe(SimpleCanalConnector.java:218) ~[classes/:na] at com.alibaba.otter.canal.example.AbstractCanalClientTest.process(AbstractCanalClientTest.java:108) ~[classes/:na] at com.alibaba.otter.canal.example.AbstractCanalClientTest$2.run(AbstractCanalClientTest.java:76) [classes/:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171] process error!com.alibaba.otter.canal.protocol.exception.CanalClientException: failed to subscribe with reason: something goes wrong with channel:[id: 0x6db99e95 /192.168.4.1:9234 => /192.168.4.21:11111] exception=java.lang.NullPointerException at com.alibaba.otter.canal.store.helper.CanalEventUtils.createPosition(CanalEventUtils.java:69) at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:375) at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:36) at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.subscribe(CanalServerWithEmbedded.java:157) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:77) ``` I am using the spring file instance xml preliminary estimate should be the destination_dir meta dat problem Meta dat written in alpha5 version ``` {"clientDatas":[{"clientIdentity":{"clientId":1001 "destination":"local_mysql" "filter":""} "cursor":{"identity":{"slaveId":-1 "sourceAddress":{"address":"192.168.4.101" "port":3306}} "postion":{"gtid":"" "included":false "journalName":"mysql-bin.000005" "position":14016 "serverId":1 "timestamp":1535189721000}}}] "destination":"local_mysql"} ``` And the meta dat written by 1 1 0 ``` {"clientDatas":[{"clientIdentity":{"clientId":1001 "destination":"local_mysql" "filter":""}}] "destination":"local_mysql"} ``` If I put the alpha5 meta dat over the 1 1 0 1 1 0, there is no upstairs problem. NPE problem We pay attention to it 2018-09-03 17:21:00.046 [main] INFO com.alibaba.otter.canal.kafka.CanalServerStarter - ## the canal server is running now ...... 2018-09-03 17:21:00.048 [main] INFO com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - ## load kafka configurations 2018-09-03 17:21:00.169 [main] INFO com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - ## start the kafka workers. 2018-09-03 17:21:00.169 [main] INFO com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - ## the kafka workers is running now ...... 2018-09-03 17:21:00.169 [pool-5-thread-1] INFO com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - ## start the canal consumer: ordertest3317. 2018-09-03 17:21:53.135 [canal-instance-scan-0] WARN o.s.beans.GenericTypeAwarePropertyDescriptor - Invalid JavaBean property 'connectionCharset' being accessed! Ambiguous write methods found next to actually used [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.lang.String)]: [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.nio.charset.Charset)] 2018-09-03 17:21:53.157 [canal-instance-scan-0] INFO c.a.o.canal.deployer.monitor.SpringInstanceConfigMonitor - auto notify reload ordertest3317 successful. 2018-09-03 17:21:54.161 [destination = ordertest3317 address = yunjitest.mysql.rds.aliyuncs.com/47.98.70.247:3306 EventParser] WARN c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - prepare to find start position just show master status 2018-09-03 17:21:54.204 [destination = ordertest3317 address = yunjitest.mysql.rds.aliyuncs.com/47.98.70.247:3306 EventParser] WARN c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - find start position : EntryPosition[included=false journalName=mysql-bin.001277 position=264457500 serverId=<null> gtid=<null> timestamp=<null>] 2018-09-03 17:21:54.230 [pool-5-thread-1] INFO com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - ## the canal consumer ordertest3317 is running now ...... **2018-09-03 17:21:55.892 [pool-5-thread-1] ERROR com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - process error! java.lang.NullPointerException: null at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.calculateSize(MemoryEventStoreWithBuffer.java:555) ~[canal.store-1.1.0.jar:na] at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.doGet(MemoryEventStoreWithBuffer.java:322) ~[canal.store-1.1.0.jar:na] at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.tryGet(MemoryEventStoreWithBuffer.java:261) ~[canal.store-1.1.0.jar:na] at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getEvents(CanalServerWithEmbedded.java:478) ~[canal.server-1.1.0.jar:na] at** com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:310) ~[canal.server-1.1.0.jar:na] at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:273) ~[canal.server-1.1.0.jar:na] at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.worker(CanalKafkaStarter.java:121) [canal.kafka-1.1.0.jar:na] at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.access$000(CanalKafkaStarter.java:26) [canal.kafka-1.1.0.jar:na] at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter$1.run(CanalKafkaStarter.java:70) [canal.kafka-1.1.0.jar:na] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_181] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_181] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_181] 2018-09-03 17:21:55.896 [pool-5-thread-1] INFO com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - ## the canal consumer ordertest3317 is running now ...... 2018-09-03 17:21:55.896 [pool-5-thread-1] ERROR com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - process error! java.lang.NullPointerException: null at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.calculateSize(MemoryEventStoreWithBuffer.java:555) ~[canal.store-1.1.0.jar:na] at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.doGet(MemoryEventStoreWithBuffer.java:322) ~[canal.store-1.1.0.jar:na] at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.tryGet(MemoryEventStoreWithBuffer.java:261) ~[canal.store-1.1.0.jar:na] at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getEvents(CanalServerWithEmbedded.java:478) ~[canal.server-1.1.0.jar:na] at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:310) ~[canal.server-1.1.0.jar:na] at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:273) ~[canal.server-1.1.0.jar:na] at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.worker(CanalKafkaStarter.java:121) [canal.kafka-1.1.0.jar:na] at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.access$000(CanalKafkaStarter.java:26) [canal.kafka-1.1.0.jar:na] at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter$1.run(CanalKafkaStarter.java:70) [canal.kafka-1.1.0.jar:na] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_181] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_181] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_181] Version 1 1 0 spring.xml /default-instance.xml I also encountered an infinite NPE error here. The key log of the error is shown in the bold section above. server After the last debugging, there was more than 1 week without using this startup. I reported the error. I cleared the zk and restarted. at com.alibaba.otter.canal.store.helper.CanalEventUtils.createPosition(CanalEventUtils.java:69) I have seen this error but I can&#39;t find the corresponding log. I changed the database instance to another test library to clear the zk startup. This error did not occur, then I re-exchange the database instance to the original problem. Set timestamp to the current time. I suspect that there is a problem with the binlog. The current time is set to skip the problematic binlog. The startup result is not reported. I want the error to reappear and change the timestamp back to the time before the problem. Clear zk but the error Did not appear again I want to locate the cause of this NPE error. After that, I can avoid or solve it in production. Please enlighten me this question. What is the problem? I am very worried that there will be problems after going online, and I will not be able to hold it. 2018-10-19 03:08:14.957 [pool-4-thread-1] INFO com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - ## the canal consumer example is running now ...... 2018-10-19 03:08:14.957 [pool-4-thread-1] ERROR com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - process error! java.lang.NullPointerException: null ![image](https://user-images.githubusercontent.com/14846522/47178034-9a082380-d34c-11e8-9b2a-d9079e6c5485.png) My version is canal.kafka-1.1.0 This is the error above. You try cat canal.properties Do not consult the wiki The default use on the top is my solution #canal.instance.global.spring.xml = classpath:spring/memory-instance.xml canal.instance.global.spring.xml = classpath:spring/file-instance.xml #canal.instance.global.spring.xml = classpath:spring/default-instance.xml ![image](https://user-images.githubusercontent.com/14846522/47182391-e311a500-d357-11e8-8d21-929ddd83e007.png) reopened but closed again just because Github was down yesterday which I think is MS 'stealling user's data
867,I encountered mysql The client will report an error when doing large-scale operations. com.alibaba.otter.canal.protocol.exception.CanalClientException: java.io.IOException: Broken pipe at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.ack(SimpleCanalConnector.java:339) at com.adups.canal.CanalHandler.handler(CanalHandler.java:72) at com.adups.canal.CanalClient$2.run(CanalClient.java:52) at java.lang.Thread.run(Thread.java:748) Caused by: java.io.IOException: Broken pipe at sun.nio.ch.FileDispatcherImpl.write0(Native Method) at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47) at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93) at sun.nio.ch.IOUtil.write(IOUtil.java:65) at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:471) at java.nio.channels.Channels.writeFullyImpl(Channels.java:78) at java.nio.channels.Channels.writeFully(Channels.java:98) at java.nio.channels.Channels.access$000(Channels.java:61) at java.nio.channels.Channels$1.write(Channels.java:174) at java.nio.channels.Channels$WritableByteChannelImpl.write(Channels.java:458) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.writeWithHeader(SimpleCanalConnector.java:382) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.writeWithHeader(SimpleCanalConnector.java:369) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.ack(SimpleCanalConnector.java:333) ... 3 common frames omitted The client processing speed is slower than the timeout period. Upgrade 1 1 0 Is the client ack too slow to cause a timeout error?
866,Canal deployer 1 1 0 version when listening to database changes, the server end reported abnormally 2018-08-23 22:52:32.366 [destination = example address = /106.12.14.74:3306 EventParser] WARN c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - find start position : EntryPosition[included=false journalName=mysql-bin.000004 position=5377 serverId=1 gtid=<null> timestamp=1535008711000] 2018-08-23 22:52:32.582 [destination = example address = /106.12.14.74:3306 EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address /106.12.14.74:3306 has an error retrying. caused by java.lang.IllegalArgumentException: null at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1314) ~[na:1.8.0_181] at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1237) ~[na:1.8.0_181] at java.util.concurrent.Executors.newFixedThreadPool(Executors.java:151) ~[na:1.8.0_181] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor.start(MysqlMultiStageCoprocessor.java:84) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:238) ~[canal.parse-1.1.0.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_181] 2018-08-23 22:52:32.582 [destination = example address = /106.12.14.74:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.lang.IllegalArgumentException at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1314) at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1237) at java.util.concurrent.Executors.newFixedThreadPool(Executors.java:151) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor.start(MysqlMultiStageCoprocessor.java:84) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:238) at java.lang.Thread.run(Thread.java:748) ] I have encountered the same problem, I don’t know if the landlord has solved it. I was still sending emails to the development team. He said to see if the parameter configuration has been modified in the canal properties and I have not modified it. Still waiting for him to reply --- from Dcein. On August 23, 2018 17:23，platypus0127<notifications@github.com> Write I have encountered the same problem, I don’t know if the landlord has solved it. — You are receiving this because you authored the thread. Reply to this email directly view it on GitHub or mute the thread. Asking if the problem is running in a single-core environment, there may be a bug. Change the canal instance parser parallel to false. @lcybo Thank you for your success. If you have this in a multi-core environment, you don’t have to set it up. Multi-core Thanks for yelling if you can monitor multiple mysql server changes, you need to configure multiple canal instance standby address = Is the monitored mysql address still configured with multiple canal instance master address? On August 23, 2018 18:20，lcybo<notifications@github.com> Write Multi-core — You are receiving this because you authored the thread. Reply to this email directly view it on GitHub or mute the thread. Daxie Hello, I really bothered you. I have time to help me solve the problem. I have opened the row mode in our linux mysql configuration file and let the operation manager open the canal related operation permission. However, after checking, the new example log still reports an error. Give trouble and give pointers 2018-08-27 18:27:16.849 [destination = example address = /192.168.100.249:3306 EventParser] WARN c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - prepare to find start position just show master status 2018-08-27 18:27:16.851 [destination = example address = /192.168.100.249:3306 EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address /192.168.100.249:3306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: command : 'show master status' has an error! pls check. you need (at least one of) the SUPER REPLICATION CLIENT privilege(s) for this operation 2018-08-27 18:27:16.851 [destination = example address = /192.168.100.249:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: command : 'show master status' has an error! pls check. you need (at least one of) the SUPER REPLICATION CLIENT privilege(s) for this operation ] Thank you for your enlightenment. | | Dcein520 | | Mailbox Dcein520 163 com | Signature by NetEase Mailbox Master custom made On August 23, 2018 18:26，Dcein520 Write Thanks for yelling if you can monitor multiple mysql server changes, you need to configure multiple canal instance standby address = Is the monitored mysql address still configured with multiple canal instance master address? On August 23, 2018 18:20，lcybo<notifications@github.com> Write Multi-core — You are receiving this because you authored the thread. Reply to this email directly view it on GitHub or mute the thread. you need (at least one of) the SUPER REPLICATION CLIENT privilege(s) for this Check mysql grant > Daxie Hello, I really bothered you. I have time to help me solve the problem. I have opened the row mode in our linux mysql configuration file and let the operation manager open the canal related operation permission. However, after checking, the new example log still reports an error. Give trouble and give pointers 2018-08-27 18:27:16.849 [destination = example address = /192.168.100.249:3306 EventParser] WARN c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - prepare to find start position just show master status 2018-08-27 18:27:16.851 [destination = example address = /192.168.100.249:3306 EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address /192.168.100.249:3306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: command : 'show master status' has an error! pls check. you need (at least one of) the SUPER REPLICATION CLIENT privilege(s) for this operation 2018-08-27 18:27:16.851 [destination = example address = /192.168.100.249:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: command : 'show master status' has an error! pls check. you need (at least one of) the SUPER REPLICATION CLIENT privilege(s) for this operation ] Thank you for your enlightenment. | | Dcein520 | | Mailbox Dcein520 163 com | Signature by NetEase Mailbox Master custom made On August 23, 2018 18:26，Dcein520 Write Thanks for yelling if you can monitor multiple mysql server changes, you need to configure multiple canal instance standby address = Is the monitored mysql address still configured with multiple canal instance master address? On August 23, 2018 18:20，lcybo<notifications@github.com> Write Multi-core — You are receiving this because you authored the thread. Reply to this email directly view it on GitHub or mute the thread. Recommended in example instance properties Modify inside canal.instance.filter.regex=.*\\..* Only monitor the database of your own instance I found that even when the rds is used, the superuser will prompt insufficient permissions. Estimated insufficient built-in library permissions Rds super account still has no permissions on the mysql library part of the table need to filter out the mysql library
865,fix #849: HBase data synchronization adaptation HBase data synchronization external adapter will be packaged under client launcher tks
864,canal-1.1.0 BioSocketChannel Timeout `2018-08-21 05:51:48.895 [destination = zaful address = /127.0.0.1:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position {"identity":{"slaveId":-1 "sourceAddress":{"address":"127.0.0.1" "port":3306}} "postion":{"gtid":"" "included":false "journalName":"binlog.002535" "position":157229530 "serverId":97153 "timestamp":1534833018000}} 2018-08-21 05:51:48.895 [destination = zaful address = /127.0.0.1:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=binlog.002535 position=157229530 serverId=97153 gtid= timestamp=1534833018000] 2018-08-21 05:52:36.469 [destination = zaful address = /127.0.0.1:3306 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - Socket timeout expired closing connection java.net.SocketTimeoutException: Timeout occurred failed to read 9471 bytes in 25000 milliseconds. at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.read(BioSocketChannel.java:123) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:174) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:85) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:206) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:240) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_91] 2018-08-21 05:52:36.470 [destination = zaful address = /127.0.0.1:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address 127.0.0.1/127.0.0.1:3306 has an error retrying. **caused by _### **### > java.net.SocketTimeoutException: Timeout occurred failed to read 9471 bytes in 25000 milliseconds **_.** at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.read(BioSocketChannel.java:123) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:174) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:85) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:206) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:240) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_91] 2018-08-21 05:52:36.470 [destination = zaful address = /127.0.0.1:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:zaful[java.net.SocketTimeoutException: Timeout occurred failed to read 9471 bytes in 25000 milliseconds. at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.read(BioSocketChannel.java:123) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:174) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:85) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:206) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:240) at java.lang.Thread.run(Thread.java:745) ]` Program startup After running for a period of time, I have been reporting this error. After each fetch timeout, I will kill the connection with Mysql and then try again and then timeout. This is the endless loop restart. It is useless, but after deleting the meta dat, it can run normally for a while. Will report a timeout error I really can&#39;t understand why reading a few kb of data can&#39;t be read for 25s without a lot of binlog files being consumed. Do you have colleagues who have the same problem? Try 1 1 0 version @fangchunsheng If your environment can often reproduce, it is recommended BioSocketChannel.read(BioSocketChannel.java:123) Is there a debug inside? input.read(data off + n len - n); This call is really timeout PS: Looked at the openjdk implementation timeout through poll or select to see if the return value is 0, so even if only 1 byte is read, it does not count timeout @agapple Sorry The typo was written as 2 1 1 The actual use is 1 1 0, but I will not have this situation when I switch to 1 0 25 @lcybo ok let me try However, we are not using openjdk online. Thank you I read 4 bytes here will timeout start example will also report this error java.net.SocketTimeoutException: Timeout occurred failed to read 4 bytes in 25000 milliseconds. at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.read(BioSocketChannel.java:123) ~[canal.parse.driver-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:174) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:77) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:213) [canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:248) [canal.parse-1.1.0.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.7.0_191] 2018-08-22 05:24:35.190 [destination = centos1 address = /127.0.0.1:3306 EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address /127.0.0.1:3306 has an error retrying. caused by java.net.SocketTimeoutException: Timeout occurred failed to read 4 bytes in 25000 milliseconds. at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.read(BioSocketChannel.java:123) ~[canal.parse.driver-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:174) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:77) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:213) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:248) ~[canal.parse-1.1.0.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.7.0_191] 2018-08-22 05:24:35.194 [destination = centos1 address = /127.0.0.1:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:centos1[java.net.SocketTimeoutException: Timeout occurred failed to read 4 bytes in 25000 milliseconds. at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.read(BioSocketChannel.java:123) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:174) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:77) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:213) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:248) at java.lang.Thread.run(Thread.java:748) @nerverbe Yes I will have this situation on my side. That is, 4 bytes will also report an error. I looked at the code and should read the header of his protocol. It will be 4 bytes. I also tried to change 25000ms to 75000ms but the same error. If you confirm that the socketInputStream read call has a timeout, you can consider the network problem. View dump Whether the network card used for connection is occupied or not @lcybo I use canal 1 1 0 The test case of the dump inside goes to the line and runs through the same binglog file and then the same position. There is no problem when running, so it should not be a network cause. @fangchunsheng Is the test case you said is MysqlDumpTest? The MysqlEventParser in the test case is the socketChannel used by the parser on the line. It is also consistent. It seems that we should also consider the factors outside of MysqlEventParser. @agapple 。
863,Set the master in the program Position error I am direct extends MysqlEventParser ```java /** * Query the current binlog location * @param mysqlConnection * @return */ private EntryPosition findEndPosition(MysqlConnection mysqlConnection) { try { // ... EntryPosition endPosition = new EntryPosition( "my-bin.000001" Long.valueOf(1L)); return endPosition; ``` Direct error reporting ```cmd 18:06:43.222 [destination = example address = /117.107.241.79:3306 EventParser] INFO c.a.o.c.p.d.mysql.MysqlConnector - KILL DUMP 1110 failure java.io.IOException: ErrorPacket [errorNumber=1094 fieldCount=-1 message=Unknown thread id: 1110 sqlState=HY000 sqlStateMarker=#] with command: KILL CONNECTION 1110 at com.alibaba.otter.canal.parse.driver.mysql.MysqlUpdateExecutor.update(MysqlUpdateExecutor.java:49) ~[classes/:na] at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.disconnect(MysqlConnector.java:107) ~[classes/:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.disconnect(MysqlConnection.java:93) [classes/:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:309) [classes/:na] ``` I am too low, it should start from 4L
862,kafka Version wiki configuration instructions are unknown The deploy version of canal_kafka configures kafka xml according to the wiki configuration method. Other reference canal independent configuration kafka Console consumption does not output canal here and there is no log feedback. Do you need special configuration? Canal_server is not directly started from the target canal bin under the deploy module, but from the target canal bin under the kafka module, after starting the server, observe whether there is data sent to kafka. The file directory in the release is just the same. When the canal is started, there is kafka. workers is running now Just don&#39;t output to kafka Client can&#39;t see the data The same question asks how to solve it or not, and there is no log output. Kafka is not able to consume data. Reference documentation https github com alibaba canal wiki Canal Kafka RocketMQ QuickStart Modify and feedback if the document is not clear
861,Client The datasource configuration item of the QPS indicator is changed from Prometheus to the datasource parameter. After the upgrade of Canal to 1 1 0, the monitoring is found in the monitoring page of Grafana. The reason why QPS&#39;s panel is not available is because of Canal The profile panel&#39;s configuration file Canal_instances_tmpl json is named Client The datasource configuration item of the QPS panel is written as a fixed value. Prometheus is modified to take the datasource parameter and the panel can be displayed normally. [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=861) <br/>All committers have signed the CLA. [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=861) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=861) before we can accept your contribution.<br/><hr/>**lixiang** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=861) it.</sub> Thank you
860,How to share zk canal in cluster mode Client side How to specify zk? An article is provided with the following method Production scene HA mode, such as using ZK as a service management, here at least specify the majority ZK Node&#39;s IP list If you have multiple Canals Cluster shares ZK, then each Canal also needs to use a unique rootpath canal.zkServers = 10.0.1.21:2818 10.0.1.22 10.0.2.21:2818/canal/g1 My own machine test canal.zkServers = localhost:2181/canal/g1 Start directly and report an error Something goes wrong when starting up the canal Server: java.lang.IllegalArgumentException: Path length must be > 0 If native support is not supported, it is only possible to isolate multiple canals by deploying multiple sets of zk. server Cluster? /canal/g1 Do not bring this I also encountered this problem. The current design looks like only one set of zk supports a set of canal server HA cluster Same set of canal Can the server&#39;s registration path in zk be modified? Canal itself supports cluster mode in A canal server After the startup is completed, there will be information about the node B in the running of the zk destination. canal If the server is started with A canal The destination of the server has the same name. There will be information about the node in the cluster of the zk destination directory. Since A is already running, this node will not pull the binlog only when A canal server After hanging, ZK will notify B to obtain the binlog running information and change to B. canal The information of A in server cluster will be gone. The client side is directly connected to the zk and destination of the corresponding destination. Name can be used to implement HA through zk scheduling when there are multiple clients. The problem has been solved before asking this question is unfamiliar with canal
859,Can you provide some related internal design documents for 1 1 0? Can you provide some related internal design documents for 1 1 0? Are in the wiki Looks like the old wiki No 1 1 0 Such as prometheus https://github.com/alibaba/canal/wiki Here Thank you very much
858,The tableMetaCache property in LogEventConvert java is null in 1 1 0 When is tableMetaCache initialized? Specific reproduction method And have you modified your code yourself? Changed to the code Direct inheritance ```java extends MysqlEventParser ``` Thanks so that tableMetaCache is initialized.
857,Add the total entrance of the client launcher module external data landing adapter The total entry of the external data landing adapter can be adapted to the data synchronization of HBase and ES.
856,ErrotCode:400 canal 1.0.26-SNAPSHOT-2 2018-08-20 09:46:27.050 [New I/O server worker #1-1] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400 Caused by : something goes wrong with channel:[id: 0x2ae8ccbc /10.31.152.38:34114 => /10.31.152.38:11111] exception=com.alibaba.otter.canal.server.exception.CanalServerException: ack error clientId:1001 batchId:36171 is not exist please check 2018-08-20 09:48:12.626 [New I/O server worker #1-3] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400 Caused by : something goes wrong with channel:[id: 0x568191e2 /10.31.152.38:34184 => /10.31.152.38:11111] exception=com.alibaba.otter.canal.server.exception.CanalServerException: ack error clientId:1001 batchId:36172 is not exist please check 2018-08-20 09:50:13.754 [New I/O server worker #1-1] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400 Caused by : something goes wrong with channel:[id: 0x4176eecb /10.31.152.38:34258 => /10.31.152.38:11111] exception=com.alibaba.otter.canal.server.exception.CanalServerException: ack error clientId:1001 batchId:36173 is not exist please check 2018-08-20 09:52:11.962 [New I/O server worker #1-3] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400 Caused by : something goes wrong with channel:[id: 0x49e5e3d0 /10.31.152.38:34332 => /10.31.152.38:11111] exception=com.alibaba.otter.canal.server.exception.CanalServerException: ack error clientId:1001 batchId:36174 is not exist please check 2018-08-20 09:54:03.646 [New I/O server worker #1-1] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400 Caused by : something goes wrong with channel:[id: 0x74b1d81d /10.31.152.38:34400 => /10.31.152.38:11111] exception=com.alibaba.otter.canal.server.exception.CanalServerException: ack error clientId:1001 batchId:36175 is not exist please check 2018-08-20 09:55:57.774 [New I/O server worker #1-3] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400 Caused by : something goes wrong with channel:[id: 0x2b72e577 /10.31.152.38:34460 => /10.31.152.38:11111] exception=com.alibaba.otter.canal.server.exception.CanalServerException: ack error clientId:1001 batchId:36176 is not exist please check 2018-08-20 09:57:52.922 [New I/O server worker #1-1] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400 Caused by : something goes wrong with channel:[id: 0x61134480 /10.31.152.38:34530 => /10.31.152.38:11111] exception=com.alibaba.otter.canal.server.exception.CanalServerException: ack error clientId:1001 batchId:36177 is not exist please check The server-side instance must have a rollback or restart behavior. Re-subscibe get ack according to the example project.
855,Remove TPS(events) in tmpl. The display layer first removes TPS events
854,Improvement based on comments This is basically the same as the comments. Delay is divided into master put get Ack carefully considered or retained the master server and mysql master 的delay  master Delay and put get The main difference between ack is that master Delay focuses on server and mysql when mysql is in idle state without delta Binlog can also be refreshed by heartbeat and put get Ic, especially the latter two are biased towards the client delay。 Documentation updated later I understand the master delay Mainly refers to the difference between the last time the binlog was received and the current time. For no binlog Update the timestamp of the last received data via heartbeat ? "expr": "rate(canal_instance_store_consume_seq{destination=~\"$destination\"}[2m])" This binlog Events TPS calculation In fact, we count the binlog that enters the eventStore. Not the original mysql binlog count For example Mysql DML will have a TableLogEvent WriteLogEvent two But this consume_seq will only record a WriteLogEvent 1. master Delay pair of masters passing the current 15s heartbeat。 2. I understand that TableLogEvent is a table Map metadata, that TPS events doesn’t seem to make much sense. I took him to thx.
853,Fixed 48-year delay when reading without execTime at startup 1. Fix bug 2. Add template 3. Update some maps
852,The fields and values ​​that canal resolve to do not match Daxie to see 呗 ![image](https://user-images.githubusercontent.com/18712087/44260217-51e25d00-a246-11e8-9622-32655791da82.png) Rowchange taken in this row Inside you can see the parsed fields Value Does not match the original library or even the type is not the original library int After parsing is date Did the ddl change? @wingerx For example Original table There is an insert but the canal is not consumed. If the original table has done ddl, then the data before canal consumption will happen. Yes, this situation can be turned on to avoid tsdb to avoid similar problems. . What does tsdb mean? Don&#39;t understand @wingerx I remember this before, and I haven’t seen this problem since I changed the table structure.
851,canal sever-client heartbeat The Idle detection on the server side only detects the blocking time of the Socket read/write channel. After the client and server subscription relationship is established, CanalConnector does not provide a pure heartbeat detection method. Only the get request can be sent to the server to prove that it is alive. If the consumption speed is too slow, the server will close the Socket connection. Can I add a simple heartbeat detection implementation? Let&#39;s adjust the timeout first in the short term.
850,The alter statement cannot be parsed Normal mysql Alter statement server side parsing error Can&#39;t skip the temporary solution that I think of now is to manually execute in the target library and then move the offset back 2018-08-14 16:53:22.500 [destination =xxxx address = /xxxx EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : ALTER TABLE `loan_withdraw_record` ADD COLUMN `remark` varchar(255) DEFAULT NULL COMMENT Remarks AFTER `is_remind_limit` ALGORITHM=inplace LOCK=NONE com.alibaba.fastsql.sql.parser.ParserException: syntax error expect TABLES or TABLE actual EQ pos 143 line 1 column 143 token = at com.alibaba.fastsql.sql.dialect.mysql.parser.MySqlStatementParser.parseStatementListDialect(MySqlStatementParser.java:863) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:483) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.apply(DatabaseTableMeta.java:104) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.apply(TableMetaCache.java:228) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseQueryEvent(LogEventConvert.java:265) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:126) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:68) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:345) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:187) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:154) [canal.parse-1.0.26-SNAPSHOT.jar:na] SHTERM: session timeoutotter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_51] Fastsql parsing problem
849,Canal support sync to hbase Hbase is currently one of the most widely used solutions in the big data field. It is expected to provide an ability to synchronize to hbase based on canal increments.
847,I reported a message after opening the server. dump address /127.0.0.1:3306 has an error What is the reason for this? [destination = example address = /127.0.0.1:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /127.0.0.1:3306 has an error retrying. caused by java.lang.IllegalArgumentException: null at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1314) ~[na:1.8.0_144] at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1237) ~[na:1.8.0_144] at java.util.concurrent.Executors.newFixedThreadPool(Executors.java:151) ~[na:1.8.0_144] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor.start(MysqlMultiStageCoprocessor.java:84) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:230) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144] 2018-08-16 12:21:45.933 [destination = example address = /127.0.0.1:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.lang.IllegalArgumentException at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1314) at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1237) at java.util.concurrent.Executors.newFixedThreadPool(Executors.java:151) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor.start(MysqlMultiStageCoprocessor.java:84) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:230) at java.lang.Thread.run(Thread.java:748) Is 1 0 26 The version of alpha is wrong Is this version Alpha several versions of alpha3 or alpha5 don&#39;t use alpha4 alpha4 problem alpha5 This version reported an error alpha5 Encountered the same problem canal.instance.parser.parallelThreadSize Need to set You were at the time of the canal instance parser parallelThreadSize This parameter is how much the configuration is. I commented out this line with the default value of the original configuration document. Also encountered this problem for help The latest 1 1 1 version has been resolved
846,V1 0 26 alpha5 client occasionally frequently reports error no alive canal server com.alibaba.otter.canal.protocol.exception.CanalClientException:no alive canal server at com.alibaba.otter.canal.client.impl.ClusterNodeAccessStrategy.nextNode(ClusterNodeAccessStrategy.java:76)~ The client occasionally reports the above error frequently. Check in Zookeeper get /otter/canal/destinations/example/running {"active":true "address":"xxx.xxx.xxx.xxx" "cid":1} It is normal for the server to report no error, but the client will report an error frequently. If the client self-recovers Check out the canal at the time Whether the server has switched or exited @agapple How to check canal Server? @agapple Check in Zookeeper get /otter/canal/destinations/example/running result Node does not exist : /otter/canal/destinations/example/running get /otter/canal/destinations/sample/running result Node does not exist : /otter/canal/destinations/sample/running This situation is a bit more frequent, although you can retry successfully, but this problem is not canal The server has a problem. Current scene Two instances of example sample are exactly the same in both instances. There is no error in the log log in the canal deployer, but the node information is not detected in the Zookeeper. Appears no alive canal Server must be canal The server has an exit behavior that needs to check the server&#39;s log.
845,Add header size for each packet. Client traffic is added to the header length MD directly to the wiki
844,Plus if (logger.isDebugEnabled()) Prevent message toString from being run every time. tks Mail can contact me jianghang115@gmail.com Invite you to join canal Collaborators Co-constructed together @rewerma
843,canal + rocksmq Can you ensure that the data is not lost? If canal pushes data to rocksmq, rmq crashes and mq does not flush and lose data in time. canal + Does rocksmq have the possibility of losing binlog? If there is no flush before the rmq crash, then the MQ design problem will definitely lose data. You need to manually return the data to supplement the data. ok @agapple
842,When will the official version of v1 0 26 be released? The v1 1 0 alapa version is still in the release version of v1 0 26 1 0 26 directly converted to 1 1 0 It is expected that this two weeks will be officially released.
841,Create canal prometheus docs. The initial document and so on will be put into the wiki. Every night, I will update part of it to make sure I can get it this week. As the current work and canal have been decoupled, I can only go home at night and take time to make progress. Please forgive me. Thank you. You can write directly to the wiki. Here https github com alibaba canal wiki
840,Upgrade Kafka Version to 2.11_1.1.1 awesome
839,Debug Memory Leak [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=839) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=839) before we can accept your contribution.<br/>**1** out of **2** committers have signed the CLA.<br/><br/>:white_check_mark: payonxp<br/>:x: yfpeng<br/><hr/>**yfpeng** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=839) it.</sub>
838,Canal 1 0 26 preview 5 Embedded Server cannot get Entries The previous 1 0 25 version is ok https://github.com/alibaba/canal/releases/tag/canal-1.0.26-preview-5 1.0.26 Optimized for performance on Entries Modifications have been made Embedded server 可以参考https github com alibaba canal blob master client src main java com alibaba otter canal client impl SimpleCanalConnector java L332 How data is processed Enn is the new version of the Message information is placed in the List ByteString rawEntries = Message getRawEntries and then get the complete information through result addEntry Entry parseFrom byteString Thank you
837,MemoryEventStoreWithBuffer fixes memory leak increase Max batch size Fixed a memory leak in MemoryEventStoreWithBuffer public void cleanUntil(Position position) throws CanalStoreException line 430: for (long next = sequence + 1; next <= maxSequence; next++) The maximum Batch length parameter is added to the CanalEventStore interface so that the otter can control the maximum length of the acquired Batch. The Dubbo request exceeds Max in the RPC usage scenario. Payload problem [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=837) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=837) before we can accept your contribution.<br/>**1** out of **2** committers have signed the CLA.<br/><br/>:white_check_mark: payonxp<br/>:x: yfpeng<br/><hr/>**yfpeng** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=837) it.</sub> [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=837) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=837) before we can accept your contribution.<br/>**1** out of **2** committers have signed the CLA.<br/><br/>:white_check_mark: payonxp<br/>:x: yfpeng<br/><hr/>**yfpeng** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=837) it.</sub> 1. The code first merges the master. Invalid branch changes are a bit more 2. otter Payload problem canal The master code can accurately control the memory size of the batch and does not require an additional batchSize to control the payload of the next service. The auto mode can determine whether to take the rpc or HTTP file download according to the data size. #839 Our business is not suitable for HTTP file downloads, so it is based on RPC. PayLoad oversized problem has been remodeled Let&#39;s take a look at the memory leak.
836,merge [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=836) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=836) before we can accept your contribution.<br/>**1** out of **2** committers have signed the CLA.<br/><br/>:white_check_mark: payonxp<br/>:x: yfpeng<br/><hr/>**yfpeng** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=836) it.</sub>
835,MemoryEventStoreWithBuffer fixes memory leak increase Max batch size Fixed a memory leak in MemoryEventStoreWithBuffer public void cleanUntil(Position position) throws CanalStoreException line 430: for (long next = sequence + 1; next <= maxSequence; next++) The maximum Batch length parameter is added to the CanalEventStore interface so that the otter can control the maximum length of the acquired Batch. The Dubbo request exceeds Max in the RPC usage scenario. Payload problem [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=835) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=835) before we can accept your contribution.<br/>**1** out of **2** committers have signed the CLA.<br/><br/>:white_check_mark: payonxp<br/>:x: yfpeng<br/><hr/>**yfpeng** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=835) it.</sub> [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=835) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=835) before we can accept your contribution.<br/>**1** out of **2** committers have signed the CLA.<br/><br/>:white_check_mark: payonxp<br/>:x: yfpeng<br/><hr/>**yfpeng** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=835) it.</sub>
834,About master - slave of bin-log problem The document states that Canal Is by simulating yourself as a “slave” Go and master Interactive bin-log Subscription and resolution If slave Also opened the bin log Canal Can it be slave Interact to achieve related functions The reason for this is to consider the current US master The physical machine performance is poor and then there is no previous bin-log Mode is not ROW, I want to ask if I can slave open Bin log then Canal Pass directly slave Subscribe and parse Thank you Yes, for the canal, your slave is the master. @lcybo thx
833,Store adds bufferSize metrics Server port configurable Such as the title
832,Will Daxie support conditional screening later? For example, to synchronize the data of a table, but I don&#39;t want to synchronize all the past. I want to do some filtering similar to where After defining some conditions that meet the conditions, we will synchronize the past. Write code in the client to do data filtering
831,Is there any plan to upgrade Google? Protocol Buffer? Want to be a client connection to c CanalServer。 You can submit a PR Can do upgrades got it
830,Entry header in executeTime Fields can achieve millisecond precision Binst inside timestamp 4 bytes precision is seconds
829,I would like to ask the example of the instance properties and rds_properties inside the database configuration is different. I remember that 1 0 24 is the rds_properties configured in the instance properties. Why is this canal instance tsdb dbUsername? Oh, help me, my brother. This new version will be replaced by a new program. : https://github.com/alibaba/canal/issues/727
828,canal Performance index collection List of indicators canal_instance_traffic_delay Delay with the master unit millisecond precision milliseconds canal_instance_transactions Number of transactions received canal_instance_row_events Received rowdata Number of entries canal_instance_rows_counter Number of rows changed canal_instance Instance list canal_instance_subscriptions Instance subscription number canal_instance_publish_blocking_time Pushlish blocking time unit millisecond precision nanosecond in parallel mode canal_instance_received_binlog_bytes Number of binlog bytes received canal_instance_parser_mode Whether parser is parallel mode canal_instance_client_packets client Instance request packets canal_instance_client_bytes To the client Instances send bytes canal_instance_client_empty_batches To the client Instances send empty packets canal_instance_client_request_error client Number of array error requests canal_instance_client_request_latency Request latency canal_instance_sink_blocking_time Sink thread put Store blocking time unit millisecond precision nanoseconds canal_instance_store_produce_seq Production serial number canal_instance_store_consume_seq The serial number of the consumption entry is used in conjunction with canal_instance_store_produce_seq canal_instance_store Store basic information canal_instance_store_produce_mem The production entry takes up the total amount of mem canal_instance_store_consume_mem The consumption entry accounts for the total amount of mem To be added The httpserver port is currently 11112 At night and the rds commit crashed so some conflict files merge 2 6 1 version for protoc Follow-up will add Prometheus expression visualization related documents test Cases, parameters such as ports can be configured, etc. Please review it. Thank you. Great ask a question METRICS_OPTS This comment Is the current monitoring information obtained by JMX or HTTP? ? I suggest you to monitor the concept of this indicator. And the case based on the indicator to locate the problem can list Afterwards, everyone can share it. The submission of rds is relatively large There are also crashes with some of my submissions. Need to pay attention to see if it is covered Submission of this RDS Mainly to solve the RDS on Alibaba Cloud Binlog deleted question Will automatically connect to the oss binlog with mysql binlog. With this mechanism In the future, docking all kinds of cloud RDS is basically barrier-free. METRICS_OPTS Originally used for javaagent is now removed Currently, the monitoring information is obtained by http. - job_name: 'canal' static_configs: - targets: ['localhost:11112'] The port or other configuration will be configured into the file later. The concept of the indicator and the case will be written together with the expression an MD and then commit including some develop Guide and visual content Like praise
827,Merge master updates [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=827) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=827) before we can accept your contribution.<br/>**2** out of **3** committers have signed the CLA.<br/><br/>:white_check_mark: agapple<br/>:white_check_mark: lcybo<br/>:x: lin848497337<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=827) it.</sub>
826,Metrics support Increase heartbeat refresh delay increase MHEARTBEAT event type。 Use version 2.6.1 Protoc compile CanalEntry Change the number of rows using the rowsCount in the pair [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=826) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=826) before we can accept your contribution.<br/>**4** out of **5** committers have signed the CLA.<br/><br/>:white_check_mark: payonxp<br/>:white_check_mark: agapple<br/>:white_check_mark: wingerx<br/>:white_check_mark: lcybo<br/>:x: yfpeng<br/><hr/>**yfpeng** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=826) it.</sub>
825,Remove dirty test case Source below com alibaba otter canal parse inbound mysql tablemeta has deleted the test The case is still having problems compiling tks
824,How does the client control the insert in the same thing? Update consumption order How does the client control the insert in the same thing? Update consumption order Sometimes the client often gets the update event and gets the insert event. The order that the client gets is the order of the binlog records in the database.
823,TableMetaCache.parseTableMetaByDesc Parsing desc table the result of NullPointer Mysql version 5 6 desc Table error in packet resolution public static final String COLUMN_NAME = "COLUMN_NAME"; public static final String COLUMN_TYPE = "COLUMN_TYPE"; public static final String IS_NULLABLE = "IS_NULLABLE"; public static final String COLUMN_KEY = "COLUMN_KEY"; public static final String COLUMN_DEFAULT = "COLUMN_DEFAULT"; public static final String EXTRA = "EXTRA"; Should be changed public static final String COLUMN_NAME = "Field"; public static final String COLUMN_TYPE = "Type"; public static final String IS_NULLABLE = "Null"; public static final String COLUMN_KEY = "Key"; public static final String COLUMN_DEFAULT = "Default"; public static final String EXTRA = "Extra"; Otherwise, I will report NullPointer in this line. meta.setColumnName(packet.getFieldValues().get(nameMaps.get(COLUMN_NAME) + i * size).intern()); mysql> select @@version ; +--------------------+ | @@version | +--------------------+ | 5.6.28-cdb2016-log | +--------------------+ 1 row in set (0.01 sec) mysql> desc test.test ; +-------+---------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +-------+---------+------+-----+---------+-------+ | id | int(11) | YES | | NULL | | +-------+---------+------+-----+---------+-------+ 1 row in set (0.01 sec) ![image](https://user-images.githubusercontent.com/33280738/43878223-88545b3a-9bd0-11e8-9f31-6dccbc75e37a.png) Take the originalName instead of the name Is NPE the result of the test? ![image](https://user-images.githubusercontent.com/7187362/43878339-234ac804-9bd1-11e8-99de-a4646bdb9c61.png) Buddy, I am here to put the packet out. Is this the inconsistency between my mysql version and your side? System.out.println(packet); UT can be measured possible https://dev.mysql.com/doc/refman/8.0/en/columns-table.html ![image](https://user-images.githubusercontent.com/33280738/43881977-9a81a8ac-9be0-11e8-9126-c6711b98eb16.png) ![image](https://user-images.githubusercontent.com/33280738/43881992-ad3b1fbe-9be0-11e8-97e7-11bad3d560c9.png) I used 5 6 28 to measure the same. The version of your side is obtained from where. Brother, I got the wrong mysqlConnection and connected to another database. The amount is a version of 8 0 11 I am embarrassed to take up your time. ok. The 80 version is currently not supported. Excuse me, the issue is closed.
822,fix tableMetaStorage NPE
821,NPE：MysqlEventParser TableMetaCacheWithStorage When the tableMetaStorageFactory is NULL, the storage value is null. Subsequent to the NullPointerException at com.alibaba.otter.canal.parse.inbound.mysql.tablemeta.TableMetaCacheWithStorage.<init>(TableMetaCacheWithStorage.java:21) ![image](https://user-images.githubusercontent.com/5847660/43820893-1ccadbde-9b1a-11e8-9979-2ab94346dfdb.png) #822 See my latest master commit has removed the tableMetaStorageFactory ok
820,Canal is supported to MySQL5 7 18? 5 7 18 or higher version is completely unsupported or there is a problem with the support of some features, there are no children&#39;s shoes used in the higher version. We are now 5.7.20 No problems found on the use
819,canal Performance monitoring changes Code I first merge to the development branch metrics_support Recently, there are some other things to be busy. The progress is a bit slow. Sorry. The current progress is a simple debug. After going home for a few days, I will try my best to test it. TOTO TODO List memo 1. Extend the EntryProtocol to increase the master&#39;s heartbeat to refresh the delay in the idle state. 2. Extend the EntryProtocol to increase the rowData count in the header header, otherwise you can only get it by deserialization. @agapple The comments have been crossed out in Pair. [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=819) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=819) before we can accept your contribution.<br/>**4** out of **5** committers have signed the CLA.<br/><br/>:white_check_mark: wingerx<br/>:white_check_mark: lcybo<br/>:white_check_mark: agapple<br/>:white_check_mark: qmz<br/>:x: Chuanyi Li L<br/><hr/>**Chuanyi Li L** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=819) it.</sub>
818,Canal instance tsdb enable in HA mode. The master and slave of the database after the master-slave switch After switching, I will report the following exception. The current solution I use is to use canal instance tsdb enable false to switch normally. Canal instance tsdb enable in HA mode. The master and slave of the database after the master-slave switch After switching, I will report the following exception. The current solution I use is to use canal instance tsdb enable false to switch normally. 2018-08-07 10:44:48.772 [destination = orderfailover address = /10.8.132.135:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position by switch ::1532919362000 2018-08-07 10:44:48.780 [destination = orderfailover address = /10.8.132.135:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - Didn't find the corresponding binlog files from mysql-bin.000020 to mysql-bin.000021 2018-08-07 10:44:48.782 [destination = orderfailover address = /10.8.132.135:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /10.8.132.135:3306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: can't find start position for orderfailover 2018-08-07 10:44:48.782 [Druid-ConnectionPool-Create-1877078260] ERROR com.alibaba.druid.pool.DruidDataSource - create connection SQLException url: errorCode 0 state null java.sql.SQLException: connect error url driverClass org.h2.Driver at com.alibaba.druid.pool.DruidAbstractDataSource.createPhysicalConnection(DruidAbstractDataSource.java:1582) ~[druid-1.1.9.jar:1.1.9] at com.alibaba.druid.pool.DruidDataSource$CreateConnectionThread.run(DruidDataSource.java:2466) ~[druid-1.1.9.jar:1.1.9] 2018-08-07 10:44:48.782 [destination = orderfailover address = /10.8.132.135:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:orderfailover[com.alibaba.otter.canal.parse.exception.CanalParseException: can't find start position for orderfailover ] 2018-08-07 10:44:49.282 [Druid-ConnectionPool-Create-1877078260] ERROR com.alibaba.druid.pool.DruidDataSource - create connection SQLException url: errorCode 0 state null java.sql.SQLException: connect error url driverClass org.h2.Driver at com.alibaba.druid.pool.DruidAbstractDataSource.createPhysicalConnection(DruidAbstractDataSource.java:1582) ~[druid-1.1.9.jar:1.1.9] at com.alibaba.druid.pool.DruidDataSource$CreateConnectionThread.run(DruidDataSource.java:2466) ~[druid-1.1.9.jar:1.1.9] 2018-08-07 10:44:49.783 [Druid-ConnectionPool-Create-1877078260] ERROR com.alibaba.druid.pool.DruidDataSource - create connection SQLException url: errorCode 0 state null java.sql.SQLException: connect error url driverClass org.h2.Driver at com.alibaba.druid.pool.DruidAbstractDataSource.createPhysicalConnection(DruidAbstractDataSource.java:1582) ~[druid-1.1.9.jar:1.1.9] at com.alibaba.druid.pool.DruidDataSource$CreateConnectionThread.run(DruidDataSource.java:2466) ~[druid-1.1.9.jar:1.1.9] 2018-08-07 10:44:50.283 [Druid-ConnectionPool-Create-1877078260] ERROR com.alibaba.druid.pool.DruidDataSource - create connection SQLException url: errorCode 0 state null java.sql.SQLException: connect error url driverClass org.h2.Driver at com.alibaba.druid.pool.DruidAbstractDataSource.createPhysicalConnection(DruidAbstractDataSource.java:1582) ~[druid-1.1.9.jar:1.1.9] at com.alibaba.druid.pool.DruidDataSource$CreateConnectionThread.run(DruidDataSource.java:2466) ~[druid-1.1.9.jar:1.1.9] 2018-08-07 10:44:50.783 [Druid-ConnectionPool-Create-1877078260] ERROR com.alibaba.druid.pool.DruidDataSource - create connection SQLException url: errorCode 0 state null java.sql.SQLException: connect error url driverClass org.h2.Driver at com.alibaba.druid.pool.DruidAbstractDataSource.createPhysicalConnection(DruidAbstractDataSource.java:1582) ~[druid-1.1.9.jar:1.1.9] at com.alibaba.druid.pool.DruidDataSource$CreateConnectionThread.run(DruidDataSource.java:2466) ~[druid-1.1.9.jar:1.1.9] 2018-08-07 10:44:51.283 [Druid-ConnectionPool-Create-1877078260] ERROR com.alibaba.druid.pool.DruidDataSource - create connection SQLException url: errorCode 0 state null The main reason is that the h2 tsdb xml depends on the canal instance in the System. The destination variable defines that there is an asynchronous field operation when the active/standby switchover is empty.
817,canal-1.026-alpha4 Startup failed OS: CentOS Linux release 7.4.1708 canal : canal-1.026-alpha4 Verify no problem in the mac environment linux environment Startup failed java stack: `Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: java.lang.IllegalArgumentException: limit excceed: 44 Caused by: java.lang.IllegalArgumentException: limit excceed: 44 at com.taobao.tddl.dbsync.binlog.LogBuffer.getUint8(LogBuffer.java:235) ~[canal.parse.dbsync-1.0.26-SNAPSHOT.jar:na] at com.taobao.tddl.dbsync.binlog.event.GtidLogEvent.<init>(GtidLogEvent.java:48) ~[canal.parse.dbsync-1.0.26-SNAPSHOT.jar:na] at com.taobao.tddl.dbsync.binlog.LogDecoder.decode(LogDecoder.java:364) ~[canal.parse.dbsync-1.0.26-SNAPSHOT.jar:na] at com.taobao.tddl.dbsync.binlog.LogDecoder.decode(LogDecoder.java:110) ~[canal.parse.dbsync-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$SimpleParserStage.onEvent(MysqlMultiStageCoprocessor.java:210) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]` Known issues please download the latest alpha 5
816,fix ci: fixed compiler java 1.7 fix ci: fixed compiler java 1.7 1. Remove canal instance spring After the dependency, you need to add druid and mysql Connector dependency 2. PropertyPlaceholderConfigurer moved from canal instance spring to db tks
815,canal HA Mode view position canal server Opened two sets of HA high availability configuration as follows But zk was accidentally deleted now active canal Still receive the binlog parsing normally but can&#39;t update the position information in zk. I would like to ask zk to accidentally delete the position data of the current instance except zk. ` ################################################# ######### common argument ############# ################################################# canal.id= 1 canal.ip= canal.port= 11111 canal.zkServers=10.100.1.10:2181 10.100.1.11:2181 10.100.1.12:2181 canal.zookeeper.flush.period = 1000 canal.file.data.dir = ${canal.conf.dir} canal.file.flush.period = 1000 canal.instance.memory.buffer.size = 16384 canal.instance.memory.buffer.memunit = 1024 ` The meta log under logs will record every consumption Meta log under logs in HA mode No record is recorded when only non-HA is recorded. Is there a problem with my HA configuration?
814,schema history storage interface In order to solve Otter&#39;s backtracking data, Rowdata does not match the current schema. Provide a persistent interface to TableMeta based on timestamp matching history schema In our mysql storage implementation by Otter Manager performs database operations CanalEmbedSelector assembles Factory and puts into CanalProperties Degraded to the original TableMetaCache without the storage implementation [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=814) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=814) before we can accept your contribution.<br/>**1** out of **2** committers have signed the CLA.<br/><br/>:white_check_mark: payonxp<br/>:x: yfpeng<br/><hr/>**yfpeng** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=814) it.</sub> [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=814) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=814) before we can accept your contribution.<br/><hr/>**yfpeng** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=814) it.</sub> @payonxp Code conflicts, trouble, merge the main code first. This feature and tsdb What&#39;s the difference Looked at the next implementation is to resolve the database table structure when parsing the DDL log at that time, do a timestamp mapping record compared to TableMeta Tsdb has a problem is to get the binlog When the DDL is back-checked, the table structure at this time and the structure corresponding to the DDL at that time are inconsistent with time GAP. Can canal1 0 26 latest implementation https github com alibaba canal wiki TableMetaTSDB @agapple I didn&#39;t take a closer look at the implementation logic but from the description it feels like the function of TSDB implementation. Get the binlog When trying to call the parsing function of TSDB in DDL, it is not always going to the database every time. The main function and TSDB almost want to put the persistence function into the otter canal itself does not achieve persistence It is recommended to submit the storage capability implemented in the otter to a PR. I re-edited the support of Otter using TableMetaTSDB https://github.com/alibaba/canal/commit/9e816bc48f9955f9e2839873993cef9627c2f389 Corresponding otter commit : https://github.com/alibaba/otter/commit/7cc897131da65aab2c1aa31b7b0b8aa7a6f65745 Thx
813,fix #802: Strip MySQL from otter Related synchronization implementation as a reference for example Very much like
812,Link failure ### wiki-like home + Related information used with Alibaba&#39;s rocketMQ Connection failure ### wiki的Introduction + Binching of knowledge science mysql The second link introduced by Log is invalid + EventParser design binlog event Structure Please refer to the connection for details. Page Not Found Taobaodba&#39;s blog There are no other connections already fixed tks @wingerx It can be said that it is very efficient. give it a like Forgive my obsessive README.md The connection to RocketMQ has not changed. https://github.com/apache/rocketmq already fixed
811,change com.alibaba.fastsql version to the last on http://central.mave… Solve the bug of issue #808 [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=811) <br/>All committers have signed the CLA. [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=811) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=811) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=811) it.</sub> The latest version of fastsql will be submitted later. Ok, then deploy it as soon as possible. I will turn this off first.
810,fix bug: kafka get row data for performance tks
809,canal ack error 2018-08-03 15:17:45.414 [New I/O server worker #1-4] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x7cec5107 /10.111.61.32:47512 :> /10.111.61.32:11111] exception=java.nio.channels.ClosedChannelException at org.jboss.netty.channel.socket.nio.NioWorker.cleanUpWriteBuffer(NioWorker.java:649) at org.jboss.netty.channel.socket.nio.NioWorker.writeFromUserCode(NioWorker.java:370) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:137) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) at org.jboss.netty.channel.Channels.write(Channels.java:611) at org.jboss.netty.channel.Channels.write(Channels.java:578) at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:46) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:174) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.cleanup(ReplayingDecoder.java:542) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.channelDisconnected(ReplayingDecoder.java:450) at org.jboss.netty.channel.Channels.fireChannelDisconnected(Channels.java:360) at org.jboss.netty.channel.socket.nio.NioWorker.close(NioWorker.java:599) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:119) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) at org.jboss.netty.channel.Channels.close(Channels.java:720) at org.jboss.netty.channel.AbstractChannel.close(AbstractChannel.java:208) at org.jboss.netty.channel.ChannelFutureListener$1.operationComplete(ChannelFutureListener.java:46) at org.jboss.netty.channel.DefaultChannelFuture.notifyListener(DefaultChannelFuture.java:381) at org.jboss.netty.channel.DefaultChannelFuture.addListener(DefaultChannelFuture.java:148) at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:48) at com.alibaba.otter.canal.server.netty.NettyUtils.error(NettyUtils.java:69) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:253) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:444) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261) at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:350) at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:748) Do a retry according to the example code.
808,com.alibaba.fastsql:fastsql:jar:2.0.0_preview_520 mvn clean Error when installing [ERROR] Failed to execute goal on project canal.parse: Could not resolve dependencies for project com.alibaba.otter:canal.parse:jar:1.0.26-SNAPSHOT: Could not find artifact com.alibaba.fastsql:fastsql:jar:2.0.0_preview_520 in central (http://repo1.maven.org/maven2) -> [Help 1] How can I solve this problem? You use 2 0 0_preview_186 2 0 0_preview_520 Not open to the outside, the maven repository center is configured as http central maven org maven2 2.0.0_preview_520 In release There is a jar package inside
807,instance with Some fine-tuning of the manager mode 1 CanalParameter MetaMode adds local file mode adaptation manager mode is instance using local meta dat storage location information 2 typo correction 3 BatchMode default correction The parameter name adjustment of CanalParameter will have a history version compatibility problem. The problem of word spelling was previously found but can&#39;t be changed unless it is compatible with the old set get method. Understand that I reset the commit. The current pr is just adapted to the manager mode. You can use the FileMixedMetaManager local file to store the meta dat site data. To avoid the current manager mode, the production environment must use the zookeeper and file instance xml default configuration. The parameters correspond to the dataDir and metaFileFlushPeriod of the new parameters of CanalParameter. tks
806,Support GTID mode to get the current gtid value as the starting value by default tks
805,About position setting I would like to set the log read position is not only need to set the journal name and position in the instance proerties on the meda dat. Why is it different from the instance proerties? Which one takes effect? I am using default instance xml Zk cluster 1. After using default instance xml, the default is to manage the location information through zk. Meda dat is invalid here 2. The mode that meda dat targets is file instance xml 3. instance.properties In the locus and zk Site Relationship Default instance xml as an example * When the zk is not present in the locus, the canal is activated. The instance properties are subject to `show master status` Take the latest location information * when zk The location information in zk is subject to the record in zk So in the case of normal data synchronization, the location of zk and the location configured in instance properties are different canal It is also the ability to achieve breakpoints through it. @wingerx Hello, I tried to delete the location information on zk and let it regenerate. According to your statement, I should set the location information in Instance properties. I have set it up but there is no reason for this. 1. The configuration in Instance properties is only related to finding a location at startup. 2. The record in zk is the client side after the successful consumption of ack Time point It is not that the site information zk is configured in the instance properties to be updated to this site.
804,How can deal with blob data types in mysql Ask God to ask how to deal with the blob data type in mysql The information found on the Internet is as follows. I don&#39;t know if it is correct. Canal will binlog The value in the sequence is serialized String Format to the downstream program so Blob Formatted data is serialized into String Forced to save space IOS_8859_0 As a code, it will cause Chinese garbled in the following cases. Synchronization service JVM used UTF-8 coding BLOB Chinese characters are stored in the field AUTHOR haitaoyao Link https www jianshu com p be3f62d4dce0 Source book The copyright of the book is owned by the author. Please contact the author for authorization and indicate the source. Ask God to guide According to iso8859 1 reverse solution to bytes Thank you for guiding but I will use sqltype for longblob type fields using iso8859 1 inverse solution to bytes and then utf 8 encoding for string is still garbled This is the content of the longblob field <img width="1128" alt="canal-blob-getvalue" src="https://user-images.githubusercontent.com/28953872/43885058-677e5658-9bea-11e8-956c-edeb50a0da5b.png"> This is done using iso8859 1 Inverse solution <img width="1123" alt="iso" src="https://user-images.githubusercontent.com/28953872/43885109-8a92545a-9bea-11e8-9102-2b5a0c10ec0d.png"> My code is as follows if(column.getMysqlType().toUpperCase().equals("LONGBLOB")){ logger.info(">>>>>> is blob\n"); logger.info("\n>>>>>> column.getValue() is: " + column.getValue()); logger.info("\n\n\n\n\n\n>>>>>> new String(column.getValue().getBytes(\"iso8859-1\") \"UTF-8\") is: " + new String(column.getValue().getBytes("iso8859-1"))); } Ask the god how to fix use getValueBytes method takes the original bytes Still the claw machine used by getBytesValue can&#39;t confirm Can you tell me more about the gods? Column no getBytesValue method should be getBytesValue ``` try { if (StringUtils.containsIgnoreCase(column.getMysqlType() "BLOB") || StringUtils.containsIgnoreCase(column.getMysqlType() "BINARY")) { // get value bytes builder.append(column.getName() + " : " + new String(column.getValue().getBytes("ISO-8859-1") "UTF-8")); } else { builder.append(column.getName() + " : " + column.getValue()); } } catch (UnsupportedEncodingException e) { } ``` public java.lang.String getValue() { java.lang.Object ref = value_; if (!(ref instanceof java.lang.String)) { com.google.protobuf.ByteString bs = (com.google.protobuf.ByteString) ref; With getValue, here is already encoded with utf8 java.lang.String s = bs.toStringUtf8(); if (bs.isValidUtf8()) { value_ = s; } return s; } else { return (java.lang.String) ref; } } public com.google.protobuf.ByteString getValueBytes() { java.lang.Object ref = value_; if (ref instanceof String) { com.google.protobuf.ByteString b = com.google.protobuf.ByteString.copyFromUtf8((java.lang.String) ref); value_ = b; return b; } else { Get the original ByteString return (com.google.protobuf.ByteString) ref; } } @alexlgj Try first @agapple Right Impression in the 1 0 22 version of getValue Varbinary will lose information when you go directly to change the code @agapple @lcybo Ok, thank you for the guidance of the two great gods. I am not very good at this time. Did an experiment byte[] bs = {-1 -2}; The byte is a negative number. This depends on the encoding of your database. ByteString bstring = ByteString.copyFrom(bs); String utf = bstring.toStringUtf8(); System.out.println(utf); Lost data turned into two 65533 String iso = new String(utf.getBytes("ISO-8859-1") "UTF-8"); System.out.println(iso); There is no way to restore it into two 63 @alexlgj The coding problem is quite interesting According to the method of agapple, the content of the blob field can be successfully decoded into Chinese Beijing. Column getvalue get "image\\\":\\\"å\\\\u008C\\\\u0097äº¬\\\" \\\ new String(column.getValue().getBytes("ISO-8859-1") UTF 8 gets Image Beijing The canal I used is 1 0 24 version. In your example, there is no access to the database. Why is the byte negative? This depends on the encoding of your database. latin1 128 255 with a byte is a negative number indicates that there is a negative number in utf 8 which represents a multi-byte is a certain specification of 128 255 sequence is likely to violate these specifications become If there is a hard-coded thing, the unordered 128 255 non-utf 8 code still takes the original byteString. If the canal put in the string is the iso 8859 1 coded that I ignored, I ran away. @lcybo I understand very much, thank you very much. @lcybo Before the server-side blob type was strongly transferred to the string type, it is more reasonable to use ByteString, but now it can not be changed to the old user&#39;s blob parsing will be garbled. understanding I still have some questions about the two great gods. 1. Is it canal? Server side will mysql All fields of binlog do iso 8859 1 encoded into string 2. I don’t understand the coding at all. Can help explain why this change code can correctly parse the string stored in the blob type. String(column.getValue().getBytes("ISO-8859-1") "UTF-8")) What is the order of execution of the different encodings in the code? My understanding is canal Server does iso 8859 1 encoding on the byte array of the blob field, but iso 8859 1 encoding can not represent Chinese Beijing garbled. At this time, executing the column getValue method will enter if (!(ref instanceof java.lang.String)) { Utf8 string for ref God, I want to ask the column getValueBytes toByteArray returns the blob field original Byte array is still always utf 8 encoding or other encoded byte array. Why can&#39;t column in the canal getValueBytes toByteArray A Chinese character corresponds to 6 bytes? This is my test result Inserted blob field value ”0123abc" Print the binary array contents of the inserted string in a java program Arrays.toString(”0123abc".getBytes()) is： [48 49 50 51 97 98 99] Binary array content obtained in canal Arrays.toString(column.getValueBytes().toByteArray() is： [48 49 50 51 97 98 99] Blob field value Arrays toString 上 getBytes  is： [-28 -72 -118] Arrays.toString(column.getValueBytes().toByteArray() is: [-61 -92 -62 -72 -62 -118] Blob field value Shanghai Arrays toString 上海 getBytes  is： [-28 -72 -118 -26 -75 -73] Arrays.toString(column.getValueBytes().toByteArray() is： [-61 -92 -62 -72 -62 -118 -61 -90 -62 -75 -62 -73] 1 description from agapple god should be only blob and binary do Another problem if you use iso encoding into string then a char in the string is actually a single byte printed out is garbled but the actual data format is not lost What you mean is that although the printout is garbled, the binary data itself remains the same. Then why is new? String(column.getValue().getBytes("UTF-8") The result of UTF 8 printing is garbled. getBytes UTF 8 is equivalent to using utf 8 to encode that bunch of garbled prints, of course, garbled I just asked the students about some coding knowledge. Now I can understand the basics. Thank you, God’s patience and guidance.
803,canalConnector subscribe will block the thread and cause stop to stop When using zookeeper HA mode to start multiple Canals If the Client is a Canal Client has succeeded subsribe, currently started Canal Client will block on the canalConnector subscribe method At this point, calling the following stop method will block the stop at the join. public void stop() { if (!running) { return; } canalConnector stopRunning This version of the canal client does not support running = false; if (thread != null) { try { **thread.join();** } catch (InterruptedException e) { e.printStackTrace(); } } KafkaProducerUtil.stop(); MDC.remove("destination"); } I see the new version of the example stop method added the following code does not know if it is to solve the above problem connector.stopRunning(); If so, how should the previous version solve the blocking problem? Canal is through this mechanism to achieve HA on the client side. Blocking is to avoid having two identical clients at the same time
802,canal Example example code to increase data synchronization Application-rich at the client level expects to provide a standard implementation based on synchronization to the database
801,canal Dockerization package support 1. Increase Dockerfile support all in One packaging method centos + jdk + canal) 2. Parameter level allows docker e variable passing and modifying Read in java via System getenv 3. Delivery form - Allow users to package source code based on Dockerfile - Upload to Docker Hub allows users to download directly from the public network The corresponding docker uses the documentation https github com alibaba canal wiki Docker QuickStart
800,Canal native support for RocketMQ docking RocketMQ： https://rocketmq.apache.org/ Canal native support for data to be written to RocketMQ Code submitted Reference document https://github.com/alibaba/canal/wiki/Canal-Kafka-RocketMQ-QuickStart
799,About the sorting problem of canal received data When the same cust_id Add in order modify delete Add Canal receives data from two data Such as cust_id Types of Sql execution time 001 add 1533002422 001 update 1533002422 001 delete 1533002423 001 add 1533002423 There is a problem with this. When I made the zipper table of the data warehouse Unable to know 001 User&#39;s order of add and delete in the case of 1533002423 milliseconds my question is In this situation There are other fields that can provide the same sql execution time. Sort the data to distinguish the order Binlog is the order relationship cust_id Is the primary key? If it is the primary key, why is it added twice? mysql The binlog mechanism is more complicated. Many internal optimization transactions only guarantee the final result after the slave is executed.
798,Transaction in canal ID acquisition and entry getHeader getLogfileOffset offset understanding Question 1 A message of canal may correspond to multiple transactions in mysql. The list entry can be obtained by the method message getEntries. Entrys entry Divided into transanctionBegin transactionEnd and rowdata Now the problem is that I am > Get transaction id< Only the normal number is obtained through transactionEnd getTransactionId but the ones obtained through transactionbegin and rowdata are all null. Will there be other ways to get the transaction in canal? Id? Question 2 Entry getHeader getLogfileOffset The offset obtained by the current entry rowdata in the log file is the number of logs or bytes, that is, the number of bytes in front of the row. Ask God to enlighten me Thank you Do it yourself through mysqlbinlog Hexdump can view the commit to have xid Offset is a byte offset Thank you, great god
797,How can can distinguish multiple records in a transaction rowdata A transaction in mysql may contain multiple records of rowdata. So how does canal distinguish these records? Is there any scn system like oracle? change Number to uniquely identify changes made to a record #751 Look at this pr is not satisfied with the scene you said pr? Excuse me, is it convenient to explain in detail? Sorry, I just didn&#39;t see the link. Thank you.
796,Local debugging SimpleCanalClientTest throws an exception java net ConnectException Connection refused: Connect solution ![image](https://user-images.githubusercontent.com/16176283/43498536-f031ab84-9579-11e8-9bc6-09ca48546cbf.png) 将com alibaba otter canal example SimpleCanalClientTest The corresponding content in the class can be solved by changing the actual content.
794,Kafka integrated canal Event always receives no data, don&#39;t know why see PR #790
793,How to convert ByteString to CanalEntry Entry example In the code ``` ... CanalEntry.Entry.parseFrom(byteString) ``` OK
792,Received EOF packet from server ![image](https://user-images.githubusercontent.com/18360996/43445241-7a85f184-94d8-11e8-87f6-1bf1f0b8ddba.png) If slaveId If the setting does not repeat, please refer to the maximum possible. #777
791,Why the insert will be parsed into ddl RowChange object information is as follows unknownFields = {UnknownFieldSet@1470} "" bitField0_ = 30 tableId_ = 0 eventType_ = {CanalEntry$EventType@1609} "INSERT" isDdl_ = true sql_ = "insert into `test`(`school_student_id` `school_group_id` `like_user_id` `school_event_id`) values('114' '1' '66889278' '61')" rowDatas_ = {Collections$EmptyList@1473} size = 0 props_ = {Collections$EmptyList@1473} size = 0 ddlSchemaName_ = "test" memoizedIsInitialized = 1 memoizedSerializedSize = -1 memoizedSize = -1 memoizedHashCode = 0 1. Check if binlog is in row mode 2. RowsQueryLogEvent event in row mode 1 0 26 has solved this problem https://github.com/alibaba/canal/wiki/FAQ
790,kafka producer adaptation row data for performance #726 tks
789,Can&#39;t start ![image](https://user-images.githubusercontent.com/18360996/43390651-4090cdb2-9421-11e8-9e18-4b1d13f8e758.png) reference #777
788,Canal docking performance sampling first edition Hi Some personal ideas are as follows 1. Plugable In order to facilitate the docking of programs other than prometheus and to use the service in the relevant code The provider mechanism CanalServerWithEmbedded loads the service with the runtime Scope specified implementation in the deployer&#39;s pom <!-- Specify the metrics for the runtime here. provider--> <dependency> <groupId>com.alibaba.otter</groupId> <artifactId>canal.prometheus</artifactId> <version>${project.version}</version> <scope>runtime</scope> </dependency> 2. Try not to change the logic code of the canal as much as possible. The current commit changes to the original code as follows a. CanalServerWithEmbedded ServiceProvider logic b. startup.sh Load a javaagent for LTW load time Weaving is currently out of the stable version c. Deployer&#39;s pom specifies that the metrics implementation is currently also marked out. Therefore, the way to start the metrics of the logical logic of the canal is not to add the mvn configuration. install。 3. Currently implemented metrics have - jvm Family bucket - canal_net_inbound_bytes The amount of data received from mysql unit byte can be calculated using rate - canal_net_outbound_bytes The amount of data sent to the client in bytes can be calculated using rate - canal_instance instance List - canal_instance_traffic_delay The delay of each instance currently has a limitation in mysql When the binlog is not updated, the delay will always increase. The current idea is to use the mysql master. heart beat Packet refresh needs to parse this package TODO - eventstore Produce and ack index canal_instance_store_produce_seq with canal_instance_store_consume_seq Can be used to calculate TPS rate canal_instance_store_consume_seq and ringbuffer remain evnets cound(canal_instance_store_produce_seq - canal_instance_store_consume_seq) Other metrics please add 4. Some TODO to be perfected - Does HttpServer need to support https? - This machine is tested with the debug UT. - Parameter configurable - ... I would like to add and comment on the comments. Best regards To debug new features in the IDE, vmarg should be added -javaagent:/pathto/aspectjweaver-${version}.jar Looked at the structure is quite good, there are several suggestions 1. It is also necessary to change the code of the main server to do some burying for monitoring. Not specific advice through aspect 2. Differentiated between the instance level and the destination level for monitoring, such as network traffic reading and writing out, can distinguish different destinations Because the business often finds out who is the trickster, basically the individual destination affects the whole @agapple Thanks for comments, I will think about how to refactor BTW ask about the disadvantages of aspects in doing burying The main reason is that some maintenance monitoring should belong to the built-in capability of the canal. Ideally, the whole binlog parser sink store client can be fully buried and sampled to obtain performance data. The peripheral is very flexible and pluggable is not very meaningful. @agapple Well, understand that simply using canal The existing mechanism of the server to do some neutral burying points has some ideas and has time to sort out
787,Use show processlist The command does not see the dump process [v1.0.26.alpha4] After the canal starts up, it starts dumping. Why use show on mysql side processlist The command does not see the dump process Confirm if you are starting to consume data. There is no find in the instance log start position : EntryPosition[included=false journalName=xxxx position=xxxx serverId=<null> gtid=<null> timestamp=<null>]. Then the meta log has no continuous refresh show slave hosts Can see the dump process of canal
786,Ask Canal to subscribe to Alibaba Cloud RDS MySql case Business scene Existing Alibaba Cloud RDS MySql database wants to install binar for RDS data by installing canal Please provide a case for God Thank you, thank you. 参考 https github com alibaba canal wiki FAQ
785,no meda.dat I accidentally deleted the file：meta.dat（Below each instance） what should I do？ set position info in instance.properties which you could find in meta.log.However there might be an overlap. the latest one in meta.log @lcybo Do I need to manually create meta.dat and copy the contents of meta.log into newly meta.dat? It would be created and maintained by metamanager automatically. @lcybo Unfortunately it doesn't automatically generate in the document. Is it because I didn't configure position and what should position fill in? @lcybo I know why I did not generate meta.dat files because I did not add the following sentence to my code: connector.subscribe (* * \ \ \ * *); @lcybo But is this not dispensable as long as the canal.instance.filter.regex in the Instance.properties should not be configured with connector.subscribe (". * \ \.. *"); position is selected by following priority: 1.meta.dat 2.instance.properties 3.the position via 'show master status'(probably events lost) the meta.dat would be created after successful subscription if not exist. @lcybo Now you can automatically generate meta.dat but now I have to write connector.subscribe () in the program; connector is the CanalConnector type. If I write this the filter attribute in Instance.properties will be rewritten into an empty string and I can't let canal.instance.filter.regex in Instance.properties now. What do I do to make it work rather than using connector.subscribe () in the program? Yes filter is not necessary. Canal server work with canal client together there is a example client in canal project. Here is the code slice: protected void process() { int batchSize = 5 * 1024; while (running) { try { MDC.put("destination" destination); connector.connect(); connector.subscribe(); while (running) { Message message = connector.getWithoutAck(batchSize); // Get the specified amount of data long batchId = message.getId(); int size = message.getEntries().size(); if (batchId == -1 || size == 0) { // try { // Thread.sleep(1000); // } catch (InterruptedException e) { // } } else { printSummary(message batchId size); printEntry(message.getEntries()); } connector.ack(batchId); // Submit confirmation // connector.rollback(batchId); // Processing failure Rollback data } } catch (Exception e) { logger.error("process error!" e); } finally { connector.disconnect(); MDC.remove("destination"); } } } @lcybo If I write connector.subscribe () in the program again the regex on the server side will be covered. How can I make regex not cover? a. If you call subscribe() then canal.instance.filter.regex is used ignore the empty string in meta.dat. b.Otherwise(subscribe(regex)) use the parameter 'regex' instead of canal.instance.filter.regex. @lcybo It's like you said：you call subscribe() then canal.instance.filter.regex is used ignore the empty string in meta.dat.The problem has been solved，Thank you very much。 :)
784,Can canalserver support an instance instance to extract multiple libraries? Canalserver uses a single library instance configuration to not group and then extract the binlog logs of multiple libraries in one instance. Is this configuration method supported? Found configuration method canal instance defaultDatabaseName db1 Db2 This configuration is applicable to the same ip port instance under the different library library names separated by commas I think you understand that there is a small problem. You are configuring multiple libraries for the default listener. But it is better to do a regular match in the following configuration item. All libraries canal.instance.filter.regex = .*\\..* db1 db2 canal.instance.filter.regex = db1\\..* db2\\..* LS Correct Answer
783,Canal seeks to explain the message entries transanctionID batchid Event and rowchange relationship Learning the use of canal, asking God to sort out the relationship between several concepts in canal At present, my understanding is that a message contains multiple entries, which together constitute an entries. An entry corresponds to an eventtype. It can be an insert delete. An entry can be parsed to get a rowchange. A rowchange can be parsed to get multiple rowdata. A rowdata can be parsed to get the before column method getBeforeColumnsList. And after the columns - Question 1 What is the internal relationship between them? - Question 2 The difference between transactionID and batchID - Question 3 Entry getHeader getSchemaName can get the schema of mysql but how to get mysql Of instance Thank you Seek the guidance of the great God A message is the batch to get to Packet batchId is its identity is canal concept transactionId is mysql transaction related mysql binlog Will be the same type of operation insert in the same table Change line of update etc. merge to an event to save meta overhead entry is canal Proto type corresponds to mysql The event rowdata corresponds to each row of data before and after the change before and after the update, both of which have both inserts only after Delete only before In some special cases, such as opening ndb some parameters update will become insert type Hello, thank you for your advice. I have a problem. I have only inserted a row in mysql but the entries size displayed in the canal. Yes 3 Then how do I know which entry corresponds to the row I just inserted? It should be there transactionBegin with transactionEnd Event can be pressed event Type filter On Jul 26 2018 18:20 +0800 alexlgj <notifications@github.com> wrote: > Hello, thank you for your advice. I have a problem. I have only inserted a row in mysql but the entries size displayed in the canal. Yes 3 Then how do I know which entry corresponds to the row I just inserted? > — > You are receiving this because you are subscribed to this thread. > Reply to this email directly view it on GitHub or mute the thread. ls Correct Answer Thanks to the reply of the Great God, I now understand the test several times and the results are as follows An insert or delete or update operation entrys.size() is: 3 Perform three consecutive insert data operations and then start the canal client entrys.size() is: 9 When the entry is transactionbegin or transactionend, the eventtype is update. When the entry is rowdata, the corresponding eventtype is the corresponding operation, ie insert delete update Also encountered a problem how to get mysql through canal The instance name of database is only found in the header of entry. By entry getHeader getSchemaName but how to get mysql Of instance Does instance refer to the mysql instance of the port distinction? Consider using serverId to distinguish Ok, thank you very much.
782,MysqlMultiStageCoprocessor robustness enhancement Hi Wenge&#39;s commit solves the problem caused by the EOF package. However, MysqlMultiStageCoprocessor still has problems in other situations. If the exception is not onEvent What happens in the body is the InterruptedException of 771. The stop in onShutdown and the reset of the dump thread will compete. If MysqlMultiStageCoprocessor is only safely called serially within the dump thread So change to accident exception. Also check by publish check by dump thread Best regard [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=782) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=782) before we can accept your contribution.<br/><hr/>**Chuanyi Li L** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=782) it.</sub> [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=782) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=782) before we can accept your contribution.<br/><hr/>**Chuanyi Li L** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=782) it.</sub> tks @lcybo Your implicit definition and the previous exception are duplicated and merged directly into one Ok, I thought that canalParseException might have a special deal. The second commit deliberately added the incident distinction. It seems that I want more. @lcybo Invite you to join the project administrator trouble private letter my mailbox contact jianghang115 gmail com @agapple Has accpet mailbox chuanyili0625 gmail com Please take care
781,The latest version of fastsql preview 520 ： com.alibaba.fastsql.sql.parser.ParserException Fastsql version Latest 520 Sql statement and Exception case 1 : sql_6 = "CREATE INDEX `idx_t_uid` on stu_score (`uid`) COMMENT '' ALGORITHM DEFAULT LOCK DEFAULT "; Exception in thread "main" com.alibaba.fastsql.sql.parser.ParserException: syntax error error in :'ITHM DEFAULT LOCK DEFAULT pos 134 line 1 column 136 token IDENTIFIER null at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:363) at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:520) at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:92) at com.test.HelloWorld.main(HelloWorld.java:266) case 2: sql = "alter table test COLLATE utf8mb4_unicode_ci" ; Exception in thread "main" com.alibaba.fastsql.sql.parser.ParserException: syntax error error in :'LATE utf8mb4_unicode_ci' expect = actual null pos 44 line 1 column 27 token IDENTIFIER utf8mb4_unicode_ci at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:363) at com.alibaba.fastsql.sql.parser.SQLParser.accept(SQLParser.java:371) at com.alibaba.fastsql.sql.dialect.mysql.parser.MySqlStatementParser.parseAlterTable(MySqlStatementParser.java:4818) at com.alibaba.fastsql.sql.dialect.mysql.parser.MySqlStatementParser.parseAlter(MySqlStatementParser.java:3544) at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:264) at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:92) at com.test.HelloWorld.main(HelloWorld.java:266) case 3： sql_8 = "alter table task AUTO_INCREMENT = 20000000 COMMENT Self-incrementing start value ; Exception in thread "main" com.alibaba.fastsql.sql.parser.ParserException: syntax error error in :'0000 COMMENT Self-incrementing start value expect ON actual = pos 52 line 1 column 52 token = at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:363) at com.alibaba.fastsql.sql.parser.SQLParser.accept(SQLParser.java:371) at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseComment(SQLStatementParser.java:3488) at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:354) at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:92) at com.test.HelloWorld.main(HelloWorld.java:266) These are some problems that have been feedback and suggested to be modified to bypass alter table test COLLATE = utf8mb4_unicode_ci alter table task AUTO_INCREMENT = 20000000 COMMENT Self-incrementing start value Add another ddl CREATE TABLE `app_info` (`id` bigint(20) NOT NULL `app_name` varchar(255) NOT NULL PRIMARY KEY (`id`) INDEX `idx` USING BTREE (`app_name`) comment '') ;
780,fix bug #776 [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=780) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=780) before we can accept your contribution.<br/><hr/>**winger** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=780) it.</sub> [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=780) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=780) before we can accept your contribution.<br/><hr/>**winger** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=780) it.</sub> @agapple Gmail mailbox jianghang115 gmail com have my message thank you tks
779,Long comments can cause data confusion The note was originally Directional setting 32 characters Each character has 3 values ​​0 Empty 1 Unlimited 2 Determined value specified No specified exclusion Excluded as specified 3 Determined value Excluded 1st character Application channel 2nd character Network environment 3rd character Mobile operating system 4th character platform version number 5th character geographical orientation 6th character gender 7th character age segment 8th character keyword 0 no 1 no limit 2 designation 3 exclusion 4 smart 9th character commercial interest Label 10th character AppList label 11th character device price label 12th character video type 13th character operator 14th character custom crowd package Change to Directional setting 32 characters Each character has 3 values ​​0 Empty 1 Unlimited 2 Determined value specified No specified exclusion Excluded as specified 3 Determined value Excluded 1st character Application channel 2nd character Network environment 3rd character Mobile operating system 4th character platform version number 5th character geographical orientation 6th character gender 7th character age segment 8th character keyword 0 no 1 no limit 2 designation 3 exclusion 4 smart 9th character commercial interest Label 10th character AppList label 11th character device price label 12th character video type 13th character operator 14th character custom crowd package 15th character application channel information version 16th character platform version number information Version There is a field with data that is garbled in the future. Give me a complete ddl sql alter table tb_ad_group modify `target_setting` varchar(64) NOT NULL DEFAULT '00000000000000000000000000000000' COMMENT Directional setting 32 characters Each character has 3 values ​​0 Empty 1 Unlimited 2 Determined value specified No specified exclusion Excluded as specified 3 Determined value Excluded 1st character Application channel 2nd character Network environment 3rd character Mobile operating system 4th character platform version number 5th character geographical orientation 6th character gender 7th character age segment 8th character keyword 0 no 1 no limit 2 designation 3 exclusion 4 smart 9th character commercial interest Label 10th character AppList label 11th character device price label 12th character video type 13th character operator 14th character custom crowd package 15th character application channel information version 16th character platform version number information Version Latest alpha 4 analysis no problem I want to ask you about it. If there is any way to change the version, the version is not particularly convenient.
778,parse row data failed canal 1.0.26-SNAPSHOT 5.5.52-MariaDB GRANT SELECT REPLICATION SLAVE REPLICATION CLIENT ON *.* TO 'czbrepclient'@'%' IDENTIFIED BY PASSWORD '*A6A643DF20257C290F26693E1A59F69448428157' Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: java.lang.IllegalArgumentException: limit excceed: 148 at com.taobao.tddl.dbsync.binlog.LogBuffer.getFullString(LogBuffer.java:1123) Provide binlog files or test data to reproduce frequently Send you gmail, please check
777,fix issue #771 #776 #756 [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=777) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=777) before we can accept your contribution.<br/><hr/>**winger** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=777) it.</sub> tks
776,[v1.0.26.alpha4] decode Event time limit Exceed error ![image](https://user-images.githubusercontent.com/8179551/43183817-099b0e50-9019-11e8-8339-2d922e48b9ed.png) decode Event time limit Exceed error Should be related to this commit https://github.com/alibaba/canal/commit/89726a636530b73a6b97cecc2b5bcee4fb464f86 The mysql version is 啥 Sorry, gaoxiangyu I rely on MySQL 5 7 tested MySQL that can cause this error The version says it is convenient, I will follow up here. Thank you. 5.6.28 @gaoxiangyu Thank you for following me. @agapple I will follow up on this issue. BTW is convenient when you look at your gmail
775,Canal support spring4 or spring5? Currently conflicting with spring4 spring5 in the project when introducing the client Currently using spring3 by default @agapple Have plans to support spring4 or spring5? Spring4 is more common with spring3. You can submit a PR to my canal dependence on spring is an IOC can be upgraded Spring5 requires java8 to pay attention to
774,[v1.0.26.alpha4]connect timed Out error 018-07-25 10:50:58.209 [destination = example address = /127.0.0.1:3306 EventParser] WARN c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - Received EOF packet from server apparent master disconnected. It's may be duplicate slaveId check instance config 2018-07-25 10:51:17.034 [destination = example address = /127.0.0.1:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position {"identity":{"slaveId":-1 "sourceAddress":{"address":"127.0.0.1" "port":3306}} "postion":{"gtid":"" "included":false "journalName":"mysql-bin.000027" "position":299492902 "serverId":2 "timestamp":1532487056000}} 2018-07-25 10:51:17.086 [destination = example address = /127.0.0.1:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000027 position=299492902 serverId=2 gtid= timestamp=1532487056000] 2018-07-25 10:51:18.081 [destination = example address = /127.0.0.1:3306 EventParser] WARN c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - Received EOF packet from server apparent master disconnected. It's may be duplicate slaveId check instance config 2018-07-25 10:51:33.753 [destination = example address = /127.0.0.1:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position {"identity":{"slaveId":-1 "sourceAddress":{"address":"127.0.0.1" "port":3306}} "postion":{"gtid":"" "included":false "journalName":"mysql-bin.000027" "position":299516130 "serverId":2 "timestamp":1532487071000}} 2018-07-25 10:51:33.760 [destination = example address = /127.0.0.1:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000027 position=299516130 serverId=2 gtid= timestamp=1532487071000] 2018-07-25 10:51:43.767 [destination = example address = /127.0.0.1:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address 127.0.0.1/127.0.0.1:3306 has an error retrying. caused by java.io.IOException: connect /127.0.0.1:3306 failure at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:77) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.reconnect(MysqlConnector.java:86) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.reconnect(MysqlConnection.java:85) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:186) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Unknown Source) [na:1.8.0_152] Caused by: java.net.SocketTimeoutException: connect timed out at java.net.TwoStacksPlainSocketImpl.socketConnect(Native Method) ~[na:1.8.0_152] at java.net.AbstractPlainSocketImpl.doConnect(Unknown Source) ~[na:1.8.0_152] at java.net.AbstractPlainSocketImpl.connectToAddress(Unknown Source) ~[na:1.8.0_152] at java.net.AbstractPlainSocketImpl.connect(Unknown Source) ~[na:1.8.0_152] at java.net.PlainSocketImpl.connect(Unknown Source) ~[na:1.8.0_152] at java.net.SocksSocketImpl.connect(Unknown Source) ~[na:1.8.0_152] at java.net.Socket.connect(Unknown Source) ~[na:1.8.0_152] at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannelPool.open(BioSocketChannelPool.java:18) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannelPool.open(SocketChannelPool.java:18) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:72) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] ... 4 common frames omitted 2018-07-25 10:51:43.774 [destination = example address = /127.0.0.1:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: connect /127.0.0.1:3306 failure at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:77) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.reconnect(MysqlConnector.java:86) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.reconnect(MysqlConnection.java:85) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:186) at java.lang.Thread.run(Unknown Source) Caused by: java.net.SocketTimeoutException: connect timed out at java.net.TwoStacksPlainSocketImpl.socketConnect(Native Method) at java.net.AbstractPlainSocketImpl.doConnect(Unknown Source) at java.net.AbstractPlainSocketImpl.connectToAddress(Unknown Source) at java.net.AbstractPlainSocketImpl.connect(Unknown Source) at java.net.PlainSocketImpl.connect(Unknown Source) at java.net.SocksSocketImpl.connect(Unknown Source) at java.net.Socket.connect(Unknown Source) at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannelPool.open(BioSocketChannelPool.java:18) at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannelPool.open(SocketChannelPool.java:18) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:72) ... 4 more ] 2018-07-25 10:52:13.657 [destination = example address = /127.0.0.1:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address 127.0.0.1/127.0.0.1:3306 has an error retrying. caused by java.io.IOException: connect /127.0.0.1:3306 failure at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:77) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:81) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:172) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Unknown Source) [na:1.8.0_152] Caused by: java.net.SocketTimeoutException: connect timed out at java.net.TwoStacksPlainSocketImpl.socketConnect(Native Method) ~[na:1.8.0_152] at java.net.AbstractPlainSocketImpl.doConnect(Unknown Source) ~[na:1.8.0_152] at java.net.AbstractPlainSocketImpl.connectToAddress(Unknown Source) ~[na:1.8.0_152] at java.net.AbstractPlainSocketImpl.connect(Unknown Source) ~[na:1.8.0_152] at java.net.PlainSocketImpl.connect(Unknown Source) ~[na:1.8.0_152] at java.net.SocksSocketImpl.connect(Unknown Source) ~[na:1.8.0_152] at java.net.Socket.connect(Unknown Source) ~[na:1.8.0_152] at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannelPool.open(BioSocketChannelPool.java:18) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannelPool.open(SocketChannelPool.java:18) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:72) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] ... 3 common frames omitted 2018-07-25 10:52:13.658 [destination = example address = /127.0.0.1:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: connect /127.0.0.1:3306 failure at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:77) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:81) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:172) at java.lang.Thread.run(Unknown Source) Caused by: java.net.SocketTimeoutException: connect timed out at java.net.TwoStacksPlainSocketImpl.socketConnect(Native Method) at java.net.AbstractPlainSocketImpl.doConnect(Unknown Source) at java.net.AbstractPlainSocketImpl.connectToAddress(Unknown Source) at java.net.AbstractPlainSocketImpl.connect(Unknown Source) at java.net.PlainSocketImpl.connect(Unknown Source) at java.net.SocksSocketImpl.connect(Unknown Source) at java.net.Socket.connect(Unknown Source) at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannelPool.open(BioSocketChannelPool.java:18) at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannelPool.open(SocketChannelPool.java:18) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:72) ... 3 more ] @agapple Native MySQL pass 127.0.0.1:3306 Can&#39;t connect? @wingerx This 127 0 0 1 is another specific IP that was just replaced when posting the log.
773,[v1.0.26.alpha4]parse events has an Error error ![image](https://user-images.githubusercontent.com/9798724/43177235-d82ca3c0-8ff9-11e8-87f1-e43826acb891.png) 2018-07-25 10:52:13.662 [destination = example address = /127.0.0.1:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - parse events has an error java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.FutureTask@869cc59 rejected from java.util.concurrent.ThreadPoolExecutor@36f77020[Terminated pool size = 0 active threads = 0 queued tasks = 0 completed tasks = 0] at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(Unknown Source) ~[na:1.8.0_152] at java.util.concurrent.ThreadPoolExecutor.reject(Unknown Source) ~[na:1.8.0_152] at java.util.concurrent.ThreadPoolExecutor.execute(Unknown Source) ~[na:1.8.0_152] at java.util.concurrent.AbstractExecutorService.submit(Unknown Source) ~[na:1.8.0_152] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor.start(MysqlMultiStageCoprocessor.java:120) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor.reset(MysqlMultiStageCoprocessor.java:187) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:306) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Unknown Source) ~[na:1.8.0_152] The same problem is solved. @agapple This is not root Cause looking for the top log No problem with v1 0 25 Should be a single-core machine startup failure problem v1 1 1 fixed @agapple When will the official version of v1 1 1 be released?
772,fix issue #771 #756 [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=772) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=772) before we can accept your contribution.<br/><hr/>**winger** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=772) it.</sub> [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=772) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=772) before we can accept your contribution.<br/><hr/>**winger** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=772) it.</sub>
771,The thread pool is working abnormally when starting the canal 2018-07-24 15:44:43.247 [destination = example address = /22.5.229.239:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position mysql-bin.000001:558240:null 2018-07-24 15:44:43.271 [destination = example address = /22.5.229.239:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000001 position=558240 serverId=<null> gtid= timestamp=<null>] 2018-07-24 15:44:43.331 [destination = example address = /22.5.229.239:3306 EventParser] WARN c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - Received EOF packet from server apparent master disconnected. It's may be duplicate slaveId check instance config 2018-07-24 15:45:00.446 [destination = example address = /22.5.229.239:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position {"identity":{"slaveId":-1 "sourceAddress":{"address":"cbjup04" "port":3306}} "postion":{"gtid":"" "included":false "journalName":"mysql-bin.000001" "position":559146 "serverId":16782861 "timestamp":1532417891000}} 2018-07-24 15:45:00.447 [destination = example address = /22.5.229.239:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000001 position=559146 serverId=16782861 gtid= timestamp=1532417891000] 2018-07-24 15:45:00.453 [destination = example address = /22.5.229.239:3306 EventParser] WARN c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - Received EOF packet from server apparent master disconnected. It's may be duplicate slaveId check instance config 2018-07-24 15:45:18.422 [destination = example address = /22.5.229.239:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position {"identity":{"slaveId":-1 "sourceAddress":{"address":"cbjup04" "port":3306}} "postion":{"gtid":"" "included":false "journalName":"mysql-bin.000001" "position":559146 "serverId":16782861 "timestamp":1532417891000}} 2018-07-24 15:45:18.422 [destination = example address = /22.5.229.239:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000001 position=559146 serverId=16782861 gtid= timestamp=1532417891000] 2018-07-24 15:45:18.426 [destination = example address = /22.5.229.239:3306 EventParser] WARN c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - Received EOF packet from server apparent master disconnected. It's may be duplicate slaveId check instance config 2018-07-24 15:45:18.440 [destination = example address = /22.5.229.239:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - parse events has an error java.util.concurrent.RejectedExecutionException: Task com.lmax.disruptor.WorkProcessor@58d43d50 rejected from java.util.concurrent.ThreadPoolExecutor@56057cbf[Terminated pool size = 0 active threads = 0 queued tasks = 0 completed tasks = 0] at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2048) ~[na:1.7.0_45] at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:821) ~[na:1.7.0_45] at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1372) ~[na:1.7.0_45] at com.lmax.disruptor.WorkerPool.start(WorkerPool.java:140) ~[disruptor-3.4.2.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor.start(MysqlMultiStageCoprocessor.java:122) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor.reset(MysqlMultiStageCoprocessor.java:187) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:306) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:744) ~[na:1.7.0_45] #756 The same problem Found the problem @agapple in fix #726 Issue will binlog_flags Set in order to BINLOG_DUMP_NON_BLOCK master has no binlog to send After the event will return an EOF package. Usually for the slave, it’s probably better to keep the connection hanging so that you can receive the newly generated binlog in a timely manner. Event I don&#39;t know agapple Based on what considerations changed the settings BINLOG_DUMP_NON_BLOCK？ Or more elegant processing EOF package Instead of printing a misleading Received EOF packet from server apparent master disconnected. It's may be duplicate slaveId check instance config" Log information @wingerx Hello, my problem is mainly in the following when creating a parse thread pool will always report java util concurrent RejectedExecutionException and pool Size is 0 My server is 4 cores. According to the default value of parallelThreadSize, 60 poolSize of the total available physical core of the server should be 2 instead of 0. The same problem is solved by the landlord. @fanpeng1100 No problem with me is calling the thread pool error @sky-mariner The reason for the thread pool error is please see my submitted pr #777。 For the latest release v1.0.26 alpha 4 first change the configuration canal.instance.parser.parallel = false Use it @wingerx Okay thank you @wingerx Correct answer
770,Specify canal instance master timestamp timestamp consumption is invalid The timestamp is specified in the instance. The client will also get the log before this timestamp. If you want to consume only the log after this timestamp, which should be configured? Can you configure the canal instance master timestamp separately? Clean up the last recorded location information such as the zookeeper node and the meta dat file
769,1.0.26 Version single machine does not use h2 db Another sql in the modification of multiple table structures and the creation of a new table parsing the entire library of exception monitoring The exception information is as follows Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: fetch failed by table Meta database_table name_new Caused by: java.io.IOException: ErrorPacket [errorNumber=1146 fieldCount=-1 message=Table Database_table name_new doesn't exist sqlState=42S02 sqlStateMarker=#] at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.query(MysqlQueryExecutor.java:61) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:94) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMetaByDB(TableMetaCache.java:89) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:32) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:62) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:52) at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) at com.google.common.cache.LocalCache.get(LocalCache.java:3937) at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941) at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824) at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:185) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:759) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:428) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:114) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:66) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:337) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:184) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:152) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) at java.lang.Thread.run(Thread.java:748) message=Table Database_table name_new doesn't exist sqlState=42S02 sqlStateMarker=#] Table not found The error message that this table does not exist is the parsing exception caused by the temporary table when the index is added to the table and the other table operation is created. Open Table TSDB to test a lot of restrictions on the previous model of the database it is good I opened h2 Seeing that there will be such an exception afterwards
768,Modify Readme md new data subscription event announcement Modify Readme md new data subscription event announcement [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=768) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=768) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=768) it.</sub>
767,Canal client connection problem ZK at 47 96 186 32 2181 Canal at 47 96 186 32 11111 No problem with the port Canal server logs are normal When the client connects ZK can successfully connect But call **canalConnector.subscribe(canalClientProperties.getSubscribeFilter());** This method throws the following exception Canal autoscan is off something goes wrong when subscribing from server:null 2018-07-23 14:14:30.100 [main-SendThread(47.96.186.32:2181)] DEBUG org.apache.zookeeper.ClientCnxn.readResponse(815) - Reading reply sessionid:0x164b770991e0058 packet:: clientPath:null serverPath:null finished:false header:: 298 4 replyHeader:: 298 12176 0 request:: '/otter/canal/destinations/example/1001/running T response:: #7b22616374697665223a747275652c2261646472657373223a223132372e302e302e313a3536333633222c22636c69656e744964223a313030317d s{12174 12175 1532326470040 1532326470047 1 0 0 100406785942093912 59 0 12174} 2018-07-23 14:14:30.100 [taskExecutor-5] WARN c.a.o.c.c.i.r.ClientRunningMonitor.check(168) - canal is running in [127.0.0.1:56363] but not in [192.168.31.207] 2018-07-23 14:14:30.101 [taskExecutor-5] ERROR t.j.m.m.s.c.CanalIncrementSyncTask.handleException(25) - com.alibaba.otter.canal.protocol.exception.CanalClientException: failed to subscribe after 3 times retry. at com.alibaba.otter.canal.client.impl.ClusterCanalConnector.subscribe(ClusterCanalConnector.java:119) at tech.jiyu.micromagasin.mc.sync.canal.CanalIncrementSyncTask.sync(CanalIncrementSyncTask.java:75) at tech.jiyu.micromagasin.mc.sync.canal.CanalIncrementSyncTask$$FastClassBySpringCGLIB$$783fbb6f.invoke(<generated>) at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:204) at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:720) at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157) at org.springframework.aop.interceptor.AsyncExecutionInterceptor$1.call(AsyncExecutionInterceptor.java:115) at java.util.concurrent.FutureTask.run$$$capture(FutureTask.java:266) at java.util.concurrent.FutureTask.run(FutureTask.java) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) com.alibaba.otter.canal.protocol.exception.CanalClientException: failed to subscribe after 3 times retry. at com.alibaba.otter.canal.client.impl.ClusterCanalConnector.subscribe(ClusterCanalConnector.java:119) at tech.jiyu.micromagasin.mc.sync.canal.CanalIncrementSyncTask.sync(CanalIncrementSyncTask.java:75) at tech.jiyu.micromagasin.mc.sync.canal.CanalIncrementSyncTask$$FastClassBySpringCGLIB$$783fbb6f.invoke(<generated>) at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:204) at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:720) at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157) at org.springframework.aop.interceptor.AsyncExecutionInterceptor$1.call(AsyncExecutionInterceptor.java:115) at java.util.concurrent.FutureTask.run$$$capture(FutureTask.java:266) at java.util.concurrent.FutureTask.run(FutureTask.java) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) The reason for finding the problem right now ![image](https://user-images.githubusercontent.com/31718669/43062693-3e5187a2-8e8c-11e8-9c3a-beb631e3d868.png) canalServer 的canal properties Ip configuration is 127 0 0 1 When the client connects to ZK, the above data connection is obtained. The address used is also 127 0 0 1 to connect to Canal. Server This becomes the client machine connecting to the local CanalServer In the beginning, this was just a guess. But when I started a CanalServer locally, I no longer throw an exception. Can connect normally the question is that I tried to configure the remote CanalServer ip as a public IP. This should be able to connect properly. But changed to public network IP after CanalServer Startup failed Prompt error "**canal cannot assign requested address**"
766,Cancal starts normal java client can connect but the client can&#39;t get data change Cancal starts normal java client can connect on but there is data change after the client can not get the change below is the record Who has encountered this situation, please help, thank you 2018-07-23 12:29:37.519 [destination = example address = /127.0.01:3307 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: ErrorPacket [errorNumber=1046 fieldCount=-1 message=No database selected sqlState=3D000 sqlStateMarker=#] with command: show create table Sbed full speed stock show create table Sbed commodity filing information table show create table Sbed Condot stock show create table Sbed no GST merchandise show create table Sbed Australian cat commodity price show create table Sbed Australian cat commodity declaration price show create table Sbed Australian cat commodity sales price show create table Sbed online store housekeeper total inventory show create table Sbed online store housekeeper commodity data show create table Sbed goods price list show create table `sbed`.`QRTZ_BLOB_TRIGGERS`;show create table `sbed`.`QRTZ_CALENDARS`;show create table `sbed`.`QRTZ_CRON_TRIGGERS`;show create table `sbed`.`QRTZ_FIRED_TRIGGERS`;show create table `sbed`.`QRTZ_JOB_DETAILS`;show create table `sbed`.`QRTZ_LOCKS`;show create table `sbed`.`QRTZ_PAUSED_TRIGGER_GRPS`;show create table `sbed`.`QRTZ_SCHEDULER_STATE`;show create table `sbed`.`QRTZ_SIMPLE_TRIGGERS`;show create table `sbed`.`QRTZ_SIMPROP_TRIGGERS`;show create table `sbed`.`QRTZ_TRIGGERS`;show create table `sbed`.`sys_add_goods_advice`;show create table `sbed`.`sys_aomao_goods`;show create table `sbed`.`sys_aomao_goods_price`;show create table `sbed`.`sys_aomao_logistics`;show create table `sbed`.`sys_aomao_trade`;show create table `sbed`.`sys_aomao_trade_goods`;show create table `sbed`.`sys_aomao_user`;show create table `sbed`.`sys_aomao_user_group`;show create table `sbed`.`sys_aomao_user_inventory`;show create table `sbed`.`sys_attachment`;show create table `sbed`.`sys_au_data`;show create table `sbed`.`sys_auspost_base`;show create table `sbed`.`sys_auspost_log`;show create table `sbed`.`sys_auspost_order`;show create table `sbed`.`sys_common_map`;show create table `sbed`.`sys_config`;show create table `sbed`.`sys_content`;show create table `sbed`.`sys_content_taxonomy`;show create table `sbed`.`sys_currency`;show create table `sbed`.`sys_customer`;show create table `sbed`.`sys_customer_address`;show create table `sbed`.`sys_customer_category`;show create table `sbed`.`sys_customer_grade`;show create table `sbed`.`sys_dept`;show create table `sbed`.`sys_documents`;show create table `sbed`.`sys_eunionpay`;show create table `sbed`.`sys_excel_export`;show create table `sbed`.`sys_express`;show create table `sbed`.`sys_finance_log`;show create table `sbed`.`sys_finance_order`;show create table `sbed`.`sys_finance_order_goods`;show create table `sbed`.`sys_finance_refund`;show create table `sbed`.`sys_goods`;show create table `sbed`.`sys_goods_brand`;show create table `sbed`.`sys_goods_category`;show create table `sbed`.`sys_goods_price`;show create table `sbed`.`sys_goods_repertory`;show create table `sbed`.`sys_goods_return`;show create table `sbed`.`sys_goods_shop_price`;show create table `sbed`.`sys_goods_stock`;show create table `sbed`.`sys_goods_supplier`;show create table `sbed`.`sys_issue`;show create table `sbed`.`sys_issue_post`;show create table `sbed`.`sys_jingdong_refund`;show create table `sbed`.`sys_log`;show create table `sbed`.`sys_menu`;show create table `sbed`.`sys_order_detail`;show create table `sbed`.`sys_purchase`;show create table `sbed`.`sys_purchase_checkout`;show create table `sbed`.`sys_purchase_checkout_detail`;show create table `sbed`.`sys_purchase_detail`;show create table `sbed`.`sys_purchaseplan`;show create table `sbed`.`sys_purchaseplan_detail`;show create table `sbed`.`sys_quansutong_clearance`;show create table `sbed`.`sys_quansutong_express`;show create table `sbed`.`sys_quansutong_token`;show create table `sbed`.`sys_redbook_repertory`;show create table `sbed`.`sys_role`;show create table `sbed`.`sys_role_menu`;show create table `sbed`.`sys_sai_goods`;show create table `sbed`.`sys_sai_shipment_data`;show create table `sbed`.`sys_sai_stock`;show create table `sbed`.`sys_sale_num_goods`;show create table `sbed`.`sys_sales`;show create table `sbed`.`sys_sales_stock`;show create table `sbed`.`sys_schedule_job`;show create table `sbed`.`sys_schedule_job_log`;show create table `sbed`.`sys_shop`;show create table `sbed`.`sys_shop_delivery`;show create table `sbed`.`sys_shop_goods`;show create table `sbed`.`sys_shop_order`;show create table `sbed`.`sys_shop_order_goods`;show create table `sbed`.`sys_status`;show create table `sbed`.`sys_stock_all`;show create table `sbed`.`sys_stock_all_log`;show create table `sbed`.`sys_stock_all_num`;show create table `sbed`.`sys_stock_all_num_view`;show create table `sbed`.`sys_stock_allocation`;show create table `sbed`.`sys_stock_allocation_main`;show create table `sbed`.`sys_stock_goods`;show create table `sbed`.`sys_stock_store`;show create table `sbed`.`sys_stock_store_position`;show create table `sbed`.`sys_store`;show create table `sbed`.`sys_store_model`;show create table `sbed`.`sys_supplier`;show create table `sbed`.`sys_taobao_goods`;show create table `sbed`.`sys_taobao_goods_detail`;show create table `sbed`.`sys_taobao_goods_detail_item_imgs`;show create table `sbed`.`sys_taobao_goods_detail_location`;show create table `sbed`.`sys_taobao_goods_detail_prop_imgs`;show create table `sbed`.`sys_taobao_goods_detail_sku`;show create table `sbed`.`sys_taobao_goods_price`;show create table `sbed`.`sys_taobao_logistics_company`;show create table `sbed`.`sys_taobao_order`;show create table `sbed`.`sys_taobao_trade`;show create table `sbed`.`sys_taxonomy`;show create table `sbed`.`sys_third_nulaxgroup`;show create table `sbed`.`sys_tianmao_activity`;show create table `sbed`.`sys_tianmao_activity_goods`;show create table `sbed`.`sys_trade`;show create table `sbed`.`sys_trade_goods`;show create table `sbed`.`sys_trade_logistics`;show create table `sbed`.`sys_user`;show create table `sbed`.`sys_user_brand`;show create table `sbed`.`sys_user_formthead`;show create table `sbed`.`sys_user_role`;show create table `sbed`.`sys_user_store`;show create table `sbed`.`sys_user_token`;show create table `sbed`.`sys_value`;show create table `sbed`.`sys_wave_house_cost`;show create table `sbed`.`sys_yeebao_pay`;show create table `sbed`.`xhs_order`;show create table `sbed`.`xhs_order_detail`;show create table `sbed`.`xhs_stock`; Caused by: java.io.IOException: ErrorPacket [errorNumber=1046 fieldCount=-1 message=No database selected sqlState=3D000 sqlStateMarker=#] with command: show create table Sbed full speed stock show create table Sbed commodity filing information table show create table Sbed Condot stock show create table Sbed no GST merchandise show create table Sbed Australian cat commodity price show create table Sbed Australian cat commodity declaration price show create table Sbed Australian cat commodity sales price show create table Sbed online store housekeeper total inventory show create table Sbed online store housekeeper commodity data show create table Sbed goods price list show create table `sbed`.`QRTZ_BLOB_TRIGGERS`;show create table `sbed`.`QRTZ_CALENDARS`;show create table `sbed`.`QRTZ_CRON_TRIGGERS`;show create table `sbed`.`QRTZ_FIRED_TRIGGERS`;show create table `sbed`.`QRTZ_JOB_DETAILS`;show create table `sbed`.`QRTZ_LOCKS`;show create table `sbed`.`QRTZ_PAUSED_TRIGGER_GRPS`;show create table `sbed`.`QRTZ_SCHEDULER_STATE`;show create table `sbed`.`QRTZ_SIMPLE_TRIGGERS`;show create table `sbed`.`QRTZ_SIMPROP_TRIGGERS`;show create table `sbed`.`QRTZ_TRIGGERS`;show create table `sbed`.`sys_add_goods_advice`;show create table `sbed`.`sys_aomao_goods`;show create table `sbed`.`sys_aomao_goods_price`;show create table `sbed`.`sys_aomao_logistics`;show create table `sbed`.`sys_aomao_trade`;show create table `sbed`.`sys_aomao_trade_goods`;show create table `sbed`.`sys_aomao_user`;show create table `sbed`.`sys_aomao_user_group`;show create table `sbed`.`sys_aomao_user_inventory`;show create table `sbed`.`sys_attachment`;show create table `sbed`.`sys_au_data`;show create table `sbed`.`sys_auspost_base`;show create table `sbed`.`sys_auspost_log`;show create table `sbed`.`sys_auspost_order`;show create table `sbed`.`sys_common_map`;show create table `sbed`.`sys_config`;show create table `sbed`.`sys_content`;show create table `sbed`.`sys_content_taxonomy`;show create table `sbed`.`sys_currency`;show create table `sbed`.`sys_customer`;show create table `sbed`.`sys_customer_address`;show create table `sbed`.`sys_customer_category`;show create table `sbed`.`sys_customer_grade`;show create table `sbed`.`sys_dept`;show create table `sbed`.`sys_documents`;show create table `sbed`.`sys_eunionpay`;show create table `sbed`.`sys_excel_export`;show create table `sbed`.`sys_express`;show create table `sbed`.`sys_finance_log`;show create table `sbed`.`sys_finance_order`;show create table `sbed`.`sys_finance_order_goods`;show create table `sbed`.`sys_finance_refund`;show create table `sbed`.`sys_goods`;show create table `sbed`.`sys_goods_brand`;show create table `sbed`.`sys_goods_category`;show create table `sbed`.`sys_goods_price`;show create table `sbed`.`sys_goods_repertory`;show create table `sbed`.`sys_goods_return`;show create table `sbed`.`sys_goods_shop_price`;show create table `sbed`.`sys_goods_stock`;show create table `sbed`.`sys_goods_supplier`;show create table `sbed`.`sys_issue`;show create table `sbed`.`sys_issue_post`;show create table `sbed`.`sys_jingdong_refund`;show create table `sbed`.`sys_log`;show create table `sbed`.`sys_menu`;show create table `sbed`.`sys_order_detail`;show create table `sbed`.`sys_purchase`;show create table `sbed`.`sys_purchase_checkout`;show create table `sbed`.`sys_purchase_checkout_detail`;show create table `sbed`.`sys_purchase_detail`;show create table `sbed`.`sys_purchaseplan`;show create table `sbed`.`sys_purchaseplan_detail`;show create table `sbed`.`sys_quansutong_clearance`;show create table `sbed`.`sys_quansutong_express`;show create table `sbed`.`sys_quansutong_token`;show create table `sbed`.`sys_redbook_repertory`;show create table `sbed`.`sys_role`;show create table `sbed`.`sys_role_menu`;show create table `sbed`.`sys_sai_goods`;show create table `sbed`.`sys_sai_shipment_data`;show create table `sbed`.`sys_sai_stock`;show create table `sbed`.`sys_sale_num_goods`;show create table `sbed`.`sys_sales`;show create table `sbed`.`sys_sales_stock`;show create table `sbed`.`sys_schedule_job`;show create table `sbed`.`sys_schedule_job_log`;show create table `sbed`.`sys_shop`;show create table `sbed`.`sys_shop_delivery`;show create table `sbed`.`sys_shop_goods`;show create table `sbed`.`sys_shop_order`;show create table `sbed`.`sys_shop_order_goods`;show create table `sbed`.`sys_status`;show create table `sbed`.`sys_stock_all`;show create table `sbed`.`sys_stock_all_log`;show create table `sbed`.`sys_stock_all_num`;show create table `sbed`.`sys_stock_all_num_view`;show create table `sbed`.`sys_stock_allocation`;show create table `sbed`.`sys_stock_allocation_main`;show create table `sbed`.`sys_stock_goods`;show create table `sbed`.`sys_stock_store`;show create table `sbed`.`sys_stock_store_position`;show create table `sbed`.`sys_store`;show create table `sbed`.`sys_store_model`;show create table `sbed`.`sys_supplier`;show create table `sbed`.`sys_taobao_goods`;show create table `sbed`.`sys_taobao_goods_detail`;show create table `sbed`.`sys_taobao_goods_detail_item_imgs`;show create table `sbed`.`sys_taobao_goods_detail_location`;show create table `sbed`.`sys_taobao_goods_detail_prop_imgs`;show create table `sbed`.`sys_taobao_goods_detail_sku`;show create table `sbed`.`sys_taobao_goods_price`;show create table `sbed`.`sys_taobao_logistics_company`;show create table `sbed`.`sys_taobao_order`;show create table `sbed`.`sys_taobao_trade`;show create table `sbed`.`sys_taxonomy`;show create table `sbed`.`sys_third_nulaxgroup`;show create table `sbed`.`sys_tianmao_activity`;show create table `sbed`.`sys_tianmao_activity_goods`;show create table `sbed`.`sys_trade`;show create table `sbed`.`sys_trade_goods`;show create table `sbed`.`sys_trade_logistics`;show create table `sbed`.`sys_user`;show create table `sbed`.`sys_user_brand`;show create table `sbed`.`sys_user_formthead`;show create table `sbed`.`sys_user_role`;show create table `sbed`.`sys_user_store`;show create table `sbed`.`sys_user_token`;show create table `sbed`.`sys_value`;show create table `sbed`.`sys_wave_house_cost`;show create table `sbed`.`sys_yeebao_pay`;show create table `sbed`.`xhs_order`;show create table `sbed`.`xhs_order_detail`;show create table `sbed`.`xhs_stock`; at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.queryMulti(MysqlQueryExecutor.java:109) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.queryMulti(MysqlConnection.java:101) at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:173) at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:84) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) at java.lang.Thread.run(Thread.java:748) ] Execute that show command to see the error The same problem canal1 1 1 How to solve the help @agapple Execute the show command that gave the error @shuaicloud Thanks agapple Executed multiple shows create table XXXXX; No error 。 Mysql5 7 18 has multiple database permissions on it Newspaper ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address xxxxx/xxxxx:3306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: ErrorPacket [errorNumber=1046 fieldCount=-1 message=No database selected sqlState=3D000 sqlStateMarker=#] mysql5.6.40 The same set of canal service has no problem permission Execute multi-statement show on 5 7 create Table is not error? > Execute multi-statement show on 5 7 create Table is not error? Yes
765,Canal monitoring docking prometheus grafana Many people have feedback on how to effectively monitor canal The initial thinking of the server is to choose the current popular prometheus grafana Monitored indicator 1. Server level status (instance list / client list) + (cpu / jvm gc / net / io ...) 2. Instance level status (delay / tps / ringbuffer / heartbeat ... ) If you have already done a similar solution I welcome everyone to submit PR to me. cpu/jvm gc These can be used directly as exports provided by simpleclient_hotspot @agapple I’m just learning about the content recently. Can I mention pr? Very welcome to submit two PR according to @agapple The comments combine with some of the feelings of their own use to implement the canal Some ideas of the server itself monitoring First list some indicators to open parallel mode analysis as an example PS By with instance Tag can collect and summarize indicators in the instance level 1. From read to store put a. Canal_inbound_bytes representing inbound traffic b. Indicates the dump thread pushlish events -> disruptor Buffer blocking time slice canal_dump_publish_blocking_time c. Indicates sink thread put events -> Store blocking time slice canal_sink_put_blocking_time d. The delegate delay can be obtained by CanalEventDownStreamHandler Canal_instance_delay various events The classification statistics of type determine whether there is a batch SQL or the like in the near future by the number of rowData attached to rowchange. 2. netty The server side uses the SessionHandler to call write to increase the ChannelFutureListener callback to sample the client&#39;s request and completion information, including destination. PacketType write amount batchId only judges whether GET is empty or not error code response time Wait a. Canal_inbound_bytes representing outbound traffic b. Classification statistics of each packetType package c. Empty packet rate d. The error occurred is as error Code statistics e. Response time 3. event Store more natural buried point a. TPS is calculated according to ackSequence b. If it is memory mode MPS is calculated according to ackMemSize c. store remain events Quantity d. store remain memory Some scenes - 1-b No obvious blocking，1-d Delay increases by 1 a Small flow --> It can be roughly inferred whether the bottleneck is insufficient in the network or the traffic is crowded. - 1-b Significant blocking，1-c No obvious blocking client spending quickly Can try to increase the canal instance parser parallelThreadSize to speed up the parsing speed - 1-b with 1 c has significant blocking Delay is increasing --> The bottleneck lies in client read and write and consumption - 2-c High air frequency --> Consider adjusting the timeout time of the optimized get - 2-b && 2-d --> Whether the client&#39;s behavior is normal Welcome comments BTW New pulled a branch metrics_support @lcybo Considered a very comprehensive point of praise :) <img width="910" alt="grafana" src="https://user-images.githubusercontent.com/15042781/44220795-30379600-a1b2-11e8-9f56-24dd8395f0bf.PNG"> Sticking a preview image of the home environment traffic is not easy to simulate complex scenes awesome perfect! <img width="251" alt="down" src="https://user-images.githubusercontent.com/15042781/44293887-3f0d6e00-a2c2-11e8-92e7-da3ccae6c991.PNG"> Generate template to increase datasource convenience configuration instanes switch Submitted template canal conf metrics Canal_instances_tmpl json Re-typed the dashboard Instructions for use have also come to an end I will fix it as soon as possible if there is a problem. ![image](https://user-images.githubusercontent.com/834743/44321609-8ab74780-a47b-11e8-80c2-6d5baab962fa.png) The final rendering can be found in https github com alibaba canal wiki Prometheus QuickStart
764,canal 1.0.26 Occasionally disconnected and then connected from the new Partial synchronization stop scenes to be used 1 Synchronization task for connecting 20 databases to a single virtual machine A large number of logs are suddenly appearing at a certain moment. But after the new connection Part of the database is in sync Partial database synchronization stops Zk&#39;s cursor does not change Stay in the disconnected place log as follows MysqlConnector - |disConnect Mysql Connection to 10.x.x.x/10.x.x.x:3306..... MysqlConnector - |connect Mysql Connection to 10.x.x.x/10.x.x.x:3306..... MysqlConnector - |handshake initialization packet received prepare the client authentication packet to send MysqlConnector - |client authentication packet is sent out. There have been many such cases at present. Unknown reason Too little information can&#39;t be located, you can look at the jstack at the time. +1 Constantly kill dump以及disConnect connect The handshake intranet test is like this @kervin521 You have encountered this problem, or let me provide more information now in accordance with the meaning of the god Analysis of jstack Reopen after having a recurring method
763,Canal instance filter regex can only grab the first item after the first item data change can not get E.g ``` canal.instance.filter.regex=vip\\.crm.* .oms.* .ser.* .sku.* .vip.* .wfe.* ``` Can only get the data change oms of the crm prefix table and can not get the following take a look https://github.com/alibaba/canal/wiki/FAQ
762,Modify Readme md Add DTS instructions [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=762) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=762) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=762) it.</sub>
761,Add Canal commercial version update instructions Add Canal commercial version update instructions [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=761) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=761) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=761) it.</sub> repeated This follow-up will increase ，hdfs/hbase/ Other interface plugins are not
760,canal+OTTER Client side batchid 1 Canal version 1 0 22 After the configuration is started, the value obtained by the client using canalConnector getWithoutAck batchSize getId is always 1 and the cursor node has not generated the normal port log under 1001node on zk. Client side log ![image](https://user-images.githubusercontent.com/41414514/42923813-0c834b50-8b5a-11e8-83f4-bf00bdb56d34.png) Set the binlog name on the canal side and the position still can&#39;t create the cursor and the file exists. Has anyone encountered such a problem? Ask for advice. Look at canal Whether the log on the server is abnormal
759,append gtid lastCommitted and sequenceNumber support [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=759) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=759) before we can accept your contribution.<br/><hr/>**winger** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=759) it.</sub> tks
758,Client switch delay problem Deployed two client terminals to listen to the same destination through zk. Look at the document. It does not seem to describe the time interval for getting data when two clients switch. The two client terminals are kept through the while true mode. Subscribe connect getMessage It is reasonable to say that the time interval should be the time when the zk handles the state change. I set it. Canal zookeeper flush period 1000 From the results, the interval is about 30S each time and the configuration is quite different. Is there a way to shorten this interval? The business has high requirements for real-time performance. This is zookeeper&#39;s sessionTimeout. You can find the parameters on the Internet to lower it.
757,used 1.0.26 Why is the p3 version? EventType.QUERY Printed sql is insert or it could be create table？ > DEBUG com.pamirs.data.AbstractCanalClient - Event Type Is Query sql --> UPDATE T_PDC_HANDOVER_INDEX_OR code show as below ```java if (eventType == EventType.QUERY || rowChage.getIsDdl()) { if (rowChage.getIsDdl()) { StringBuilder exceptionMessage = new StringBuilder(); exceptionMessage.append("mysql has an ddl sql is [ ").append(rowChage.getSql()).append(" ]"); ExceptionHandle.handleException(new ExceptionMessageEntity(entityTask.getTableTask().getId() ExceptionMessageEntityDesc.PAMIRS_DATA_MYSQL_PULL_DDL_MESSAGE ExceptionMessageTypeEnum.pamirs_data exceptionMessage.toString())); } else { logger.debug("Event Type Is Query sql --> {}" rowChage.getSql()); } return; } ``` RowsQueryLogEvent ， After 5 6 the row mode binlog will be mixed with statement and row. For insert update delete, the corresponding original SQL will also be recorded. This sql is the goods. @agapple How should we distinguish?
756,com.lmax.disruptor.FatalExceptionHandler handleEventException Just understand canal directly with canal kafka 1 0 26 SNAPSHOT tar package will report a com alibaba druid pool DruidDataSource - testWhileIdle is true validationQuery not Set but can still be used but there is an error when running CanalLauncher java with kafka package Image https user images githubusercontent com 26325971 42809370 19b008e0 89e8 11e8 8ba0 bc520efcb083 png Is there anything I am doing wrong? Really can&#39;t find the error where the landlord helps me。 Image https user images githubusercontent com 26325971 42810158 d57bd4ae 89e9 11e8 95a9 97a38b5d6402 png Profile Information ![image](https://user-images.githubusercontent.com/26325971/42810224-04150a7e-89ea-11e8-954b-1855e2d8fe0b.png) ![image](https://user-images.githubusercontent.com/26325971/42813766-a1392646-89f4-11e8-82ea-3279b7b1e13a.png) How can I solve this problem? ![image](https://user-images.githubusercontent.com/834743/42819117-28463a7e-8a05-11e8-9058-64224e83e062.png) Duplicate slaveId Optimized the log output to remove the useless I also saw this duplicate slaveId error but I only have one in the Instance prorerties that only defines one Image https user images githubusercontent com 26325971 42854088 b68f65c8 8a6c 11e8 94c2 abc81424ca81 png Where else to change?
755,The configured filter regex does not take effect. The server does not filter out the sys library data. Sometimes the mysql library data is not filtered out. Version 1 0 26 Binlog mode ROW Instance properties configuration canal.instance.filter.regex=blacklist3\\..* canal.instance.filter.black.regex= Server log 2018-07-17 15:45:09.388 [destination = xxxx address = /xxxx:xxxx EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:xxxx[com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: ErrorPacket [errorNumber=1142 fieldCount=-1 message=SHOW VIEW command denied to user 'xxxx'@'xxxx for table 'host_summary' sqlState=42000 sqlStateMarker=#] with command: show create table `sys`.`host_summary`;show create table `sys`.`host_summary_by_file_io`;show create table `sys`.`host_summary_by_file_io_type`;show create table `sys`.`host_summary_by_stages`;show create table `sys`.`host_summary_by_statement_latency`;show create table `sys`.`host_summary_by_statement_type`;show create table `sys`.`innodb_buffer_stats_by_schema`;show create table `sys`.`innodb_buffer_stats_by_table`;show create table `sys`.`innodb_lock_waits`;show create table `sys`.`io_by_thread_by_latency`;show create table `sys`.`io_global_by_file_by_bytes`;show create table `sys`.`io_global_by_file_by_latency`;show create table `sys`.`io_global_by_wait_by_bytes`;show create table `sys`.`io_global_by_wait_by_latency`;show create table `sys`.`latest_file_io`;show create table `sys`.`memory_by_host_by_current_bytes`;show create table `sys`.`memory_by_thread_by_current_bytes`;show create table `sys`.`memory_by_user_by_current_bytes`;show create table `sys`.`memory_global_by_current_bytes`;show create table `sys`.`memory_global_total`;show create table `sys`.`metrics`;show create table `sys`.`processlist`;show create table `sys`.`ps_check_lost_instrumentation`;show create table `sys`.`schema_auto_increment_columns`;show create table `sys`.`schema_index_statistics`;show create table `sys`.`schema_object_overview`;show create table `sys`.`schema_redundant_indexes`;show create table `sys`.`schema_table_lock_waits`;show create table `sys`.`schema_table_statistics`;show create table `sys`.`schema_table_statistics_with_buffer`;show create table `sys`.`schema_tables_with_full_table_scans`;show create table `sys`.`schema_unused_indexes`;show create table `sys`.`session`;show create table `sys`.`session_ssl_status`;show create table `sys`.`statement_analysis`;show create table `sys`.`statements_with_errors_or_warnings`;show create table `sys`.`statements_with_full_table_scans`;show create table `sys`.`statements_with_runtimes_in_95th_percentile`;show create table `sys`.`statements_with_sorting`;show create table `sys`.`statements_with_temp_tables`;show create table `sys`.`sys_config`;show create table `sys`.`user_summary`;show create table `sys`.`user_summary_by_file_io`;show create table `sys`.`user_summary_by_file_io_type`;show create table `sys`.`user_summary_by_stages`;show create table `sys`.`user_summary_by_statement_latency`;show create table `sys`.`user_summary_by_statement_type`;show create table `sys`.`version`;show create table `sys`.`wait_classes_global_by_avg_latency`;show create table `sys`.`wait_classes_global_by_latency`;show create table `sys`.`waits_by_host_by_latency`;show create table `sys`.`waits_by_user_by_latency`;show create table `sys`.`waits_global_by_latency`;show create table `sys`.`x$host_summary`;show create table `sys`.`x$host_summary_by_file_io`;show create table `sys`.`x$host_summary_by_file_io_type`;show create table `sys`.`x$host_summary_by_stages`;show create table `sys`.`x$host_summary_by_statement_latency`;show create table `sys`.`x$host_summary_by_statement_type`;show create table `sys`.`x$innodb_buffer_stats_by_schema`;show create table `sys`.`x$innodb_buffer_stats_by_table`;show create table `sys`.`x$innodb_lock_waits`;show create table `sys`.`x$io_by_thread_by_latency`;show create table `sys`.`x$io_global_by_file_by_bytes`;show create table `sys`.`x$io_global_by_file_by_latency`;show create table `sys`.`x$io_global_by_wait_by_bytes`;show create table `sys`.`x$io_global_by_wait_by_latency`;show create table `sys`.`x$latest_file_io`;show create table `sys`.`x$memory_by_host_by_current_bytes`;show create table `sys`.`x$memory_by_thread_by_current_bytes`;show create table `sys`.`x$memory_by_user_by_current_bytes`;show create table `sys`.`x$memory_global_by_current_bytes`;show create table `sys`.`x$memory_global_total`;show create table `sys`.`x$processlist`;show create table `sys`.`x$ps_digest_95th_percentile_by_avg_us`;show create table `sys`.`x$ps_digest_avg_latency_distribution`;show create table `sys`.`x$ps_schema_table_statistics_io`;show create table `sys`.`x$schema_flattened_keys`;show create table `sys`.`x$schema_index_statistics`;show create table `sys`.`x$schema_table_lock_waits`;show create table `sys`.`x$schema_table_statistics`;show create table `sys`.`x$schema_table_statistics_with_buffer`;show create table `sys`.`x$schema_tables_with_full_table_scans`;show create table `sys`.`x$session`;show create table `sys`.`x$statement_analysis`;show create table `sys`.`x$statements_with_errors_or_warnings`;show create table `sys`.`x$statements_with_full_table_scans`;show create table `sys`.`x$statements_with_runtimes_in_95th_percentile`;show create table `sys`.`x$statements_with_sorting`;show create table `sys`.`x$statements_with_temp_tables`;show create table `sys`.`x$user_summary`;show create table `sys`.`x$user_summary_by_file_io`;show create table `sys`.`x$user_summary_by_file_io_type`;show create table `sys`.`x$user_summary_by_stages`;show create table `sys`.`x$user_summary_by_statement_latency`;show create table `sys`.`x$user_summary_by_statement_type`;show create table `sys`.`x$wait_classes_global_by_avg_latency`;show create table `sys`.`x$wait_classes_global_by_latency`;show create table `sys`.`x$waits_by_host_by_latency`;show create table `sys`.`x$waits_by_user_by_latency`;show create table `sys`.`x$waits_global_by_latency`; Caused by: java.io.IOException: ErrorPacket [errorNumber=1142 fieldCount=-1 message=SHOW VIEW command denied to user 'xxxx'@'xxxx' for table 'host_summary' sqlState=42000 sqlStateMarker=#] with command: show create table `sys`.`host_summary`;show create table `sys`.`host_summary_by_file_io`;show create table `sys`.`host_summary_by_file_io_type`;show create table `sys`.`host_summary_by_stages`;show create table `sys`.`host_summary_by_statement_latency`;show create table `sys`.`host_summary_by_statement_type`;show create table `sys`.`innodb_buffer_stats_by_schema`;show create table `sys`.`innodb_buffer_stats_by_table`;show create table `sys`.`innodb_lock_waits`;show create table `sys`.`io_by_thread_by_latency`;show create table `sys`.`io_global_by_file_by_bytes`;show create table `sys`.`io_global_by_file_by_latency`;show create table `sys`.`io_global_by_wait_by_bytes`;show create table `sys`.`io_global_by_wait_by_latency`;show create table `sys`.`latest_file_io`;show create table `sys`.`memory_by_host_by_current_bytes`;show create table `sys`.`memory_by_thread_by_current_bytes`;show create table `sys`.`memory_by_user_by_current_bytes`;show create table `sys`.`memory_global_by_current_bytes`;show create table `sys`.`memory_global_total`;show create table `sys`.`metrics`;show create table `sys`.`processlist`;show create table `sys`.`ps_check_lost_instrumentation`;show create table `sys`.`schema_auto_increment_columns`;show create table `sys`.`schema_index_statistics`;show create table `sys`.`schema_object_overview`;show create table `sys`.`schema_redundant_indexes`;show create table `sys`.`schema_table_lock_waits`;show create table `sys`.`schema_table_statistics`;show create table `sys`.`schema_table_statistics_with_buffer`;show create table `sys`.`schema_tables_with_full_table_scans`;show create table `sys`.`schema_unused_indexes`;show create table `sys`.`session`;show create table `sys`.`session_ssl_status`;show create table `sys`.`statement_analysis`;show create table `sys`.`statements_with_errors_or_warnings`;show create table `sys`.`statements_with_full_table_scans`;show create table `sys`.`statements_with_runtimes_in_95th_percentile`;show create table `sys`.`statements_with_sorting`;show create table `sys`.`statements_with_temp_tables`;show create table `sys`.`sys_config`;show create table `sys`.`user_summary`;show create table `sys`.`user_summary_by_file_io`;show create table `sys`.`user_summary_by_file_io_type`;show create table `sys`.`user_summary_by_stages`;show create table `sys`.`user_summary_by_statement_latency`;show create table `sys`.`user_summary_by_statement_type`;show create table `sys`.`version`;show create table `sys`.`wait_classes_global_by_avg_latency`;show create table `sys`.`wait_classes_global_by_latency`;show create table `sys`.`waits_by_host_by_latency`;show create table `sys`.`waits_by_user_by_latency`;show create table `sys`.`waits_global_by_latency`;show create table `sys`.`x$host_summary`;show create table `sys`.`x$host_summary_by_file_io`;show create table `sys`.`x$host_summary_by_file_io_type`;show create table `sys`.`x$host_summary_by_stages`;show create table `sys`.`x$host_summary_by_statement_latency`;show create table `sys`.`x$host_summary_by_statement_type`;show create table `sys`.`x$innodb_buffer_stats_by_schema`;show create table `sys`.`x$innodb_buffer_stats_by_table`;show create table `sys`.`x$innodb_lock_waits`;show create table `sys`.`x$io_by_thread_by_latency`;show create table `sys`.`x$io_global_by_file_by_bytes`;show create table `sys`.`x$io_global_by_file_by_latency`;show create table `sys`.`x$io_global_by_wait_by_bytes`;show create table `sys`.`x$io_global_by_wait_by_latency`;show create table `sys`.`x$latest_file_io`;show create table `sys`.`x$memory_by_host_by_current_bytes`;show create table `sys`.`x$memory_by_thread_by_current_bytes`;show create table `sys`.`x$memory_by_user_by_current_bytes`;show create table `sys`.`x$memory_global_by_current_bytes`;show create table `sys`.`x$memory_global_total`;show create table `sys`.`x$processlist`;show create table `sys`.`x$ps_digest_95th_percentile_by_avg_us`;show create table `sys`.`x$ps_digest_avg_latency_distribution`;show create table `sys`.`x$ps_schema_table_statistics_io`;show create table `sys`.`x$schema_flattened_keys`;show create table `sys`.`x$schema_index_statistics`;show create table `sys`.`x$schema_table_lock_waits`;show create table `sys`.`x$schema_table_statistics`;show create table `sys`.`x$schema_table_statistics_with_buffer`;show create table `sys`.`x$schema_tables_with_full_table_scans`;show create table `sys`.`x$session`;show create table `sys`.`x$statement_analysis`;show create table `sys`.`x$statements_with_errors_or_warnings`;show create table `sys`.`x$statements_with_full_table_scans`;show create table `sys`.`x$statements_with_runtimes_in_95th_percentile`;show create table `sys`.`x$statements_with_sorting`;show create table `sys`.`x$statements_with_temp_tables`;show create table `sys`.`x$user_summary`;show create table `sys`.`x$user_summary_by_file_io`;show create table `sys`.`x$user_summary_by_file_io_type`;show create table `sys`.`x$user_summary_by_stages`;show create table `sys`.`x$user_summary_by_statement_latency`;show create table `sys`.`x$user_summary_by_statement_type`;show create table `sys`.`x$wait_classes_global_by_avg_latency`;show create table `sys`.`x$wait_classes_global_by_latency`;show create table `sys`.`x$waits_by_host_by_latency`;show create table `sys`.`x$waits_by_user_by_latency`;show create table `sys`.`x$waits_global_by_latency`; at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.queryMulti(MysqlQueryExecutor.java:109) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.queryMulti(MysqlConnection.java:101) at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:173) at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:84) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) at java.lang.Thread.run(Thread.java:745) Debug look at the DatabaseTableMeta dumpTableMeta whether the filter condition is not correctly received by the client&#39;s subscribe condition Should be this problem Switching between two instances on the server side The client side configuration will affect the server side as follows. ``` String filter = ".*"; String filter = "\\.*"; String filter = "\\.\\*"; String filter = ".*\\\\..*"; ``` Looked at the source code If set to null, it will not affect String filter = null; The specific impact is that the server that is started later will subscribe to the table other than the server-side filter, resulting in no permission to report the error. The client getMessage is always empty. Two server instance configurations canal.properties In addition to canal id Canal port is the same instance.properties apart from Canal instance mysql slaveId is the same `canal.instance.filter.regex=blacklist3\\..*`
754,fix issue #751 [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=754) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=754) before we can accept your contribution.<br/><hr/>**winger** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=754) it.</sub> [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=754) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=754) before we can accept your contribution.<br/><hr/>**winger** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=754) it.</sub> tks
753,com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed Running for a while Always report this error Version 1 0 26 com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4832) ~[guava-18.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:160) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:759) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:428) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:114) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:66) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:337) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:184) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:145) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] Caused by: java.io.IOException: should execute connector.connect() first at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.<init>(MysqlQueryExecutor.java:30) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:87) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMetaByDB(TableMetaCache.java:80) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:30) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:55) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:50) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache.get(LocalCache.java:3937) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830) ~[guava-18.0.jar:na] ... 10 common frames omitted 2018-07-17 10:48:26.646 [destination = example address = /172.16.203.11:3309 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address 172.16.203.11/172.16.203.11:3309 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4832) ~[guava-18.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:160) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:759) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:428) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:114) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:66) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:337) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:184) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:145) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] Caused by: java.io.IOException: should execute connector.connect() first at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.<init>(MysqlQueryExecutor.java:30) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:87) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMetaByDB(TableMetaCache.java:80) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:30) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:55) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:50) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache.get(LocalCache.java:3937) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830) ~[guava-18.0.jar:na] ... 10 common frames omitted 2018-07-17 10:48:26.646 [destination = example address = /172.16.203.11:3309 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4832) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:160) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:759) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:428) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:114) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:66) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:337) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:184) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:145) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) at java.lang.Thread.run(Thread.java:748) Caused by: java.io.IOException: should execute connector.connect() first at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.<init>(MysqlQueryExecutor.java:30) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:87) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMetaByDB(TableMetaCache.java:80) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:30) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:55) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:50) at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) at com.google.common.cache.LocalCache.get(LocalCache.java:3937) at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941) at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824) at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830) ... 10 more ] You can&#39;t download the latest 26 version of the code line.
752,GTID cannot be obtained ![6b06917b225fc61a76d0b901b656ecc9](https://user-images.githubusercontent.com/5210147/42793757-40d64d60-89ae-11e8-8090-f69238c5ed5a.jpg) My master and slave are both open gtid synchronization is normal only when there is transaction id in transactionend, other times can not get gtid and transaction id How did you get the gtid method? Have you seen the example code in the extended attribute field? Brother example entry.getHeader().getGtid() Printed out as an empty string Need to open canal Gtid&#39;s subscription instance.property canal.instance.gtidon=true Open this ok thanks！！！
751,canal Increase the tag within a transaction row data count Hi @agapple For large transaction scenarios, since the canal exceeds a certain number in the delivery process, the default 1024 will be sent in batches. For some business scenarios, it is necessary to know the row within a large transaction. The amount of data is then merged
750,Canal deployer 1 0 26 SNAPSHOT version after running the report point error The details of the false information are as follows: something goes wrong with channel:[id: 0x1d0b2315 /xx.xx.xx.xx:xxxx => /xx.xx.xx.xx:xxxx] exception=com.alibaba.otter.canal.server.exception.CanalServerException: ack error clientId:1001 batchId:261365 is not exist please check [New I/O server worker #1-12] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x1d0b2315 /xx.xx.xx.xx:xxxx :> /xx.xx.xx.xx:xxxx] exception=java.nio.channels.ClosedChannelException at org.jboss.netty.channel.socket.nio.NioWorker.cleanUpWriteBuffer(NioWorker.java:649) at org.jboss.netty.channel.socket.nio.NioWorker.writeFromUserCode(NioWorker.java:370) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:137) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) at org.jboss.netty.channel.Channels.write(Channels.java:611) at org.jboss.netty.channel.Channels.write(Channels.java:578) at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:28) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:144) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.cleanup(ReplayingDecoder.java:542) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.channelDisconnected(ReplayingDecoder.java:450) at org.jboss.netty.channel.Channels.fireChannelDisconnected(Channels.java:360) at org.jboss.netty.channel.socket.nio.NioWorker.close(NioWorker.java:599) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:119) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) at org.jboss.netty.channel.Channels.close(Channels.java:720) at org.jboss.netty.channel.AbstractChannel.close(AbstractChannel.java:208) at org.jboss.netty.channel.ChannelFutureListener$1.operationComplete(ChannelFutureListener.java:46) at org.jboss.netty.channel.DefaultChannelFuture.notifyListener(DefaultChannelFuture.java:381) at org.jboss.netty.channel.DefaultChannelFuture.addListener(DefaultChannelFuture.java:148) at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:30) at com.alibaba.otter.canal.server.netty.NettyUtils.error(NettyUtils.java:51) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:200) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:444) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261) at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:350) at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) clientId:1001 batchId:261365 is not exist please check The server estimates that an instance restart has occurred. Why does the restart occur and how to avoid this restart?
749,Fastsql 2 0 0_preview_371 in which maven download ```xml <dependency> <groupId>com.alibaba.fastsql</groupId> <artifactId>fastsql</artifactId> <version>2.0.0_preview_371</version> </dependency> ``` modify pom.xml replace fastsql Node content <dependency> <groupId>com.alibaba.fastsql</groupId> <artifactId>fastsql</artifactId> <version>2.0.0_preview_186</version> </dependency> 371 is in my binary In the master branch, is it necessary to change the fastsql dependency from 371 to a usable version? Otherwise, the import project can not find the jar. Missing artifact com.alibaba.fastsql:fastsql:jar:2.0.0_preview_520 Now I am missing this package, and I will pass it to maven.
748,How can the deal of the canal be too slow? MySQL show master status mysql-bin.000093 Canal&#39;s dump in zk parse mysql-bin.000064 Each binlog size is 100M dump entry: 30~40 event/second Show in MySQL full processlist: Show canal dump for # writing to net Are you a new version? @WithLin V1 0 25 v1 0 26 There is a problem with non-stop disconnecting the database and database connection timeout problem @agapple @kervin521 Specifically describe the problem of v1 0 26 You are too general
747,add restart.sh restart canal server [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=747) <br/>All committers have signed the CLA. [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=747) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=747) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=747) it.</sub> tks
746,canal server Connection database timeout Server connection database timeout and then no longer reconnected after about two hours Automatically reconnected. Which parameter can be set? How long does it take to reconnect after a timeout? You changed to v1 0 25 version, v1 0 26 has a problem v1.0.25 Is there any parameter setting? I have no problem, look at the log, is there an error? The 25 version has been marked as not recommending the use of nio @WithLin The 26 version of this non-stop disconnection is too big. After 7 pm, this situation is more changed to 25 version, no problem to see if there is a problem with the timer. @WithLin Reporting is a simple connection timeout connect timeout Feeling may be caused by network fluctuations but It took 2 hours to regain the connection. Is it too long after the timeout? How long does it take to re-acquire the connection without parameters that can be set by myself? It’s okay to retry the network. while(true) catch How long does it take to rest and try again?
745,canal Service deletes the temp log under the log and the service does not restart, causing disk space to be occupied. Such as canal The server found that the disk was occupied but did not find the specific occupation problem through lsof |grep Delete found a large amount of the following information java 37924 37925 appops 2w REG 254 1 50965216675 136005 /home/appops/canal/logs/canal/canal.log6671827459413464.tmp (deleted) Checked that the file is deleted and the process is still alive and thus the space is occupied. It is necessary to kill the process to recover. But the service is always restored by killing. It is definitely not convenient. Is there any good solution?
744,1 0 26 server is always repeated something goes wrong with channel Server 1 0 26 2018-07-10 15:50:20.197 [destination = example address = /110.110.10.27:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position {"identity":{"slaveId":-1 "sourceAddress":{"address":"10.110.10.27" "port":3306}} "postion":{"gtid":"" "included":false "journalName":"mysql-bin.000007" "position":313931 "serverId":27 "timestamp":1531208924000}} 2018-07-10 15:50:20.203 [destination = example address = /10.110.10.27:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000007 position=313931 serverId=27 gtid= timestamp=1531208924000] 2018-07-10 15:50:21.402 [New I/O server worker #1-1] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400 Caused by : something goes wrong with channel:[id: 0x65065848 /192.168.37.1:60600 => /192.168.37.129:11111] exception=com.alibaba.otter.canal.store.CanalStoreException: no match ack positionLogPosition[identity=LogIdentity[sourceAddress=10.110.10.27/10.110.10.27:3306 slaveId=-1] postion=EntryPosition[included=false journalName=mysql-bin.000007 position=313931 serverId=27 gtid= timestamp=1531208924000]] 2018-07-10 15:50:31.406 [New I/O server worker #1-1] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x65065848 /192.168.37.1:60600 :> /192.168.37.129:11111] exception=java.nio.channels.ClosedChannelException at org.jboss.netty.channel.socket.nio.NioWorker.cleanUpWriteBuffer(NioWorker.java:649) at org.jboss.netty.channel.socket.nio.NioWorker.writeFromUserCode(NioWorker.java:370) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:137) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) at org.jboss.netty.channel.Channels.write(Channels.java:611) at org.jboss.netty.channel.Channels.write(Channels.java:578) at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:28) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:144) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.cleanup(ReplayingDecoder.java:542) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.channelDisconnected(ReplayingDecoder.java:450) at org.jboss.netty.channel.Channels.fireChannelDisconnected(Channels.java:360) at org.jboss.netty.channel.socket.nio.NioWorker.close(NioWorker.java:599) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:119) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) at org.jboss.netty.channel.Channels.close(Channels.java:720) at org.jboss.netty.channel.AbstractChannel.close(AbstractChannel.java:208) at org.jboss.netty.channel.ChannelFutureListener$1.operationComplete(ChannelFutureListener.java:46) at org.jboss.netty.channel.DefaultChannelFuture.notifyListener(DefaultChannelFuture.java:381) at org.jboss.netty.channel.DefaultChannelFuture.addListener(DefaultChannelFuture.java:148) at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:30) at com.alibaba.otter.canal.server.netty.NettyUtils.error(NettyUtils.java:51) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:200) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:444) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261) at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:350) at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) 2018-07-10 15:50:32.315 [destination = example address = /10.110.10.27:3306 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 4052 sqlstate = HY000 errmsg = A slave with the same server_uuid/server_id as this slave has connected to the master; the first event 'mysql-bin.000007' at 313931 the last event read from 'mysql-bin.000007' at 313931 the last byte read from 'mysql-bin.000007' at 313962. at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_102] 2018-07-10 15:50:32.317 [destination = example address = /10.110.10.27:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address 10.110.10.27/10.110.10.27:3306 has an error retrying. caused by java.io.IOException: Received error packet: errno = 4052 sqlstate = HY000 errmsg = A slave with the same server_uuid/server_id as this slave has connected to the master; the first event 'mysql-bin.000007' at 313931 the last event read from 'mysql-bin.000007' at 313931 the last byte read from 'mysql-bin.000007' at 313962. at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_102] 2018-07-10 15:50:32.318 [destination = example address = /10.110.10.27:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: Received error packet: errno = 4052 sqlstate = HY000 errmsg = A slave with the same server_uuid/server_id as this slave has connected to the master; the first event 'mysql-bin.000007' at 313931 the last event read from 'mysql-bin.000007' at 313931 the last byte read from 'mysql-bin.000007' at 313962. at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) at java.lang.Thread.run(Thread.java:745) ] 2018-07-10 15:50:41.415 [New I/O server worker #1-2] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400 Caused by : something goes wrong with channel:[id: 0x650a9f4a /192.168.37.1:60616 => /192.168.37.129:11111] exception=com.alibaba.otter.canal.store.CanalStoreException: no match ack positionLogPosition[identity=LogIdentity[sourceAddress=10.110.10.27/10.110.10.27:3306 slaveId=-1] postion=EntryPosition[included=false journalName=mysql-bin.000007 position=313931 serverId=27 gtid= timestamp=1531208924000]] 2018-07-10 15:50:46.048 [destination = example address = /10.110.10.27:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position {"identity":{"slaveId":-1 "sourceAddress":{"address":"10.110.10.27" "port":3306}} "postion":{"gtid":"" "included":false "journalName":"mysql-bin.000007" "position":313931 "serverId":27 "timestamp":1531208924000}} 2018-07-10 15:50:46.052 [destination = example address = /10.110.10.27:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000007 position=313931 serverId=27 gtid= timestamp=1531208924000] 2018-07-10 15:50:51.418 [New I/O server worker #1-2] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x650a9f4a /192.168.37.1:60616 :> /192.168.37.129:11111] exception=java.nio.channels.ClosedChannelException at org.jboss.netty.channel.socket.nio.NioWorker.cleanUpWriteBuffer(NioWorker.java:649) at org.jboss.netty.channel.socket.nio.NioWorker.writeFromUserCode(NioWorker.java:370) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:137) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) at org.jboss.netty.channel.Channels.write(Channels.java:611) at org.jboss.netty.channel.Channels.write(Channels.java:578) at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:28) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:144) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.cleanup(ReplayingDecoder.java:542) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.channelDisconnected(ReplayingDecoder.java:450) at org.jboss.netty.channel.Channels.fireChannelDisconnected(Channels.java:360) at org.jboss.netty.channel.socket.nio.NioWorker.close(NioWorker.java:599) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:119) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) at org.jboss.netty.channel.Channels.close(Channels.java:720) at org.jboss.netty.channel.AbstractChannel.close(AbstractChannel.java:208) at org.jboss.netty.channel.ChannelFutureListener$1.operationComplete(ChannelFutureListener.java:46) at org.jboss.netty.channel.DefaultChannelFuture.notifyListener(DefaultChannelFuture.java:381) at org.jboss.netty.channel.DefaultChannelFuture.addListener(DefaultChannelFuture.java:148) at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:30) at com.alibaba.otter.canal.server.netty.NettyUtils.error(NettyUtils.java:51) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:200) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:444) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261) at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:350) at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) 2018-07-10 15:51:00.260 [destination = example address = /10.110.10.27:3306 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 4052 sqlstate = HY000 errmsg = A slave with the same server_uuid/server_id as this slave has connected to the master; the first event 'mysql-bin.000007' at 313931 the last event read from 'mysql-bin.000007' at 313931 the last byte read from 'mysql-bin.000007' at 313962. at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_102] 2018-07-10 15:51:00.261 [destination = example address = /10.110.10.27:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address 10.110.10.27/10.110.10.27:3306 has an error retrying. caused by java.io.IOException: Received error packet: errno = 4052 sqlstate = HY000 errmsg = A slave with the same server_uuid/server_id as this slave has connected to the master; the first event 'mysql-bin.000007' at 313931 the last event read from 'mysql-bin.000007' at 313931 the last byte read from 'mysql-bin.000007' at 313962. at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_102] 2018-07-10 15:51:00.261 [destination = example address = /10.110.10.27:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: Received error packet: errno = 4052 sqlstate = HY000 errmsg = A slave with the same server_uuid/server_id as this slave has connected to the master; the first event 'mysql-bin.000007' at 313931 the last event read from 'mysql-bin.000007' at 313931 the last byte read from 'mysql-bin.000007' at 313962. at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) at java.lang.Thread.run(Thread.java:745) ] 2018-07-10 15:51:01.425 [New I/O server worker #1-1] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400 Caused by : something goes wrong with channel:[id: 0x3f8a301b /192.168.37.1:60627 => /192.168.37.129:11111] exception=com.alibaba.otter.canal.store.CanalStoreException: no match ack positionLogPosition[identity=LogIdentity[sourceAddress=10.110.10.27/10.110.10.27:3306 slaveId=-1] postion=EntryPosition[included=false journalName=mysql-bin.000007 position=313931 serverId=27 gtid= timestamp=1531208924000]] 2018-07-10 15:51:11.430 [New I/O server worker #1-1] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x3f8a301b /192.168.37.1:60627 :> /192.168.37.129:11111] exception=java.nio.channels.ClosedChannelException at org.jboss.netty.channel.socket.nio.NioWorker.cleanUpWriteBuffer(NioWorker.java:649) at org.jboss.netty.channel.socket.nio.NioWorker.writeFromUserCode(NioWorker.java:370) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:137) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) at org.jboss.netty.channel.Channels.write(Channels.java:611) at org.jboss.netty.channel.Channels.write(Channels.java:578) at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:28) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:144) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.cleanup(ReplayingDecoder.java:542) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.channelDisconnected(ReplayingDecoder.java:450) at org.jboss.netty.channel.Channels.fireChannelDisconnected(Channels.java:360) at org.jboss.netty.channel.socket.nio.NioWorker.close(NioWorker.java:599) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:119) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) at org.jboss.netty.channel.Channels.close(Channels.java:720) at org.jboss.netty.channel.AbstractChannel.close(AbstractChannel.java:208) at org.jboss.netty.channel.ChannelFutureListener$1.operationComplete(ChannelFutureListener.java:46) at org.jboss.netty.channel.DefaultChannelFuture.notifyListener(DefaultChannelFuture.java:381) at org.jboss.netty.channel.DefaultChannelFuture.addListener(DefaultChannelFuture.java:148) at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:30) at com.alibaba.otter.canal.server.netty.NettyUtils.error(NettyUtils.java:51) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:200) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:444) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261) at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:350) at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) Client official exampledemo ---------------- END ----> transaction id: 17715 ================> binlog[mysql-bin.00000732Ï:313931] executeTime : 1531208924000(2018-07-10 15:48:44) gtid : () delay : 317551ms ---------------- END ----> transaction id: 17715 ================> binlog[mysql-bin.000007:313931] executeTime : 1531208924000(2018-07-10 15:48:44) gtid : () delay : 317552ms process error!com.alibaba.otter.canal.protocol.exception.CanalClientException: something goes wrong with reason: something goes wrong with channel:[id: 0x092dc999 /192.168.37.1:60768 => /192.168.37.129:11111] exception=com.alibaba.otter.canal.store.CanalStoreException: no match ack positionLogPosition[identity=LogIdentity[sourceAddress=10.110.10.27/10.110.10.27:3306 slaveId=-1] postion=EntryPosition[included=false journalName=mysql-bin.000007 position=313931 serverId=27 gtid= timestamp=1531208924000]] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.receiveMessages(SimpleCanalConnector.java:338) ~[classes/:na] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:315) ~[classes/:na] at com.alibaba.otter.canal.example.AbstractCanalClientTest.process(AbstractCanalClientTest.java:123) ~[classes/:na] at com.alibaba.otter.canal.example.AbstractCanalClientTest$2.run(AbstractCanalClientTest.java:87) [classes/:na] at java.lang.Thread.run(Thread.java:722) [na:1.7.0_17] Database 10 2 12 MariaDB What problem is the big god to solve Received error packet: errno = 4052 sqlstate = HY000 errmsg = A slave with the same server_uuid/server_id as this slave has connected to the master; the first event 'mysql-bin.000007' at 313931 the last event read from 'mysql-bin.000007' at 313931 the last byte read from 'mysql-bin.000007' at 313962.
743,About MySQL8 support Consult us this project has MySQL8 Is there a plan to provide support? Will support You have started using mysql8. @agapple Ok Already started to replace 8
742,Mysql and canal deployed problems on Tencent Cloud just now The mysql canal service deployed on Tencent Cloud is also a startup canal on Tencent Cloud. The error message after the server is as follows Also let the operation of the home and the canal account 2018-07-10 11:03:29.165 [Druid-ConnectionPool-Create-525230202] ERROR com.alibaba.druid.pool.DruidDataSource - create connection SQLException url: jdbc:h2:../conf/yunjiradartrace899/h2;CACHE_SIZE=1000;MODE=MYSQL; errorCode 28000 state 28000 org.h2.jdbc.JdbcSQLException: Wrong user name or password [28000-196] at org.h2.message.DbException.getJdbcSQLException(DbException.java:345) ~[h2-1.4.196.jar:1.4.196] at org.h2.message.DbException.get(DbException.java:179) ~[h2-1.4.196.jar:1.4.196] at org.h2.message.DbException.get(DbException.java:155) ~[h2-1.4.196.jar:1.4.196] at org.h2.message.DbException.get(DbException.java:144) ~[h2-1.4.196.jar:1.4.196] at org.h2.engine.Engine.validateUserAndPassword(Engine.java:336) ~[h2-1.4.196.jar:1.4.196] at org.h2.engine.Engine.createSessionAndValidate(Engine.java:162) ~[h2-1.4.196.jar:1.4.196] at org.h2.engine.Engine.createSession(Engine.java:137) ~[h2-1.4.196.jar:1.4.196] at org.h2.engine.Engine.createSession(Engine.java:27) ~[h2-1.4.196.jar:1.4.196] at org.h2.engine.SessionRemote.connectEmbeddedOrServer(SessionRemote.java:354) ~[h2-1.4.196.jar:1.4.196] at org.h2.jdbc.JdbcConnection.<init>(JdbcConnection.java:116) ~[h2-1.4.196.jar:1.4.196] at org.h2.jdbc.JdbcConnection.<init>(JdbcConnection.java:100) ~[h2-1.4.196.jar:1.4.196] at org.h2.Driver.connect(Driver.java:69) ~[h2-1.4.196.jar:1.4.196] at com.alibaba.druid.pool.DruidAbstractDataSource.createPhysicalConnection(DruidAbstractDataSource.java:1513) ~[druid-1.1.9.jar:1.1.9] at com.alibaba.druid.pool.DruidAbstractDataSource.createPhysicalConnection(DruidAbstractDataSource.java:1578) ~[druid-1.1.9.jar:1.1.9] at com.alibaba.druid.pool.DruidDataSource$CreateConnectionThread.run(DruidDataSource.java:2466) ~[druid-1.1.9.jar:1.1.9] determine Account password is not wrong Close tablemeta first Tsdb&#39;s ability This error is mainly caused by the local H2 error. If you have a better understanding of H2, you can try to locate or fix it. H2 as a table The historical version of ddl will be stored based on the default jdbc configuration of H2. ``` canal.instance.tsdb.dir=${canal.file.data.dir:../conf}/${canal.instance.destination:} canal.instance.tsdb.url=jdbc:h2:${canal.instance.tsdb.dir}/h2;CACHE_SIZE=1000;MODE=MYSQL; canal.instance.tsdb.dbUsername=canal canal.instance.tsdb.dbPassword=canal ``` Interpretation is the first time you will create conf instance instance mv db and set the access password to canal canal. If the second reopen will verify xx mv db whether there are multiple processes at the same time will appear java lang IllegalStateException The file is Locked will also verify that the access password for this time is the same as when it was first created. If you really encounter some inexplicable problems, the omnipotent solution removes the conf corresponding xx mv db will reinitialize an h2 local file
741,Canal throughput is reduced sharply 2018-07-09 08:10:22.493 - clientId:1001 cursor:[mysql-bin.000274 21149132 1530956958000] address[/10.132.27.103:3306] 2018-07-09 10:11:27.493 - clientId:1001 cursor:[mysql-bin.000274 25141059 1530957528000] address[/10.132.27.103:3306] Found canal log error 2018-07-09 13:39:29.043 [destination = cube address = /10.132.27.103:3306 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Unexpected End Stream at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) ~[canal.parse-1.0.21.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:78) ~[canal.parse-1.0.21.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) [canal.parse-1.0.21.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) [canal.parse-1.0.21.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.7.0_55] 2018-07-09 13:39:29.044 [destination = cube address = /10.132.27.103:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /10.132.27.103:3306 has an error retrying. caused by java.io.IOException: Unexpected End Stream at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) ~[canal.parse-1.0.21.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:78) ~[canal.parse-1.0.21.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) ~[canal.parse-1.0.21.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) ~[canal.parse-1.0.21.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.7.0_55] 2018-07-09 13:39:29.047 [destination = cube address = /10.132.27.103:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:cube[java.io.IOException: Unexpected End Stream at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:78) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) at java.lang.Thread.run(Thread.java:745) Today, on the 9th, I found that the synchronous super slow is going to update dozens of times per hour. It takes 1 2 hours for today&#39;s synchronization. Can the normal producers of the normal environment of the producers also have the guidance of the experts to check the problem? Can test 1 0 26 SNAPSHOT Synchronized for high volume data DML It is recommended to increase the jvm memory From my test, as long as the memory resources are large enough The resolution speed is still very stable Refer to the performance in the FAQ https github com alibaba canal wiki FAQ
740,Some questions in the wiki AdminGuide https://github.com/alibaba/canal/wiki/AdminGuide Right Is there a slight blur in the introduction of the difference in spring assembly? ![image](https://user-images.githubusercontent.com/13183268/42429351-1335062e-836b-11e8-9d1e-d0566544cbf8.png) Store is not only memory mode, the store storage here should be only memory It won&#39;t be stored in zookeeper and file. Your understanding is correct Store is only the ability of memory
739,Canal client 1 0 26 Which maven is SNAPSHOT pulled? ```xml <dependency> <groupId>com.alibaba.otter</groupId> <artifactId>canal.client</artifactId> <version>1.0.26-SNAPSHOT</version> </dependency> ``` The client version of 1 0 26 is not available in which maven library. mvn clean install My client program should use 1 0 26 SNAPSHOT Is it possible to package the project and then pull it 1 0 26 SNAPSHOT jar package to join its own project? Can not directly pull from the maven library? 1 0 25 SNAPSHOT in the public network maven Have
738,How does Alibaba Cloud RDS use GTID The RDS version used on Alibaba Cloud is 5 6 before using binlog position to synchronize data and try to set it. After the canal instance gtidon true ``` java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = The slave is connecting using CHANGE MASTER TO MASTER_AUTO_POSITION = 1 but the master has purged binary logs containing GTIDs that the slave requires. ``` Is it necessary to modify the configuration of the RDS? The version of my canal is 1 0 26 This requires mysql to set the gtid ability You can search for errmsg. Or look at my gtid issue I also encountered a similar problem. Checked that gtid has been enabled. There is a search because the GIT_PURGED value on the main library is causing the slave library to be unable to synchronize. Other posts do not know the reason. @agapple
737,Perfection of Parallel parameter details Some small details 60%processor Anyway will be :16 Overwrite Second 60 processor * 16 Most likely not meeting the ringbuffer limit I am very careful to see it.
736,com.alibaba.fastsql.sql.parser.ParserException Scene TSDB is turned on Fastsql version 371 I have encountered two types of exceptions after upgrading to the 371 version of fastsql. One Student-service 2018-07-05 17:10:13.017 WARN ???? [6 EventParser] c.a.o.c.p.i.m.t.MemoryTableMeta : [] parse faield : ALTER TABLE `platform`.`notice` CHANGE COLUMN `content` `content` VARCHAR(3000) CHARACTER SET 'utf8mb4' COLLATE 'utf8mb4_unicode_ci' NOT NULL DEFAULT unfilled COMMENT default com.alibaba.fastsql.sql.parser.ParserException at com.alibaba.fastsql.sql.parser.SQLExprParser.parseCharTypeRest(SQLExprParser.java:2910) ~[Student-service.jar!/:?] at com.alibaba.fastsql.sql.parser.SQLExprParser.parseDataType(SQLExprParser.java:2785) ~[Student-service.jar!/:?] at com.alibaba.fastsql.sql.parser.SQLExprParser.parseDataType(SQLExprParser.java:2666) ~[Student-service.jar!/:?] at com.alibaba.fastsql.sql.dialect.mysql.parser.MySqlExprParser.parseColumn(MySqlExprParser.java:463) ~[Student-service.jar!/:?] at com.alibaba.fastsql.sql.dialect.mysql.parser.MySqlStatementParser.parseAlterTable(MySqlStatementParser.java:4315) ~[Student-service.jar!/:?] at com.alibaba.fastsql.sql.dialect.mysql.parser.MySqlStatementParser.parseAlter(MySqlStatementParser.java:3307) ~[Student-service.jar!/:?] at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:248) ~[Student-service.jar!/:?] at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[Student-service.jar!/:?] at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[Student-service.jar!/:?] two Student-service 2018-07-05 17:32:42.210 WARN ???? [6 EventParser] c.a.o.c.p.i.m.t.MemoryTableMeta : [] parse faield : ALTER TABLE `student` DROP COLUMN `display_name` DROP COLUMN `system` MODIFY COLUMN `id` bigint(20) UNSIGNED NOT NULL AUTO_INCREMENT FIRST CHANGE COLUMN `name` `student_name` varchar(255) CHARACTER SET utf8 COLLATE utf8_unicode_ci NOT NULL DEFAULT '' COMMENT Student name AFTER `id` MODIFY COLUMN `description` varchar(255) CHARACTER SET utf8 COLLATE utf8_unicode_ci NOT NULL DEFAULT '' COMMENT School description AFTER `student_name` MODIFY COLUMN `created_at` timestamp NOT NULL AFTER `description` MODIFY COLUMN `updated_at` timestamp NOT NULL AFTER `created_at` CHANGE COLUMN `creator` `created_by` varchar(50) CHARACTER SET utf8 COLLATE utf8_unicode_ci NOT NULL AFTER `updated_at` CHANGE COLUMN `updated_name` `updated_by` varchar(50) CHARACTER SET utf8 COLLATE utf8_unicode_ci NOT NULL AFTER `created_by` ADD COLUMN `is_deleted` tinyint(1) NOT NULL DEFAULT 0 COMMENT Delete tag 0 not deleted 1 delete AFTER `description` com.alibaba.fastsql.sql.parser.ParserException: syntax error error in :'MODIFY COLUMN `id` bigint(20) UNSIGN pos 86 line 4 column 9 token COLUMN at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:361) ~[Student-service.jar!/:?] at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:498) ~[Student-service.jar!/:?] at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[Student-service.jar!/:?] at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[Student-service.jar!/:?] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:72) [Student-service.jar!/:?] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.apply(DatabaseTableMeta.java:104) [Student-service.jar!/:?] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.apply(TableMetaCache.java:228) [Student-service.jar!/:?] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseQueryEvent(LogEventConvert.java:265) [Student-service.jar!/:?] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:126) [Student-service.jar!/:?] Your second SQL is also complicated enough. The exception caused by the first parseCharTypeRest is more preferential than the second common trouble. Test found Alter In Table COLLATE Will cause parsing to fail The 2 0 0_preview_186 version also has this problem and is not related to TSDB. This is the problem of fastsql&#39;s parsing support for DDL. I will record and feedback The latest fastsql has been fixed Will the latest sql version be ？
735,High CPU usage after starting the client Start client connection canal The server&#39;s corresponding java process usage rate in the cpu instantly increased to 40 - 50 does not seem normal between the two, I see the client The inside of the demo uses while Infinite loops may be the reason why cpu is high. Does the client obtain data through continuous training of the server? Client code is as follows CanalConnector connector = CanalConnectors.newSingleConnector(new InetSocketAddress(canalServerIp canalServerPort) serverDestination canalUserName canalPassword); new Thread( () -> { while(true){ try{ connector.connect(); connector.subscribe(monitorDatabase+"\\..*"); while(true){ Message message = connector.getWithoutAck(batchSize); // Get the specified amount of data long batchId = message.getId(); int size = message.getEntries().size(); if (batchId != -1 && size != 0) { try{ processData(message.getEntries()); connector.ack(batchId); // Submit confirmation //Thread.sleep(500); }catch (Exception e){ e.printStackTrace(); Connector rollback If an exception occurs, you need to roll back the last read binlog location } } } }catch (Exception e){ e.printStackTrace(); Logger error and canal Server interaction exception }finally { connector.disconnect(); } } } ).start(); The following is the performance parameters of my machine, please pay attention to the corresponding java process Processes: 346 total 6 running 340 sleeping 2038 threads 15:09:09 Load Avg: 5.62 5.26 5.03 CPU usage: 58.75% user 19.6% sys 22.18% idle SharedLibs: 185M resident 57M data 27M linkedit. MemRegions: 78596 total 7644M resident 207M private 1706M shared. PhysMem: 16G used (2031M wired) 136M unused. VM: 1565G vsize 1110M framework vsize 0(0) swapins 0(0) swapouts. Networks: packets: 38214974/2633M in 38184606/2541M out. Disks: 284609/7890M read 251982/9950M written. PID COMMAND %CPU TIME #TH #WQ #PORT MEM PURG CMPRS PGRP 3399 mdworker 0.1 00:00.06 4 2 44 3252K+ 0B 0B 3399 3398 mdworker 0.0 00:00.06 4 2 44 3272K 0B 0B 3398 3397 mdworker 0.1 00:00.06 4 2 44 3260K+ 0B 0B 3397 3391 com.apple.au 0.0 00:00.01 2 2 16 972K 0B 0B 3391 3353 Google Chrom 0.0 00:00.10 15 1 113 13M 0B 0B 526 **3347 java 25.9 01:28.67 144/2 1 326 1327M+ 0B 0B 665** 3344 top 4.1 00:23.44 1/1 0 30 3668K+ 0B 0B 3344 **3333 java 38.9 03:29.43 30 1 98 260M 0B 0B 665 3322 java 48.9 04:08.49 33/1 1 104 606M 0B 0B 3312** 3295 tail 0.0 00:00.14 1 0 10 360K 0B 0B 3295 Also my version is v1.0.24 After the client starts, the server will start parsing the work.
734,add create database support Increase create database Statement support [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=734) <br/>All committers have signed the CLA. tks
733,v1.0.24 Canal instance filter regex is invalid Use version 1 0 24 Just need to listen to the binlog of two tables Set in the instance properties file canal.instance.defaultDatabaseName = mall canal.instance.filter.regex = mall.users mall.users_extra_info The client code did not write a connector subscribe statement But still received the binlog of all the tables in the mall database. Solve Regular should only be hardcoded to be ok. The other regular does not support complex cautiousness.
732,When is the version of oracle supported open source? Such as the title Oracle this piece will not open source Can refer to https github com alibaba yugong based on materialized view to make increments The rest can refer to Oracle&#39;s LogMiner tool or CDC to achieve or
731,Canal shows that the connection is successful but the database message is changed and the subscription message is not received. Canal start does not report an error, but no matter how the database is operated, there is no database change log printed out to ask God to advise ![image](https://user-images.githubusercontent.com/8399936/42198827-071f8a10-7ebd-11e8-95ee-d0438707b334.png) I also encountered a similar problem with the 26 version of the client version 24 server zk cluster connection startup application can be consumed but after a period of time through the stack found that the client program is blocked in the semaphore acquisition where is not handled well Deadlocked 2 client instance consumption solution already settled It is recommended to use the new version of v1 1 1
730,canal increase create Database support Pulled out 1 0 26 preview3 branch test currently canal in processing Query event create Database does not parser the build statement Call logic LogEventConvert#parseQueryEvent -> DruidDdlParser.parse(queryString event.getDbName()); -> fastsql_preview366#SQLCreateDatabaseStatement At present, there is no code for fastsql, which is roughly modified as follows. ![qq 20180702202624](https://user-images.githubusercontent.com/5847660/42163743-3e061f4e-7e36-11e8-8126-24af0d1386cf.jpg) The latest trunk Master also did not deal with this problem fastsql when open source _ You submit a MR to me. #734 Raised a PR GitHub MR how to operate
729,update Why is the statement ddl ？ ![image](https://user-images.githubusercontent.com/5965173/42151092-603695a8-7e0e-11e8-8386-8920b4bc4d70.png) As shown mysql 5.5.24 Is there any problem with this version support? Found that there will be problems with updated data loss and parsing exceptions. ![image](https://user-images.githubusercontent.com/5965173/42151207-cd8a93f2-7e0e-11e8-8250-6ac2445ef3a3.png) DDL syntax parsing errors do not affect DML data Manually update the database to observe that some data can be in canal client Successfully received some conclusions that did not result in data loss The current canal open source version supports 5 7 and below versions of Ali internal mysql 5.7.13 5.6.10 mysql 5 5 18 and 5 1 40 48 Therefore 5 5 24 The official version does not guarantee that the reliability of the data is lost. Currently canal test has supported mysql 5 7 13 5 6 10 and below mariab 5 5 35 and 10 0 7 theoretically support the following versions But here&#39;s a look at 5 5 24 It should be supported, so I don’t know what the reason is. Please answer it. Thank you D. This error is not related to mariadb. It is a simple SQL syntax parsing error that cannot accurately filter data and is not tight. Using Mysql5 5 24 ， How to locate the situation when data update is lost? PS AC can&#39;t be added, can you pull it? 469673467 Thank you ：D
728,Jar package conflict spring code和canal common Reference canal client 1 0 12 after the code is reported org.springframework.core.MethodParameter（Cannot resolve method 'hasParameterAnnotation(java.lang.Class<>） spring-code 4.3.14.RELEASE operating <dependency> <groupId>com.alibaba.otter</groupId> <artifactId>canal.client</artifactId> <version>1.0.12</version> <exclusions> ...... <exclusion> <artifactId>canal.common</artifactId> <groupId>com.alibaba.otter</groupId> </exclusion> </exclusions> Normal after dependency Run code error java lang NoClassDefFoundError com/alibaba/otter/canal/common/utils/BooleanMutex How to solve You are also a bit low in the canal version. Upgrade to 1 0 26
727,Canal support aliyun Rds binlog subscription Aliyun is a great company and is currently the highest proportion of the domestic cloud market. There are too many small partners in front and I have feedback to expect more comprehensive support for RDS. Binlog subscription aliyun RDS mainly meets the user&#39;s simple and convenient use of MySQL. For the rapid development of user services, it is expected to use the aliyun for future storage computing scalability. [DRDS（ Distributed Relational Database Service https www aliyun com product drds one based on MySQL Sharding&#39;s distributed database solution can also be used for data subscription based on canal because the storage itself is MySQL. First, let&#39;s clear a few questions that are usually faced by binlog subscriptions for this type of cloud MySQL. 1. Account permission problem solved * Canal&#39;s strategy is to simulate MySQL Slave&#39;s behavior therefore needs to have a SELECT REPLICATION SLAVE REPLICATION CLIENT permissions * Solution The current account created by RDS on aliyun has its own permissions for RDS. 5 6 5 7 high-privilege instance can use the root account for additional authorization and authorization operations. Refer to QuickStart. 2. Binlog deleted problem solved * Corresponding binlog cleanup strategy https help aliyun com knowledge_detail 41815 html After more than 18 hours, it will be deleted and backed up to oss. If the canal task stops for more than 18 hours, it will encounter xx similar error. * Solution RDS provides oss by default for a while Binlog downloadability reference documentation https help aliyun com document_detail 26291 html canal can identify the timestamp in the site to compare RDS show binary The earliest binlog in the logs will be downloaded to the machine through the oss interface for parsing and smoothing the historical binlog before switching to RDS. Continue to consume in binlog Canal code will support ![image](https://user-images.githubusercontent.com/834743/43991319-e6de1772-9d9c-11e8-9407-fe74e48da1ce.png) 3. Problems caused by active/standby switchover solved * The general cloud MySQL master and backup solution uses the vip mode to shield the active/standby switch between the backend physical nodes. That is, for the canal, only the single node MySQL is seen. Ip will encounter xx similar errors when holding the old main library location for physical and standby switching. * Solutions Canal can be used for MySQL5 6 The gtid subscription method is not supported by local binlog parsing when there is a problem 2. Or it is recommended to automatically identify the active/standby switch based on the serverId. Check the serverId and the current database node in the bin each time the binlog subscription is performed. Whether the serverId is consistent. If the inconsistency indicates that the server generates the active/standby switchover, you can relocate the corresponding binlog in the new main library based on the timestamp and continue the subsequent consumption. Here you need to consider the case where the binlog is deleted when locating the site. Code submission https github com alibaba canal commit ea6391d1c3231406e241ef09217ea1c78f885373 Specific aliyun Rds using the documentation https github com alibaba canal wiki aliyun RDS QuickStart
726,Canal overall performance optimization There are more friends before Feedback can&#39;t keep up with the speed of large-scale data DML changes Feedback list of questions 1. https://github.com/alibaba/canal/issues/672 2. https://github.com/alibaba/canal/issues/547 3. https://github.com/alibaba/canal/issues/355 4. https://github.com/alibaba/canal/issues/267 Here will be a relatively complete test for targeted optimization and also welcome everyone&#39;s participation and code MR to work together to solve the problem of performance and stability. ps. 1 0 26 will be a milestone version ---- Final optimization results https github com alibaba canal wiki Performance ![image](https://user-images.githubusercontent.com/834743/42986955-1f40b0d6-8c2a-11e8-8eb5-c5b885d8040a.png) Like praise Looking forward to ing looking forward to ing awesome Let me first throw a brick to talk about the issue and some ideas I have encountered before. The scene is using load backup File way to fill data while using canal for synchronization One or more files per table because the rows in each file belong to the same event belonging to a table. INSERT mysql binlog will merge a certain amount of rows depending on binlog row event max size to the same event Single sentence DML affects multiple rows of data should also apply The compressed event is parsed into a protobuf object. The memory usage is exploding and the fgc is frequent and even oom. In MySQL The official version of 5 6 defaults the binlog row event max size from 1024 to 8192. At that time we were bypassed by setting the binlog row event max size back to 1024. If the server client Netty3 upgrade to 4 with PooledByteBuf should be able to ease a small part but big head or protobuf this memory hogs. Add ringbuffer Size is set to 4096 memory is 4g due to load backup After file is smooth normal traffic, so I don&#39;t want to adjust the memory or further reduce the ringbuffer The first step of network optimization is only to do header analysis to identify the packet size and other information without specific record level resolution. The result is the 32k socket set by default. Buffer for large throughput transmission is a bit too small to remove the default settings for socket self-coordination by default on the 24c96g physical machine test receiveBuffer coordination result is 180k Performance throughput comparison before and after adjustment 18MB VS 117MB provides 6 times more sockets Buffer optimization can basically run full network card The second step of optimizing the parsing ability ran a simple object parsing without protobuf object construction. The initial speed is 20MB. The main optimization time field is improved to 45MB. At present, there is a bottleneck in the observation system. The load cpu is up to 1 5 cores and jvm. Gc is mainly concentrated in the next optimization of the young area. Multi-thread parallel analysis maximizes the use of system load for optimization. The third step of the overall concurrency model design according to the previous optimization network and object analysis bottlenecks are in the depth analysis of the object. If you want to maximize the performance, you must introduce the concurrent design. ![image](https://user-images.githubusercontent.com/834743/42207319-3e58ac2c-7edc-11e8-966e-33f46c57e4bc.png) The concurrency model based on ringbuffer will be divided into four phases. The phases are separated from each other and depend on the analysis of the most consumable bottleneck. This multi-threaded model is used to accelerate the entire optimization. The entire analysis can run through the entire Gigabit network card. ps. If you have a 10G network card model, you can also help run it. @lcybo What optimization suggestions do you have for memory usage? Replace protobuf Personal immature thoughts Protostuff based on protobuf and java can be used without cross-language Relative advantage - Faster than the protobuf takes up less memory size。 - Can use POJO without proto file generated by protobuf Code is too unfriendly to people Insufficient region - Community is not very active - To be added haha Post some details // Re-use (manage) this buffer to avoid allocating on every serialization LinkedBuffer buffer = LinkedBuffer.allocate(512); Contrast protobuf byte[] result = new byte[this.getSerializedSize()]; In addition, POJO can also refer to netty&#39;s Recycler or other object pool theory. The peak of the Entry object in the ringbuffer event is the number of objects that the ObjectPool needs to cache. ProtostuffIOUtil.mergeFrom(protostuff **fooParsed** schema); fooParsed is available for deserialization that can benefit from ObjectPool Of course, doing this code will be more complicated. The multiplexing of byte is The idea based on ObjectPool has never been considered before. The various nested structures in the object are also different. Do not understand the principle of reuse @lcybo Parallel parsing how to guarantee binlog Event ordering agapple The method is quite a chestnut to put the result as satellite data countdownlatch or other blocking or parallelstream, and finally no reorder Agapple is not convenient to go back to the codeword on the outside claw machine @lan1994 The serial and parallel combination of serials completes the basic object parsing and mainly processes some time-consuming long-term parsing of specific field parsing protobuf objects such as DML. At present, the preliminary test results of 24 concurrent can basically put a 24core physical machine cpu to run full throughput can run to 80MB s From the binlog received to generate CanalEntry overall optimization before about 7MB s to increase an order of magnitude the current bottleneck is basically on the cpu string serialization ratio is very high afternoon will continue to test from mysql -> canal server -> canal Overall throughput of the client About nested structures Suppose Row nests Column&#39;s list Both classes use ObjectPool to implement the Recycable interface and the recycle method. Clear data fields when calling recycle on Row. Call recycle for each Column nest and then list clear It should be noted that the embedded Column life cycle should not be longer than Row. About Netty&#39;s Recycler Every object new will be bind ThreadLocal&#39;s reclaim stack If there is a recycle get in the same thread then there will be no competition at all. If the two are separated, it will involve the WeakOrderQueue shared by the potential threads. According to the serial shallow parsing concurrent parsing proto object ringbuffer serial get The processing logic serializes the point in the SessionHandler&#39;s get will cause the depth to resolve the competition between the threads. When the thread stack is empty, they will go to the WeakOrderQueue to steal some objects. It looks like at least the netty&#39;s Recycler is not suitable for this application unless the serialization is early. Put the result into the ringbuffer reference io.netty.util.Recycler<T> Object reference io.netty.util.internal.RecyclableArrayList Serialization in BTW sessionhandler NettyUtils.write(ctx.getChannel() packetBuilder.build().toByteArray() null);// Output Data packetBuilder build toByteArray every time new Byte should have optimized space The third step optimization has completed the analysis. This introduces the ringbufer model and is divided into multiple stages. The network receives the simple parsing DML depth parsing and delivery to the store. The most time-consuming DML depth parsing in the middle is replaced by the concurrent parsing model. Received a test from the binlog to generate the Entry object. 1. About 5MB s before parallelization Single thread 2. Do parallelization transformation 16 concurrent parsing is probably 80MB S Basically positive correlation linear expansion Basically run the cpu bottleneck. If you want to further optimize, you can only optimize the construction protocol of the Entry object. The fourth step optimization has begun to focus on the throughput of the binlog parsing to the client receiving data. The current step-by-step pressure measurement is probably binlog download throughput in the 8MB s Entry object throughput is about 55MB s is about 1 7 data expansion rate At present, the bottleneck seen by the profile is mainly when the server serializes the Entry object. Compared with the network packet, the record of 50 tests is about 100 bytes. The converted tps is about 20w. Record s My test data is more compact in batch insert and update binlog ---- If you optimize this theoretical Entry object, you can run full network bandwidth and expect to increase the overall 150. Compared to the unoptimized because the network transmission bottleneck at the last end is relatively large, so I have eaten the improvement brought by the previous optimizations. If we want to further optimize the design of the protobuf protocol. ---- Preliminary optimization 1. Multithreading constructs the serialization result of the Entry in advance to avoid the client. Temporary serialization when get operation Because there is a bottleneck in single thread 2. If you change the protobuf protocol, you can compress some of the data storage by designing some dictionary tables. ![image](https://user-images.githubusercontent.com/834743/42286259-d9dede60-7fe4-11e8-9220-4ae70469bd12.png) Read the code parallel parameter some details Pr 737 https github com alibaba canal pull 737 awesome I welcome everyone to submit PR. I am thinking about some optimization details of protobuf. The main bottleneck is the serialization and byte amplification of protobuf. I will try to ensure compatibility, but it does not rule out the incompatibility of protocol design changes in extreme cases. so. If you have a good idea, you can give me feedback as soon as possible. enum Compression { NONE = 1; ZLIB = 2; GZIP = 3; LZF = 4; } The compression is reserved in proto. If it is concurrently parsed, this step is additionally serialized and concurrently compressed. SessionHandler there _packetBuilder setBody compressed data _ Then the client serially receives the parallel decompression and deserialization Parallel compression network transmission VS Uncompressed network transmission does not know how to perform 1. Single-threaded compression and network transmission will be a balance point 2. Data compression ratio after protobuf serialization also needs to be evaluated The fourth step of optimization has been carried out for more than half of the approximate effect compared to the total unoptimized before the increase of 35 throughput can go to 8900 TPS This pressure measurement data and the previous batch of inserts have chosen a random library of the business to run a 19MB binlog. The network transmission is about 35MB. Optimization point 1. The serialization of the CanalEntry object is completed before entering the ringbuffer. The final sessionHandler only makes a ByteString copy. 2. SimpleCanalConnector adds lazyParseEntry parameter to support lazy parsing of CanalEntry object to reduce the cost of the entire get ack serial operation to maximize serial throughput The biggest bottleneck in the current profile analysis is that there are multiple array copies when constructing network transmissions. Current code ``` Packet.Builder packetBuilder = CanalPacket.Packet.newBuilder(); packetBuilder.setType(PacketType.MESSAGES); Messages.Builder messageBuilder = CanalPacket.Messages.newBuilder(); messageBuilder.setBatchId(message.getId()); if (message.getId() != -1) { if (message.isRaw()) { // for performance if (!CollectionUtils.isEmpty(message.getRawEntries())) { messageBuilder.addAllMessages(message.getRawEntries()); } } else { if (!CollectionUtils.isEmpty(message.getEntries())) { for (Entry entry : message.getEntries()) { messageBuilder.addMessages(entry.toByteString()); } } } } packetBuilder.setBody(messageBuilder.build().toByteString()); NettyUtils.write(ctx.getChannel() packetBuilder.build().toByteArray() null);// Output Data ``` 1. messageBuilder build toByteString will copy all the data of message getRawEntries to a byte. 2. packetBuilder build toByteArray will copy the entire data of the message to a packet byte. 3. NettyUtils.write Network sending The original 10MB record will be at least the original 3x targeted optimization is straight code one-time construction of the protobuf data format rather than through the builder + The way toByteString is expected to increase by about 10 Try to bypass the multiple copies of protobuf with a byte array. The next step is to optimize how to reuse the byte array to avoid opening a new byte array for each request. Open a 10MB memory block at a time or have some overhead including the client level. Test conclusion https://github.com/alibaba/canal/wiki/Performance ![image](https://user-images.githubusercontent.com/834743/42986934-0bae041a-8c2a-11e8-8f31-f3f8e016bc8d.png)
725,EventParser] ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta EventParser] ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - pls submit github issue show create table ddl:CREATE TABLE `batch_job_seq` ( `ID` bigint(20) NOT NULL `UNIQUE_KEY` char(1) NOT NULL UNIQUE KEY `UNIQUE_KEY_UN` (`UNIQUE_KEY`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 compare failed . db : TableMeta [schema=test table=batch_job_seq fileds= FieldMeta [columnName=ID columnType=bigint(20) defaultValue=null nullable=false key=false] FieldMeta [columnName=UNIQUE_KEY columnType=char(1) defaultValue=null nullable=false key=true] ] mem : TableMeta [schema=test table=batch_job_seq fileds= FieldMeta [columnName=ID columnType=bigint(20) defaultValue=null nullable=false key=false] FieldMeta [columnName=UNIQUE_KEY columnType=char(1) defaultValue=null nullable=false key=false] ] You are the old version 25. Upgrade to 26 already fixed @qingmu2017 Will you solve this problem after upgrading to 26? I am using 26 SNAPSHOT or have this problem.
724,scheudle applySnapshotToDB faield ![image](https://user-images.githubusercontent.com/9798724/42080735-3392c10c-7bb6-11e8-8f56-23829972429c.png) 2018-06-27 22:09:34.820 [[scheduler-table-meta-snapshot]] ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - scheudle applySnapshotToDB faield com.alibaba.otter.canal.parse.exception.CanalParseException: java.net.SocketException: Broken pipe (Write failed) Caused by: java.net.SocketException: Broken pipe (Write failed) at java.net.SocketOutputStream.socketWrite0(Native Method) ~[na:1.8.0_144] at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111) ~[na:1.8.0_144] at java.net.SocketOutputStream.write(SocketOutputStream.java:143) ~[na:1.8.0_144] at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.write(BioSocketChannel.java:35) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.writeBody0(PacketManager.java:42) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.writeBody(PacketManager.java:35) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.query(MysqlQueryExecutor.java:55) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:94) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.compareTableMetaDbAndMemory(DatabaseTableMeta.java:296) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.applySnapshotToDB(DatabaseTableMeta.java:251) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.access$100(DatabaseTableMeta.java:45) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta$2.run(DatabaseTableMeta.java:84) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_144] at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) [na:1.8.0_144] at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) [na:1.8.0_144] at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) [na:1.8.0_144] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_144] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_144] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144] This is a scheduled schedule task Tomorrow will be the table in memory Meta data to do checkpoint In the process of the process will compare the memory value and the table structure of the database This exception is generated when the database table structure is obtained. pipe I will add a retry to ensure the validity of the network. My solution at the time was commented out. Canal properties about tsdb Comments such as canal instance tsdb spring xml and setting canal instance tsdb enable false This is not to understand the complete turn off the table Meta time series versioning function @agapple Is there any solution to this problem? @qingmu2017 Just set canal instance tsdb enable false to close the table Meta time series function
723,EventParser Always parser when encountering SQL statement execution error 2018-06-28 11:43:52.463 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2018-06-28 11:43:52.470 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties] 2018-06-28 11:43:52.727 [main] WARN o.s.beans.GenericTypeAwarePropertyDescriptor - Invalid JavaBean property 'connectionCharset' being accessed! Ambiguous write methods found next to actually used [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.nio.charset.Charset)]: [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.lang.String)] 2018-06-28 11:43:52.817 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2018-06-28 11:43:52.817 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties] 2018-06-28 11:43:53.154 [main] ERROR com.alibaba.druid.pool.DruidDataSource - testWhileIdle is true validationQuery not set 2018-06-28 11:43:53.598 [main] INFO c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-example 2018-06-28 11:43:53.743 [main] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - subscribe filter change to .*\..* 2018-06-28 11:43:53.743 [main] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - start successful.... 2018-06-28 11:43:53.779 [destination = example address = /172.16.0.2:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position {"identity":{"slaveId":-1 "sourceAddress":{"address":"172.16.0.2" "port":3306}} "postion":{"gtid":"" "included":false "journalName":"mysql-bin.000004" "position":15872 "serverId":1 "timestamp":1530090101000}} 2018-06-28 11:43:54.357 [destination = example address = /172.16.0.2:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : GRANT ALL PRIVILEGES ON *.* TO 'canal'@'%' IDENTIFIED WITH 'mysql_native_password' AS '*E3619321C1A937C46A0D8BD1DAC39F93B27D4458' com.alibaba.fastsql.sql.parser.ParserException: syntax error error in :'IDENTIFIED WITH 'mysql_native_password' A' expect BY actual WITH pos 58 line 1 column 55 token WITH at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:361) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.parser.SQLParser.accept(SQLParser.java:369) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseGrant(SQLStatementParser.java:1022) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:266) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.applyHistoryOnMemory(DatabaseTableMeta.java:382) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:121) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:84) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171] 2018-06-28 11:43:54.357 [destination = example address = /172.16.0.2:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : GRANT SELECT REPLICATION SLAVE REPLICATION CLIENT ON *.* TO 'canal'@'%' IDENTIFIED WITH 'mysql_native_password' AS '*E3619321C1A937C46A0D8BD1DAC39F93B27D4458' com.alibaba.fastsql.sql.parser.ParserException: syntax error error in :'IDENTIFIED WITH 'mysql_native_password' A' expect BY actual WITH pos 89 line 1 column 86 token WITH at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:361) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.parser.SQLParser.accept(SQLParser.java:369) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseGrant(SQLStatementParser.java:1022) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:266) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.applyHistoryOnMemory(DatabaseTableMeta.java:382) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:121) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:84) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171] 2018-06-28 11:43:54.384 [destination = example address = /172.16.0.2:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000004 position=15872 serverId=1 gtid= timestamp=1530090101000] 2018-06-28 11:43:54.543 [destination = example address = /172.16.0.2:3306 EventParser] WARN c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : CREATE TABLE `t_comment` ( `id` bigint NOT NULL AUTO_INCREMENT `chapter_id` bigint not NULL `comment_member_id` bigint not NULL `comment_nickname` varchar(32) not NULL `comment_headicon` varchar(256) not NULL `is_reply` int(11) DEFAULT 0 COMMENT Number of responses `content` TEXT NOT NULL COMMENT content `agrees` int(11) NOT NULL DEFAULT 0 COMMENT Endorsed `product_time` bigint(20) DEFAULT '0' COMMENT Production time `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8 COMMENT = Comment form **2018-06-28 11:43:54.577 [destination = example address = /172.16.0.2:3306 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Cannot replicate anonymous transaction when @@GLOBAL.GTID_MODE = ON at file ./mysql-bin.000004 position 16561.; the first event 'mysql-bin.000004' at 15872 the last event read from './mysql-bin.000004' at 16626 the last byte read from './mysql-bin.000004' at 16626.** at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171] 2018-06-28 11:43:54.577 [destination = example address = /172.16.0.2:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /172.16.0.2:3306 has an error retrying. caused by java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Cannot replicate anonymous transaction when @@GLOBAL.GTID_MODE = ON at file ./mysql-bin.000004 position 16561.; the first event 'mysql-bin.000004' at 15872 the last event read from './mysql-bin.000004' at 16626 the last byte read from './mysql-bin.000004' at 16626. at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171] 2018-06-28 11:43:54.580 [destination = example address = /172.16.0.2:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Cannot replicate anonymous transaction when @@GLOBAL.GTID_MODE = ON at file ./mysql-bin.000004 position 16561.; the first event 'mysql-bin.000004' at 15872 the last event read from './mysql-bin.000004' at 16626 the last byte read from './mysql-bin.000004' at 16626. at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) at java.lang.Thread.run(Thread.java:748) ] = Cannot replicate anonymous transaction when @@GLOBAL.GTID_MODE = ON at file ./mysql-bin.000004 position 16561.; the first event 'mysql-bin.000004' at 15872 the last event read from './mysql-bin.000004' at 16626 the last byte read from './mysql-bin.000004' at 16626. MySQL exception
722,canal Server and mysql deployed on the same machine to resolve binlog failed Canal version 1 0 25 Mysql version 5 7 10 At the beginning I will be canal The server is deployed on the same machine as mysql to start the normal operation of the database. The binlog log is updated but the client of the canal is not read. The server is deployed on other machines to capture the binlog information. It is not clear why this phenomenon occurs or is it that I missed the configuration? canal 25 version is not recommended Replaced a version 1 0 24 indeed
721,canal Cannot read bin log OpenJDK 64-Bit Server VM warning: ignoring option PermSize=96m; support was removed in 8.0 OpenJDK 64-Bit Server VM warning: ignoring option MaxPermSize=256m; support was removed in 8.0 OpenJDK 64-Bit Server VM warning: UseCMSCompactAtFullCollection is deprecated and will likely be removed in a future release. 2018-06-27 14:20:28.945 [main] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## set default uncaught exception handler 2018-06-27 14:20:29.015 [main] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## load canal configurations 2018-06-27 14:20:29.020 [main] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## start the canal server. 2018-06-27 14:20:29.096 [main] INFO com.alibaba.otter.canal.deployer.CanalController - ## start the canal server[172.16.0.2:8765] 2018-06-27 14:20:29.794 [main] WARN o.s.beans.GenericTypeAwarePropertyDescriptor - Invalid JavaBean property 'connectionCharset' being accessed! Ambiguous write methods found next to actually used [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.nio.charset.Charset)]: [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.lang.String)] 2018-06-27 14:20:30.194 [main] ERROR com.alibaba.druid.pool.DruidDataSource - testWhileIdle is true validationQuery not set 2018-06-27 14:20:30.634 [destination = example address = /172.16.0.2:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /172.16.0.2:3306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: connect /172.16.0.2:3306 failure Caused by: java.io.IOException: connect /172.16.0.2:3306 failure at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:77) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:79) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:87) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:163) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171] Caused by: java.io.IOException: Error When doing Client Authentication:ErrorPacket [errorNumber=1045 fieldCount=-1 message=Access denied for user 'canal'@'172.16.0.2' (using password: YES) sqlState=28000 sqlStateMarker=#] at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:199) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:74) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] ... 4 common frames omitted 2018-06-27 14:20:30.645 [destination = example address = /172.16.0.2:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: connect /172.16.0.2:3306 failure Caused by: java.io.IOException: connect /172.16.0.2:3306 failure at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:77) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:79) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:87) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:163) at java.lang.Thread.run(Thread.java:748) Caused by: java.io.IOException: Error When doing Client Authentication:ErrorPacket [errorNumber=1045 fieldCount=-1 message=Access denied for user 'canal'@'172.16.0.2' (using password: YES) sqlState=28000 sqlStateMarker=#] at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:199) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:74) ... 4 more ] 2018-06-27 14:20:30.659 [main] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## the canal server is running now ...... You want mysql to create a canal with a name GRANT SELECT REPLICATION SLAVE REPLICATION CLIENT ON *.* TO 'canal'@'%' IDENTIFIED BY 'canal';
720,canal.instance.filter.regex Is there a limit to the length of this property value? For example, because the same database table that the same mysql needs to take in the demand is different, I am worried that this value is getting bigger and bigger. Is there a length limit? Regular expressions have a length limit. I think it is 4k.
719,Why can&#39;t clients connect to different instances? 2018-06-27 11:05:51 165 INFO (ZooKeeper.java:438)- Initiating client connection connectString=10.204.52.173:2181 10.204.52.174:2181 10.204.52.175:2181 sessionTimeout=90000 watcher=com.alibaba.otter.canal.common.zookeeper.ZkClientx@2ca65ce4 2018-06-27 11:05:51 184 INFO (ClientCnxn.java:1032)- Opening socket connection to server 10.204.52.174/10.204.52.174:2181. Will not attempt to authenticate using SASL (unknown error) 2018-06-27 11:05:51 186 INFO (ClientCnxn.java:876)- Socket connection established to 10.204.52.174/10.204.52.174:2181 initiating session 2018-06-27 11:05:51 190 INFO (ClientCnxn.java:1299)- Session establishment complete on server 10.204.52.174/10.204.52.174:2181 sessionid = 0x263fdc9445e0090 negotiated timeout = 40000 2018-06-27 11:05:51 191 INFO (ZkClient.java:449)- zookeeper state changed (SyncConnected) I have two clients connected to the same instance of the same server, why is it stuck? zookeeper state changed (SyncConnected)
718,v1.0.26 alpha 3 how to use v1.0.26 alpha 3 how to use Install example Configured in the middle Data is written to kafka Incomplete Missing data or
717,Ddl new field exception com mysql jdbc exceptions jdbc4 MySQLSyntaxErrorException Duplicate column name 'name2' Prerequisites configured multiple channels The source tables of each channel are the same table. The target table is a different table of the same library of another cluster. mysql5.7.20 Canal 1 0 26 ddl new field error is as follows 2018-06-26 15:02:25.895 [pipelineId = 15 taskName = LoadWorker] ERROR com.alibaba.otter.node.etl.load.LoadTask - [15] loadWork executor is error! data:EtlEventData[currNid=8 nextNid=8 desc=[MemoryPipeKey[identity=Identity[channelId=15 pipelineId=15 processId=89] time=1529996545875 dataType=DB_BATCH]] processId=89 startTime=1529996545271 endTime=<null> firstTime=1529996545000 batchId=6 number=1 size=<null> exts=<null> pipelineId=15] com.alibaba.otter.node.etl.load.exception.LoadException: java.util.concurrent.ExecutionException: com.alibaba.otter.node.etl.load.exception.LoadException: com.alibaba.otter.node.etl.load.exception.LoadException: org.springframework.jdbc.BadSqlGrammarException: StatementCallback; bad SQL grammar []; nested exception is com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Duplicate column name 'name2' Caused by: java.util.concurrent.ExecutionException: com.alibaba.otter.node.etl.load.exception.LoadException: com.alibaba.otter.node.etl.load.exception.LoadException: org.springframework.jdbc.BadSqlGrammarException: StatementCallback; bad SQL grammar []; nested exception is com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Duplicate column name 'name2' at java.util.concurrent.FutureTask.report(FutureTask.java:122) [na:1.8.0_102] at java.util.concurrent.FutureTask.get(FutureTask.java:192) [na:1.8.0_102] at com.alibaba.otter.node.etl.load.loader.db.DataBatchLoader.load(DataBatchLoader.java:107) ~[node.etl-4.2.16-SNAPSHOT.jar:na] at com.alibaba.otter.node.etl.load.loader.OtterLoaderFactory.load(OtterLoaderFactory.java:50) ~[node.etl-4.2.16-SNAPSHOT.jar:na] at com.alibaba.otter.node.etl.load.LoadTask$1.run(LoadTask.java:85) ~[node.etl-4.2.16-SNAPSHOT.jar:na] at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_102] at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_102] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_102] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_102] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_102] Caused by: com.alibaba.otter.node.etl.load.exception.LoadException: com.alibaba.otter.node.etl.load.exception.LoadException: org.springframework.jdbc.BadSqlGrammarException: StatementCallback; bad SQL grammar []; nested exception is com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Duplicate column name 'name2' Caused by: com.alibaba.otter.node.etl.load.exception.LoadException: org.springframework.jdbc.BadSqlGrammarException: StatementCallback; bad SQL grammar []; nested exception is com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Duplicate column name 'name2' It is estimated that DDL is repeatedly executed. Ignoring the current DDL exception can be skipped. How to ignore this? I am the otter configuration implementation of the otter only skip the select exception and skip the Load exception. The above is the load exception. But if you skip the load exception, you will be afraid to skip all load exceptions instead of just How to solve this problem for ddl exceptions? There is an option to ignore DDL exceptions Skipping exceptions will appear without syncing For example, the same source table is synchronized to different table targets of the same target library. Table 1 Synchronization success target table 2 is skipped and the exception is not synchronized.
716,tableMetaTSDB finds a place to enter an infinite loop In this class MysqlEventParser The following method **### /** * Find a transaction start position that is closest to the timestamp in the specified binlog according to the given timestamp. * EndPosition is given for the last binlog to avoid endless queries */ private EntryPosition findAsPerTimestampInSpecificLogFile(MysqlConnection mysqlConnection final Long startTimestamp final EntryPosition endPosition final String searchBinlogFile final Boolean justForPositionTimestamp)** I specified in my configuration file. canal.instance.master.journal.name=mysql-bin.000344 canal.instance.master.position=106199547 After turning on the tsdb function When the canal service starts The program will enter here to find the starting point Since the program is not found, it will be consistently found even larger than 106199547. Still looking for the lead has been out of the initialization phase and then it will appear Client cannot create cursor 。https://github.com/alibaba/canal/issues/715 Is the site not present or is it I am looking for this order. show binlog events in'mysql-bin.000344' mysql-bin.000344 106199500 Rotate 609344520 106199547 mysql-bin.000345;pos=4 This is the last record Then went to take 106199547 As canal instance master position Then the above source code enters the infinite loop Did you have no new data after getting a spot? Dataable But already entered the next file 106199547 Is the last site of mysql bin 000344 But here is Mysql bin 000345 file The latest 26 version optimizes the judgment exit condition as
715,canal1.0.25 After the otter client starts In zookeeper Not created in cursor Do not update binlog location information We have encountered this problem at the moment. https://github.com/alibaba/canal/issues/541 Disable tsdb function 1 0 25 version has some problems for 1 0 26
714,Filter does not work The server specifies the canal instance filter regex Client connector subscribe But when the server parses the binlog When encountering a log of a table that has been dropped by a table that does not need to be synchronized Still will report an error Say desc Db_name table_name failed In fact, these tables will not appear in the canal instance filter regex I don&#39;t know how to solve it.
713,CanalParseException: parse row data failed. CanalParseException: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first The synchronization process suddenly reports an error Mysql5 7 20 canal 1 0 25 related installation package manager deployer 4 2 15 tar gz node deployer 4 2 15 tar gz ps This canal nesting inside if the canal has a fixed version in the high version, the manage and node will also provide The corresponding repair version or the installation node canal independent deployment canal non-nested installation instructions The error message is as follows com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4832) ~[guava-18.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:160) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:759) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:428) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:114) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:66) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:337) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:184) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:145) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) [canal.parse-1.0.25.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_102] Caused by: java.io.IOException: should execute connector.connect() first at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.<init>(MysqlQueryExecutor.java:30) ~[canal.parse.driver-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:87) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMetaByDB(TableMetaCache.java:80) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:30) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:55) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:50) ~[canal.parse-1.0.25.jar:na] at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache.get(LocalCache.java:3937) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830) ~[guava-18.0.jar:na] ... 10 common frames omitted Look at Issue604 https github com alibaba canal issues 640 Upgrade to version 26 Ok, I will try it first. Thank you.
712,org.h2.message.DbException: General error: "java.lang.IllegalStateException: The file is locked: nio:/usr/local/canal/conf/business/h2.mv.db Version 1 0 25 The canal service starts with 2 instances and the following 2 problems occur. 1.canal Unauthenticated after server startup user Non-authenticated user connection 2 lock file 2018-06-22 14:08:32 database: flush org.h2.message.DbException: General error: "java.lang.IllegalStateException: The file is locked: nio:/usr/local/canal/conf/business/h2.mv.db [1.4.196/7]" [50000-196] at org.h2.message.DbException.get(DbException.java:168) at org.h2.message.DbException.convert(DbException.java:295) at org.h2.mvstore.db.MVTableEngine$1.uncaughtException(MVTableEngine.java:95) at org.h2.mvstore.MVStore.panic(MVStore.java:378) at org.h2.mvstore.MVStore.<init>(MVStore.java:361) at org.h2.mvstore.MVStore$Builder.open(MVStore.java:2930) at org.h2.mvstore.db.MVTableEngine$Store.open(MVTableEngine.java:155) at org.h2.mvstore.db.MVTableEngine.init(MVTableEngine.java:100) at org.h2.engine.Database.getPageStore(Database.java:2476) at org.h2.engine.Database.open(Database.java:697) at org.h2.engine.Database.openDatabase(Database.java:276) at org.h2.engine.Database.<init>(Database.java:270) at org.h2.engine.Engine.openSession(Engine.java:64) at org.h2.engine.Engine.openSession(Engine.java:176) at org.h2.engine.Engine.createSessionAndValidate(Engine.java:154) at org.h2.engine.Engine.createSession(Engine.java:137) at org.h2.engine.Engine.createSession(Engine.java:27) at org.h2.engine.SessionRemote.connectEmbeddedOrServer(SessionRemote.java:354) at org.h2.jdbc.JdbcConnection.<init>(JdbcConnection.java:116) at org.h2.jdbc.JdbcConnection.<init>(JdbcConnection.java:100) at org.h2.Driver.connect(Driver.java:69) at com.alibaba.druid.pool.DruidAbstractDataSource.createPhysicalConnection(DruidAbstractDataSource.java:1510) at com.alibaba.druid.pool.DruidAbstractDataSource.createPhysicalConnection(DruidAbstractDataSource.java:1575) at com.alibaba.druid.pool.DruidDataSource$CreateConnectionThread.run(DruidDataSource.java:2450) Caused by: org.h2.jdbc.JdbcSQLException: General error: "java.lang.IllegalStateException: The file is locked: nio:/usr/local/canal/conf/business/h2.mv.db [1.4.196/7]" [50000-196] at org.h2.message.DbException.getJdbcSQLException(DbException.java:345) ... 24 more Caused by: java.lang.IllegalStateException: The file is locked: nio:/usr/local/canal/conf/business/h2.mv.db [1.4.196/7] at org.h2.mvstore.DataUtils.newIllegalStateException(DataUtils.java:765) at org.h2.mvstore.FileStore.open(FileStore.java:173) at org.h2.mvstore.MVStore.<init>(MVStore.java:347) ... 19 more @agapple This error is still the first time to see if the last canal has an abnormal crash. This is the first time that these two situations have occurred in the official environment. @agapple The file is Locked is estimated to repeatedly start the same instance to check this file /usr/local/canal/conf/business/h2.mv.db The corresponding file handle is opened by which process
711,Canal connects Alibaba Cloud RDS The relevant rds configuration file does not seem to be ah, another canal instance rds open url https rds aliyuncs com This configuration is for the external network Is there an intranet address? This is RDS The configuration of binlog&#39;s openapi download is not yet fully formed. If you need to use it, you can understand the code. 参考 https github com alibaba canal wiki FAQ
710,java.io.IOException: connect /xxx:3306 failure:java.io.IOException: Unexpected End Start the canal service and report this error canal version 1 0 22 2018-06-19 11:26:44.358 [destination = example address = /xxx:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /xxx:3306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: connect /xxx:3306 failure:java.io.IOException: Unexpected End Stream at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.readBytesAsBuffer(PacketManager.java:22) at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.readHeader(PacketManager.java:13) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:199) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:71) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:52) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:85) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:158) at java.lang.Thread.run(Thread.java:745) Caused by: java.io.IOException: connect /xxx:3306 failure:java.io.IOException: Unexpected End Stream at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.readBytesAsBuffer(PacketManager.java:22) at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.readHeader(PacketManager.java:13) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:199) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:71) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:52) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:85) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:158) at java.lang.Thread.run(Thread.java:745) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:74) ~[canal.parse.driver-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:52) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:85) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:158) ~[canal.parse-1.0.22.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_11] Port 3306 is open java.io.IOException: connect /xxx:3306 failure:java.io.IOException: Unexpected End Stream Can try 1 0 26
709,Increase the running status monitoring of the canal kafka client Increase the running status monitoring of the canal kafka client kafka Client runs cluster and also canal The client waits for the first to start and waits for the blocking to start. based on Topic groupId preempts the directory in zookeeper as /otter/canal/topics/example/groupId tks @rewerma For the use of kafka, you can write a simple explanatory document for me. @agapple Instructions for use After packaging, use the canal folder under the kafka target as the server conf canal properties Configuration modification canal.withoutNetty = True other configuration items are the same as before Added conf kafka yml configuration servers: slave1:6667 slave2:6667 slave3:6667 # kafka Server address retries: 0 # kafka number of retries batchSize: 16384 # kafka Batch size lingerMs: 1 bufferMemory: 33554432 canalBatchSize: 50 # Lot size unit of canal k If the database write operation is large, it is recommended to change to 1M. filterTransactionEntry: true canalDestinations: - canalDestination: example # Canal instance topic: example # kafka topic partition: # Partition topics: # A destination can correspond to multiple topics - topic: example partition: canal kafka Client reference test Moderate com.alibaba.otter.canal.kafka.client.running.CanalKafkaClientExample
708,Data processing failure will be repeated after the rollback Because it is pressed Batch id If ack and rollback fail in one of the batches, the process starts to roll back and then reprocesses the batch data that has just failed to process. If the processing of the batch fails, the processing is successful. Repeated processing @agapple Repeat cannot be avoided That can only be weighted in your own code. I have encountered this situation I am persisting the data of the current batch to the database record exception information after catching the exception. Then submit the data of the batch normally In order to avoid repeated processing In addition to this measure, you need to avoid the situation of abnormal crash when you deal with half of it.
707,Canal starts normally Canal client subscription stuck Error log 2018 06 20 13:43:46.730 [ZkClient-EventThread-26-vm153:2181 vm154:2181 vm155:2181] ERROR org.I0Itec.zkclient.ZkEventThread - Error handling event ZkEvent[Data of /otter/canal/destinations/yuantong_order_mysql_10.1.240.87_3307_0/1001/running changed sent to com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor$1@4e7d0621] com.alibaba.otter.canal.protocol.exception.CanalClientException: something goes wrong in initRunning method. at com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor.initRunning(ClientRunningMonitor.java:142) at com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor$1.handleDataDeleted(ClientRunningMonitor.java:71) at org.I0Itec.zkclient.ZkClient$6.run(ZkClient.java:549) at org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71) Caused by: com.alibaba.otter.canal.protocol.exception.CanalClientException: java.net.ConnectException: Refuse to connect at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.doConnect(SimpleCanalConnector.java:171) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.access$000(SimpleCanalConnector.java:48) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector$1.processActiveEnter(SimpleCanalConnector.java:396) at com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor.processActiveEnter(ClientRunningMonitor.java:207) at com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor.initRunning(ClientRunningMonitor.java:118) ... 3 common frames omitted Caused by: java.net.ConnectException: Refuse to connect at sun.nio.ch.Net.connect0(Native Method) at sun.nio.ch.Net.connect(Net.java:454) at sun.nio.ch.Net.connect(Net.java:446) at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.doConnect(SimpleCanalConnector.java:132) ... 7 common frames omitted Sometimes it will always be stuck on this error and need to be restarted to be normal. But sometimes this error will occur but the last version that can be successfully subscribed to does not need to restart the canal is 24 I sometimes have this problem. Can you solve it? Try the latest v1 1 1
706,Why do I start two java client connections canal Server can&#39;t connect Start two client connections canalserver The second startup has been stuck Subcribe did not start there successfully
705,mysql5.7.22 canal prompt java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not open log file 2018-06-20 12:02:29.240 [destination = dev address = /192.168.255.2:3306 EventParser] INFO c.a.otter.canal.parse.inbound.mysql.MysqlConnection - COM_BINLOG_DUMP with position:BinlogDumpCommandPacket[binlogPosition=309144234 slaveServerId=1201 binlogFileName=mysql-bin.000333 command=18] 2018-06-20 12:02:29.244 [destination = dev address = /192.168.255.2:3306 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not open log file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) ~[canal.parse-1.0.25-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122) [canal.parse-1.0.25-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) [canal.parse-1.0.25-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171] 2018-06-20 12:02:29.244 [destination = dev address = /192.168.255.2:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /192.168.255.2:3306 has an error retrying. caused by java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not open log file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) ~[canal.parse-1.0.25-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122) ~[canal.parse-1.0.25-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) ~[canal.parse-1.0.25-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171] 2018-06-20 12:02:29.244 [destination = dev address = /192.168.255.2:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:dev[java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not open log file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) at java.lang.Thread.run(Thread.java:748) ] 2018-06-20 12:02:29.244 [destination = dev address = /192.168.255.2:3306 EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - disConnect MysqlConnection to /192.168.255.2:3306... Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not open log file Check the validity of binlog files and sites
704,Canal does not recognize the column when the primary key set by the table limits the length of the field CREATE TABLE `job_execution_script` ( `app_id` INT(11) NOT NULL `job_id` INT(11) NOT NULL `script_name` VARCHAR(512) NOT NULL DEFAULT '' `committer_name` VARCHAR(128) NOT NULL DEFAULT '' PRIMARY KEY (`app_id` `job_id` `script_name`(100) `committer_name`) ) ENGINE=INNODB DEFAULT CHARSET=utf8; When the table structure is as above, canal will report “unknow column `script_name`(100)” ERROR Thank you The exception stack is 2018-06-20 10:51:32.960 [destination = ido001 address = ido001/180.137.128.151:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:ido001[java.lang.RuntimeException: unknow column : `script_name`(100) at com.alibaba.otter.canal.parse.inbound.TableMeta.getFieldMetaByName(TableMeta.java:73) at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.processTableElement(MemoryTableMeta.java:215) at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.parse(MemoryTableMeta.java:151) at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.find(MemoryTableMeta.java:106) at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.compareTableMetaDbAndMemory(DatabaseTableMeta.java:288) at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.applySnapshotToDB(DatabaseTableMeta.java:251) at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:129) at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) at java.lang.Thread.run(Thread.java:745) ] You are using the latest version 1 0 26, which I remember has been fixed. I am using the 1 0 25 version of this problem still exists Try the 1 0 26 version
703,RDS binlog retention period RDS binlog defaults to 18 hours if I want to backtrack Re-spent from the specified cursor Should be unable to find the binlog file or the corresponding location RDSbinlog saved in oss needs to download oss ​​for local parsing 参考 https github com alibaba canal wiki FAQ
702,Specify whether the Filter does not work or can get the monitoring data of the entire library. `connector.subscribe("databaseName.tableName");` This code I specified the filter can still receive the binlog data of all the tables. Is there any problem with the configuration? Search for history issues The filter uses a regular expression. You need to escape this regex to use. For example, your example databaseName tableName needs to be converted into > databaseName\\\\.tableName Can be used in the regular expression to represent the meaning of any character is the point because a single backslash represents the escaping and therefore needs to be escaped, so it becomes the same as above. take a look FAQ : https://github.com/alibaba/canal/wiki/FAQ
701,canal It will appear after running for two days or so. Abnormally unstable Can&#39;t see what effect the canal Is there a mechanism for error recovery? com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:145) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:87) [canal.parse-1.0.25.jar:na] Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4832) ~[guava-18.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:759) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:160) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:428) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:114) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:66) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:337) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:184) ~[canal.parse-1.0.25.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171] Caused by: java.io.IOException: should execute connector.connect() first at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.<init>(MysqlQueryExecutor.java:30) ~[canal.parse.driver-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMetaByDB(TableMetaCache.java:80) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:30) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:55) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:50) ~[canal.parse-1.0.25.jar:na] at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache.get(LocalCache.java:3937) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830) ~[guava-18.0.jar:na] ... 10 common frames omitted At the same time, 3 database instances are connected. Only one of the instances is down and there seems to be no way to recover. Try the 1 0 26 version
700,Reduce the kafka dependent version to declare that the server&#39;s running is volatile tks
699,Could not find first log file name in binary log index file mysql:10.1.25-MariaDB Canal tried 1 0 26alpha3 1 0 25 Position is not configured in the configuration file Already rummaged through similar issues in the issues solution to try to remove canal Meta file h2 mv db file will not work After the mysql side changes canal Example also reported this first com.alibaba.fastsql.sql.parser.ParserException: syntax error error in :'utf8 PAGE_CHECKSUM=0 pos 626 line 13 column 50 token IDENTIFIER PAGE_CHECKSUM Next report this Could not find first log file name in binary log index file More specific anomalies 2018-06-15 11:30:11.805 [destination = example address = /10.10.38.39:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just show master status 2018-06-15 11:32:09.338 [destination = example address = /10.10.38.39:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TEMPORARY TABLE `ALL_PLUGINS` ( `PLUGIN_NAME` varchar(64) NOT NULL DEFAULT '' `PLUGIN_VERSION` varchar(20) NOT NULL DEFAULT '' `PLUGIN_STATUS` varchar(16) NOT NULL DEFAULT '' `PLUGIN_TYPE` varchar(80) NOT NULL DEFAULT '' `PLUGIN_TYPE_VERSION` varchar(20) NOT NULL DEFAULT '' `PLUGIN_LIBRARY` varchar(64) DEFAULT NULL `PLUGIN_LIBRARY_VERSION` varchar(20) DEFAULT NULL `PLUGIN_AUTHOR` varchar(64) DEFAULT NULL `PLUGIN_DESCRIPTION` longtext `PLUGIN_LICENSE` varchar(80) NOT NULL DEFAULT '' `LOAD_OPTION` varchar(64) NOT NULL DEFAULT '' `PLUGIN_MATURITY` varchar(12) NOT NULL DEFAULT '' `PLUGIN_AUTH_VERSION` varchar(80) DEFAULT NULL ) ENGINE=Aria DEFAULT CHARSET=utf8 PAGE_CHECKSUM=0 com.alibaba.fastsql.sql.parser.ParserException: syntax error error in :'utf8 PAGE_CHECKSUM=0 pos 723 line 15 column 50 token IDENTIFIER PAGE_CHECKSUM at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:361) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:498) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:84) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:32:09.340 [destination = example address = /10.10.38.39:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TEMPORARY TABLE `COLUMNS` ( `TABLE_CATALOG` varchar(512) NOT NULL DEFAULT '' `TABLE_SCHEMA` varchar(64) NOT NULL DEFAULT '' `TABLE_NAME` varchar(64) NOT NULL DEFAULT '' `COLUMN_NAME` varchar(64) NOT NULL DEFAULT '' `ORDINAL_POSITION` bigint(21) unsigned NOT NULL DEFAULT '0' `COLUMN_DEFAULT` longtext `IS_NULLABLE` varchar(3) NOT NULL DEFAULT '' `DATA_TYPE` varchar(64) NOT NULL DEFAULT '' `CHARACTER_MAXIMUM_LENGTH` bigint(21) unsigned DEFAULT NULL `CHARACTER_OCTET_LENGTH` bigint(21) unsigned DEFAULT NULL `NUMERIC_PRECISION` bigint(21) unsigned DEFAULT NULL `NUMERIC_SCALE` bigint(21) unsigned DEFAULT NULL `DATETIME_PRECISION` bigint(21) unsigned DEFAULT NULL `CHARACTER_SET_NAME` varchar(32) DEFAULT NULL `COLLATION_NAME` varchar(32) DEFAULT NULL `COLUMN_TYPE` longtext NOT NULL `COLUMN_KEY` varchar(3) NOT NULL DEFAULT '' `EXTRA` varchar(27) NOT NULL DEFAULT '' `PRIVILEGES` varchar(80) NOT NULL DEFAULT '' `COLUMN_COMMENT` varchar(1024) NOT NULL DEFAULT '' ) ENGINE=Aria DEFAULT CHARSET=utf8 PAGE_CHECKSUM=0 com.alibaba.fastsql.sql.parser.ParserException: syntax error error in :'utf8 PAGE_CHECKSUM=0 pos 1078 line 22 column 50 token IDENTIFIER PAGE_CHECKSUM at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:361) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:498) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:84) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:32:09.341 [destination = example address = /10.10.38.39:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TEMPORARY TABLE `EVENTS` ( `EVENT_CATALOG` varchar(64) NOT NULL DEFAULT '' `EVENT_SCHEMA` varchar(64) NOT NULL DEFAULT '' `EVENT_NAME` varchar(64) NOT NULL DEFAULT '' `DEFINER` varchar(189) NOT NULL DEFAULT '' `TIME_ZONE` varchar(64) NOT NULL DEFAULT '' `EVENT_BODY` varchar(8) NOT NULL DEFAULT '' `EVENT_DEFINITION` longtext NOT NULL `EVENT_TYPE` varchar(9) NOT NULL DEFAULT '' `EXECUTE_AT` datetime DEFAULT NULL `INTERVAL_VALUE` varchar(256) DEFAULT NULL `INTERVAL_FIELD` varchar(18) DEFAULT NULL `SQL_MODE` varchar(8192) NOT NULL DEFAULT '' `STARTS` datetime DEFAULT NULL `ENDS` datetime DEFAULT NULL `STATUS` varchar(18) NOT NULL DEFAULT '' `ON_COMPLETION` varchar(12) NOT NULL DEFAULT '' `CREATED` datetime NOT NULL DEFAULT '0000-00-00 00:00:00' `LAST_ALTERED` datetime NOT NULL DEFAULT '0000-00-00 00:00:00' `LAST_EXECUTED` datetime DEFAULT NULL `EVENT_COMMENT` varchar(64) NOT NULL DEFAULT '' `ORIGINATOR` bigint(10) NOT NULL DEFAULT '0' `CHARACTER_SET_CLIENT` varchar(32) NOT NULL DEFAULT '' `COLLATION_CONNECTION` varchar(32) NOT NULL DEFAULT '' `DATABASE_COLLATION` varchar(32) NOT NULL DEFAULT '' ) ENGINE=Aria DEFAULT CHARSET=utf8 PAGE_CHECKSUM=0 com.alibaba.fastsql.sql.parser.ParserException: syntax error error in :'utf8 PAGE_CHECKSUM=0 pos 1234 line 26 column 50 token IDENTIFIER PAGE_CHECKSUM at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:361) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:498) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:84) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:32:09.342 [destination = example address = /10.10.38.39:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TEMPORARY TABLE `PARAMETERS` ( `SPECIFIC_CATALOG` varchar(512) NOT NULL DEFAULT '' `SPECIFIC_SCHEMA` varchar(64) NOT NULL DEFAULT '' `SPECIFIC_NAME` varchar(64) NOT NULL DEFAULT '' `ORDINAL_POSITION` int(21) NOT NULL DEFAULT '0' `PARAMETER_MODE` varchar(5) DEFAULT NULL `PARAMETER_NAME` varchar(64) DEFAULT NULL `DATA_TYPE` varchar(64) NOT NULL DEFAULT '' `CHARACTER_MAXIMUM_LENGTH` int(21) DEFAULT NULL `CHARACTER_OCTET_LENGTH` int(21) DEFAULT NULL `NUMERIC_PRECISION` int(21) DEFAULT NULL `NUMERIC_SCALE` int(21) DEFAULT NULL `DATETIME_PRECISION` bigint(21) unsigned DEFAULT NULL `CHARACTER_SET_NAME` varchar(64) DEFAULT NULL `COLLATION_NAME` varchar(64) DEFAULT NULL `DTD_IDENTIFIER` longtext NOT NULL `ROUTINE_TYPE` varchar(9) NOT NULL DEFAULT '' ) ENGINE=Aria DEFAULT CHARSET=utf8 PAGE_CHECKSUM=0 com.alibaba.fastsql.sql.parser.ParserException: syntax error error in :'utf8 PAGE_CHECKSUM=0 pos 853 line 18 column 50 token IDENTIFIER PAGE_CHECKSUM at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:361) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:498) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:84) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:32:09.342 [destination = example address = /10.10.38.39:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TEMPORARY TABLE `PARTITIONS` ( `TABLE_CATALOG` varchar(512) NOT NULL DEFAULT '' `TABLE_SCHEMA` varchar(64) NOT NULL DEFAULT '' `TABLE_NAME` varchar(64) NOT NULL DEFAULT '' `PARTITION_NAME` varchar(64) DEFAULT NULL `SUBPARTITION_NAME` varchar(64) DEFAULT NULL `PARTITION_ORDINAL_POSITION` bigint(21) unsigned DEFAULT NULL `SUBPARTITION_ORDINAL_POSITION` bigint(21) unsigned DEFAULT NULL `PARTITION_METHOD` varchar(18) DEFAULT NULL `SUBPARTITION_METHOD` varchar(12) DEFAULT NULL `PARTITION_EXPRESSION` longtext `SUBPARTITION_EXPRESSION` longtext `PARTITION_DESCRIPTION` longtext `TABLE_ROWS` bigint(21) unsigned NOT NULL DEFAULT '0' `AVG_ROW_LENGTH` bigint(21) unsigned NOT NULL DEFAULT '0' `DATA_LENGTH` bigint(21) unsigned NOT NULL DEFAULT '0' `MAX_DATA_LENGTH` bigint(21) unsigned DEFAULT NULL `INDEX_LENGTH` bigint(21) unsigned NOT NULL DEFAULT '0' `DATA_FREE` bigint(21) unsigned NOT NULL DEFAULT '0' `CREATE_TIME` datetime DEFAULT NULL `UPDATE_TIME` datetime DEFAULT NULL `CHECK_TIME` datetime DEFAULT NULL `CHECKSUM` bigint(21) unsigned DEFAULT NULL `PARTITION_COMMENT` varchar(80) NOT NULL DEFAULT '' `NODEGROUP` varchar(12) NOT NULL DEFAULT '' `TABLESPACE_NAME` varchar(64) DEFAULT NULL ) ENGINE=Aria DEFAULT CHARSET=utf8 PAGE_CHECKSUM=0 com.alibaba.fastsql.sql.parser.ParserException: syntax error error in :'utf8 PAGE_CHECKSUM=0 pos 1323 line 27 column 50 token IDENTIFIER PAGE_CHECKSUM at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:361) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:498) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:84) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:32:09.343 [destination = example address = /10.10.38.39:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TEMPORARY TABLE `PLUGINS` ( `PLUGIN_NAME` varchar(64) NOT NULL DEFAULT '' `PLUGIN_VERSION` varchar(20) NOT NULL DEFAULT '' `PLUGIN_STATUS` varchar(16) NOT NULL DEFAULT '' `PLUGIN_TYPE` varchar(80) NOT NULL DEFAULT '' `PLUGIN_TYPE_VERSION` varchar(20) NOT NULL DEFAULT '' `PLUGIN_LIBRARY` varchar(64) DEFAULT NULL `PLUGIN_LIBRARY_VERSION` varchar(20) DEFAULT NULL `PLUGIN_AUTHOR` varchar(64) DEFAULT NULL `PLUGIN_DESCRIPTION` longtext `PLUGIN_LICENSE` varchar(80) NOT NULL DEFAULT '' `LOAD_OPTION` varchar(64) NOT NULL DEFAULT '' `PLUGIN_MATURITY` varchar(12) NOT NULL DEFAULT '' `PLUGIN_AUTH_VERSION` varchar(80) DEFAULT NULL ) ENGINE=Aria DEFAULT CHARSET=utf8 PAGE_CHECKSUM=0 com.alibaba.fastsql.sql.parser.ParserException: syntax error error in :'utf8 PAGE_CHECKSUM=0 pos 719 line 15 column 50 token IDENTIFIER PAGE_CHECKSUM at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:361) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:498) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:84) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:32:09.344 [destination = example address = /10.10.38.39:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TEMPORARY TABLE `PROCESSLIST` ( `ID` bigint(4) NOT NULL DEFAULT '0' `USER` varchar(128) NOT NULL DEFAULT '' `HOST` varchar(64) NOT NULL DEFAULT '' `DB` varchar(64) DEFAULT NULL `COMMAND` varchar(16) NOT NULL DEFAULT '' `TIME` int(7) NOT NULL DEFAULT '0' `STATE` varchar(64) DEFAULT NULL `INFO` longtext `TIME_MS` decimal(22 3) NOT NULL DEFAULT '0.000' `STAGE` tinyint(2) NOT NULL DEFAULT '0' `MAX_STAGE` tinyint(2) NOT NULL DEFAULT '0' `PROGRESS` decimal(7 3) NOT NULL DEFAULT '0.000' `MEMORY_USED` int(7) NOT NULL DEFAULT '0' `EXAMINED_ROWS` int(7) NOT NULL DEFAULT '0' `QUERY_ID` bigint(4) NOT NULL DEFAULT '0' `INFO_BINARY` blob `TID` bigint(4) NOT NULL DEFAULT '0' ) ENGINE=Aria DEFAULT CHARSET=utf8 PAGE_CHECKSUM=0 com.alibaba.fastsql.sql.parser.ParserException: syntax error error in :'utf8 PAGE_CHECKSUM=0 pos 774 line 19 column 50 token IDENTIFIER PAGE_CHECKSUM at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:361) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:498) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:84) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:32:09.344 [destination = example address = /10.10.38.39:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TEMPORARY TABLE `ROUTINES` ( `SPECIFIC_NAME` varchar(64) NOT NULL DEFAULT '' `ROUTINE_CATALOG` varchar(512) NOT NULL DEFAULT '' `ROUTINE_SCHEMA` varchar(64) NOT NULL DEFAULT '' `ROUTINE_NAME` varchar(64) NOT NULL DEFAULT '' `ROUTINE_TYPE` varchar(9) NOT NULL DEFAULT '' `DATA_TYPE` varchar(64) NOT NULL DEFAULT '' `CHARACTER_MAXIMUM_LENGTH` int(21) DEFAULT NULL `CHARACTER_OCTET_LENGTH` int(21) DEFAULT NULL `NUMERIC_PRECISION` int(21) DEFAULT NULL `NUMERIC_SCALE` int(21) DEFAULT NULL `DATETIME_PRECISION` bigint(21) unsigned DEFAULT NULL `CHARACTER_SET_NAME` varchar(64) DEFAULT NULL `COLLATION_NAME` varchar(64) DEFAULT NULL `DTD_IDENTIFIER` longtext `ROUTINE_BODY` varchar(8) NOT NULL DEFAULT '' `ROUTINE_DEFINITION` longtext `EXTERNAL_NAME` varchar(64) DEFAULT NULL `EXTERNAL_LANGUAGE` varchar(64) DEFAULT NULL `PARAMETER_STYLE` varchar(8) NOT NULL DEFAULT '' `IS_DETERMINISTIC` varchar(3) NOT NULL DEFAULT '' `SQL_DATA_ACCESS` varchar(64) NOT NULL DEFAULT '' `SQL_PATH` varchar(64) DEFAULT NULL `SECURITY_TYPE` varchar(7) NOT NULL DEFAULT '' `CREATED` datetime NOT NULL DEFAULT '0000-00-00 00:00:00' `LAST_ALTERED` datetime NOT NULL DEFAULT '0000-00-00 00:00:00' `SQL_MODE` varchar(8192) NOT NULL DEFAULT '' `ROUTINE_COMMENT` longtext NOT NULL `DEFINER` varchar(189) NOT NULL DEFAULT '' `CHARACTER_SET_CLIENT` varchar(32) NOT NULL DEFAULT '' `COLLATION_CONNECTION` varchar(32) NOT NULL DEFAULT '' `DATABASE_COLLATION` varchar(32) NOT NULL DEFAULT '' ) ENGINE=Aria DEFAULT CHARSET=utf8 PAGE_CHECKSUM=0 com.alibaba.fastsql.sql.parser.ParserException: syntax error error in :'utf8 PAGE_CHECKSUM=0 pos 1603 line 33 column 50 token IDENTIFIER PAGE_CHECKSUM at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:361) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:498) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:84) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:32:09.345 [destination = example address = /10.10.38.39:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TEMPORARY TABLE `SYSTEM_VARIABLES` ( `VARIABLE_NAME` varchar(64) NOT NULL DEFAULT '' `SESSION_VALUE` varchar(2048) DEFAULT NULL `GLOBAL_VALUE` varchar(2048) DEFAULT NULL `GLOBAL_VALUE_ORIGIN` varchar(64) NOT NULL DEFAULT '' `DEFAULT_VALUE` varchar(2048) DEFAULT NULL `VARIABLE_SCOPE` varchar(64) NOT NULL DEFAULT '' `VARIABLE_TYPE` varchar(64) NOT NULL DEFAULT '' `VARIABLE_COMMENT` varchar(2048) NOT NULL DEFAULT '' `NUMERIC_MIN_VALUE` varchar(21) DEFAULT NULL `NUMERIC_MAX_VALUE` varchar(21) DEFAULT NULL `NUMERIC_BLOCK_SIZE` varchar(21) DEFAULT NULL `ENUM_VALUE_LIST` longtext `READ_ONLY` varchar(3) NOT NULL DEFAULT '' `COMMAND_LINE_ARGUMENT` varchar(64) DEFAULT NULL ) ENGINE=Aria DEFAULT CHARSET=utf8 PAGE_CHECKSUM=0 com.alibaba.fastsql.sql.parser.ParserException: syntax error error in :'utf8 PAGE_CHECKSUM=0 pos 768 line 16 column 50 token IDENTIFIER PAGE_CHECKSUM at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:361) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:498) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:84) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:32:09.346 [destination = example address = /10.10.38.39:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TEMPORARY TABLE `TRIGGERS` ( `TRIGGER_CATALOG` varchar(512) NOT NULL DEFAULT '' `TRIGGER_SCHEMA` varchar(64) NOT NULL DEFAULT '' `TRIGGER_NAME` varchar(64) NOT NULL DEFAULT '' `EVENT_MANIPULATION` varchar(6) NOT NULL DEFAULT '' `EVENT_OBJECT_CATALOG` varchar(512) NOT NULL DEFAULT '' `EVENT_OBJECT_SCHEMA` varchar(64) NOT NULL DEFAULT '' `EVENT_OBJECT_TABLE` varchar(64) NOT NULL DEFAULT '' `ACTION_ORDER` bigint(4) NOT NULL DEFAULT '0' `ACTION_CONDITION` longtext `ACTION_STATEMENT` longtext NOT NULL `ACTION_ORIENTATION` varchar(9) NOT NULL DEFAULT '' `ACTION_TIMING` varchar(6) NOT NULL DEFAULT '' `ACTION_REFERENCE_OLD_TABLE` varchar(64) DEFAULT NULL `ACTION_REFERENCE_NEW_TABLE` varchar(64) DEFAULT NULL `ACTION_REFERENCE_OLD_ROW` varchar(3) NOT NULL DEFAULT '' `ACTION_REFERENCE_NEW_ROW` varchar(3) NOT NULL DEFAULT '' `CREATED` datetime DEFAULT NULL `SQL_MODE` varchar(8192) NOT NULL DEFAULT '' `DEFINER` varchar(189) NOT NULL DEFAULT '' `CHARACTER_SET_CLIENT` varchar(32) NOT NULL DEFAULT '' `COLLATION_CONNECTION` varchar(32) NOT NULL DEFAULT '' `DATABASE_COLLATION` varchar(32) NOT NULL DEFAULT '' ) ENGINE=Aria DEFAULT CHARSET=utf8 PAGE_CHECKSUM=0 com.alibaba.fastsql.sql.parser.ParserException: syntax error error in :'utf8 PAGE_CHECKSUM=0 pos 1228 line 24 column 50 token IDENTIFIER PAGE_CHECKSUM at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:361) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:498) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:84) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:32:09.347 [destination = example address = /10.10.38.39:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TEMPORARY TABLE `VIEWS` ( `TABLE_CATALOG` varchar(512) NOT NULL DEFAULT '' `TABLE_SCHEMA` varchar(64) NOT NULL DEFAULT '' `TABLE_NAME` varchar(64) NOT NULL DEFAULT '' `VIEW_DEFINITION` longtext NOT NULL `CHECK_OPTION` varchar(8) NOT NULL DEFAULT '' `IS_UPDATABLE` varchar(3) NOT NULL DEFAULT '' `DEFINER` varchar(189) NOT NULL DEFAULT '' `SECURITY_TYPE` varchar(7) NOT NULL DEFAULT '' `CHARACTER_SET_CLIENT` varchar(32) NOT NULL DEFAULT '' `COLLATION_CONNECTION` varchar(32) NOT NULL DEFAULT '' `ALGORITHM` varchar(10) NOT NULL DEFAULT '' ) ENGINE=Aria DEFAULT CHARSET=utf8 PAGE_CHECKSUM=0 com.alibaba.fastsql.sql.parser.ParserException: syntax error error in :'utf8 PAGE_CHECKSUM=0 pos 626 line 13 column 50 token IDENTIFIER PAGE_CHECKSUM at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:361) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:498) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:84) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:32:17.341 [destination = example address = /10.10.38.39:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000001×{G? position=4239821 serverId=3839 gtid=<null> timestamp=1529033212000] 2018-06-15 11:32:18.653 [destination = example address = /10.10.38.39:3306 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:32:18.655 [destination = example address = /10.10.38.39:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /10.10.38.39:3306 has an error retrying. caused by java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:32:18.658 [destination = example address = /10.10.38.39:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) at java.lang.Thread.run(Thread.java:748) ] 2018-06-15 11:32:30.620 [destination = example address = /10.10.38.39:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just show master status 2018-06-15 11:32:45.110 [destination = example address = /10.10.38.39:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000001×{G? position=4240327 serverId=3839 gtid=<null> timestamp=1529033528000] 2018-06-15 11:32:45.287 [destination = example address = /10.10.38.39:3306 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:32:45.288 [destination = example address = /10.10.38.39:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /10.10.38.39:3306 has an error retrying. caused by java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:32:45.289 [destination = example address = /10.10.38.39:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) at java.lang.Thread.run(Thread.java:748) ] 2018-06-15 11:32:56.095 [destination = example address = /10.10.38.39:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just show master status 2018-06-15 11:35:34.307 [destination = example address = /10.10.38.39:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000001×{G? position=4240471 serverId=3839 gtid=<null> timestamp=1529033565000] 2018-06-15 11:35:34.724 [destination = example address = /10.10.38.39:3306 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:35:34.725 [destination = example address = /10.10.38.39:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /10.10.38.39:3306 has an error retrying. caused by java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:35:34.725 [destination = example address = /10.10.38.39:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) at java.lang.Thread.run(Thread.java:748) ] 2018-06-15 11:35:52.169 [destination = example address = /10.10.38.39:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just show master status 2018-06-15 11:50:06.738 [destination = example address = /10.10.38.39:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000001×{G? position=4240821 serverId=3839 gtid=<null> timestamp=1529033734000] 2018-06-15 11:50:07.002 [destination = example address = /10.10.38.39:3306 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:50:07.003 [destination = example address = /10.10.38.39:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /10.10.38.39:3306 has an error retrying. caused by java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:50:07.006 [destination = example address = /10.10.38.39:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) at java.lang.Thread.run(Thread.java:748) ] 2018-06-15 11:50:24.984 [destination = example address = /10.10.38.39:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just show master status 2018-06-15 11:51:40.902 [destination = example address = /10.10.38.39:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000001×{G? position=4241171 serverId=3839 gtid=<null> timestamp=1529034606000] 2018-06-15 11:51:41.216 [destination = example address = /10.10.38.39:3306 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:51:41.217 [destination = example address = /10.10.38.39:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /10.10.38.39:3306 has an error retrying. caused by java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] PAGE_CHECKSUM This is the keyword of mariadb. Is the keyword of mariadb Look at this error is not recognized when creating a local memory table What if you need to replace the in-memory database with mysql? I closed tsdb and it is normal now. In the instance properties configured canal instance defaultDatabaseName aaa or will receive changes to all databases on the current instance. The meaning of this default is This default has no meaning I have seen the same problem.There is my log: environment:mysql5.7.16-log Source distribution canal:canal.deployer-1.0.24 2018-07-11 16:44:26.054 [destination = example address = /***.***.**.**:3307 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) [canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210) [canal.parse-1.0.22.jar:na] at java.lang.Thread.run(Unknown Source) [na:1.8.0_161] 2018-07-11 16:44:26.054 [destination = example address = /***.***.**.**:3307 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /***.***.**.**:3307 has an error retrying. caused by java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210) ~[canal.parse-1.0.22.jar:na] at java.lang.Thread.run(Unknown Source) [na:1.8.0_161] 2018-07-11 16:44:26.055 [destination = example address = /***.***.**.**:3307 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210) at java.lang.Thread.run(Unknown Source) ] sqlstate = HY000 errmsg = Could not find first log file name in binary log index file Binlog is deleted but I can execute the sql to find out that file show binary logs
698,ClientRunningMonitor shutdown delayExector threadpool #697 #697 tks
697,ClientRunningMonitor shutdown delayExector threadpool Version：1.0.26.alpha2 When the client has a ha switch in cluster mode, com alibaba otter canal client impl running ClientRunningMonitor Will submit Job to delayExector must execute delayExector shutdown in the stop method. Otherwise, the application cannot be gracefully shut down. Submit pr please assign the issue to me. Thanx.
696,Problems with canal1 0 24 and 1 0 25 Spring scene default-instance.xml The 1 0 24 version of the connector subscribe method sometimes gets stuck and does not get the binlog subscription message. The information on the zookeeper is normal but the binlog message is not consumed. 1 0 25 version will not have this problem Now the production environment is using 1 0 24. I want to ask if this is a bug already in 1 0 24? Kill a thread Dump to see where the card is generally not getting the running lock on zk is a few client nodes @nbqyqx As far as a client does not need zk direct connection, it can&#39;t receive binlog message for 1 025 and can receive 1 0 24. Now nothing can be done. 2018-06-15 10:32:57.010 [destination = szctest address = /47.97.185.74:3306 EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - connect MysqlConnection to /47.97.185.74:3306... 2018-06-15 10:32:57.072 [destination = szctest address = /47.97.185.74:3306 EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - handshake initialization packet received prepare the client authentication packet to send 2018-06-15 10:32:57.073 [destination = szctest address = /47.97.185.74:3306 EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - client authentication packet is sent out. 2018-06-15 10:32:57.105 [destination = szctest address = /47.97.185.74:3306 EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - disConnect MysqlConnection to /47.97.185.74:3306... 2018-06-15 10:32:57.105 [destination = szctest address = /47.97.185.74:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /47.97.185.74:3306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: connect /47.97.185.74:3306 failure:java.io.IOException: Error When doing Client Authentication:ErrorPacket [errorNumber=1045 fieldCount=-1 message=Access denied for user 'retl'@'113.116.141.191' (using password: YES) sqlState=28000 sqlStateMarker=#] at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:208) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:71) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:56) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:86) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:157) at java.lang.Thread.run(Unknown Source) Caused by: java.io.IOException: connect /47.97.185.74:3306 failure:java.io.IOException: Error When doing Client Authentication:ErrorPacket [errorNumber=1045 fieldCount=-1 message=Access denied for user 'retl'@'113.116.141.191' (using password: YES) sqlState=28000 sqlStateMarker=#] at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:208) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:71) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:56) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:86) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:157) at java.lang.Thread.run(Unknown Source) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:74) ~[canal.parse.driver-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:56) ~[canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:86) ~[canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:157) ~[canal.parse-1.0.24.jar:na] at java.lang.Thread.run(Unknown Source) [na:1.8.0_172] Silent 47 97 185 74 3306 This is my mysql But this is 啥 'retl'@'113.116.141.191' The reason is that the configuration in my instance is different from the configuration in the default instance xml. My instance is canal instance master dbUsername And xml uses canal instance dbUsername prior to When changing group Didn&#39;t get it back
695,Increase kafka on canal Binlog information generation consumption support 1 in canal The server directly subscribes to the destination to get the message and send it to kafka. 2 Add the canal withoutNetty attribute to the canal properties to close the netty service on the server side. 3 increase kafka&#39;s client to directly consume message data The original canal server -- netty -- message --> nio -- canal client Architecture Architecture supported by kafka canal server -- kafka prducer -- message --> KAFKA --> kafka consumer -- canal kafka client Great support! Looking forward to accept the patch. https://github.com/alibaba/canal/issues/669 Very nice PR, I will take a closer look Has been merged High efficiency ：）
694,Canal resolution speed problem In a library within 40 minutes today, we did 2kw data insertion operation resulting in a large number of binglog generation, but the canal parsing speed is very slow. It took nearly 6 hours to catch up with the actual site. I don&#39;t know how to tune this. Refer to the performance in the FAQ https github com alibaba canal wiki FAQ
693,Cannot monitor binlog Use 1 0 23 to listen when mysql binlog configuration log bin mysql186 bin this structure can not be monitored whether it is related to the name of the binlog file No one has reported any similar questions, please describe the steps to reproduce. It should be that I switched the cluster mode back and forth to clean up the files generated under zk.
692,The problem specified by the timestamp when the specified site is synchronized If you only use the timestamp to specify that the binlog file name is not used, the auxiliary code will get the binlog file at the head and tail position. If the dba is coarsely managed or otherwise, the binlog file will be physically deleted and the binlog start file will not be obtained. Key code ``` class MysqlEventParser 。。。 // Find the binlog location based on time private EntryPosition findByStartTimeStamp(MysqlConnection mysqlConnection Long startTimestamp) { EntryPosition endPosition = findEndPosition(mysqlConnection); EntryPosition startPosition = findStartPosition(mysqlConnection); 。。。 /* Query the current binlog location */ private EntryPosition findStartPosition(MysqlConnection mysqlConnection) { try { ResultSetPacket packet = mysqlConnection.query("show binlog events limit 1"); List<String> fields = packet.getFieldValues(); if (CollectionUtils.isEmpty(fields)) { throw new CanalParseException("command : 'show binlog events limit 1' has an error! pls check. you need (at least one of) the SUPER REPLICATION CLIENT privilege(s) for this operation"); } EntryPosition endPosition = new EntryPosition(fields.get(0) Long.valueOf(fields.get(1))); return endPosition; } catch (IOException e) { throw new CanalParseException("command : 'show binlog events limit 1' has an error!" e); } } ``` show binlog events limit 1 Will report an error > MySQL [(none)]> show binlog events limit 1; > ERROR 29 (HY000): File './mysqlmaster-bin.000001' not found (Errcode: 2 - No such file or directory) Is it used? show binary logs; Command acquisition size more than the 0 Files then use show binlog events It will be better to get the starting point information. Consider submitting a PR to me
691,Java cluster link error com.alibaba.otter.canal.protocol.exception.CanalClientException: java.io.IOException: end of stream when reading header at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:277) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:248) at com.alibaba.otter.canal.client.impl.ClusterCanalConnector.getWithoutAck(ClusterCanalConnector.java:174) at com.dangdang.canal.DDPimJob.run(DDPimJob.java:55) at java.lang.Thread.run(Unknown Source) Caused by: java.io.IOException: end of stream when reading header at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.read(SimpleCanalConnector.java:374) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.readNextPacket(SimpleCanalConnector.java:365) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.receiveMessages(SimpleCanalConnector.java:282) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:275) You still report the version number. I think it should be a template or a variety of questions. Look at what you are using is the 25 version
690,Can the canal instance filter regex support matching multiple schemas? For example, I have 3 database sub-libraries, respectively, pay_n_1 pay_n_2 Pay_n_3 Other libraries are not my concern, I only care about these three canal.instance.filter.regex How to write, I write as canal instance filter regex pay_n_ Does not work pay_n_\\\w+\\..\* that&#39;s fine
689,Can the canal instance be created manually? If the instance of the question is to be created manually each time, if it is automatically configured How should I configure it every time I seem to only automatically generate a folder but the client will report an error when connecting?
688,After the canal instance defaultDatabaseName is set to the specified library, you can also receive messages for the addition and deletion of other libraries. As the title I only specified an instance instance, only one library is specified, but all library changes can be monitored when consumed. Check out the history of the issue @agapple Thank you
687,Canal on windows The server will report the error from time to time. The remote host forcibly closes an existing connection. The error log is the instance log. The canal log does not report the error log as follows 2018-06-11 19:21:30.110 [New I/O server worker #1-1] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x0c80035e /10.204.241.135:58426 => /10.204.241.135:11111] exception=java.io.IOException: The remote host forcibly closed an existing connection at sun.nio.ch.SocketDispatcher.write0(Native Method) at sun.nio.ch.SocketDispatcher.write(Unknown Source) at sun.nio.ch.IOUtil.writeFromNativeBuffer(Unknown Source) at sun.nio.ch.IOUtil.write(Unknown Source) at sun.nio.ch.SocketChannelImpl.write(Unknown Source) at org.jboss.netty.channel.socket.nio.SocketSendBufferPool$PooledSendBuffer.transferTo(SocketSendBufferPool.java:243) at org.jboss.netty.channel.socket.nio.NioWorker.write0(NioWorker.java:470) at org.jboss.netty.channel.socket.nio.NioWorker.writeFromUserCode(NioWorker.java:388) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:137) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) at org.jboss.netty.channel.Channels.write(Channels.java:611) at org.jboss.netty.channel.Channels.write(Channels.java:578) at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:28) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:144) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:444) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261) at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:350) at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) at java.lang.Thread.run(Unknown Source) Mainly the timeout mechanism causes the 26 version to do some optimization Https github com alibaba canal issues 640 Take a look at this
686,canal server Swap is full and causes disk space 100 to be used Such as the question v1 0 26 alpha 2 version canal server Swap is full and causes disk space 100 to be used Swap memory %Cpu(s): 0.2 us 0.2 sy 0.0 ni 99.6 id 0.0 wa 0.0 hi 0.0 si 0.0 st KiB Mem: 8193776 total 8050240 used 143536 free 87860 buffers KiB Swap: 0 total 0 used 0 free 6344652 cached canal This swap of server cached Only increase or decrease, resulting in a final disk space usage of 100 Current rootfs 60G 12G 46G 20% / Restart canal Server later swap The cached is 0. The disk space is back to normal and then the next round is only increasing.
685,Database connection problem @agapple Hello, I am based on V25 now. Canceled the client Changed to eventStore2Kafka Now that the database connection has a problem, can you help me to see it? ![gf vm9d xhrh z s5 f ps](https://user-images.githubusercontent.com/24663485/41216549-63ed2b40-6d87-11e8-99d9-d1c750d0d7d9.png)
684,Turn on detection and often report Connection reset by peer See other issues saying that it is recommended to turn on the heartbeat, so turn it on canal.instance.detecting.enable =true canal.instance.detecting.heartbeatHaEnable = true The result is very easy to make mistakes when it does not affect the data capture. The next day, you can&#39;t capture the data. 1 0 25 version of my two parameters are false or unstable. This is the client interval for a while, even the server will be out of Connection. reset by Peer, but it is not necessarily in our alpha environment. I tested it in my local area for one afternoon. Recommended to update to version 26 Https github com alibaba canal issues 640 Take a look at this
683,Ask v1 0 26 alpha The 2 version has consumption accumulation and the server side disk is full. As the title asks at v1 0 26 alpha The 2 version yesterday had about four hours to find that the consumption of canal was delayed. The current consumption data is mainly due to the update data. The delay is the database that was updated two hours ago and then canal. Client only consumes insert data no problem other canal The server&#39;s disk does not know that it was occupied by the file. After the restart of the server, the disk space returned to normal. However, the service was not restored after a period of time. It was automatically recovered and no obvious error was found. So I would like to ask if you have encountered such a problem. And what is the cause of the problem? Thank you.
682,Fix #631 Boolean java type support tks
681,fix issue #680 [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=681) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=681) before we can accept your contribution.<br/><hr/>**winger** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=681) it.</sub> [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=681) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=681) before we can accept your contribution.<br/><hr/>**winger** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=681) it.</sub> This trunk has fixed a non-such change mode.
680,When table Unique key Tsdb parsing error when it is a non-standard field name Table structure as shown below ![image](https://user-images.githubusercontent.com/33280738/41092395-58152322-6a7b-11e8-9d54-3208c4863e27.png) canal Via fastsql The results of the analysis are as follows ![image](https://user-images.githubusercontent.com/33280738/41092506-9d2a60bc-6a7b-11e8-918b-407415af0e6d.png) And at this time canal by Name 200 does not correspond to the field name when comparing the memory with the actual table field Download the latest 1 0 26 release package
679,Merge pull request #1 from alibaba/master merge from master [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=679) <br/>All committers have signed the CLA. [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=679) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=679) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=679) it.</sub>
678,com.alibaba.otter.canal.parse.exception.CanalParseException: can't find start position for example ![image](https://user-images.githubusercontent.com/9798724/41083138-0893606a-6a63-11e8-89e7-9fad84702144.png) ![image](https://user-images.githubusercontent.com/9798724/41083182-1f12c8f8-6a63-11e8-81f8-66bb0c8e52d3.png) Can&#39;t find a suitable place
677,canal.instance.gtidon This default value does not force configuration to ensure forward compatibility version v1.0.26 alpha 3 2018-06-07 12:25:29.861 [main] ERROR c.a.o.c.common.zookeeper.running.ServerRunningMonitor - start failed com.google.common.collect.ComputationException: com.alibaba.otter.canal.common.CanalException: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'instance' defined in class path resource [spring/default-instance.xml]: Cannot resolve reference to bean 'eventParser' while setting bean property 'eventParser'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'eventParser' defined in class path resource [spring/default-instance.xml]: Initialization of bean failed; nested exception is org.springframework.beans.TypeMismatchException: Failed to convert property value of type 'java.lang.String' to required type 'boolean' for property 'isGTIDMode'; nested exception is java.lang.IllegalArgumentException: Invalid boolean value [] at com.google.common.collect.MapMaker$ComputingMapAdapter.get(MapMaker.java:889) ~[guava-18.0.jar:na] at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.start(CanalServerWithEmbedded.java:98) ~[canal.server-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.deployer.CanalController$2$1.processActiveEnter(CanalController.java:128) ~[canal.deployer-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.common.zookeeper.running.ServerRunningMonitor.processActiveEnter(ServerRunningMonitor.java:245) ~[canal.common-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.common.zookeeper.running.ServerRunningMonitor.initRunning(ServerRunningMonitor.java:150) ~[canal.common-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.common.zookeeper.running.ServerRunningMonitor.start(ServerRunningMonitor.java:104) ~[canal.common-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.deployer.CanalController.start(CanalController.java:415) [canal.deployer-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.deployer.CanalLauncher.main(CanalLauncher.java:38) [canal.deployer-1.0.26-SNAPSHOT.jar:na] Caused by: com.alibaba.otter.canal.common.CanalException: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'instance' defined in class path resource [spring/default-instance.xml]: Cannot resolve reference to bean 'eventParser' while setting bean property 'eventParser'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'eventParser' defined in class path resource [spring/default-instance.xml]: Initialization of bean failed; nested exception is org.springframework.beans.TypeMismatchException: Failed to convert property value of type 'java.lang.String' to required type 'boolean' for property 'isGTIDMode'; nested exception is java.lang.IllegalArgumentException: Invalid boolean value [] Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'instance' defined in class path resource [spring/default-instance.xml]: Cannot resolve reference to bean 'eventParser' while setting bean property 'eventParser'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'eventParser' defined in class path resource [spring/default-instance.xml]: Initialization of bean failed; nested exception is org.springframework.beans.TypeMismatchException: Failed to convert property value of type 'java.lang.String' to required type 'boolean' for property 'isGTIDMode'; nested exception is java.lang.IllegalArgumentException: Invalid boolean value [] at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:334) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE] at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:108) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE] at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyPropertyValues(AbstractAutowireCapableBeanFactory.java:1417) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE] at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1158) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE] at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:519) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE] at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:458) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE] at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:296) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE] at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:223) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE] at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:293) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE] at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:194) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE] at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:633) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE] at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:932) ~[spring-context-3.2.9.RELEASE.jar:3.2.9.RELEASE]
676,This is also 404 ah manager deployer xyz tar gz https://github.com/alibaba/otter/releases/download/otter-x.y.z/manager.deployer-x.y.z.tar.gz Replace xyz with a specific version
675,Server multiple instances can start up to 5 jobs at a time a canal server 15 instances are deployed. Only 5 instances can get the location information and restart the five are not fixed. Have one of them reported SHOW VIEW Command does not have permission to open this permission to the account Only select replication in the documentation slave，replication Client permission 2018-06-06 19:18:22.458 [destination = mysql.05.upanb.cn address = mysql.05.upanb.cn/192.168.250.214:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address mysql.05.upanb.cn/192.168.250.214:3306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: ErrorPacket [errorNumber=1142 fieldCount=-1 message=SHOW VIEW command denied to user 'canal'@'192.168.249.54' for table 'taglist' sqlState=42000 sqlStateMarker=#] with command: show create table `pointstore`.`admin`;show create table `pointstore`.`blacklist`;show create table `pointstore`.`clearuk`;show create table `pointstore`.`duibaorder`;show create table `pointstore`.`point_detail_description`;show create table `pointstore`.`returnpoint`;show create table `pointstore`.`s_admin`;show create table `pointstore`.`t_adminlog`;show create table `pointstore`.`t_broker`;show create table `pointstore`.`t_broker_copy`;show create table `pointstore`.`t_brokercompany`;show create table `pointstore`.`t_exchangerule`;show create table `pointstore`.`t_luckproduct`;show create table `pointstore`.`t_luckrecord`;show create table `pointstore`.`t_luckrecord2`;show create table `pointstore`.`t_luckrecord3`;show create table `pointstore`.`t_luckrecord4`;show create table `pointstore`.`t_oauthinfo`;show create table `pointstore`.`t_oauthinfo_bak1`;show create table `pointstore`.`t_oauthinfo_copy`;show create table `pointstore`.`t_pointactivity`;show create table `pointstore`.`t_pointdetail_new`;show create table `pointstore`.`t_pointmain_new`;show create table `pointstore`.`t_pointmain_new_20180212`;show create table `pointstore`.`t_pointmain_new_bak`;show create table `pointstore`.`t_pointmain_new_copy`;show create table `pointstore`.`t_pointproduct`;show create table `pointstore`.`t_pointrule`;show create table `pointstore`.`t_pointsetting`;show create table `pointstore`.`t_pointstoreadminauthority`;show create table `pointstore`.`t_pointstoreinvoice`;show create table `pointstore`.`t_pointstorevisitlog`;show create table `pointstore`.`t_productinventory`;show create table `pointstore`.`t_productpic`;show create table `pointstore`.`t_productscategory`;show create table `pointstore`.`t_productsexchange_new`;show create table `pointstore`.`t_productsexpand`;show create table `pointstore`.`t_productshow`;show create table `pointstore`.`t_provincecityarea`;show create table `pointstore`.`t_ruletocity`;show create table `pointstore`.`taglist`;show create table `pointstore`.`test`;show create table `pointstore`.`timelist`;show create table `pointstore`.`v_brokerpoint`;show create table `pointstore`.`v_luckrecord`;show create table `pointstore`.`v_pointstoreinvoicetotal`;show create table `pointstore`.`v_pointstoreinvoicetotal_a`;show create table `pointstore`.`v_productshow`;show create table `pointstore`.`v_productshowlist`;show create table `pointstore`.`v_productwarehouse`;show create table `pointstore`.`yearlist`; Caused by: java.io.IOException: ErrorPacket [errorNumber=1142 fieldCount=-1 message=SHOW VIEW command denied to user 'canal'@'192.168.249.54' for table 'taglist' sqlState=42000 sqlStateMarker=#] 2018-06-06 19:18:22.459 [destination = mysql.05.upanb.cn address = mysql.05.upanb.cn/192.168.250.214:3306 EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - disConnect MysqlConnection to mysql.05.upanb.cn/192.168.250.214:3306... 2018-06-06 19:18:22.459 [destination = mysql.05.upanb.cn address = mysql.05.upanb.cn/192.168.250.214:3306 EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - disConnect MysqlConnection to mysql.05.upanb.cn/192.168.250.214:3306.. Tsdb often report errors can not be used? 2018-06-07 00:44:16.671 [destination = mysql.13.upanb.cn address = mysql.13.upanb.cn/192.168.250.226:3306 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - pls submit github issue show create table ddl:CREATE TABLE `group_order_history_detail_new` ( `result_id` int(11) NOT NULL DEFAULT '0' COMMENT order number `year` int(11) NOT NULL DEFAULT '2018' COMMENT year `month` int(11) NOT NULL DEFAULT '2' COMMENT month `order_id` int(11) NOT NULL DEFAULT '0' COMMENT Order center order number `product_id` int(11) NOT NULL DEFAULT '0' COMMENT Project ID `product_name` varchar(50) NOT NULL DEFAULT '' COMMENT project name `broker_id` int(11) NOT NULL DEFAULT '0' COMMENT Broker ID `broker_name` varchar(50) NOT NULL DEFAULT '' COMMENT Broker name `broker_phone` varchar(50) NOT NULL DEFAULT '' COMMENT Broker phone number `building_id` int(11) NOT NULL DEFAULT '0' COMMENT Property ID `sale_id` int(11) NOT NULL DEFAULT '0' COMMENT Sales ID `sale_name` varchar(50) NOT NULL DEFAULT '' COMMENT Salesperson name `sale_group_id` int(11) NOT NULL DEFAULT '0' COMMENT Sales team ID `sale_group_name` varchar(45) NOT NULL DEFAULT '' COMMENT Sales team name `expand_id` int(11) NOT NULL DEFAULT '0' COMMENT Extended ID `expand_name` varchar(50) NOT NULL DEFAULT '' COMMENT Developer name `expand_group_id` int(11) NOT NULL DEFAULT '0' COMMENT Expansion team ID `expand_group_name` varchar(45) NOT NULL DEFAULT '' COMMENT Expansion team name `accounts_amount` bigint(20) NOT NULL DEFAULT '0' COMMENT Receivables `out_accounts_amount` bigint(20) NOT NULL DEFAULT '0' COMMENT Payable `actual_ascription_amount` bigint(20) NOT NULL DEFAULT '0' COMMENT Attributable income receivable `sale_actual_ascription_amount` bigint(20) NOT NULL DEFAULT '0' COMMENT Sales vesting income `expand_actual_ascription_amount` bigint(20) NOT NULL DEFAULT '0' COMMENT Expand vesting income `public_amount` bigint(20) NOT NULL DEFAULT '0' COMMENT Public pool income attributable income `cav_accounts_amount` bigint(20) NOT NULL DEFAULT '0' COMMENT Actually received less than 180 days `out_cav_accounts_amount` bigint(20) NOT NULL DEFAULT '0' COMMENT More than 180 days of actual receipts `sale_divide_rate` double(5 4) NOT NULL DEFAULT '0.0000' COMMENT Sales expansion share `subscription_time` timestamp NOT NULL DEFAULT '0000-00-00 00:00:00' ON UPDATE CURRENT_TIMESTAMP COMMENT Subscription time `handle_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT Customer transaction date `confirm_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT Upload receipt time `confirmation_cycle` int(11) NOT NULL DEFAULT '0' COMMENT Confirmation cycle `sale_actual_accounts_amount` bigint(20) NOT NULL DEFAULT '0' COMMENT Sales team receivable amount `sale_actual_cav_accounts_amount` bigint(20) NOT NULL DEFAULT '0' COMMENT Sales team received amount `expand_actual_accounts_amount` bigint(20) NOT NULL DEFAULT '0' COMMENT Expansion group receivable amount `expand_actual_cav_accounts_amount` bigint(20) NOT NULL DEFAULT '0' COMMENT Expansion team received amount `is_baoxiao` tinyint(4) NOT NULL DEFAULT '0' COMMENT Whether the order is underwritten Non-underwriting 0 Underwriting 1 Class underwriting `actual_cost` bigint(20) NOT NULL DEFAULT '0' COMMENT The actual sales expense is actually reported in the actual express report. `fixed_amount` bigint(20) NOT NULL DEFAULT '0' COMMENT fixed income `cav_fixed_amount` bigint(20) NOT NULL DEFAULT '0' COMMENT Returned fixed income `receivable_commission` bigint(20) NOT NULL DEFAULT '0' COMMENT Commission receivable `cav_commission` bigint(20) NOT NULL DEFAULT '0' COMMENT Commissioned `pay_commission` bigint(20) NOT NULL DEFAULT '0' COMMENT Commission payable `cav_commission` bigint(20) NOT NULL DEFAULT '0' COMMENT Commissioned `pay_commission` bigint(20) NOT NULL DEFAULT '0' COMMENT Commission payable `receivable_deal` bigint(20) NOT NULL DEFAULT '0' COMMENT Receivable award `cav_deal` bigint(20) NOT NULL DEFAULT '0' COMMENT Returned prize `pay_deal` bigint(20) NOT NULL DEFAULT '0' COMMENT Coping Deal Award `receivable_jump` bigint(20) NOT NULL DEFAULT '0' COMMENT Point of hop `cav_jump` bigint(20) NOT NULL DEFAULT '0' COMMENT Jumpback point `pay_jump` bigint(20) NOT NULL DEFAULT '0' COMMENT Coping point `receivable_premium` bigint(20) NOT NULL DEFAULT '0' COMMENT Receivable premium `cav_premium` bigint(20) NOT NULL DEFAULT '0' COMMENT Returned premium `pay_premium` bigint(20) NOT NULL DEFAULT '0' COMMENT Pay premium `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT Creation time `close_time` timestamp NULL DEFAULT NULL ON UPDATE CURRENT_TIMESTAMP COMMENT Order closing time `is_close` tinyint(4) NOT NULL DEFAULT '0' COMMENT 0 Order not closed 1 Order closed `sys_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT system time `is_delete` tinyint(4) NOT NULL DEFAULT '0' COMMENT delete or not 0 not deleted 1 delete UNIQUE KEY `index` (`result_id` `year` `month`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 compare failed . db : TableMeta [schema=tops_data_gdm table=group_order_history_detail_new fileds= FieldMeta [columnName=result_id columnType=int(11) defaultValue=0 nullable=false key=true] FieldMeta [columnName=year columnType=int(11) defaultValue=2018 nullable=false key=true] FieldMeta [columnName=month columnType=int(11) defaultValue=2 nullable=false key=true] FieldMeta [columnName=order_id columnType=int(11) defaultValue=0 nullable=false key=false] FieldMeta [columnName=product_id columnType=int(11) defaultValue=0 nullable=false key=false] FieldMeta [columnName=product_name columnType=varchar(50) defaultValue= nullable=false key=false] FieldMeta [columnName=broker_id columnType=int(11) defaultValue=0 nullable=false key=false] FieldMeta [columnName=broker_name columnType=varchar(50) defaultValue= nullable=false key=false] FieldMeta [columnName=broker_phone columnType=varchar(50) defaultValue= nullable=false key=false] FieldMeta [columnName=building_id columnType=int(11) defaultValue=0 nullable=false key=false] FieldMeta [columnName=sale_id columnType=int(11) defaultValue=0 nullable=false key=false] FieldMeta [columnName=sale_name columnType=varchar(50) defaultValue= nullable=false key=false] FieldMeta [columnName=sale_group_id columnType=int(11) defaultValue=0 nullable=false key=false] FieldMeta [columnName=sale_group_name columnType=varchar(45) defaultValue= nullable=false key=false] FieldMeta [columnName=expand_id columnType=int(11) defaultValue=0 nullable=false key=false] FieldMeta [columnName=expand_name columnType=varchar(50) defaultValue= nullable=false key=false] FieldMeta [columnName=expand_group_id columnType=int(11) defaultValue=0 nullable=false key=false] FieldMeta [columnName=expand_group_name columnType=varchar(45) defaultValue= nullable=false key=false] FieldMeta [columnName=accounts_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=out_accounts_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=actual_ascription_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=sale_actual_ascription_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=expand_actual_ascription_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=public_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=cav_accounts_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=out_cav_accounts_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=cav_accounts_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=out_cav_accounts_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=sale_divide_rate columnType=double(5 4) defaultValue=0.0000 nullable=false key=false] FieldMeta [columnName=subscription_time columnType=timestamp defaultValue=0000-00-00 00:00:00 nullable=false key=false] FieldMeta [columnName=handle_time columnType=timestamp defaultValue=CURRENT_TIMESTAMP nullable=false key=false] FieldMeta [columnName=confirm_time columnType=timestamp defaultValue=CURRENT_TIMESTAMP nullable=false key=false] FieldMeta [columnName=confirmation_cycle columnType=int(11) defaultValue=0 nullable=false key=false] FieldMeta [columnName=sale_actual_accounts_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=sale_actual_cav_accounts_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=expand_actual_accounts_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=expand_actual_cav_accounts_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=is_baoxiao columnType=tinyint(4) defaultValue=0 nullable=false key=false] FieldMeta [columnName=actual_cost columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=fixed_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=cav_fixed_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=receivable_commission columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=cav_commission columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=pay_commission columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=receivable_deal columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=cav_deal columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=pay_deal columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=receivable_jump columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=cav_jump columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=pay_jump columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=receivable_premium columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=cav_premium columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=pay_premium columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=create_time columnType=timestamp defaultValue=CURRENT_TIMESTAMP nullable=false key=false] FieldMeta [columnName=close_time columnType=timestamp defaultValue=null nullable=true key=false] FieldMeta [columnName=is_close columnType=tinyint(4) defaultValue=0 nullable=false key=false] FieldMeta [columnName=sys_time columnType=timestamp defaultValue=CURRENT_TIMESTAMP nullable=false key=false] FieldMeta [columnName=is_delete columnType=tinyint(4) defaultValue=0 nullable=false key=false] ] mem : TableMeta [schema=tops_data_gdm table=group_order_history_detail_new fileds= FieldMeta [columnName=result_id columnType=int(11) defaultValue=0 nullable=false key=false] FieldMeta [columnName=year columnType=int(11) defaultValue=2018 nullable=false key=false] FieldMeta [columnName=month columnType=int(11) defaultValue=2 nullable=false key=false] FieldMeta [columnName=order_id columnType=int(11) defaultValue=0 nullable=false key=false] FieldMeta [columnName=product_id columnType=int(11) defaultValue=0 nullable=false key=false] FieldMeta [columnName=product_name columnType=varchar(50) defaultValue= nullable=false key=false] FieldMeta [columnName=broker_id columnType=int(11) defaultValue=0 nullable=false key=false] FieldMeta [columnName=broker_name columnType=varchar(50) defaultValue= nullable=false key=false] FieldMeta [columnName=broker_phone columnType=varchar(50) defaultValue= nullable=false key=false] FieldMeta [columnName=building_id columnType=int(11) defaultValue=0 nullable=false key=false] FieldMeta [columnName=sale_id columnType=int(11) defaultValue=0 nullable=false key=false] FieldMeta [columnName=building_id columnType=int(11) defaultValue=0 nullable=false key=false] FieldMeta [columnName=sale_id columnType=int(11) defaultValue=0 nullable=false key=false] FieldMeta [columnName=sale_name columnType=varchar(50) defaultValue= nullable=false key=false] FieldMeta [columnName=sale_group_id columnType=int(11) defaultValue=0 nullable=false key=false] FieldMeta [columnName=sale_group_name columnType=varchar(45) defaultValue= nullable=false key=false] FieldMeta [columnName=expand_id columnType=int(11) defaultValue=0 nullable=false key=false] FieldMeta [columnName=expand_name columnType=varchar(50) defaultValue= nullable=false key=false] FieldMeta [columnName=expand_group_id columnType=int(11) defaultValue=0 nullable=false key=false] FieldMeta [columnName=expand_group_name columnType=varchar(45) defaultValue= nullable=false key=false] FieldMeta [columnName=accounts_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=out_accounts_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=actual_ascription_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=sale_actual_ascription_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=expand_actual_ascription_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=public_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=cav_accounts_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=out_cav_accounts_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=sale_divide_rate columnType=double(5 4) defaultValue=0.0000 nullable=false key=false] FieldMeta [columnName=subscription_time columnType=timestamp defaultValue=0000-00-00 00:00:00 nullable=false key=false] FieldMeta [columnName=handle_time columnType=timestamp defaultValue=CURRENT_TIMESTAMP nullable=false key=false] FieldMeta [columnName=confirm_time columnType=timestamp defaultValue=CURRENT_TIMESTAMP nullable=false key=false] FieldMeta [columnName=confirmation_cycle columnType=int(11) defaultValue=0 nullable=false key=false] FieldMeta [columnName=sale_actual_accounts_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=sale_actual_cav_accounts_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=expand_actual_accounts_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=expand_actual_cav_accounts_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=is_baoxiao columnType=tinyint(4) defaultValue=0 nullable=false key=false] FieldMeta [columnName=actual_cost columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=fixed_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=cav_fixed_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=receivable_commission columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=cav_commission columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=pay_commission columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=receivable_deal columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=cav_deal columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=pay_deal columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=receivable_jump columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=cav_jump columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=pay_jump columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=receivable_premium columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=cav_premium columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=pay_premium columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=create_time columnType=timestamp defaultValue=CURRENT_TIMESTAMP nullable=false key=false] FieldMeta [columnName=close_time columnType=timestamp defaultValue=null nullable=true key=false] FieldMeta [columnName=is_close columnType=tinyint(4) defaultValue=0 nullable=false key=false] FieldMeta [columnName=close_time columnType=timestamp defaultValue=null nullable=true key=false] FieldMeta [columnName=is_close columnType=tinyint(4) defaultValue=0 nullable=false key=false] FieldMeta [columnName=sys_time columnType=timestamp defaultValue=CURRENT_TIMESTAMP nullable=false key=false] FieldMeta [columnName=is_delete columnType=tinyint(4) defaultValue=0 nullable=false key=false] ] 2018-06-07 00:59:47.133 [destination = mysql.04.upanb.cn address = mysql.04.upanb.cn/192.168.250.213:3306 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - pls submit github issue show create table ddl:CREATE TABLE `tk_broker_tag` ( `broker_id` bigint(20) NOT NULL COMMENT Broker id `tag_id` bigint(20) NOT NULL COMMENT Tag id The primary key of the table tk_tag `tag_count` bigint(20) NOT NULL DEFAULT '0' COMMENT The number of times the broker was evaluated for the tag `sys_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP UNIQUE KEY `IX_broker_tag` (`broker_id` `tag_id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 compare failed . db : TableMeta [schema=tops_kber table=tk_broker_tag fileds= FieldMeta [columnName=broker_id columnType=bigint(20) defaultValue=null nullable=false key=true] FieldMeta [columnName=tag_id columnType=bigint(20) defaultValue=null nullable=false key=true] FieldMeta [columnName=tag_count columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=sys_time columnType=timestamp defaultValue=CURRENT_TIMESTAMP nullable=false key=false] ] mem : TableMeta [schema=tops_kber table=tk_broker_tag fileds= FieldMeta [columnName=broker_id columnType=bigint(20) defaultValue=null nullable=false key=false] FieldMeta [columnName=tag_id columnType=bigint(20) defaultValue=null nullable=false key=false] FieldMeta [columnName=tag_count columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=sys_time columnType=timestamp defaultValue=CURRENT_TIMESTAMP nullable=false key=false] ] You are this version of the new version has fixed this problem 1.0.25 Neal Hu nealhu@apache.org > in June 7, 2018 09 51 agapple <notifications@github.com> Write > > You are this version of the new version has fixed this problem > > — > You are receiving this because you authored the thread. > Reply to this email directly view it on GitHub or mute the thread. Use the latest 1 0 26
674,v1.0.26 alpha The 2 version of the client has a jar package corresponding to the maven library in which maven library? ![image](https://user-images.githubusercontent.com/9798724/41032611-83024bf2-69b6-11e8-89fd-542e572f7ed1.png) Download code for local clean Install
673,Local test connection remote canal does not seem to get the message I found that I can receive messages when I use the canal service locally. Locally seems to have never received a message when using canal on a remote server `CanalConnectors.newSingleConnector(new InetSocketAddress("xxx.xxx.xxx.xxx" 11111) "example" "" "");` You can see such a log in the example example log every time you connect to the local remote. `2018-06-06 17:57:15.309 [New I/O server worker #1-1] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - subscribe filter change to .*\..*` This seems to have been connected to the remote cannal service but the local has not received the bin log message. canal Did you see the log? Is there an error? I changed it again this morning. 1 0 26 alpha3 There is no problem on a MySQL5 6 40. The local test seems to be no problem. I downloaded the same version of the canal on another server and found that MySQL5 7 21 seems to be not stable sometimes. Local can receive the message but most of the time is not enough. I don&#39;t know if it is compatible with 5 7 21 for canal. Have a look at canal Whether the server&#39;s log has some links or parsing exceptions **example.log** ``` 2018-06-07 11:03:51.958 [Thread-4] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - stop successful.... 2018-06-07 11:04:30.064 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2018-06-07 11:04:30.068 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties] 2018-06-07 11:04:30.218 [main] WARN o.s.beans.GenericTypeAwarePropertyDescriptor - Invalid JavaBean property 'connectionCharset' being accessed! Ambiguous write methods found next to actually used [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.nio.charset.Charset)]: [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.lang.String)] 2018-06-07 11:04:30.268 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2018-06-07 11:04:30.268 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties] 2018-06-07 11:04:30.493 [main] ERROR com.alibaba.druid.pool.DruidDataSource - testWhileIdle is true validationQuery not set 2018-06-07 11:04:30.769 [main] INFO c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-example 2018-06-07 11:04:30.882 [main] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - subscribe filter change to .*\..* 2018-06-07 11:04:30.882 [main] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - start successful.... 2018-06-07 11:04:30.908 [destination = example address = /127.0.0.1:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position {"identity":{"slaveId":-1 "sourceAddress":{"address":"localhost" "port":3306}} "postion":{"gtid":"" "included":false "journalName":"mysql-bin.000002" "position":5818 "serverId":1 "timestamp":1528340336000}} 2018-06-07 11:04:31.332 [destination = example address = /127.0.0.1:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000002 position=5818 serverId=1 gtid= timestamp=1528340336000] 2018-06-07 11:05:17.176 [New I/O server worker #1-2] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - subscribe filter change to .*\..* 2018-06-07 11:07:50.895 [New I/O server worker #1-3] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - subscribe filter change to .*\..* ``` **canal.log** ``` 2018-06-07 11:03:51.829 [Thread-4] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## stop the canal server 2018-06-07 11:03:51.962 [Thread-4] INFO com.alibaba.otter.canal.deployer.CanalController - ## stop the canal Server IP security processing 11111 2018-06-07 11:03:51.962 [Thread-4] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## canal server is down. Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=96m; support was removed in 8.0 Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=256m; support was removed in 8.0 Java HotSpot(TM) 64-Bit Server VM warning: UseCMSCompactAtFullCollection is deprecated and will likely be removed in a future release. 2018-06-07 11:04:29.664 [main] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## set default uncaught exception handler 2018-06-07 11:04:29.707 [main] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## load canal configurations 2018-06-07 11:04:29.707 [main] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## start the canal server. 2018-06-07 11:04:29.749 [main] INFO com.alibaba.otter.canal.deployer.CanalController - ## start the canal Server IP security processing 11111 2018-06-07 11:04:30.218 [main] WARN o.s.beans.GenericTypeAwarePropertyDescriptor - Invalid JavaBean property 'connectionCharset' being accessed! Ambiguous write methods found next to actually used [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.nio.charset.Charset)]: [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.lang.String)] 2018-06-07 11:04:30.493 [main] ERROR com.alibaba.druid.pool.DruidDataSource - testWhileIdle is true validationQuery not set 2018-06-07 11:04:30.908 [destination = example address = /127.0.0.1:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position {"identity":{"slaveId":-1 "sourceAddress":{"address":"localhost" "port":3306}} "postion":{"gtid":"" "included":false "journalName":"mysql-bin.000002" "position":5818 "serverId":1 "timestamp":1528340336000}} 2018-06-07 11:04:30.933 [main] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## the canal server is running now ...... 2018-06-07 11:04:31.332 [destination = example address = /127.0.0.1:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000002 position=5818 serverId=1 gtid= timestamp=1528340336000] ``` Meta log to see the consumption situation The problem is solved because a Java program on this server of MySQL5 7 21 has been connected to the 11111 port. I saw the log and found that the data was consumed by the program, which caused me to consume less locally. Summarize suggestions for others 1） Because the local can not consume the remote canal information, please upgrade to the 1 0 26 version of the upgrade and solve the problem that I can not consume a remote 5 6 40 bin log data. 2 If you still can&#39;t consume data after the upgrade, please check if other applications are consuming bin log data. Thanks everyone.
672,Mysql generated binlog large amount of data when syncing to Canal slow how to solve this Did some large amount of data testing, such as updating a 100W table to synchronize to 8 8 seconds to reach Canal When I get the data, the batchid is 1 and then I guess the RingBuffer on the server side. doPut should not be put to the data I think it should be Mysql to Canal Server will have a blocking process. Is there any way to reduce the blocking time? For example, mysql5 7 multi-threaded parallel operation Help analyze several processes written to binlog -> Canal received -> Canal parsing storage to memory -> Client receives a look at the specific bottleneck point 1. Writing binlog involves small IO should not be a problem 2. canal Network IO IO Thread when going to fetch 3. When client When setting the value is relatively large The IO operation designed by the above three points We recently found through testing while (fetcher.fetch()) Pull the data qps at 1000 Because the canal read data is single-threaded, if the insertion of millions of data in a moment can be extended to the canal client, the extension is still very large. I don’t know if the landlord solved this problem. You are delaying now how long My 1 million data is about 25 points. 50W data 5 minutes or so 50W data 5 minutes or so Is the single threaded method used? Synchronize data to es Still redis Add me wx aleenjava Explore I did this by opening 2 threads. A thread keeps fetching data and puts it in the blocking queue. Then the second thread is consumed from the blocking queue and then sent to MQ in order. @DevWithLin Is your side based on canal analysis completed docking kafka? I am using it here. Based on the configuration interface, it is currently useless for rabbitmq and activemq kafka. Follow-up will complete kafka @agapple I also intend Let canalClient support Sky Walking I did this by opening 2 threads. A thread keeps fetching data and puts it in the blocking queue. Then the second thread is consumed from the blocking queue and then sent to MQ in order. This is sent to MQ in a single-threaded consumption order What is the difference? When the MQ transmission takes longer than from Canal Server This makes sense when you get data. Where is the code? Do you share it? It&#39;s so simple that you don&#39;t need code to open two threads and a blocking queue to simulate production consumption. I just use this way Achieved Didn’t reach your effect 1 million data probably 20 minutes I am here very quickly. Where did you send it? Write to es in Written in kafka Almost like maxwell @chenglinjava68 Guess so ES bottleneck, I wrote more than 8000 messages on rabbitmq 8000 per second It’s a bit exaggerated.
671,canal V1 0 25 version error java nio channels ClosedByInterruptException null 2018-06-05 00:50:56.419 [destination = example address = testrds1.amazonaws.com/10.8.3.18:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address testrds1.corlbeqxlnts.us-west-2.rds.amazonaws.com/10.8.3.180:3306 has an error retrying. caused by java.nio.channels.ClosedByInterruptException: null at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannel.read(SocketChannel.java:49) ~[canal.parse.driver-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:151) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:77) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:137) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) ~[canal.parse-1.0.25.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144] 2018-06-05 00:50:56.425 [destination = example address = testrds1.amazonaws.com/10.8.3.18:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.nio.channels.ClosedByInterruptException at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannel.read(SocketChannel.java:49) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:151) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:77) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:137) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) at java.lang.Thread.run(Thread.java:748) ] Try 26 version 看下 Issue604 https github com alibaba canal issues 640 
670,Support mongo es synchronization [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=670) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=670) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=670) it.</sub> [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=670) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=670) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=670) it.</sub> 1. This change is a bit large and contains mongo&#39;s parsing and synchronization features should be split into client code. 2. The code has some conflicts
669,Canal Design based on Server What is the purpose of Client mode? Canal Design based on Server What is the purpose of Client mode? Is it more reliable and efficient to parse binlog directly on the server side and consume it? The question of the landlord can ask the meaning of the distributed system. In addition to making the programmable client more lightweight, what else to consider, the server can also be made distributed, and currently supports the server. Ha In order to facilitate the operation and maintenance management can also be directly embedded into the server directly connected to mysql For such efficiency and reliability will be higher, we intend to package canal The client&#39;s code support is configured as an embedded server. It also abstracts the extensible outconnector. The spi mechanism is currently doing the hbase es log4j connector to consider contributing to the community. Very welcome to submit PR The corresponding kafka related code has been merged Welcome to download https github com alibaba canal releases tag canal 1 0 26 preview 3
668,Canal Server Will the binlog offset information be synchronized when HA is switched? Will be able to record the location in the zookeeper Switch to default instance xml Record position information with zk Switch to StandBy DB Binlog It will not be right for this situation. It needs manual processing. I always thought that the HA here is automatic, but it would not be so simple. Forgive me for being in this closed The issue continues to ask because there is no place where it can communicate directly and efficiently.
667,canal Broken pipe Look at the code seems to be to update the meta_snapshot in MySQL What is the use of this table? ## canal Log ```bash 2018-06-03 21:24:30.826 [[scheduler-table-meta-snapshot]] ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - scheudle applySnapshotToDB faield com.alibaba.otter.canal.parse.exception.CanalParseException: java.net.SocketException: Broken pipe (Write failed) Caused by: java.net.SocketException: Broken pipe (Write failed) at java.net.SocketOutputStream.socketWrite0(Native Method) ~[na:1.8.0_171] at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111) ~[na:1.8.0_171] at java.net.SocketOutputStream.write(SocketOutputStream.java:143) ~[na:1.8.0_171] at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.write(BioSocketChannel.java:35) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.writeBody0(PacketManager.java:42) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.writeBody(PacketManager.java:35) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.query(MysqlQueryExecutor.java:55) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:94) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.compareTableMetaDbAndMemory(DatabaseTableMeta.java:296) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.applySnapshotToDB(DatabaseTableMeta.java:251) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.access$100(DatabaseTableMeta.java:45) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta$2.run(DatabaseTableMeta.java:84) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_171] at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) [na:1.8.0_171] at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) [na:1.8.0_171] at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) [na:1.8.0_171] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_171] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_171] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171] ``` ##canal-client ```bash mory=0.024801254 InstallDirSize=11.233233 LastUpdatetime=2018-05-31 17:01:28.0}] old=[\{LastUpdatetime=2018-05-31 17:01:23.0}]} 10:11:45.401 [pool-6-thread-1] INFO com.tops001.foundation.canal.connector.CanalLog4JConnector - \{"data":[\{"Id":1 NodeName new node "NodeCreateTime":1484977299000 "NodeIP":"WIN-8RG5KGC1JNM" "NodeLastUpdatetime":1527757293000 "IfCheckState":false}] "database":"tops_bops_task" "old":[\{"NodeLastUpdatetime":1527757288000}] "sql":"" "table":"task_node" "ts":1528078305401 "type":"UPDATE"} 10:11:45.401 [pool-6-thread-1] DEBUG com.tops001.foundation.canal.connector.es.syn.ElasticsearchSynService - Dml\{database='tops_bops_task' table='task_node' type='UPDATE' ts=1528078305401 sql='' data=[\{Id=1 NodeName new node NodeCreateTime=2017-01-21 13:41:39.0 NodeIP=WIN-8RG5KGC1JNM NodeLastUpdatetime=2018-05-31 17:01:33.0 IfCheckState=false}] old=[\{NodeLastUpdatetime=2018-05-31 17:01:28.0}]} 10:11:45.402 [Thread-12] WARN com.alibaba.otter.canal.client.impl.ClusterCanalConnector - something goes wrong when getWithoutAck data from server:null com.alibaba.otter.canal.protocol.exception.CanalClientException: java.io.IOException: end of stream when reading header at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:291) ~[canal.client-1.0.26.4.jar!/:?] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:264) ~[canal.client-1.0.26.4.jar!/:?] at com.alibaba.otter.canal.client.impl.ClusterCanalConnector.getWithoutAck(ClusterCanalConnector.java:182) ~[canal.client-1.0.26.4.jar!/:?] at com.tops001.foundation.canal.service.CanalWorker.process(CanalWorker.java:120) ~[classes!/:?] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_171] Caused by: java.io.IOException: end of stream when reading header at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.read(SimpleCanalConnector.java:396) ~[canal.client-1.0.26.4.jar!/:?] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.readNextPacket(SimpleCanalConnector.java:384) ~[canal.client-1.0.26.4.jar!/:?] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.readNextPacket(SimpleCanalConnector.java:368) ~[canal.client-1.0.26.4.jar!/:?] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.receiveMessages(SimpleCanalConnector.java:296) ~[canal.client-1.0.26.4.jar!/:?] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:289) ~[canal.client-1.0.26.4.jar!/:?] ... 4 more 10:11:50.417 [Thread-12] INFO com.alibaba.otter.canal.client.impl.ClusterCanalConnector - restart the connector for next round retry. ``` You opened the table of the table The ability of tsdb is a timed task non-mainline affects the reason why the client reads failed.
666,Compiling problems yourself in the Mac environment Dependent on this file can not get http://code.alibabatech.com/mvn/releases/com/alibaba/fastsql/fastsql/2.0.0_preview_228/fastsql-2.0.0_preview_228.pom Trouble putting this dependency on a reliable source [ERROR] Failed to execute goal on project canal.parse: Could not resolve dependencies for project com.alibaba.otter:canal.parse:jar:1.0.26-SNAPSHOT: Failed to collect dependencies at com.alibaba.fastsql:fastsql:jar:2.0.0_preview_228: Failed to read artifact descriptor for com.alibaba.fastsql:fastsql:jar:2.0.0_preview_228: Could not transfer artifact com.alibaba.fastsql:fastsql:pom:2.0.0_preview_228 from/to alibaba (http://code.alibabatech.com/mvn/releases/): Connect to code.alibabatech.com:80 [code.alibabatech.com/119.38.217.15] failed: Operation timed out -> [Help 1] same here same Https github com alibaba canal releases tag canal 1 0 26 preview 3 The corresponding tar package has a corresponding jar Fastsql will also open source the strongest sql Parser tool mysql resolution compatibility is the best
665,Cancan Whether to support cascading replication rt Canal Server from mysql Slave node copy binlog Should be supported I will test it first. As long as mysql opens the log_slave_update feature
664,About two canal Server with the same instance of the question Two canal The server is configured with two instance example1 and example2 to start one of the canal Server At this time, the server can synchronize the data of the two libraries of example1 and example2. At this time, the second canal is started. Server At this time, the server cannot consume example1 and example2. At this time, the cluster does not make much sense. Can the canal do operations like rebalance, such as rebalance after canal Server1 and canal Server2 synchronizes the data of one library separately
663,1.0.26 com.alibaba.fastsql.sql.parser.ParserException: TODO pos 545 line 9 column 83 token NULL durid Has been manually upgraded to 1 1 9 Start or report an error ` 2018-05-29 17:29:36.484 [destination = devmysql4308 address = /192.168.100.4:3308 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : /* ApplicationName=IntelliJ IDEA 2017.2.5 */ CREATE TABLE best_sign_cont_task ( id INTEGER PRIMARY KEY NOT NULL COMMENT Primary key AUTO_INCREMENT docid VARCHAR(50) NOT NULL DEFAULT '' COMMENT The original id of the original contract pdfid VARCHAR(50) NOT NULL DEFAULT '' COMMENT The id of the contract pdf contract_id VARCHAR(50) NOT NULL DEFAULT '' COMMENT Sign the contract id finish TINYINT(1) NOT NULL DEFAULT 0 COMMENT 0 Party A has not signed 1 Party A has signed Indicates completion of the task remark VARCHAR(100) NOT NULL DEFAULT '' COMMENT Remarks sys_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP NOT NULL COMMENT system time )ENGINE=INNODB DEFAULT CHARSET=utf8 COMMENT Create a task record on the contract com.alibaba.fastsql.sql.parser.ParserException: TODO pos 545 line 9 column 83 token NULL at com.alibaba.fastsql.sql.parser.SQLExprParser.notRationalRest(SQLExprParser.java:2557) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] at com.alibaba.fastsql.sql.parser.SQLExprParser.relationalRest(SQLExprParser.java:2323) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] at com.alibaba.fastsql.sql.parser.SQLExprParser.exprRest(SQLExprParser.java:109) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] at com.alibaba.fastsql.sql.parser.SQLExprParser.expr(SQLExprParser.java:97) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] at com.alibaba.fastsql.sql.dialect.mysql.parser.MySqlExprParser.parseColumnRest(MySqlExprParser.java:462) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] at com.alibaba.fastsql.sql.parser.SQLExprParser.parseColumnRest(SQLExprParser.java:2801) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] at com.alibaba.fastsql.sql.dialect.mysql.parser.MySqlExprParser.parseColumnRest(MySqlExprParser.java:545) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] at com.alibaba.fastsql.sql.dialect.mysql.parser.MySqlExprParser.parseColumn(MySqlExprParser.java:455) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] at com.alibaba.fastsql.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:170) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] at com.alibaba.fastsql.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:239) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:165) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:76) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:469) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:331) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:71) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.applyHistoryOnMemory(DatabaseTableMeta.java:382) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:121) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171] ` I downloaded it manually. Fastsql_2 0 0_preview_186 is targeting the problem Problem sql ```sql CREATE TABLE corp_best_sign_info ( sys_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP not null COMMENT system time )ENGINE=INNODB DEFAULT CHARSET=utf8 COMMENT Enterprise opening and signing information form ``` Adjusted after passing sql ```sql CREATE TABLE corp_best_sign_info ( sys_time TIMESTAMP not null DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT system time )ENGINE=INNODB DEFAULT CHARSET=utf8 COMMENT Enterprise opening and signing information form ``` The difference is not null Wrote in question on update current_timestamp After analogy with the other sqlparse This grammar is OK, please see if you can fix it. Is indeed a problem
662,Current master branch SimpleCanalConnector connect 方法 logical error # The main method running SimpleCanalClientTest will be abnormal. `process error!java.lang.NullPointerException: null at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.writeWithHeader(SimpleCanalConnector.java:392) ~[classes/:na] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.writeWithHeader(SimpleCanalConnector.java:380) ~[classes/:na] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:292) ~[classes/:na] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:280) ~[classes/:na] at com.alibaba.otter.canal.example.AbstractCanalClientTest.process(AbstractCanalClientTest.java:125) ~[classes/:na] at com.alibaba.otter.canal.example.AbstractCanalClientTest$2.run(AbstractCanalClientTest.java:85) [classes/:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171]` Troubleshoot is a connection method of SimpleCanalConnector ``` public void connect() throws CanalClientException { if (connected) { return; } if (runningMonitor != null) { if (!runningMonitor.isStart()) { runningMonitor.start(); } } else { waitClientRunning(); if (!running) { return; } doConnect(); if (filter != null) { // If there is a conditional description, it is automatically switched based on the last conditional subscription once. subscribe(filter); } if (rollbackOnConnect) { rollback(); } } connected = true; } private void waitClientRunning() { try { if (zkClientx != null) { if (!connected) {// Connected not throw new CanalClientException("should connect first"); } running = true; mutex.get();// Blocking waiting } } catch (InterruptedException e) { Thread.currentThread().interrupt(); throw new CanalClientException(e); } } ``` Running is initialized to false and only two methods change his value to waitClientRunning with stopRunning() 。 The connect method calls waitClientRunning when zkClienttx == null So running is still false so it doesn&#39;t call doConnect so it doesn&#39;t assign a value to writableChannel and causes a null pointer exception in the exception stack. The problem is the https github com alibaba canal commit 4141049bb90ce6dc1ce05a58c95f2d974fdd5865 https github com alibaba canal commit 4141049bb90ce6dc1ce05a58c95f2d974fdd5865 @rewerma 1 Block two lines of code on the line First place public void connect() throws CanalClientException { if (connected) { return; } if (runningMonitor != null) { if (!runningMonitor.isStart()) { runningMonitor.start(); } } else { waitClientRunning(); // if (!running) { // return; // } doConnect(); if (filter != null) { // If there is a conditional description, it is automatically switched based on the last conditional subscription once. subscribe(filter); } if (rollbackOnConnect) { rollback(); } } connected = true; } Second place public void subscribe(String filter) throws CanalClientException { waitClientRunning(); // if (!running) { // return; // } try { writeWithHeader(Packet.newBuilder() .setType(PacketType.SUBSCRIPTION) .setBody(Sub.newBuilder() .setDestination(clientIdentity.getDestination()) .setClientId(String.valueOf(clientIdentity.getClientId())) .setFilter(filter != null ? filter : "") .build() .toByteString()) .build() .toByteArray()); // Packet p = Packet.parseFrom(readNextPacket()); Ack ack = Ack.parseFrom(p.getBody()); if (ack.getErrorCode() > 0) { throw new CanalClientException("failed to subscribe with reason: " + ack.getErrorMessage()); } clientIdentity.setFilter(filter); } catch (IOException e) { throw new CanalClientException(e); } } The reason above has answered zookeeper to determine that there is a problem with a single machine can have no zookeeper already fixed
661,After generating the data backlog, after synchronizing for a period of time, an error will be reported, and then the synchronization will be resumed from the starting point, resulting in the inability to continue the subsequent synchronization. Version canal canal 1 0 26 something goes wrong when acking data from server:null com.alibaba.otter.canal.protocol.exception.CanalClientException: java.io.IOException: Broken pipe at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.ack(SimpleCanalConnector.java:339) at com.alibaba.otter.canal.client.impl.ClusterCanalConnector.ack(ClusterCanalConnector.java:246) at ins.framework.mysqltoes.client.AbstractCanalClientTest.process(AbstractCanalClientTest.java:140) at ins.framework.mysqltoes.client.AbstractCanalClientTest$2.run(AbstractCanalClientTest.java:93) at java.lang.Thread.run(Thread.java:745) Caused by: java.io.IOException: Broken pipe at sun.nio.ch.FileDispatcherImpl.write0(Native Method) at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47) at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93) at sun.nio.ch.IOUtil.write(IOUtil.java:65) at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:471) at java.nio.channels.Channels.writeFullyImpl(Channels.java:78) at java.nio.channels.Channels.writeFully(Channels.java:98) at java.nio.channels.Channels.access$000(Channels.java:61) at java.nio.channels.Channels$1.write(Channels.java:174) at java.nio.channels.Channels$WritableByteChannelImpl.write(Channels.java:458) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.writeWithHeader(SimpleCanalConnector.java:382) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.writeWithHeader(SimpleCanalConnector.java:369) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.ack(SimpleCanalConnector.java:333) ... 4 common frames omitted restart the connector for next round retry. something goes wrong when getWithoutAck data from server:null com.alibaba.otter.canal.protocol.exception.CanalClientException: something goes wrong with reason: something goes wrong with channel:[id: 0x58c3e872 /10.10.56.23:50475 => /10.10.56.23:11111] exception=com.alibaba.otter.canal.server.exception.CanalServerException: ack error clientId:1001 batchId:926 is not exist please check at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.receiveMessages(SimpleCanalConnector.java:317) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:294) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:269) at com.alibaba.otter.canal.client.impl.ClusterCanalConnector.getWithoutAck(ClusterCanalConnector.java:177) at ins.framework.mysqltoes.client.AbstractCanalClientTest.process(AbstractCanalClientTest.java:126) at ins.framework.mysqltoes.client.AbstractCanalClientTest$2.run(AbstractCanalClientTest.java:93) at java.lang.Thread.run(Thread.java:745) restart the connector for next round retry. ------------------------------------------------------------------------------------------------------------- The server side of the canal generates the thread error log hs_err_pid12248 log to record part of the stack information but there are the following exceptions. Is there some code missing? ------------------------------------------------------------------------------------------------------------- Internal exceptions (10 events): Event: 13725.231 Thread 0x00007f65bc006800 Exception <a 'java/lang/ClassNotFoundException': com/alibaba/otter/canal/protocol/position/PositionBeanInfo> (0x0000000715146718) thrown at [/HUDSON/workspace/8-2-build-linux-amd64/jdk8u51/3951/hotspot/src/share/vm/classfile/systemDictionary.cpp line 210 Event: 13725.231 Thread 0x00007f65bc006800 Exception <a 'java/lang/ClassNotFoundException': com/alibaba/otter/canal/protocol/position/PositionCustomizer> (0x0000000715156278) thrown at [/HUDSON/workspace/8-2-build-linux-amd64/jdk8u51/3951/hotspot/src/share/vm/classfile/systemDictionary.cpp line 2 Event: 13725.231 Thread 0x00007f65bc006800 Exception <a 'java/lang/ClassNotFoundException': com/alibaba/otter/canal/protocol/position/TimePositionCustomizer> (0x0000000715168aa0) thrown at [/HUDSON/workspace/8-2-build-linux-amd64/jdk8u51/3951/hotspot/src/share/vm/classfile/systemDictionary.cpp li Event: 13725.232 Thread 0x00007f65bc006800 Exception <a 'java/lang/ClassNotFoundException': com/alibaba/otter/canal/protocol/position/EntryPositionCustomizer> (0x000000071517e530) thrown at [/HUDSON/workspace/8-2-build-linux-amd64/jdk8u51/3951/hotspot/src/share/vm/classfile/systemDictionary.cpp l Event: 13725.413 Thread 0x00007f659c1da000 Exception <a 'java/lang/ClassCastException': sun.misc.Cleaner cannot be cast to java.lang.Runnable> (0x00000007168aa620) thrown at [/HUDSON/workspace/8-2-build-linux-amd64/jdk8u51/3951/hotspot/src/share/vm/interpreter/interpreterRuntime.cpp line 381] Event: 13725.416 Thread 0x00007f659c1da000 Exception <a 'java/lang/NoClassDefFoundError': javassist/ClassPath> (0x00000007168c0840) thrown at [/HUDSON/workspace/8-2-build-linux-amd64/jdk8u51/3951/hotspot/src/share/vm/classfile/systemDictionary.cpp line 199] Event: 13730.219 Thread 0x00007f65a000a000 Implicit null exception at 0x00007f66653cf735 to 0x00007f66653cfa61 Event: 13743.542 Thread 0x00007f659c1da000 Exception <a 'java/lang/InterruptedException'> (0x0000000709c92318) thrown at [/HUDSON/workspace/8-2-build-linux-amd64/jdk8u51/3951/hotspot/src/share/vm/runtime/objectMonitor.cpp line 1683] Event: 13743.548 Thread 0x00007f6594003800 Implicit null exception at 0x00007f6665ecfa12 to 0x00007f6665ed0209 Event: 13743.558 Thread 0x00007f6594003800 Implicit null exception at 0x00007f6665b17426 to 0x00007f6665b19e6d You are openjdk ？ ack error clientId:1001 batchId:926 is not exist please check， Should be canal The server side has abnormally restarted the instance Checked the canalserver log found some unsupported format format warning but does not affect the synchronization but once the above exception occurs, the server will also report this exception
660,1 0 26 latest version something goes wrong with channel:******clientId:1001 batchId:50560 is not exist please check Found this morning The canal log file in the logs canal directory is nearly 200M. The exception is the following. However, the meta log below each instance directory is the normal record. The description of the consumption site is normal consumption, but why does this exception persist? 2018-05-29 09:48:25.896 [New I/O server worker #1-1] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400 Caused by : something goes wrong with channel:[id: 0x01f1bec3 /127.0.0.1:35010 => /127.0.0.1:11111] exception=com.alibaba.otter.canal.server.exception.CanalServerException: ack error clientId:1001 batchId:50560 is not exist please check 2018-05-29 09:48:25.907 [New I/O server worker #1-1] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x01f1bec3 /127.0.0.1:35010 :> /127.0.0.1:11111] exception=java.nio.channels.ClosedChannelException at org.jboss.netty.channel.socket.nio.NioWorker.cleanUpWriteBuffer(NioWorker.java:649) at org.jboss.netty.channel.socket.nio.NioWorker.writeFromUserCode(NioWorker.java:370) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:137) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) at org.jboss.netty.channel.Channels.write(Channels.java:611) at org.jboss.netty.channel.Channels.write(Channels.java:578) at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:28) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:144) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.cleanup(ReplayingDecoder.java:542) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.channelDisconnected(ReplayingDecoder.java:450) at org.jboss.netty.channel.Channels.fireChannelDisconnected(Channels.java:360) at org.jboss.netty.channel.socket.nio.NioWorker.close(NioWorker.java:599) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:119) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) at org.jboss.netty.channel.Channels.close(Channels.java:720) at org.jboss.netty.channel.AbstractChannel.close(AbstractChannel.java:208) at org.jboss.netty.channel.ChannelFutureListener$1.operationComplete(ChannelFutureListener.java:46) at org.jboss.netty.channel.DefaultChannelFuture.notifyListener(DefaultChannelFuture.java:381) at org.jboss.netty.channel.DefaultChannelFuture.addListener(DefaultChannelFuture.java:148) at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:30) at com.alibaba.otter.canal.server.netty.NettyUtils.error(NettyUtils.java:51) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:200) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:444) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261) at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:350) at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745) This is a bug, when can I update it? The server side must have generated an exception or restarted the instance client reference demo to do a retry @agapple doConnect() Method is to retry Take a look at the client demo @agapple I looked at the connector disconnect method in AbstractCanalClientTest and tried it, but the server is still the same exception. protected void process() { int batchSize = 5 * 1024; while (running) { try { MDC.put("destination" destination); connector.connect(); connector.subscribe(); waiting = false; while (running) { Message message = connector.getWithoutAck(batchSize); // Get the specified amount of data long batchId = message.getId(); int size = message.getEntries().size(); if (batchId == -1 || size == 0) { // try { // Thread.sleep(1000); // } catch (InterruptedException e) { // } } else { printSummary(message batchId size); printEntry(message.getEntries()); } connector.ack(batchId); // Submit confirmation // connector.rollback(batchId); // Processing failure Rollback data } } catch (Exception e) { logger.error("process error!" e); } finally { **connector.disconnect();** MDC.remove("destination"); } } } @agapple Instance will restart the data of the restarted production line synchronization and repeat the analysis sent to kafka Face The extractive system of the distributed system is very difficult, generally requires business considerations of idempotent
659,Initialize sql error when synchronizing 1.0.25 Default configuration in the version documentation H2 database ` 2018-05-28 14:36:38.957 [destination = myexample address = /172.16.9.11:3307 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `columns_priv` ( `Host` char(60) COLLATE utf8_bin NOT NULL DEFAULT '' `Db` char(64) COLLATE utf8_bin NOT NULL DEFAULT '' `User` char(16) COLLATE utf8_bin NOT NULL DEFAULT '' `Table_name` char(64) COLLATE utf8_bin NOT NULL DEFAULT '' `Column_name` char(64) COLLATE utf8_bin NOT NULL DEFAULT '' `Timestamp` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP `Column_priv` set('Select' 'Insert' 'Update' 'References') CHARACTER SET utf8 NOT NULL DEFAULT '' PRIMARY KEY (`Host` `Db` `User` `Table_name` `Column_name`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='Column privileges' com.alibaba.druid.sql.parser.ParserException: error pos 428 line 8 column 17 token SET at com.alibaba.druid.sql.parser.SQLExprParser.name(SQLExprParser.java:1334) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLExprParser.parseDataType(SQLExprParser.java:2310) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLExprParser.parseDataType(SQLExprParser.java:2301) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlExprParser.parseColumn(MySqlExprParser.java:493) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:115) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.25.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101] 2018-05-28 14:36:38.960 [destination = myexample address = /172.16.9.11:3307 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `db` ( `Host` char(60) COLLATE utf8_bin NOT NULL DEFAULT '' `Db` char(64) COLLATE utf8_bin NOT NULL DEFAULT '' `User` char(16) COLLATE utf8_bin NOT NULL DEFAULT '' `Select_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Insert_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Update_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Delete_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Create_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Drop_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Grant_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `References_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Index_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Alter_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Create_tmp_table_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Lock_tables_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Create_view_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Show_view_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Create_routine_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Alter_routine_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Execute_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Event_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Trigger_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' PRIMARY KEY (`Host` `Db` `User`) KEY `User` (`User`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='Database privileges' com.alibaba.druid.sql.parser.ParserException: syntax error error in :''Y') CHARACTER SET utf8 NOT NULL DE' expect RPAREN actual IDENTIFIER pos 225 line 5 column 31 token IDENTIFIER CHARACTER at com.alibaba.druid.sql.parser.SQLParser.printError(SQLParser.java:284) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLParser.accept(SQLParser.java:292) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:191) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.25.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101] 2018-05-28 14:36:38.961 [destination = myexample address = /172.16.9.11:3307 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `event` ( `db` char(64) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL DEFAULT '' `name` char(64) NOT NULL DEFAULT '' `body` longblob NOT NULL `definer` char(77) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL DEFAULT '' `execute_at` datetime DEFAULT NULL `interval_value` int(11) DEFAULT NULL `interval_field` enum('YEAR' 'QUARTER' 'MONTH' 'DAY' 'HOUR' 'MINUTE' 'WEEK' 'SECOND' 'MICROSECOND' 'YEAR_MONTH' 'DAY_HOUR' 'DAY_MINUTE' 'DAY_SECOND' 'HOUR_MINUTE' 'HOUR_SECOND' 'MINUTE_SECOND' 'DAY_MICROSECOND' 'HOUR_MICROSECOND' 'MINUTE_MICROSECOND' 'SECOND_MICROSECOND') DEFAULT NULL `created` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP `modified` timestamp NOT NULL DEFAULT '0000-00-00 00:00:00' `last_executed` datetime DEFAULT NULL `starts` datetime DEFAULT NULL `ends` datetime DEFAULT NULL `status` enum('ENABLED' 'DISABLED' 'SLAVESIDE_DISABLED') NOT NULL DEFAULT 'ENABLED' `on_completion` enum('DROP' 'PRESERVE') NOT NULL DEFAULT 'DROP' `sql_mode` set('REAL_AS_FLOAT' 'PIPES_AS_CONCAT' 'ANSI_QUOTES' 'IGNORE_SPACE' 'NOT_USED' 'ONLY_FULL_GROUP_BY' 'NO_UNSIGNED_SUBTRACTION' 'NO_DIR_IN_CREATE' 'POSTGRESQL' 'ORACLE' 'MSSQL' 'DB2' 'MAXDB' 'NO_KEY_OPTIONS' 'NO_TABLE_OPTIONS' 'NO_FIELD_OPTIONS' 'MYSQL323' 'MYSQL40' 'ANSI' 'NO_AUTO_VALUE_ON_ZERO' 'NO_BACKSLASH_ESCAPES' 'STRICT_TRANS_TABLES' 'STRICT_ALL_TABLES' 'NO_ZERO_IN_DATE' 'NO_ZERO_DATE' 'INVALID_DATES' 'ERROR_FOR_DIVISION_BY_ZERO' 'TRADITIONAL' 'NO_AUTO_CREATE_USER' 'HIGH_NOT_PRECEDENCE' 'NO_ENGINE_SUBSTITUTION' 'PAD_CHAR_TO_FULL_LENGTH') NOT NULL DEFAULT '' `comment` char(64) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL DEFAULT '' `originator` int(10) unsigned NOT NULL `time_zone` char(64) CHARACTER SET latin1 NOT NULL DEFAULT 'SYSTEM' `character_set_client` char(32) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL `collation_connection` char(32) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL `db_collation` char(32) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL `body_utf8` longblob PRIMARY KEY (`db` `name`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COMMENT='Events' com.alibaba.druid.sql.parser.ParserException: error pos 1035 line 16 column 14 token SET at com.alibaba.druid.sql.parser.SQLExprParser.name(SQLExprParser.java:1334) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLExprParser.parseDataType(SQLExprParser.java:2310) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLExprParser.parseDataType(SQLExprParser.java:2301) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlExprParser.parseColumn(MySqlExprParser.java:493) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:115) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.25.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101] 2018-05-28 14:36:38.964 [destination = myexample address = /172.16.9.11:3307 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `func` ( `name` char(64) COLLATE utf8_bin NOT NULL DEFAULT '' `ret` tinyint(1) NOT NULL DEFAULT '0' `dl` char(128) COLLATE utf8_bin NOT NULL DEFAULT '' `type` enum('function' 'aggregate') CHARACTER SET utf8 NOT NULL PRIMARY KEY (`name`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='User defined functions' com.alibaba.druid.sql.parser.ParserException: syntax error error in :'te') CHARACTER SET utf8 NOT NULL ' expect RPAREN actual IDENTIFIER pos 221 line 5 column 39 token IDENTIFIER CHARACTER at com.alibaba.druid.sql.parser.SQLParser.printError(SQLParser.java:284) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLParser.accept(SQLParser.java:292) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:191) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.25.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101] 2018-05-28 14:36:38.967 [destination = myexample address = /172.16.9.11:3307 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `proc` ( `db` char(64) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL DEFAULT '' `name` char(64) NOT NULL DEFAULT '' `type` enum('FUNCTION' 'PROCEDURE') NOT NULL `specific_name` char(64) NOT NULL DEFAULT '' `language` enum('SQL') NOT NULL DEFAULT 'SQL' `sql_data_access` enum('CONTAINS_SQL' 'NO_SQL' 'READS_SQL_DATA' 'MODIFIES_SQL_DATA') NOT NULL DEFAULT 'CONTAINS_SQL' `is_deterministic` enum('YES' 'NO') NOT NULL DEFAULT 'NO' `security_type` enum('INVOKER' 'DEFINER') NOT NULL DEFAULT 'DEFINER' `param_list` blob NOT NULL `returns` longblob NOT NULL `body` longblob NOT NULL `definer` char(77) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL DEFAULT '' `created` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP `modified` timestamp NOT NULL DEFAULT '0000-00-00 00:00:00' `sql_mode` set('REAL_AS_FLOAT' 'PIPES_AS_CONCAT' 'ANSI_QUOTES' 'IGNORE_SPACE' 'NOT_USED' 'ONLY_FULL_GROUP_BY' 'NO_UNSIGNED_SUBTRACTION' 'NO_DIR_IN_CREATE' 'POSTGRESQL' 'ORACLE' 'MSSQL' 'DB2' 'MAXDB' 'NO_KEY_OPTIONS' 'NO_TABLE_OPTIONS' 'NO_FIELD_OPTIONS' 'MYSQL323' 'MYSQL40' 'ANSI' 'NO_AUTO_VALUE_ON_ZERO' 'NO_BACKSLASH_ESCAPES' 'STRICT_TRANS_TABLES' 'STRICT_ALL_TABLES' 'NO_ZERO_IN_DATE' 'NO_ZERO_DATE' 'INVALID_DATES' 'ERROR_FOR_DIVISION_BY_ZERO' 'TRADITIONAL' 'NO_AUTO_CREATE_USER' 'HIGH_NOT_PRECEDENCE' 'NO_ENGINE_SUBSTITUTION' 'PAD_CHAR_TO_FULL_LENGTH') NOT NULL DEFAULT '' `comment` text CHARACTER SET utf8 COLLATE utf8_bin NOT NULL `character_set_client` char(32) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL `collation_connection` char(32) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL `db_collation` char(32) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL `body_utf8` longblob PRIMARY KEY (`db` `name` `type`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COMMENT='Stored Procedures' com.alibaba.druid.sql.parser.ParserException: error pos 864 line 16 column 14 token SET at com.alibaba.druid.sql.parser.SQLExprParser.name(SQLExprParser.java:1334) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLExprParser.parseDataType(SQLExprParser.java:2310) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLExprParser.parseDataType(SQLExprParser.java:2301) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlExprParser.parseColumn(MySqlExprParser.java:493) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:115) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.25.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101] 2018-05-28 14:36:38.968 [destination = myexample address = /172.16.9.11:3307 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `procs_priv` ( `Host` char(60) COLLATE utf8_bin NOT NULL DEFAULT '' `Db` char(64) COLLATE utf8_bin NOT NULL DEFAULT '' `User` char(16) COLLATE utf8_bin NOT NULL DEFAULT '' `Routine_name` char(64) CHARACTER SET utf8 NOT NULL DEFAULT '' `Routine_type` enum('FUNCTION' 'PROCEDURE') COLLATE utf8_bin NOT NULL `Grantor` char(77) COLLATE utf8_bin NOT NULL DEFAULT '' `Proc_priv` set('Execute' 'Alter Routine' 'Grant') CHARACTER SET utf8 NOT NULL DEFAULT '' `Timestamp` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP PRIMARY KEY (`Host` `Db` `User` `Routine_name` `Routine_type`) KEY `Grantor` (`Grantor`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='Procedure privileges' com.alibaba.druid.sql.parser.ParserException: syntax error error in :'RE') COLLATE utf8_bin NOT NULL `' expect RPAREN actual IDENTIFIER pos 313 line 6 column 47 token IDENTIFIER COLLATE at com.alibaba.druid.sql.parser.SQLParser.printError(SQLParser.java:284) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLParser.accept(SQLParser.java:292) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:191) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.25.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101] 2018-05-28 14:36:38.972 [destination = myexample address = /172.16.9.11:3307 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `tables_priv` ( `Host` char(60) COLLATE utf8_bin NOT NULL DEFAULT '' `Db` char(64) COLLATE utf8_bin NOT NULL DEFAULT '' `User` char(16) COLLATE utf8_bin NOT NULL DEFAULT '' `Table_name` char(64) COLLATE utf8_bin NOT NULL DEFAULT '' `Grantor` char(77) COLLATE utf8_bin NOT NULL DEFAULT '' `Timestamp` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP `Table_priv` set('Select' 'Insert' 'Update' 'Delete' 'Create' 'Drop' 'Grant' 'References' 'Index' 'Alter' 'Create View' 'Show view' 'Trigger') CHARACTER SET utf8 NOT NULL DEFAULT '' `Column_priv` set('Select' 'Insert' 'Update' 'References') CHARACTER SET utf8 NOT NULL DEFAULT '' PRIMARY KEY (`Host` `Db` `User` `Table_name`) KEY `Grantor` (`Grantor`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='Table privileges' com.alibaba.druid.sql.parser.ParserException: error pos 422 line 8 column 16 token SET at com.alibaba.druid.sql.parser.SQLExprParser.name(SQLExprParser.java:1334) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLExprParser.parseDataType(SQLExprParser.java:2310) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLExprParser.parseDataType(SQLExprParser.java:2301) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlExprParser.parseColumn(MySqlExprParser.java:493) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:115) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.25.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101] 2018-05-28 14:36:38.974 [destination = myexample address = /172.16.9.11:3307 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `user` ( `Host` char(60) COLLATE utf8_bin NOT NULL DEFAULT '' `User` char(16) COLLATE utf8_bin NOT NULL DEFAULT '' `Password` char(41) CHARACTER SET latin1 COLLATE latin1_bin NOT NULL DEFAULT '' `Select_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Insert_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Update_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Delete_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Create_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Drop_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Reload_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Shutdown_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Process_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `File_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Grant_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `References_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Index_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Alter_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Show_db_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Super_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Create_tmp_table_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Lock_tables_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Execute_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Repl_slave_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Repl_client_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Create_view_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Show_view_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Create_routine_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Alter_routine_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Create_user_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Event_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Trigger_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Create_tablespace_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `ssl_type` enum('' 'ANY' 'X509' 'SPECIFIED') CHARACTER SET utf8 NOT NULL DEFAULT '' `ssl_cipher` blob NOT NULL `x509_issuer` blob NOT NULL `x509_subject` blob NOT NULL `max_questions` int(11) unsigned NOT NULL DEFAULT '0' `max_updates` int(11) unsigned NOT NULL DEFAULT '0' `max_connections` int(11) unsigned NOT NULL DEFAULT '0' `max_user_connections` int(11) unsigned NOT NULL DEFAULT '0' `plugin` char(64) COLLATE utf8_bin DEFAULT '' `authentication_string` text COLLATE utf8_bin `password_expired` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' PRIMARY KEY (`Host` `User`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='Users and global privileges' com.alibaba.druid.sql.parser.ParserException: syntax error error in :''Y') CHARACTER SET utf8 NOT NULL DE' expect RPAREN actual IDENTIFIER pos 256 line 5 column 31 token IDENTIFIER CHARACTER at com.alibaba.druid.sql.parser.SQLParser.printError(SQLParser.java:284) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLParser.accept(SQLParser.java:292) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:191) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.25.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101] 2018-05-28 14:38:19.380 [destination = myexample address = /172.16.9.11:3307 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - pls submit github issue show create table ddl:CREATE TABLE `hierquerysellist` ( `fieldtype_id` bigint(20) NOT NULL `whereclause` longtext NOT NULL `filtervariants` char(1) NOT NULL UNIQUE KEY `fieldtype_id` (`fieldtype_id`) USING BTREE ) ENGINE=MyISAM DEFAULT CHARSET=utf8 compare failed . db : TableMeta [schema=bboss table=hierquerysellist fileds= FieldMeta [columnName=fieldtype_id columnType=bigint(20) defaultValue=null nullable=false key=true] FieldMeta [columnName=whereclause columnType=longtext defaultValue=null nullable=false key=false] FieldMeta [columnName=filtervariants columnType=char(1) defaultValue=null nullable=false key=false] ] mem : TableMeta [schema=bboss table=hierquerysellist fileds= FieldMeta [columnName=fieldtype_id columnType=bigint(20) defaultValue=null nullable=false key=false] FieldMeta [columnName=whereclause columnType=longtext defaultValue=null nullable=false key=false] FieldMeta [columnName=filtervariants columnType=char(1) defaultValue=null nullable=false key=false] ] 2018-05-28 14:38:19.590 [destination = myexample address = /172.16.9.11:3307 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - pls submit github issue show create table ddl:CREATE TABLE `hierquerysellist_fields` ( `fieldtype_id` bigint(20) NOT NULL `sequencenr` smallint(6) NOT NULL `fieldname` varchar(50) NOT NULL UNIQUE KEY `fieldtype_id` (`fieldtype_id` `sequencenr`) USING BTREE ) ENGINE=MyISAM DEFAULT CHARSET=utf8 compare failed . db : TableMeta [schema=bboss table=hierquerysellist_fields fileds= FieldMeta [columnName=fieldtype_id columnType=bigint(20) defaultValue=null nullable=false key=true] FieldMeta [columnName=sequencenr columnType=smallint(6) defaultValue=null nullable=false key=true] FieldMeta [columnName=fieldname columnType=varchar(50) defaultValue=null nullable=false key=false] ] mem : TableMeta [schema=bboss table=hierquerysellist_fields fileds= FieldMeta [columnName=fieldtype_id columnType=bigint(20) defaultValue=null nullable=false key=false] FieldMeta [columnName=sequencenr columnType=smallint(6) defaultValue=null nullable=false key=false] FieldMeta [columnName=fieldname columnType=varchar(50) defaultValue=null nullable=false key=false] ] 2018-05-28 14:38:20.005 [destination = myexample address = /172.16.9.11:3307 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - pls submit github issue show create table ddl:CREATE TABLE `linkquerysellist` ( `fieldtype_id` bigint(20) NOT NULL `whereclause` longtext NOT NULL `filtervariants` char(1) NOT NULL UNIQUE KEY `fieldtype_id` (`fieldtype_id`) USING BTREE ) ENGINE=MyISAM DEFAULT CHARSET=utf8 compare failed . db : TableMeta [schema=bboss table=linkquerysellist fileds= FieldMeta [columnName=fieldtype_id columnType=bigint(20) defaultValue=null nullable=false key=true] FieldMeta [columnName=whereclause columnType=longtext defaultValue=null nullable=false key=false] FieldMeta [columnName=filtervariants columnType=char(1) defaultValue=null nullable=false key=false] ] mem : TableMeta [schema=bboss table=linkquerysellist fileds= FieldMeta [columnName=fieldtype_id columnType=bigint(20) defaultValue=null nullable=false key=false] FieldMeta [columnName=whereclause columnType=longtext defaultValue=null nullable=false key=false] FieldMeta [columnName=filtervariants columnType=char(1) defaultValue=null nullable=false key=false] ] 2018-05-28 14:38:20.837 [destination = myexample address = /172.16.9.11:3307 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - pls submit github issue show create table ddl:CREATE TABLE `parentlinkedsellist` ( `fieldtype_id` bigint(20) NOT NULL `whereclause` longtext NOT NULL `filtervariants` char(1) NOT NULL `linkfield` varchar(50) NOT NULL UNIQUE KEY `fieldtype_id` (`fieldtype_id`) USING BTREE ) ENGINE=MyISAM DEFAULT CHARSET=utf8 compare failed . db : TableMeta [schema=bboss table=parentlinkedsellist fileds= FieldMeta [columnName=fieldtype_id columnType=bigint(20) defaultValue=null nullable=false key=true] FieldMeta [columnName=whereclause columnType=longtext defaultValue=null nullable=false key=false] FieldMeta [columnName=filtervariants columnType=char(1) defaultValue=null nullable=false key=false] FieldMeta [columnName=linkfield columnType=varchar(50) defaultValue=null nullable=false key=false] ] mem : TableMeta [schema=bboss table=parentlinkedsellist fileds= FieldMeta [columnName=fieldtype_id columnType=bigint(20) defaultValue=null nullable=false key=false] FieldMeta [columnName=whereclause columnType=longtext defaultValue=null nullable=false key=false] FieldMeta [columnName=filtervariants columnType=char(1) defaultValue=null nullable=false key=false] FieldMeta [columnName=linkfield columnType=varchar(50) defaultValue=null nullable=false key=false] ] 2018-05-28 14:38:21.359 [destination = myexample address = /172.16.9.11:3307 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - pls submit github issue show create table ddl:CREATE TABLE `querysellist` ( `fieldtype_id` bigint(20) NOT NULL `query` longtext NOT NULL `filtervariants` char(1) NOT NULL `sort_order` varchar(1) NOT NULL UNIQUE KEY `fieldtype_id` (`fieldtype_id`) USING BTREE ) ENGINE=MyISAM DEFAULT CHARSET=utf8 compare failed . db : TableMeta [schema=bboss table=querysellist fileds= FieldMeta [columnName=fieldtype_id columnType=bigint(20) defaultValue=null nullable=false key=true] FieldMeta [columnName=query columnType=longtext defaultValue=null nullable=false key=false] FieldMeta [columnName=filtervariants columnType=char(1) defaultValue=null nullable=false key=false] FieldMeta [columnName=sort_order columnType=varchar(1) defaultValue=null nullable=false key=false] ] mem : TableMeta [schema=bboss table=querysellist fileds= FieldMeta [columnName=fieldtype_id columnType=bigint(20) defaultValue=null nullable=false key=false] FieldMeta [columnName=query columnType=longtext defaultValue=null nullable=false key=false] FieldMeta [columnName=filtervariants columnType=char(1) defaultValue=null nullable=false key=false] FieldMeta [columnName=sort_order columnType=varchar(1) defaultValue=null nullable=false key=false] ] 2018-05-28 14:38:28.334 [destination = myexample address = /172.16.9.11:3307 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - compare failed check log 2018-05-28 14:38:28.356 [destination = myexample address = /172.16.9.11:3307 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /172.16.9.11:3307 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: can't find init table meta for myexample with position : EntryPosition[included=false journalName=mysql-bin.000001 position=1314 serverId=1 timestamp=1527489302000] 2018-05-28 14:38:28.359 [destination = myexample address = /172.16.9.11:3307 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:myexample[com.alibaba.otter.canal.parse.exception.CanalParseException: can't find init table meta for myexample with position : EntryPosition[included=false journalName=mysql-bin.000001 position=1314 serverId=1 timestamp=1527489302000] ] 2018-05-28 14:38:40.151 [destination = myexample address = /172.16.9.11:3307 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just show master status 2018-05-28 14:40:01.773 [destination = myexample address = /172.16.9.11:3307 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `columns_priv` ( `Host` char(60) COLLATE utf8_bin NOT NULL DEFAULT '' `Db` char(64) COLLATE utf8_bin NOT NULL DEFAULT '' `User` char(16) COLLATE utf8_bin NOT NULL DEFAULT '' `Table_name` char(64) COLLATE utf8_bin NOT NULL DEFAULT '' `Column_name` char(64) COLLATE utf8_bin NOT NULL DEFAULT '' `Timestamp` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP `Column_priv` set('Select' 'Insert' 'Update' 'References') CHARACTER SET utf8 NOT NULL DEFAULT '' PRIMARY KEY (`Host` `Db` `User` `Table_name` `Column_name`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='Column privileges' com.alibaba.druid.sql.parser.ParserException: error pos 428 line 8 column 17 token SET at com.alibaba.druid.sql.parser.SQLExprParser.name(SQLExprParser.java:1334) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLExprParser.parseDataType(SQLExprParser.java:2310) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLExprParser.parseDataType(SQLExprParser.java:2301) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlExprParser.parseColumn(MySqlExprParser.java:493) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:115) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.25.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101] 2018-05-28 14:40:01.775 [destination = myexample address = /172.16.9.11:3307 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `db` ( `Host` char(60) COLLATE utf8_bin NOT NULL DEFAULT '' `Db` char(64) COLLATE utf8_bin NOT NULL DEFAULT '' `User` char(16) COLLATE utf8_bin NOT NULL DEFAULT '' `Select_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Insert_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Update_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Delete_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Create_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Drop_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Grant_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `References_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Index_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Alter_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Create_tmp_table_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Lock_tables_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Create_view_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Show_view_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Create_routine_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Alter_routine_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Execute_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Event_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Trigger_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' PRIMARY KEY (`Host` `Db` `User`) KEY `User` (`User`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='Database privileges' com.alibaba.druid.sql.parser.ParserException: syntax error error in :''Y') CHARACTER SET utf8 NOT NULL DE' expect RPAREN actual IDENTIFIER pos 225 line 5 column 31 token IDENTIFIER CHARACTER at com.alibaba.druid.sql.parser.SQLParser.printError(SQLParser.java:284) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLParser.accept(SQLParser.java:292) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:191) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.25.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101] 2018-05-28 14:40:01.776 [destination = myexample address = /172.16.9.11:3307 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `event` ( `db` char(64) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL DEFAULT '' `name` char(64) NOT NULL DEFAULT '' `body` longblob NOT NULL `definer` char(77) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL DEFAULT '' `execute_at` datetime DEFAULT NULL `interval_value` int(11) DEFAULT NULL `interval_field` enum('YEAR' 'QUARTER' 'MONTH' 'DAY' 'HOUR' 'MINUTE' 'WEEK' 'SECOND' 'MICROSECOND' 'YEAR_MONTH' 'DAY_HOUR' 'DAY_MINUTE' 'DAY_SECOND' 'HOUR_MINUTE' 'HOUR_SECOND' 'MINUTE_SECOND' 'DAY_MICROSECOND' 'HOUR_MICROSECOND' 'MINUTE_MICROSECOND' 'SECOND_MICROSECOND') DEFAULT NULL `created` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP `modified` timestamp NOT NULL DEFAULT '0000-00-00 00:00:00' `last_executed` datetime DEFAULT NULL `starts` datetime DEFAULT NULL `ends` datetime DEFAULT NULL `status` enum('ENABLED' 'DISABLED' 'SLAVESIDE_DISABLED') NOT NULL DEFAULT 'ENABLED' `on_completion` enum('DROP' 'PRESERVE') NOT NULL DEFAULT 'DROP' `sql_mode` set('REAL_AS_FLOAT' 'PIPES_AS_CONCAT' 'ANSI_QUOTES' 'IGNORE_SPACE' 'NOT_USED' 'ONLY_FULL_GROUP_BY' 'NO_UNSIGNED_SUBTRACTION' 'NO_DIR_IN_CREATE' 'POSTGRESQL' 'ORACLE' 'MSSQL' 'DB2' 'MAXDB' 'NO_KEY_OPTIONS' 'NO_TABLE_OPTIONS' 'NO_FIELD_OPTIONS' 'MYSQL323' 'MYSQL40' 'ANSI' 'NO_AUTO_VALUE_ON_ZERO' 'NO_BACKSLASH_ESCAPES' 'STRICT_TRANS_TABLES' 'STRICT_ALL_TABLES' 'NO_ZERO_IN_DATE' 'NO_ZERO_DATE' 'INVALID_DATES' 'ERROR_FOR_DIVISION_BY_ZERO' 'TRADITIONAL' 'NO_AUTO_CREATE_USER' 'HIGH_NOT_PRECEDENCE' 'NO_ENGINE_SUBSTITUTION' 'PAD_CHAR_TO_FULL_LENGTH') NOT NULL DEFAULT '' `comment` char(64) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL DEFAULT '' `originator` int(10) unsigned NOT NULL `time_zone` char(64) CHARACTER SET latin1 NOT NULL DEFAULT 'SYSTEM' `character_set_client` char(32) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL `collation_connection` char(32) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL `db_collation` char(32) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL `body_utf8` longblob PRIMARY KEY (`db` `name`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COMMENT='Events' com.alibaba.druid.sql.parser.ParserException: error pos 1035 line 16 column 14 token SET at com.alibaba.druid.sql.parser.SQLExprParser.name(SQLExprParser.java:1334) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLExprParser.parseDataType(SQLExprParser.java:2310) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLExprParser.parseDataType(SQLExprParser.java:2301) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlExprParser.parseColumn(MySqlExprParser.java:493) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:115) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.25.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101] 2018-05-28 14:40:01.778 [destination = myexample address = /172.16.9.11:3307 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `func` ( `name` char(64) COLLATE utf8_bin NOT NULL DEFAULT '' `ret` tinyint(1) NOT NULL DEFAULT '0' `dl` char(128) COLLATE utf8_bin NOT NULL DEFAULT '' `type` enum('function' 'aggregate') CHARACTER SET utf8 NOT NULL PRIMARY KEY (`name`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='User defined functions' com.alibaba.druid.sql.parser.ParserException: syntax error error in :'te') CHARACTER SET utf8 NOT NULL ' expect RPAREN actual IDENTIFIER pos 221 line 5 column 39 token IDENTIFIER CHARACTER at com.alibaba.druid.sql.parser.SQLParser.printError(SQLParser.java:284) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLParser.accept(SQLParser.java:292) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:191) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.25.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101] 2018-05-28 14:40:01.780 [destination = myexample address = /172.16.9.11:3307 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `proc` ( `db` char(64) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL DEFAULT '' `name` char(64) NOT NULL DEFAULT '' `type` enum('FUNCTION' 'PROCEDURE') NOT NULL `specific_name` char(64) NOT NULL DEFAULT '' `language` enum('SQL') NOT NULL DEFAULT 'SQL' `sql_data_access` enum('CONTAINS_SQL' 'NO_SQL' 'READS_SQL_DATA' 'MODIFIES_SQL_DATA') NOT NULL DEFAULT 'CONTAINS_SQL' `is_deterministic` enum('YES' 'NO') NOT NULL DEFAULT 'NO' `security_type` enum('INVOKER' 'DEFINER') NOT NULL DEFAULT 'DEFINER' `param_list` blob NOT NULL `returns` longblob NOT NULL `body` longblob NOT NULL `definer` char(77) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL DEFAULT '' `created` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP `modified` timestamp NOT NULL DEFAULT '0000-00-00 00:00:00' `sql_mode` set('REAL_AS_FLOAT' 'PIPES_AS_CONCAT' 'ANSI_QUOTES' 'IGNORE_SPACE' 'NOT_USED' 'ONLY_FULL_GROUP_BY' 'NO_UNSIGNED_SUBTRACTION' 'NO_DIR_IN_CREATE' 'POSTGRESQL' 'ORACLE' 'MSSQL' 'DB2' 'MAXDB' 'NO_KEY_OPTIONS' 'NO_TABLE_OPTIONS' 'NO_FIELD_OPTIONS' 'MYSQL323' 'MYSQL40' 'ANSI' 'NO_AUTO_VALUE_ON_ZERO' 'NO_BACKSLASH_ESCAPES' 'STRICT_TRANS_TABLES' 'STRICT_ALL_TABLES' 'NO_ZERO_IN_DATE' 'NO_ZERO_DATE' 'INVALID_DATES' 'ERROR_FOR_DIVISION_BY_ZERO' 'TRADITIONAL' 'NO_AUTO_CREATE_USER' 'HIGH_NOT_PRECEDENCE' 'NO_ENGINE_SUBSTITUTION' 'PAD_CHAR_TO_FULL_LENGTH') NOT NULL DEFAULT '' `comment` text CHARACTER SET utf8 COLLATE utf8_bin NOT NULL `character_set_client` char(32) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL `collation_connection` char(32) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL `db_collation` char(32) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL `body_utf8` longblob PRIMARY KEY (`db` `name` `type`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COMMENT='Stored Procedures' com.alibaba.druid.sql.parser.ParserException: error pos 864 line 16 column 14 token SET at com.alibaba.druid.sql.parser.SQLExprParser.name(SQLExprParser.java:1334) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLExprParser.parseDataType(SQLExprParser.java:2310) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLExprParser.parseDataType(SQLExprParser.java:2301) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlExprParser.parseColumn(MySqlExprParser.java:493) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:115) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.25.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101] 2018-05-28 14:40:01.783 [destination = myexample address = /172.16.9.11:3307 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `procs_priv` ( `Host` char(60) COLLATE utf8_bin NOT NULL DEFAULT '' `Db` char(64) COLLATE utf8_bin NOT NULL DEFAULT '' `User` char(16) COLLATE utf8_bin NOT NULL DEFAULT '' `Routine_name` char(64) CHARACTER SET utf8 NOT NULL DEFAULT '' `Routine_type` enum('FUNCTION' 'PROCEDURE') COLLATE utf8_bin NOT NULL `Grantor` char(77) COLLATE utf8_bin NOT NULL DEFAULT '' `Proc_priv` set('Execute' 'Alter Routine' 'Grant') CHARACTER SET utf8 NOT NULL DEFAULT '' `Timestamp` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP PRIMARY KEY (`Host` `Db` `User` `Routine_name` `Routine_type`) KEY `Grantor` (`Grantor`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='Procedure privileges' com.alibaba.druid.sql.parser.ParserException: syntax error error in :'RE') COLLATE utf8_bin NOT NULL `' expect RPAREN actual IDENTIFIER pos 313 line 6 column 47 token IDENTIFIER COLLATE at com.alibaba.druid.sql.parser.SQLParser.printError(SQLParser.java:284) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLParser.accept(SQLParser.java:292) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:191) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.25.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101] 2018-05-28 14:40:01.789 [destination = myexample address = /172.16.9.11:3307 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTab` use mysql After configuration There are the following errors `2018-05-28 15:06:27.531 [destination = myexample address = /172.16.9.11:3307 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `user` ( `Host` char(60) COLLATE utf8_bin NOT NULL DEFAULT '' `User` char(16) COLLATE utf8_bin NOT NULL DEFAULT '' `Password` char(41) CHARACTER SET latin1 COLLATE latin1_bin NOT NULL DEFAULT '' `Select_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Insert_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Update_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Delete_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Create_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Drop_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Reload_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Shutdown_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Process_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `File_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Grant_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `References_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Index_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Alter_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Show_db_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Super_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Create_tmp_table_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Lock_tables_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Execute_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Repl_slave_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Repl_client_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Create_view_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Show_view_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Create_routine_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Alter_routine_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Create_user_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Event_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Trigger_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Create_tablespace_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `ssl_type` enum('' 'ANY' 'X509' 'SPECIFIED') CHARACTER SET utf8 NOT NULL DEFAULT '' `ssl_cipher` blob NOT NULL `x509_issuer` blob NOT NULL `x509_subject` blob NOT NULL `max_questions` int(11) unsigned NOT NULL DEFAULT '0' `max_updates` int(11) unsigned NOT NULL DEFAULT '0' `max_connections` int(11) unsigned NOT NULL DEFAULT '0' `max_user_connections` int(11) unsigned NOT NULL DEFAULT '0' `plugin` char(64) COLLATE utf8_bin DEFAULT '' `authentication_string` text COLLATE utf8_bin `password_expired` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' PRIMARY KEY (`Host` `User`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='Users and global privileges'` Try to change to version 26. This is a bug in version 25. I upgraded to 26 is also the problem, but I feel like this is a permission problem. I gave all the permissions to the canal user in mysql. No problem. GRANT ALL PRIVILEGES ON *.* TO 'canal'@'%' ; FLUSH PRIVILEGES; LS Correct Answer Version v1 0 25 Open After tsdb It’s good to have this error deleted.
