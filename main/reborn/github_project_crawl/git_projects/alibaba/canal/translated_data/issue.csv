issue_id,issue_content
1086,Application yml configuration package settings default template
1084,fix #1083
1083,Flat Send RocketMQ appears to send empty packets. Kafka should have the same problem. [https://github.com/alibaba/canal/blob/master/server/src/main/java/com/alibaba/otter/canal/rocketmq/CanalRocketMQProducer.java](url) Code The value returned in the array of arrays at line 96 may be empty but there is no judgment below to send a null message to mq. Kafka should also have the same problem. Another question, please ask God to help answer FlatMessage This object Attribute data length Under what circumstances is greater than 1 There is a dml operation data Size will be greater than 1
1082,Cana kafak1 1 0 version build kafka consumer client how to parse the ROWDATA type storeValue 2018-10-31 15:49:04.318 [Thread-1] INFO c.a.o.canal.client.running.kafka.CanalKafkaClientExample - Message[id=12 entries=[header { version: 1 logfileName: "mysql-bin.000002" logfileOffset: 1206761 serverId: 1 serverenCode: "UTF-8" executeTime: 1540972144000 sourceType: MYSQL schemaName: "" tableName: "" eventLength: 79 } entryType: TRANSACTIONBEGIN storeValue: " _" header { version: 1 logfileName: "mysql-bin.000002" logfileOffset: 1206909 serverId: 1 serverenCode: "UTF-8" executeTime: 1540972144000 sourceType: MYSQL schemaName: "smart_meter" tableName: "sys_region_new" eventLength: 46 eventType: INSERT props { key: "rowsCount" value: "1" } } entryType: ROWDATA storeValue: "\b\212\001\020\001P\000b\217\001\022\036\b\000\020\004\032\002id \001(\0010\000B\00530026R\aint(11)\022\037\b\001\020\f\032\004name \000(\0010\000B\000R\vvarchar(20)\022!\b\002\020\004\032\tparent_id \000(\0010\000B\0012R\aint(11)\022)\b\003\020\371\377\377\377\377\377\377\377\377\001\032\005level \000(\0010\000B\0011R\ntinyint(1)" header { version: 1 logfileName: "mysql-bin.000002" logfileOffset: 1206955 serverId: 1 serverenCode: "UTF-8" executeTime: 1540972144000 sourceType: MYSQL schemaName: "" tableName: "" eventLength: 31 } entryType: TRANSACTIONEND storeValue: "\022\0049961" ] raw=true rawEntries=[]] I want to know how to resolve storeValue I have been tossing for a long time. Can refer to MessageUtil.parse4Dml
1081,10 1 22 MariaDB version database journalName garbled Hello, I am using 10 1 22 MariaDB and 1 1 1 version canal after booting the following error 2018-11-01 09:40:42.663 [destination = cloud address = /192.168.1.21:3306 EventParser] WARN c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - prepare to find start position just last position {"identity":{"slaveId":-1 "sourceAddress":{"address":"192.168.1.21" "port":3306}} "postion":{"gtid":"" "included":false **"journalName":"mysql-bin.000607Æ\u009E´U"** "position":91598031 "serverId":1 "timestamp":1541028888000}} 2018-11-01 09:40:42.674 [destination = cloud address = /192.168.1.21:3306 EventParser] WARN c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - find start position : EntryPosition[included=false journalName=mysql-bin.000607Æ´U position=91598031 serverId=1 gtid= timestamp=1541028888000] 2018-11-01 09:40:42.690 [destination = cloud address = /192.168.1.21:3306 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:102) ~[canal.parse-1.1.1.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:216) [canal.parse-1.1.1.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:252) [canal.parse-1.1.1.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-11-01 09:40:42.691 [destination = cloud address = /192.168.1.21:3306 EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address /192.168.1.21:3306 has an error retrying. caused by java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:102) ~[canal.parse-1.1.1.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:216) ~[canal.parse-1.1.1.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:252) ~[canal.parse-1.1.1.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-11-01 09:40:42.692 [destination = cloud address = /192.168.1.21:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:cloud[java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:102) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:216) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:252) at java.lang.Thread.run(Thread.java:748) ] Seeing that there is garbled journalName, how can I solve it? Local has not been heavy about the existing debug ability, you can try to see the resolution process of RotateLogEvent I changed the EntryPosition class Added an analytical method ``` public EntryPosition(String journalName Long position Long timestamp){ super(timestamp); this.journalName = filterLogFileName(journalName); this.position = position; } public EntryPosition(String journalName Long position Long timestamp Long serverId){ this(journalName position timestamp); this.serverId = serverId; } public String filterLogFileName(String logfileName) { Pattern p = Pattern.compile("(mysql-bin\\.[0-9]{6})."); Matcher m = p.matcher(logfileName); while(m.find()){ if(m.groupCount() >= 1) { return m.group(1); } } return logfileName; } public String getJournalName() { return filterLogFileName(journalName); } public void setJournalName(String journalName) { this.journalName = filterLogFileName(journalName); } ``` Then it’s so good This reform is a hard-coded process of saving the country without finding the root cause. @haopangxu Can try my change main reason Mariadb is not quite consistent with mysql in handling checksum behavior. For the rotate_event binlog object mariab will execute checksum and mysql will not need special judgment.
1079,Application yml is not packaged into jar Application yml is not packaged into jar Solve the merge error in the application yml and config in the jar
1078,canal-server Will a reboot in the backlog event discard the backlog in memory? ## problem canal-server In cooperation ZooKeeperLogPositionManager is read after restarting binlog The locus is server Last time parse Position still client ack s position according to https://github.com/alibaba/canal/wiki/Introduction#eventparser%E8%AE%BE%E8%AE%A1 Description of EventParser recorded binlog The last time is parse Does this behavior at the site of the site lead to server Drop event on reboot ## phenomenon This question was raised because it was encountered in practice. binlog The problem of missing events is as follows in canal-server There is an event backlog about 20 M restarts when the set memory limit is not reached canal server canal client zk 使用 docker-compose Deploy the server client with zk Single instance server Use mirroring as canal/canal-server:v1.1.0），canal Some events will be discarded ## binlog Lost reason guess according to https://github.com/alibaba/canal/wiki/Introduction#eventparser%E8%AE%BE%E8%AE%A1 Description of EventParser Record only parse Arrived binlog The reason for the site guess is server The terminal restarts after restarting the event from the last time parse The consumption of the site is caused by the memory parse、sink、store But not yet get of binlog Lost ## client Code client based on commit node 82f8a9f - fixed issue #483 show slave hosts Modify use com.alibaba.otter.canal.client.ClientLauncher With custom CanalOuterAdapter Make a purchase in order to record the received binlog Site pair com.alibaba.otter.canal.client.adapter.loader.CanalAdapterWorker The following modifications were made ```java while (running) { try { // if (switcher != null) { // switcher.get(); // } logger.info("=============> Start to connect destination: {} <=============" this.canalDestination); connector.connect(); logger.info("=============> Start to subscribe destination: {} <=============" this.canalDestination); connector.subscribe(); logger.info("=============> Subscribe destination: {} succeed <=============" this.canalDestination); while (running) { // try { // if (switcher != null) { // switcher.get(); // } // } catch (TimeoutException e) { // break; // } // Server configuration canal instance network soTimeout default 30s) // The server does not interact with the server within the scope. The socket connection will be closed. // Below is the added part Message message = connector.getWithoutAck(BATCH_SIZE); // Get the specified amount of data for (CanalEntry.Entry entry : message.getEntries()) { if (!entry.getEntryType().equals(CanalEntry.EntryType.ROWDATA)) { continue; } logger.info("receive binlog event file {} offset {}" entry.getHeader().getLogfileName() entry.getHeader().getLogfileOffset()); } ``` an examination client Log found received binlog Discontinuous range canal Totally discarded binlog The same range because it is client Printed at the entrance binlog Site so I think client Not received binlog Event server Discarded this part of the incident ## Configuration instance.xml versus canal/deployer/src/main/resources/spring/default-instance.xml of diff as follows ```xml 112 120c112 113 < <bean class="com.alibaba.otter.canal.parse.index.FailbackLogPositionManager"> < <constructor-arg> < <bean class="com.alibaba.otter.canal.parse.index.MemoryLogPositionManager" /> < </constructor-arg> < <constructor-arg> < <bean class="com.alibaba.otter.canal.parse.index.MetaLogPositionManager"> < <constructor-arg ref="metaManager"/> < </bean> < </constructor-arg> --- > <bean class="com.alibaba.otter.canal.parse.index.ZooKeeperLogPositionManager"> > <constructor-arg index="0" ref="zkClientx"/> 129c122 < <bean class="com.alibaba.otter.canal.parse.support.AuthenticationInfo" init-method="initPwd"> --- > <bean class="com.alibaba.otter.canal.parse.support.AuthenticationInfo"> 133 134d125 < <property name="pwdPublicKey" value="${canal.instance.pwdPublicKey:retl}" /> < <property name="enableDruid" value="${canal.instance.enableDruid:false}" /> 139c130 < <bean class="com.alibaba.otter.canal.parse.support.AuthenticationInfo" init-method="initPwd"> --- > <bean class="com.alibaba.otter.canal.parse.support.AuthenticationInfo"> 143 144d133 < <property name="pwdPublicKey" value="${canal.instance.pwdPublicKey:retl}" /> < <property name="enableDruid" value="${canal.instance.enableDruid:false}" /> ``` ## Please ask the gods to be confused, is my configuration wrong? server Really lose backlog of events after reboot The storage of the canal server event is based on the memory implementation of the EventStore Losing is inevitable, look at the wiki. The wiki is written very clearly. Restart reading the last client The ack&#39;s memory data will be lost but will continue to be dump based on the last successful consumption. Binlog So there is no message loss on the architecture as long as the correct ack site You are a configuration problem. You must refer to the file default instance xml in the com alibaba otter canal parse index FailbackLogPositionManager configuration. It will read MetaLogPositionManager when restarting. Ack&#39;s locus > You are a configuration problem. You must refer to the file default instance xml in the com alibaba otter canal parse index FailbackLogPositionManager configuration. It will read MetaLogPositionManager when restarting. Ack&#39;s locus Thank God for guiding the problem has been solved
1076,About rds oss binlog Offline read problem hi Hello I configured it in the instance configuration file. #rds oss binlog canal.instance.rds.accesskey= canal.instance.rds.secretkey= canal.instance.rds.instanceId= Restart canal The cluster still can&#39;t get the binlog of oss What is the reason for this? Look at the instance log and there is no information about oss. This is the error message 2018-10-30 16:33:20.395 [destination = instance address = address EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:instance[java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:102) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:153) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:262) at java.lang.Thread.run(Thread.java:745) Canal instance rds accesskey canal instance rds secretkey operation ID to the application rds instanceId instance id is also no problem oss Binlog&#39;s backtracking currently only supports timestamp-based positioning. It will determine the current mysql. If binlog has the data of the timestamp, it will automatically download binlog from oss and then parse it locally until local mysql Time can be connected in the binlog ps. If you give a wrong binlog site, you are not sure whether to give the site error or the binlog is deleted. You can then optimize the RDS case. @agapple Thank you for your reply. Now there is still a problem that I canal The normal consumption binlog of the clients has been running for a while, but I haven’t updated the location where the instance was consumed in zk. What is the reason? The HA of the canal is made with three nodes. As shown ![2121](https://user-images.githubusercontent.com/12511065/47762190-91bdca00-dcf6-11e8-8aaa-aec578ff67d1.jpg) Yesterday afternoon, I saw this, there is no update position. canal What is the update strategy for the ha site? I see the documentation saying so. **canal After receiving the ack of the client, the server will record the last site submitted by the client. But my site has not been updated 1. You can&#39;t open canal instance filter transaction entry true on your server side. 2. Check the meta log corresponding to the server node of the current run. If you have an ack, the information will be recorded. @agapple 1. I set canal instance filter transaction entry true for each node. 2 did not see the running node has a corresponding meta log ... Well, there is a problem with my cluster setup. 1. Canal instance filter transaction entry true This set of questions filters the header and tail without updates. Since the bit record is recorded, the new end v1 1 1 is compatible with this parameter. Filter does not filter all but keeps a transaction event for a few seconds. 2. rds The problem with the binlog site has been compatible. The normal site is deleted. As long as the timestamp is in the site, the oss will be downloaded automatically. binlog @agapple Thanks again Zk site update can already be caused by the problem of canal instance filter transaction entry true > oss Binlog&#39;s backtracking currently only supports timestamp-based positioning. It will determine the current mysql. If binlog has the data of the timestamp, it will automatically download binlog from oss and then parse it locally until local mysql Time can be connected in the binlog > > ps. If you give a wrong binlog site, you are not sure whether to give the site error or the binlog is deleted. You can then optimize the RDS case. What do you say about RDS optimization? I still can&#39;t determine if it has gone to oss to download the binlog I want. Optimization I have submitted the code and can re-package based on the master
1075,Kafka canal instance filter transaction entry problem The kafka mode is set after the canal instance filter transaction entry true in the canal properties file. Masked TransactionBegin event but can also receive Transactionend event Is the use of flatMessage false can try flat case is true
1071,Canal1 1 1 running client KafkaClientRunningTest error In the 1 1 0 version is Message message = connector.getWithoutAck(3L TimeUnit.SECONDS); In the 1 1 1 version is List Message messages = connector.getList(3L TimeUnit.SECONDS); There is no problem running the 1 1 0 version. You can see that the message packet runs 1 1 1 and the error is reported at this line of code. Is it still not a stable version? Set flatMessage in the configuration mq yml file in deploy to false KafkaClientRunningTest for native message reception Configure mq yml file to set flatMessage to false KafkaClientRunningTest to native message connector.getWithoutAck(batchSize 5L TimeUnit.SECONDS); Execution can&#39;t run in 2018-10-30 09:45:54，"rewerma" <notifications@github.com> Write Set flatMessage in the configuration mq yml file in deploy to false KafkaClientRunningTest for native message reception — You are receiving this because you authored the thread. Reply to this email directly view it on GitHub or mute the thread. 报com alibaba otter canal protocol exception CanalClientException  mq not support this method connector.getWithoutAck(batchSize 5L TimeUnit SECONDS This is not a method provided by kafkaConnector Test class do not modify There is no problem with using this code. ``` executor.submit(new Runnable() { @Override public void run() { connector.connect(); connector.subscribe(); while (running) { try { List<Message> messages = connector.getList(3L TimeUnit.SECONDS); if (messages != null) { System.out.println(messages); } connector.ack(); } catch (WakeupException e) { // ignore } } connector.unsubscribe(); connector.disconnect(); } }); ``` Use the above test class code flatMessage set to false after running, no response to change the catch block to Exception After seeing the error message is `Caused by: com.alibaba.otter.canal.protocol.exception.CanalClientException: deserializer failed at com.alibaba.otter.canal.client.CanalMessageDeserializer.deserializer(CanalMessageDeserializer.java:52) at com.alibaba.otter.canal.client.CanalMessageDeserializer.deserializer(CanalMessageDeserializer.java:14) at com.alibaba.otter.canal.client.kafka.MessageDeserializer.deserialize(MessageDeserializer.java:24) at com.alibaba.otter.canal.client.kafka.MessageDeserializer.deserialize(MessageDeserializer.java:1) at org.apache.kafka.common.serialization.ExtendedDeserializer$Wrapper.deserialize(ExtendedDeserializer.java:65) at org.apache.kafka.common.serialization.ExtendedDeserializer$Wrapper.deserialize(ExtendedDeserializer.java:55) at org.apache.kafka.clients.consumer.internals.Fetcher.parseRecord(Fetcher.java:967) at org.apache.kafka.clients.consumer.internals.Fetcher.access$3300(Fetcher.java:93) at org.apache.kafka.clients.consumer.internals.Fetcher$PartitionRecords.fetchRecords(Fetcher.java:1144) at org.apache.kafka.clients.consumer.internals.Fetcher$PartitionRecords.access$1400(Fetcher.java:993) at org.apache.kafka.clients.consumer.internals.Fetcher.fetchRecords(Fetcher.java:527) at org.apache.kafka.clients.consumer.internals.Fetcher.fetchedRecords(Fetcher.java:488) at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1155) at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1115) at com.alibaba.otter.canal.client.kafka.KafkaCanalConnector.getListWithoutAck(KafkaCanalConnector.java:173) at com.alibaba.otter.canal.client.kafka.KafkaCanalConnector.getList(KafkaCanalConnector.java:159) at com.alibaba.otter.canal.client.running.kafka.KafkaClientRunningTest$1.run(KafkaClientRunningTest.java:44) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) Caused by: com.google.protobuf.InvalidProtocolBufferException: Protocol message end-group tag did not match expected tag. at com.google.protobuf.InvalidProtocolBufferException.invalidEndTag(InvalidProtocolBufferException.java:110) at com.google.protobuf.CodedInputStream$ArrayDecoder.checkLastTagWas(CodedInputStream.java:660) at com.google.protobuf.CodedInputStream$ArrayDecoder.readGroup(CodedInputStream.java:869) at com.google.protobuf.UnknownFieldSet$Builder.mergeFieldFrom(UnknownFieldSet.java:541)` groupId another The old faltMessage you are consuming Ok, thank you master
1069,pass useDruidDdlFilter parameter to LogEventConvert Description of tsdb function changes 1. Tsdb should only care about ddl Dml and dcl should be skipped image #450 Such problems will not appear again, of course fastsql should also support dcl 2. Regardless of whether ddl filters tsdb functions, it should ensure correctness. 3. ddl The history write is placed behind the filter to ensure that there is no permission problem when the table structure is compared. [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=1069) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=1069) before we can accept your contribution.<br/><hr/>**wuwo** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=1069) it.</sub> [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=1069) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=1069) before we can accept your contribution.<br/><hr/>**wuwo** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=1069) it.</sub> Such a change will have a risk if fastsql Parse is parsing normal DDL statements when there is an exception DdlResult returns the default is Query, you will ignore it here, even a little log is not If the ddl parsed exception plus the ddl parsed exception is ignored and then directly return Query Ddl exception plus effect, I think there is no difference between it and before. If the druidFilter can be parsed normally, the unsubscribed table has been filtered. I rolled back the changes about tsdb but currently as you said if the normal ddl parsing failed Will return directly to Query. If dcl filtering is enabled, this ddl change will still lose the normal ddl resolution. If ddl filtering is enabled. Ddl changes will also be lost I still think that the correctness of the tsdb function should not be tied to the filterQueryDdl filterQueryDml filterQueryDcl. It is estimated that many users are not aware of this problem. I think about other elegant adjustments. tks
1066,Ddl is lost during binlog synchronization Mysql version 5.6.26 Canalserver version 1.0.24 In the process of real-time synchronization binlog processing dml, tens of seconds sent a ddl statement and found that the client did not get this ddl statement. What is the reason? canal.instance.get.ddl.isolation Is it related to this configuration? I use the default value of false. Close the issue, I found it myself. @yxzhang666 > Close the issue, I found it myself. @yxzhang666 The logic of our DBA add table field is inconsistent with our processing. We pass the CanalEntry EventType ALTER Get ddl And deal with the ddl statement dba is to add the field after the table name is replaced by the clone table and then the previous table drop and then change the name of the table before the clone table name. The new table name we did not pay attention to, so did not get the ddl strictly speaking Canal The server did not lose the binlog We didn’t get a false alarm Canal instance get ddl isolation This isolation we want to open there is no suggestion Is this not the practice of pt schema change online and ghost? Canal instance get ddl isolation true If it is open it will be separated from other DML multiple batches A batch a ddl No difference between the essence can be used for data synchronization > A batch of a ddl is not the essence of the difference can be used for data synchronization Thank you, I&#39;ve now understood > Is this not the practice of pt schema change online and ghost? Don&#39;t know much about this piece
1065,fix: fix NPE When using CanalServerWithManager, do not pass the masterPosition in GTID mode. NPE is generated when the masterPosition is not passed in the GTID mode when using the CanalServerWithManager. [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=1065) <br/>All committers have signed the CLA. tks Your CanalServerWithManager docked your own console @agapple We are a platform based on canal for incremental and full data synchronization. I made a set of consoles myself. > I made a set of consoles myself. I haven&#39;t done the page yet, but I have done core functions.
1064,1 1 0 version of the golang client Error message Fatal error: proto: Messages: illegal tag 0 (wire type 0)exit status 1 Can you give me a picture of how you called it? you can go [canal-go-issue](https://github.com/CanalClient/canal-go/issues) Issue Looked at this error occurred when the get operation proto decoding and there is no operation at the time I am testing the pass here. Is it still wrong? Occasionally there is not always there and there should be no data at the time when the database is not operating. Has been resolved
1063,RocketMQ partition sending No commit RocketMQ partition sending No commit [https://github.com/alibaba/canal/blob/master/server/src/main/java/com/alibaba/otter/canal/rocketmq/CanalRocketMQProducer.java](url)
1061,Abnormal full Gc problem We are using the 1 10 version to do a simple secondary development to achieve data transmission to kafka multi-partition through the pinpoint monitoring tool to find that the server will trigger full when the heap memory is used to 1 5G Gc but the default setting of the maximum available memory is 3G consumer does not have this problem start start sh content has not been modified for specific reasons are being investigated, do other people have this problem
1060,AdminGuide document not updated 1. operating system a. Pure java development windows linux can support b. Jdk recommends using a stable version of 1 6 25 or higher. Currently Alibaba uses this version basically. 2. Mysql request a. Currently canal supports mysql 5 5 version below mysql5 6 does not support mysql4 x version has not been rigorously tested theoretically compatible Fixed
1059,CanalConnector interface removes the stopRunning method CanalConnector interface removes the stopRunning method The stopRunning method is only called internally
1058,Doubts about the Docker pattern 1、docker stop Container id can&#39;t stop the container 2 Using docker rm -f Remove the container but use the image to start the container again. It shows that it has just been removed. It already exists. Sometimes there is still 11111 port occupied. You need to modify the container name in the startup script to start the container. Thank you for explaining Look for the basic operation of docker online. close
1057,kafka canal-adapter Distributed switch bug Solve the problem that the distributed synchronous switch is off synchronously Restart kafka Sync after the canal adapter Missing data bug
1056,group The thread pool is not initialized.
1055,Code sorting
1054,[ISSUE1027]Add flat message support to rocketmq Added support for rocketmq queues after hashing so that it can distribute Flat message. tks
1053,Whether can support transaction grouping In the database operation, the multi-table operation with transaction nature can not group or package this part of data into one packet when the data is output or group it according to the transaction id. The data transaction has a begin end for parcel service and can be combined based on this block. > The data transaction has a begin end for parcel service and can be combined based on this block. Whether can support python docking to synchronize data to kafka The new 1 1 1 version will support direct delivery of data to kafka and can be consumed by the python client corresponding to kafka. > The new 1 1 1 version will support direct delivery of data to kafka and can be consumed by the python client corresponding to kafka. Thank you, I can directly configure the data from kafka through python. Yes > Yes I saw the commit record in the 1 1 1 version on GitHub and wrote to remove kafka The server side TRANSACTIONBEGIN and TRANSACTIONEND judge this version to handle the transaction operation, then by what way to determine the transaction id?
1052,Canal start normal client can not get binlog # Mysql version 5.7 # canal 1.1.1 # logs/example/example.log ``` 2018-10-28 09:20:28.615 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2018-10-28 09:20:28.630 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties] 2018-10-28 09:20:28.860 [main] WARN o.s.beans.GenericTypeAwarePropertyDescriptor - Invalid JavaBean property 'connectionCharset' being accessed! Ambiguous write methods found next to actually used [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.lang.String)]: [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.nio.charset.Charset)] 2018-10-28 09:20:28.967 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2018-10-28 09:20:28.967 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties] 2018-10-28 09:20:29.289 [main] ERROR com.alibaba.druid.pool.DruidDataSource - testWhileIdle is true validationQuery not set 2018-10-28 09:20:29.736 [main] INFO c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-example 2018-10-28 09:20:29.916 [main] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - subscribe filter change to .*\..* 2018-10-28 09:20:29.916 [main] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - start successful.... 2018-10-28 09:20:29.918 [destination = example address = /192.168.255.129:3306 EventParser] WARN c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - prepare to find start position just show master status 2018-10-28 09:20:59.976 [New I/O server worker #1-1] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - subscribe filter change to .*\..* 2018-10-28 09:23:20.777 [destination = example address = /192.168.255.129:3306 EventParser] WARN c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - find start position : EntryPosition[included=false journalName=mysql-bin.000009 position=383 serverId=1 gtid=<null> timestamp=1540689592000] 2018-10-28 09:25:09.385 [New I/O server worker #1-2] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - subscribe filter change to .*\..* ~ ``` Canal canal log error ``` 2018-10-28 12:13:15.534 [New I/O server worker #1-2] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x26e93058 /192.168.255.1:65515 => /192.168.255.129:11111] exception=java.io.IOException: Connection reset by peer at sun.nio.ch.FileDispatcherImpl.read0(Native Method) at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39) at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223) at sun.nio.ch.IOUtil.read(IOUtil.java:192) at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380) at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:322) at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) ``` Connection reset by peer Estimated free link is closed > Connection reset by peer Estimated free link is closed How to solve please advise solved
1051,Binlog change client is not responding Mysql canal use docker mode to deploy both startups are normal mysql Binlog file mapping in the host var lib mysql directory operation database client side no response always print empty Whether count is a problem with the binlog file location Mysql version is 5 7 The same problem Check canal Corresponding log in the server I am using stand-alone mode and need to set the parallel parameter to false but the error log only shows a null pointer error message is not friendly
1050,Modify the hbase configuration item name
1049,1 1 1 version configuration kafka What is the groupId in AbstractKafkaTest?
1047,tablemeta Tsdb data increases expiration cleanup ability TableMetaTSDB介绍 https github com alibaba canal wiki TableMetaTSDB Thinking about the problem tablemeta will do checkpoint 24 hours by default to generate a snapshot data persistence. As the running time gets longer and longer, the snapshot data will continue to expand. It is necessary to add an expire strategy to periodically clean up the stale data. Design ideas 1. Add two parameters canal instance tsdb snapshot interval / canal.instance.tsdb.snapshot.expire 2. Periodically clean the snapshot data that exceeds the expire time within the interval of the interval. Note that the first snapshot of the initial init will be retained here. The first binlog_timestamp 1 Avoid all snapshot data being cleaned up
1046,Column type is tinyint 1 Unsigned but the column value obtained by canal is incorrect when the actual data value is greater than 127 Column type is tinyint 1 Unsigned but the actual data value is greater than 127. Between 128 and 255 canal is processed according to boolean and directly converted to string value becomes negative. In LogEventConvert java, line 674 is recommended to be modified as follows. // if (isSingleBit && javaType == Types.TINYINT) { // javaType = Types.BIT; // } if (buffer.isNull()) { columnBuilder.setIsNull(true); } else { final Serializable value = buffer.getValue(); if (isSingleBit && javaType == Types.TINYINT && ((Number) value).intValue() >= 0) { javaType = Types.BIT; } //... https://github.com/alibaba/canal/blob/master/parse/src/main/java/com/alibaba/otter/canal/parse/inbound/mysql/dbsync/LogEventConvert.java Convenient to provide SQL for testing
1045,Modify the package configuration tks
1044,canal Filtered expression Expression is too large. Caused by: com.alibaba.otter.canal.filter.exception.CanalFilterException: org.apache.oro.text.regex.MalformedPatternException: Expression is too large. Caused by: org.apache.oro.text.regex.MalformedPatternException: Expression is too large. at org.apache.oro.text.regex.Perl5Compiler.compile(Unknown Source) at org.apache.oro.text.regex.Perl5Compiler.compile(Unknown Source) at com.alibaba.otter.canal.filter.PatternUtils$1.apply(PatternUtils.java:30) at com.alibaba.otter.canal.filter.PatternUtils$1.apply(PatternUtils.java:25) at com.google.common.collect.ComputingConcurrentHashMap$ComputingValueReference.compute(ComputingConcurrentHashMap.java:356) at com.google.common.collect.ComputingConcurrentHashMap$ComputingSegment.compute(ComputingConcurrentHashMap.java:182) at com.google.common.collect.ComputingConcurrentHashMap$ComputingSegment.getOrCompute(ComputingConcurrentHashMap.java:151) at com.google.common.collect.ComputingConcurrentHashMap.getOrCompute(ComputingConcurrentHashMap.java:67) at com.google.common.collect.MapMaker$ComputingMapAdapter.get(MapMaker.java:885) at com.alibaba.otter.canal.filter.PatternUtils.getPattern(PatternUtils.java:41) at com.alibaba.otter.canal.filter.aviater.RegexFunction.call(RegexFunction.java:24) at Script_1540529734651_0.execute0(Unknown Source) at com.googlecode.aviator.ClassExpression.execute(ClassExpression.java:53) at com.alibaba.otter.canal.filter.aviater.AviaterRegexFilter.filter(AviaterRegexFilter.java:74) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEventForTableMeta(LogEventConvert.java:456) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$SimpleParserStage.onEvent(MysqlMultiStageCoprocessor.java:247) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$SimpleParserStage.onEvent(MysqlMultiStageCoprocessor.java:222) at com.lmax.disruptor.BatchEventProcessor.processEvents(BatchEventProcessor.java:168) at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:125) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) This question is very strange. I will have no problem after I execute it later. How long is your expression? Wait, let me revisit it carefully. I am debugging it. canal Expression calculation module pattern="^pub_operation_log2.tb_operation_log_429$|^pub_operation_log2.tb_operation_log_428$|^pub_operation_log0.tb_operation_log_201$|^pub_operation_log2.tb_operation_log_427$|^pub_operation_log0.tb_operation_log_200$|^pub_operation_log2.tb_operation_log_426$|^pub_operation_log2.tb_operation_log_425$|^pub_operation_log2.tb_operation_log_424$|^pub_operation_log0.tb_operation_log_205$|^pub_operation_log2.tb_operation_log_423$|^pub_operation_log0.tb_operation_log_204$|^pub_operation_log2.tb_operation_log_422$|^pub_operation_log0.tb_operation_log_203$|^pub_operation_log2.tb_operation_log_421$|^pub_operation_log0.tb_operation_log_202$|^pub_operation_log2.tb_operation_log_420$|^pub_operation_log0.tb_operation_log_209$|^pub_operation_log0.tb_operation_log_208$|^pub_operation_log0.tb_operation_log_207$|^pub_operation_log0.tb_operation_log_206$|^pub_operation_log2.tb_operation_log_419$|^pub_operation_log2.tb_operation_log_418$|^pub_operation_log2.tb_operation_log_417$|^pub_operation_log2.tb_operation_log_41 。。。。。。" target= "tts_operation_log0.tb_operation_log_244" I&#39;m here otter In the configuration, the sub-library is also used. pub_operation_log[0-4] | tb_operation_log_[0-499] I saw it really is The table name is too long 2500 个 pub_operation_log2 tb_operation_log_417  Similar to this expression I am running a demo I can&#39;t run anymore. idea Directly say Error 109 60) java: Constant string is too long When using sub-database sub-tables, you need to pay attention to the following strings can not be too long You are a sub-library You can use regular matching to match
1043,Canal can&#39;t parse mysql5 6 binlog for God help Operating system Centos6 5 Database version mysql5 6 First of all, I added these three in the etc my cnf file of mysql. [mysqld] server-id=1 log-bin=mysql-bin binlog-format=ROW After the configuration is finished, look at the picture ![image](https://user-images.githubusercontent.com/32409300/47547957-f4dfe300-d929-11e8-9cfb-c6bac9679f58.png) Added a canal account to my mysql Open all permissions Repl_slave_priv Y ![image](https://user-images.githubusercontent.com/32409300/47548081-6750c300-d92a-11e8-942b-9b5bba436b97.png) Monitor canal&#39;s log tail -f logs/example/example.log Start to start canal The log content is as follows ![image](https://user-images.githubusercontent.com/32409300/47548180-bdbe0180-d92a-11e8-9a64-7608dccd144f.png) My canal configuration item ![image](https://user-images.githubusercontent.com/32409300/47548240-05dd2400-d92b-11e8-8dc2-d7a1bced6de3.png) This parameter is not in the screenshot canal instance filter regex Executing sql to view the files in the binlog log is subject to change. ![image](https://user-images.githubusercontent.com/32409300/47548333-5785ae80-d92b-11e8-9d0f-5ed4882955b9.png) In the above figure, the position of the binlog is a bit problematic. The correct posture is usr bin mysqlbinlog. -v mysql-bin.000003 ![image](https://user-images.githubusercontent.com/32409300/47548446-a7647580-d92b-11e8-9bd1-69c495fb00b7.png) Test is my database name user is my table name After I executed sql Canal log has not changed The canal client can connect to the canal server normally, but the canal server can&#39;t parse the mysql binlog. If the description is not clear, please correct me. Example project is to print to another directory carefully looking for The big god that prints to another directory means that if the binlog log is generated, it is not visible in the log file example example log. @agapple ![image](https://user-images.githubusercontent.com/32409300/47554077-1dbca400-d93b-11e8-88ce-4cf6082b9646.png)
1042,canal adapter readme.md
1041,Canal example 1 1 0 The startup bat script under the tar gz package prompts that the command syntax is incorrect. I am currently testing in windows7 environment. Grammar error message content or submit a repaired PR to me ![2018-10-29_134009](https://user-images.githubusercontent.com/12081348/47631335-a710e800-db80-11e8-8825-06467c7045da.png) Very low-level questions basic bat usage you first google
1040,When the canal cluster is hung up, the zk node will be deleted. Will the data be repeated after another sever? When the canal cluster is hung up, the zk node will be deleted. Will the data be repeated after another sever? Data will be repeated > Data will be repeated Repeat is normal @agapple Can&#39;t avoid data not repeating more wiki
1039,Specification note tks
1038,Canal Send to Kafka Topic Implementing Partition at the same time as the message Partition Canal 1.1.0 Version messages are sent to Kafka while supporting specified messages by Partition Key Distribute to different partitions? Https github com alibaba canal wiki Canal Kafka RocketMQ QuickStart Upcoming 1 1 1 supports such hashing
1037,Improve the adapter launcher and hbase adaper related functions Perfect adapter launcher Perfect hbase adaper Add hbase Etl function Add client sync switch Ettl sync lock and other functions tks
1036,Clients can&#39;t connect after Docker deploys canal First canal has been deployed in docker to start the service port 11112 Then docker has a host IP with Virtual IP native computer access is via virtual IP 192 168 102 111 6633 Then the client uses the 192 168 102 111 6633 connection canal to start the service, the client does not receive the information and the canal server always outputs the ca otter canal server netty handler SessionHandler - message receives in session handler... How to solve the IP problem? Follow Docker The script in QuickStart is started as host mode and port mapping is done.
1034,How will canal The CanalEntry received by the client Parsed into executable sql Statement Master Mysql A Executed in sql then Slave canal Received CanalEntry How the object parses the object become Executable sql Then sync to the database mysql B has cost a lot of effort to assemble sql I found that if I had to assemble it, I wouldn’t have a different situation. Is there any convenient way? 1.1.0 In the version DDL operations can get sql directly Can the DML operation get sql directly? In the canal example project, see the sample code synchronized to the DB https github com alibaba canal tree master example src main java com alibaba otter canal example db
1032,v1.1.1-alpha 1 The version does not add a configurable row parameter. The default is true. v1.1.1-alpha 1 The version does not add a configurable row parameter. The default is true. There is still an Enable parsing performance issue with CanalServerWithEmbedded ![Uploading image.png…]() ![image](https://user-images.githubusercontent.com/34024756/47475509-474bd180-d84e-11e8-88f7-bc45e2df4483.png) ![image](https://user-images.githubusercontent.com/34024756/47475517-4f0b7600-d84e-11e8-8266-425e7162b102.png)
1031,Added adapter launcher project Add adapter launcher Springboot project
1030,Start up always does not move or error log has this 2018-10-24 11:08:47.331 [destination = canal-car address = rm-m5e5o46l2f13j7an8.mysql.rds.aliyuncs.com/172.31.19.222:3306 EventParser] WARN c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - prepare to find start position just show master status 2018-10-24 11:08:52.651 [destination = canal-car address = rm-m5e5o46l2f13j7an8.mysql.rds.aliyuncs.com/172.31.19.222:3306 EventParser] WARN c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - find start position : EntryPosition[included=false journalName=mysql- bin.000570 position=1244577 serverId=1797136470 gtid=<null> timestamp=1540350517000] This does not affect
1029,V1 1 0 Update the t_canal_table table on mysql about 30w records of the same field capacity 755B Number of fields 79 canal parsing bin Log error column size is not match for table: database_canal.t_canal_table 79 vs 78 Update statement sql = "UPDATE t_canal_table SET update_time= " + "'" + str(datetime.datetime.now()) + "'" Exception information Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: column size is not match for table:glocalme_css_tcanal.t_css_vsim 79 vs 78 ] 2018-10-10 11:37:10.242 [destination = canalEsInstance address = /192.168.0.135:3306 EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address 192.168.0.135/192.168.0.135:3306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Table exists in the ddl column delete operation needs to open the TSDB to support @agapple This operation does not perform DDL delete operation
1028,Fix Kafka FlatMessage Model Null Value set to "" canal Use kafka model And flatMessage in the case of The database null value is set to The phenomenon is as follows ![image](https://user-images.githubusercontent.com/7855069/47401555-6d04a800-d774-11e8-99a6-5b89b0338123.png) description Is null Case kafka message body Value It is recommended to return null Otherwise varchar Type null cannot be distinguished ![image](https://user-images.githubusercontent.com/7855069/47401616-a9d09f00-d774-11e8-9305-e2ba1c94d932.png) [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=1028) <br/>All committers have signed the CLA. [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=1028) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=1028) before we can accept your contribution.<br/><hr/>**titeng.jiang** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=1028) it.</sub>
1027,RocketMQ Increase support for FlatMessage In order to improve rocketmq Binlog distribution performance needs to increase data fragmentation support If the fragmentation data consistency is increased, it is not guaranteed. Whether this fragmentation should be based on the choices used to make the table level open for the primary key will not be modified or just to monitor the event itself does not pay attention to the order should be more suitable At present, we want to make a topic for each mysql database instance. Different database instances have different topics. Then Tag uses the library name table name to distinguish the table for the individual needs. The specified queue is sent. If the library has such a table, then The entire library is sent to a queue. The corresponding configuration needs to be transferred to the expmle for configuration. Code submitted
1026,canal varchar Null processing Use canal kafka to receive the message that the column value is null is set to int date Type received Can be understood as null by default For varchar How to distinguish text types Null and A single column object has a getisNull method to determine whether the value corresponding to this column is null. CanalEntry.Column This class LS Correct Answer
1025,destination:user_cards[com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.TableIdNotFoundException: not found tableId:21005 Environment Ubuntu 16.04 jdk：1.8 Canal environment a canal Server two instances Canal version canal kafka 1 1 0 The cause of the problem 1 delete the otter on zk znode Purpose to reset the binlog of the instance File and Position Detailed log 2018-10-23 16:05:48.880 [pool-6-thread-2] ERROR com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - process error! java.lang.NullPointerException: null at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.calculateSize(MemoryEventStoreWithBuffer.java:555) ~[canal.store-1.1.0.jar:na] at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.doGet(MemoryEventStoreWithBuffer.java:322) ~[canal.store-1.1.0.jar:na] at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.tryGet(MemoryEventStoreWithBuffer.java:261) ~[canal.store-1.1.0.jar:na] at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getEvents(CanalServerWithEmbedded.java:478) ~[canal.server-1.1.0.jar:na] at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:310) ~[canal.server-1.1.0.jar:na] at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:273) ~[canal.server-1.1.0.jar:na] at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.worker(CanalKafkaStarter.java:121) [canal.kafka-1.1.0.jar:na] at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.access$000(CanalKafkaStarter.java:26) [canal.kafka-1.1.0.jar:na] at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter$1.run(CanalKafkaStarter.java:70) [canal.kafka-1.1.0.jar:na] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_181] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_181] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_181] 2018-10-23 16:05:48.881 [pool-6-thread-2] INFO com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - ## the canal consumer user_cards is running now ...... 2018-10-23 16:05:48.881 [pool-6-thread-2] ERROR com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - process error! java.lang.NullPointerException: null at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.calculateSize(MemoryEventStoreWithBuffer.java:555) ~[canal.store-1.1.0.jar:na] at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.doGet(MemoryEventStoreWithBuffer.java:322) ~[canal.store-1.1.0.jar:na] at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.tryGet(MemoryEventStoreWithBuffer.java:261) ~[canal.store-1.1.0.jar:na] at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getEvents(CanalServerWithEmbedded.java:478) ~[canal.server-1.1.0.jar:na] at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:310) ~[canal.server-1.1.0.jar:na] at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:273) ~[canal.server-1.1.0.jar:na] at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.worker(CanalKafkaStarter.java:121) [canal.kafka-1.1.0.jar:na] at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.access$000(CanalKafkaStarter.java:26) [canal.kafka-1.1.0.jar:na] at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter$1.run(CanalKafkaStarter.java:70) [canal.kafka-1.1.0.jar:na] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_181] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_181] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_181] I violently deleted the znode in zk. When I restart the canal, he will reload the instance and canal. Server configuration? Stop canal Server then delete zknode and finally restart this to validate the location definition in the instance ps. Why do I need to stop the server and then delete because the server will refresh zknode if it does not stop, so deleting zknode in advance does not make any sense. Btw I stopped canal Delete the znode after the server and then from the upstream database show master Status takes out the binlog File and Position restart canal Server no problem canal Server and instance are not reported But once the upstream library has an update operation canal Server will always report this error instance without error 。 Use the latest v1 1 1 version Canal deployer deploys kafka synchronization Known issues fixed ps. The new version has merged the canal kafka project into the canal deployer Mainly modify the canal serverMode kafka in canal properties Thank you for this question 2018-10-23 22:02:58.642 [pool-4-thread-1] ERROR com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - process error! java.lang.NullPointerException: null at com.alibaba.otter.canal.store.helper.CanalEventUtils.createPosition(CanalEventUtils.java:69) ~[canal.store-1.1.0.jar:na] at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:375) ~[canal.store-1.1.0.jar:na] at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:36) ~[canal.store-1.1.0.jar:na] at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:307) ~[canal.server-1.1.0.jar:na] at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:273) ~[canal.server-1.1.0.jar:na] at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.worker(CanalKafkaStarter.java:121) [canal.kafka-1.1.0.jar:na] at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.access$000(CanalKafkaStarter.java:26) [canal.kafka-1.1.0.jar:na] at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter$1.run(CanalKafkaStarter.java:70) [canal.kafka-1.1.0.jar:na] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_131] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_131] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131]
1024,canal How to support the DRDS database [destination = example1 address = *************************:3306 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 1047 sqlstate = HY000 errmsg = [dc26f11c5801000][172.16.22.194:3306][zbtsteam_test]Unknown command @daoshunli Canal is a binlog that simulates mysql The dump protocol DRDS does not currently have direct support for the binlog protocol. It can directly subscribe to the RDS under the DRDS to directly subscribe.
1023,c.a.o.canal.parse.inbound.mysql.dbsync.LogEventConvert - table parser error WARN c.a.o.canal.parse.inbound.mysql.dbsync.LogEventConvert - table parser error : header{} Suddenly encountered such an abnormality today, this table synchronization is unsuccessful. I was still normal today and suddenly found out that this abnormality is going on. It is estimated that the corresponding table in the binlog is deleted. The reset site is skipped and the TSDB can be opened to avoid the TSDB.
1022,FlatMessage is directly produced to kafka and then kafka consumer end analysis efficiency is not higher, it feels that probuf is more efficient than json analysis. Object serialization will have many kinds of information message body is about 3 times larger than json The throughput dropped significantly
1021,1.0.24 canal Parsing mysql in hv mode Timestamp problem Parsing mysql in a single machine timestamp Data return is correct Instance mysql time 2018-10-19 19:09:09 Single machine parsing over canal gets 2018-10-19 19:09:09 Hv mode The parsing gets 2018 10 19 07:02:09 The difference is 12 hours Stand-alone and hv are the same server Hv three server time is CTC time zone Time is also correct. Do you know what the problem is? Hv is 啥canal is based on the data in the binlog can first look at the original binlog data through mysqlbinlog Wrong HA mode Yesterday, the mailbox has not been verified, I did not reply yesterday. Because of the production environment problem, I re-installed the canal environment HA mode in the test environment to resolve the timestamp type.
1020,canal-kafka Data synchronization to kafka after kafka Topic garbled ![default](https://user-images.githubusercontent.com/6134206/47210704-34587d80-d3c6-11e8-805c-0c7ed3772058.png) Data can be synchronized to kafka after canal startup But the synchronized data is garbled The canal version is 1 1 0 local view topic garbled for single node deployment The default transmission of kafka is the serialized Message bytecode. Not visible characters Need to deserialize to a Message with the kafka module in the client Good String and Message will be supported in the next version. You can change the code of the main branch of the kafka related code in the canal server package. Available First-arrival arrival Just wait for the new version to come out. String and Message will be supported in the next version. You can change the code of the main branch of the kafka related code in the canal server package. Available First-arrival arrival Just wait for the new version to come out. I use python client consumption kafka hope to support clear text send json data guide kafka convenient consumption without considering deserialization
1019,canal 1.1.0 Entry resolution performance issues Here entry toByteString is based on what we consider from canal 1 0 24 upgrade to 1 1 0 found that performance is much worse I saw that this place has already got the Entry object. It should be cached so that there is no need to do a parse again in the business. ``` public Event(LogIdentity logIdentity CanalEntry.Entry entry){ this.logIdentity = logIdentity; this.entryType = entry.getEntryType(); this.executeTime = entry.getHeader().getExecuteTime(); this.journalName = entry.getHeader().getLogfileName(); this.position = entry.getHeader().getLogfileOffset(); this.serverId = entry.getHeader().getServerId(); this.gtid = entry.getHeader().getGtid(); this.eventType = entry.getHeader().getEventType(); // build raw this.rawEntry = entry.toByteString(); this.rawLength = rawEntry.size(); if (entryType == EntryType.ROWDATA) { List<CanalEntry.Pair> props = entry.getHeader().getPropsList(); if (props != null) { for (CanalEntry.Pair p : props) { if ("rowsCount".equals(p.getKey())) { rowsCount = Integer.parseInt(p.getValue()); break; } } } } } ``` Similarly, there is already a RowChange but it has been serialized into bytes. ``` RowChange rowChange = rowChangeBuider.build(); if (tableError) { Entry entry = createEntry(header EntryType.ROWDATA ByteString.EMPTY); logger.warn("table parser error : {}storeValue: {}" entry.toString() rowChange.toString()); return null; } else { Entry entry = createEntry(header EntryType.ROWDATA rowChange.toByteString()); return entry; } ``` We are using CanalServerWithEmbedded Entry toByteString is mainly for the deployment mode of the server client to avoid the client serialization period is too long to cause the client Not going on tps Consider adding a configuration that allows the entry of an entry object, which introduces memory overhead.
1017,RDS configuration properties for the 1 1 0 version of the canal Support for the use of manager? canalInstanceWithManager This form of configuration Is it still not supported by the 3 configurations newly added by RDS? Consider submitting a PR to me already fixed
1016,Encountered once a day should execute connector.connect() first What is the cause of the exception? Otter encounters the following exception every day. The stack is connected and the interrupt needs to be rejoined. However, when this error is encountered, sometimes the synchronization is quickly restored. Sometimes the synchronization will be interrupted for about 10 minutes. What is the reason? pid:4 nid:3 exception:canal:hd_canal_236:com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4832) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:160) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:759) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:428) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:114) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:66) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:337) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:184) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:145) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) at java.lang.Thread.run(Thread.java:724) Caused by: java.io.IOException: should execute connector.connect() first at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.(MysqlQueryExecutor.java:30) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:87) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMetaByDB(TableMetaCache.java:80) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:30) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:55) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:50) at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) at com.google.common.cache.LocalCache.get(LocalCache.java:3937) at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941) at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824) at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830) The corresponding otter version is 啥 It is recommended to upgrade the latest version already processed
1015,Seek to guide the transaction number tranactionID obtained by canal and the transaction number transactionID in mysql are inconsistent. 1 canal&#39;s transactionID acquisition method TransactionEnd txEnd = TransactionEnd.parseFrom(entry.getStoreValue()); String txID = txEnd.getTransactionId(); Result 46 2. Query results in mysql Trx id counter 88363 How to find a query method in your mysql is obtained from the mysqlbinlog command Thank you for your reply, I am sorry that the method of the previous query is wrong. The transactionID obtained by canal has no problem. Current query method sudo mysqlbinlog --no-defaults /usr/local/mysql/data/mysql-bin.000149 Just MySQL&#39;s DDL statement is non-transactional. Can&#39;t roll back the DLL statement. How does canal handle ddl? Canal only parses the content recorded in the DDL binlog in the binlog. The transaction has been successfully submitted.
1014,Repair through show master Status Get multiple GTIDs When the carriage return causes the UUID Parsing failed bug as title tks
1013,canal.kafka Received garbled code with bin kafka console consumer sh command After inserting a record in the mysql database, use the command bin kafka console consumer sh --zookeeper 192.168.206.128:2181 --topic canal1 From beginning see the consumer output garbled `*: mysql-bin.000027*UTF-80BJP229 *7 mysql-bin.000027*UTF-80BJPF mysql-bin.000027*UTF-80Bcan_dbJ user_testP+Xb _-+_C-+++1Pb_id (0B6Ri++(11) +a+e (0BdafdR +a_cha_(40)age (0B15Ri++(11): +y_-+-bi+.000027 *UTF-80 8BJP261 *7 +y_-+-bi+.000027 *UTF-80 8BJPF  +y_-+-bi+.000027 *UTF-80 8Bca+_dbJ +_e__+e_+P*Xb _-+_C-+++1Pb^id (0B7Ri++(11) +a+e (0BeeeR +a_cha_(40)age (0B11Ri++(11): +y_-+-bi+.000027 *UTF-80 8BJP262` Then read the code in the kafka client and changed the corresponding configuration parameters of AbstractKafkaTest java. `public static String topic = "canal1"; public static Integer partition = 0; public static String groupId = ""; public static String servers = "192.168.206.128:6667"; public static String zkServers = "192.168.206.128:2181";` After inserting a new record into mysql but running KafkaClientRunningTest java did not receive the packet, have you ever encountered this situation? canal kafka The mode default is that the serialized byte of the Send Message object is not a visible string. Client receives deserialization To send a visible string Please use flatMessage mode Will send json format data @brightsong I have encountered the same problem here. Is your problem solved? > After inserting a record in the mysql database, use the command bin kafka console consumer sh --zookeeper 192.168.206.128:2181 --topic canal1 From beginning see the consumer output garbled > `�*��: ���mysql-bin.000027�*UTF-80BJP����229 �*�7 ���mysql-bin.000027�*UTF-80BJPF���� �����mysql-bin.000027�*UTF-80B�can_dbJ user_testP+X�b _-+_C-+++��1������Pb_�����id �(�0B�6Ri++(11)��� ��+a+e (�0B�dafdR +a_cha_(40)�����age (�0B�15Ri++(11)�: ���+y_-+-bi+.000027�� �*UTF-80 8�BJP����261 �*��7 ���+y_-+-bi+.000027�� �*UTF-80 8�BJPF���� ��� ���+y_-+-bi+.000027�� �*UTF-80 8�B�ca+_dbJ +_e__+e_+P*X�b _-+_C-+++��1������Pb^�����id �(�0B�7Ri++(11)��� ��+a+e (�0B�eeeR +a_cha_(40)�����age (�0B�11Ri++(11)�: ���+y_-+-bi+.000027�� �*UTF-80 8�BJP����262` > Then read the code in the kafka client and changed the corresponding configuration parameters of AbstractKafkaTest java. > `public static String topic = "canal1"; public static Integer partition = 0; public static String groupId = ""; public static String servers = "192.168.206.128:6667"; public static String zkServers = "192.168.206.128:2181";` > After inserting a new record into mysql but running KafkaClientRunningTest java did not receive the packet, have you ever encountered this situation? I encountered the same problem as you did when changing the receiving format to ConsumerRecords String String> Can receive the output but the native ConsumerRecords String Message> No news, is your problem solved? > canal kafka The mode default is that the serialized byte of the Send Message object is not a visible string. Client receives deserialization > To send a visible string Please use flatMessage mode Will send json format data Which configuration item is corresponding to which configuration file? Solved the problem of changing the virtual machine cpu to a larger point, if you can not modify the canal instance parser parallel parameter in 2018-10-22 11:06:28，"jkl0898" <notifications@github.com> Write @brightsong I have encountered the same problem here. Is your problem solved? After inserting a record in the mysql database, use the command bin kafka console consumer sh --zookeeper 192.168.206.128:2181 --topic canal1 From beginning see the consumer output garbled �*��: ���mysql-bin.000027�*UTF-80BJP����229 �*�7 ���mysql-bin.000027�*UTF-80BJPF���� �����mysql-bin.000027�*UTF-80B�can_dbJ user_testP+X�b _-+_C-+++��1������Pb_�����id �(�0B�6Ri++(11)��� ��+a+e (�0B�dafdR +a_cha_(40)�����age (�0B�15Ri++(11)�: ���+y_-+-bi+.000027�� �*UTF-80 8�BJP����261 �*��7 ���+y_-+-bi+.000027�� �*UTF-80 8�BJPF���� ��� ���+y_-+-bi+.000027�� �*UTF-80 8�B�ca+_dbJ +_e__+e_+P*X�b _-+_C-+++��1������Pb^�����id �(�0B�7Ri++(11)��� ��+a+e (�0B�eeeR +a_cha_(40)�����age (�0B�11Ri++(11)�: ���+y_-+-bi+.000027�� �*UTF-80 8�BJP����262 Then read the code in the kafka client and changed the corresponding configuration parameters of AbstractKafkaTest java. public static String topic = "canal1"; public static Integer partition = 0; public static String groupId = ""; public static String servers = "192.168.206.128:6667"; public static String zkServers = "192.168.206.128:2181"; After inserting a new record into mysql but running KafkaClientRunningTest java did not receive the packet, have you ever encountered this situation? I encountered the same problem as you did when changing the receiving format to ConsumerRecords String String> Can receive the output but the native ConsumerRecords String Message> No news, is your problem solved? — You are receiving this because you were mentioned. Reply to this email directly view it on GitHub or mute the thread. Sorry to read wrong, it has not been resolved yet. in 2018-10-22 11:06:28，"jkl0898" <notifications@github.com> Write @brightsong I have encountered the same problem here. Is your problem solved? After inserting a record in the mysql database, use the command bin kafka console consumer sh --zookeeper 192.168.206.128:2181 --topic canal1 From beginning see the consumer output garbled �*��: ���mysql-bin.000027�*UTF-80BJP����229 �*�7 ���mysql-bin.000027�*UTF-80BJPF���� �����mysql-bin.000027�*UTF-80B�can_dbJ user_testP+X�b _-+_C-+++��1������Pb_�����id �(�0B�6Ri++(11)��� ��+a+e (�0B�dafdR +a_cha_(40)�����age (�0B�15Ri++(11)�: ���+y_-+-bi+.000027�� �*UTF-80 8�BJP����261 �*��7 ���+y_-+-bi+.000027�� �*UTF-80 8�BJPF���� ��� ���+y_-+-bi+.000027�� �*UTF-80 8�B�ca+_dbJ +_e__+e_+P*X�b _-+_C-+++��1������Pb^�����id �(�0B�7Ri++(11)��� ��+a+e (�0B�eeeR +a_cha_(40)�����age (�0B�11Ri++(11)�: ���+y_-+-bi+.000027�� �*UTF-80 8�BJP����262 Then read the code in the kafka client and changed the corresponding configuration parameters of AbstractKafkaTest java. public static String topic = "canal1"; public static Integer partition = 0; public static String groupId = ""; public static String servers = "192.168.206.128:6667"; public static String zkServers = "192.168.206.128:2181"; After inserting a new record into mysql but running KafkaClientRunningTest java did not receive the packet, have you ever encountered this situation? I encountered the same problem as you did when changing the receiving format to ConsumerRecords String String> Can receive the output but the native ConsumerRecords String Message> No news, is your problem solved? — You are receiving this because you were mentioned. Reply to this email directly view it on GitHub or mute the thread. > canal kafka The mode default is that the serialized byte of the Send Message object is not a visible string. Client receives deserialization > To send a visible string Please use flatMessage mode Will send json format data Where can I configure it? I also did not configure my content to be parsed after receiving the Message object and then put the content in json and send it to another kafka. in 2018-10-23 17:12:59，"liugaozy" <notifications@github.com> Write canal kafka The mode default is that the serialized byte of the Send Message object is not a visible string. Client receives deserialization To send a visible string Please use flatMessage mode Will send json format data Where can I configure it? — You are receiving this because you were mentioned. Reply to this email directly view it on GitHub or mute the thread.
1012,Canal can get LSN Log in mysql sequence Number Binlog does not have this Thank you so much
1011,Kafka mode client Offset does not advance Kafka mode client Offset does not advance, go back and forth, consume a few pieces of data, no error
1010,The latest 1 1 1 SNAPSHOT enable gtid mode will still report errno = 1236 1 1 1 SNAPSHOT version enables gtid mode source database gtid_purged has value after startup error can not be synchronized ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = The slave is connecting using CHANGE MASTER TO MASTER_AUTO_POSITION = 1 but the master has purged binary logs containing GTIDs that the slave requires. at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:102) Holding this exception area google mysql configuration problem > Holding this exception area google mysql configuration problem Mysql is configured gtid mode gtid_purged also has value online search for this error get the solution is to stop slave sync and then set gtid_purged on the slave But how do you set gtid_purged for a canal that simulates a slave? https://dev.mysql.com/doc/refman/5.6/en/replication-mode-change-online-enable-gtids.html Refer to this operation to set mysql server
1009,meta.dat not update [canal 1.1.0] Canal server and client consumes data normally but the timestamp and content of meta.dat not change which will cause an exception 'Could not find first log file name in binary log index file' when restart canal server. Pls check the configurations in attachment. [instance.log](https://github.com/alibaba/canal/files/2486428/instance.log) [canal.log](https://github.com/alibaba/canal/files/2486429/canal.log) Could not find first log file name in binary log index file The binlog on the mysql host has been deleted. I got more information with adding log in canal server. something unexpected occurs the updateCursor does not execute. ![image](https://user-images.githubusercontent.com/9636488/47079731-b9ae2780-d238-11e8-8fe6-41fedf6ac8df.png) finally I compile a new canal server with source code and deploy the meta.dat was updated normally.
1008,For mysql Enum type Canal returns int by default ie 1 2 3 How to return the value in enum For mysql Enum type Canal defaults to return int ie 1 2 3 Now you need to return the specific value of the enum type field. How to achieve it? // *********** My previous practice 1. Modify the mysqlToJavaType method of the rowslogbuffer of the parser module to return Types CHAR for enum 2. Modify the fetchValue method of rowslogbuffer. For the enum type, modify the string type to return the string but report the error. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: java.lang.IllegalArgumentException: limit excceed: 63 at com.taobao.tddl.dbsync.binlog.LogBuffer.getFullString(LogBuffer.java:1122) <img width="1125" alt="enum1" src="https://user-images.githubusercontent.com/28953872/47005872-44beed00-d167-11e8-83a9-bb56a2cae283.png"> <img width="1121" alt="enum2" src="https://user-images.githubusercontent.com/28953872/47005873-45578380-d167-11e8-8512-1b0eee9b05cc.png"> <img width="981" alt="enum3" src="https://user-images.githubusercontent.com/28953872/47005875-45578380-d167-11e8-84a4-b942af2f2816.png"> Ask the god how to modify it? Do not modify in the binlog parsing can get the table structure in the periphery based on the int table to find the specific text to replace such as LogEventConvert The big god you said to find the specific text to replace is not based on the enum subscript index from the fieldMeta getColumnType parsing the corresponding content as shown below fieldMeta.getColumnType() is: enum('100' '200' '300' '400' '500' '600') >>>>>> columnType.startsWith("enum") >>>>>> String.valueOf(value): 3 From 100 '200' '300' '400' '500' Parsing the third element 300 in 600 But in case the elements in the enum contain special characters such as commas Or how to do single quotes Still have other ways Ask for advice @agapple Actually test the various cases of enumerating special characters Good thank you guide
1007,restart canal-server canal stuck at at sun.nio.ch.Net.poll(Native Method) Server and client works normally restart server client will disconnect and try to connect and subscribe while I found client thread stuck. Do someone have got this or come up with some ideas about this. Callstack and information as follow: 2018-10-16 10:01:41 ERROR [CustomCanalClient] Main Loop outter Exception.com.alibaba.otter.canal.protocol.exception.CanalClientException: java.io.IOException: end of stream when reading header 2018-10-16 10:01:41 WARN [HttpConnectionPool] object == null 2018-10-16 10:01:41 TRACEBACK [CustomCanalClient]com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:317) com.alibaba.otter.canal.custom.CustomCanalClient.process(CustomCanalClient.java:106) com.alibaba.otter.canal.custom.CustomCanalClient$1.run(CustomCanalClient.java:67) java.lang.Thread.run(Thread.java:748) 2018-10-16 10:01:43 ERROR [CustomCanalClient] Main Loop outter Exception.com.alibaba.otter.canal.protocol.exception.CanalClientException: java.net.ConnectException: Connection refused 2018-10-16 10:01:43 WARN [HttpConnectionPool] object == null 2018-10-16 10:01:43 TRACEBACK [CustomCanalClient]com.alibaba.otter.canal.client.impl.SimpleCanalConnector.doConnect(SimpleCanalConnector.java:190) com.alibaba.otter.canal.client.impl.SimpleCanalConnector.connect(SimpleCanalConnector.java:114) com.alibaba.otter.canal.custom.CustomCanalClient.process(CustomCanalClient.java:101) com.alibaba.otter.canal.custom.CustomCanalClient$1.run(CustomCanalClient.java:67) java.lang.Thread.run(Thread.java:748) java.lang.Thread.State: RUNNABLE at sun.nio.ch.Net.poll(Native Method) at sun.nio.ch.SocketChannelImpl.poll(SocketChannelImpl.java:954) - locked <0x0000000094054d90> (a java.lang.Object) at sun.nio.ch.SocketAdaptor$SocketInputStream.read(SocketAdaptor.java:204) - locked <0x0000000094054d80> (a java.lang.Object) at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103) - locked <0x0000000094054e98> (a sun.nio.ch.SocketAdaptor$SocketInputStream) at java.nio.channels.Channels$ReadableByteChannelImpl.read(Channels.java:385) - locked <0x0000000094054f08> (a java.lang.Object) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.read(SimpleCanalConnector.java:429) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.readNextPacket(SimpleCanalConnector.java:419) - locked <0x00000000c038e900> (a java.lang.Object) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.readNextPacket(SimpleCanalConnector.java:403) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.receiveMessages(SimpleCanalConnector.java:322) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:315) at com.alibaba.otter.canal.custom.CustomCanalClient.process(CustomCanalClient.java:106) at com.alibaba.otter.canal.custom.CustomCanalClient$1.run(CustomCanalClient.java:67) at java.lang.Thread.run(Thread.java:748) client example : https://github.com/alibaba/canal/blob/master/example/src/main/java/com/alibaba/otter/canal/example/SimpleCanalClientTest.java [jstack_log.log](https://github.com/alibaba/canal/files/2481661/jstack_log.log) I add a shell script 'test.sh' to start canal.example.SimpleCanalClientTest class but the same result comes out. ![image](https://user-images.githubusercontent.com/834743/46998952-b3e01580-d156-11e8-89a9-5f964f6e4eb9.png) if no binlog message you can add sleep time . ex : Thread.sleep(1000) Thank you
1006,support golang canal client address link as title tks
1005,binlog-rows-query-log-events=1 Queryevent event in less library name row data less GTID 5 6 database contains configuration binlog-rows-query-log-events=1 The queryevent event in the library name row data less GTID I saw binlog binlog really do not have these two canal help to fill these two properties? header { version: 1 logfileName: "mysql-bin.000945" logfileOffset: 430773470 serverId: 1346306 serverenCode: "UTF-8" executeTime: 1539165635000 sourceType: MYSQL schemaName: "" Less library name tableName: "product_sc_id" eventLength: 374 eventType: QUERY gtid: "816fb525-8e3c-11e7-b7b0-525400730122:1-212612172 73aa9f66-8e18-11e7-b6c5-525400730120:1-358980 b064380d-0559-11e6-b5be-005056996b87:1-1593948705" } entryType: ROWDATA storeValue: "xxx" header Less gtid version: 1 logfileName: "mysql-bin.000945" logfileOffset: 430773918 serverId: 1346306 serverenCode: "UTF-8" executeTime: 1539165635000 sourceType: MYSQL schemaName: "gome_jiaoyu" tableName: "product_sc_id" eventLength: 126 eventType: UPDATE props { key: "rowsCount" value: "1" } } entryType: ROWDATA storeValue: "xxx" binlog-rows-query-log-events The schema can&#39;t get the binlog. Adding gtid to ROWDATA has been added but needs to understand a semantic of gtid. If a row in the transaction records gtid as a bit, it will cause the remaining records of the current transaction to be discarded after being sent to mysql.
1004,Support Mysql8? Have you tested support for Mysql8? Currently not supported mysql8 What is the highest version of mysql supported? Read more wiki
1003,Canal writes kafka&#39;s logic without considering kafka&#39;s message Size limit https://github.com/alibaba/canal/blob/e4b6385dcc439fcf495c81b0c0f89da858dd377b/server/src/main/java/com/alibaba/otter/canal/kafka/CanalKafkaProducer.java#L104-L113 If the size of the record that needs to be sent exceeds the kafka limit, the default is 1MB. Is it impossible to continue processing? I am dealing with it myself. canal batchSize configuration should not exceed 1M ``` servers: localhost:9092 #for rocketmq: means the nameserver retries: 0 batchSize: 16384 lingerMs: 1 bufferMemory: 33554432 # Canal&#39;s batch size Default 50K Due to the maximum message body limitation of kafka, please do not exceed 1M 900K or less. canalBatchSize: 50 ``` @rewerma Hello, still have some doubts, need to ask What is the meaning of canalBatchSize? The granularity of control is the transaction level or event level or Row level or Field level if a field exceeds 1M How to deal with it? canalBatchSize is the batch size of the send that pulls the canal pull Please refer to canal for details. Configuration instructions If there is a single field greater than 1M, you can temporarily adjust the upper limit of the message body limit of kafka.
1002,1. AbstractEventParser Should stop the multiStageCoprocess every time at the end of the while instead of rese 1. AbstractEventParser The multiStageCoprocess should be stopped at the end of the while instead of reset every time. Reset will recreate two thread pools in mysqlMultiStageCoprocessor If there is an error in the dump Will create an infinite number of thread pools and do not stop without terminal Resulting that system resources are occupied Note that the while loop will create a new mysqlMultiStageCoprocessor every time. 2. In the MySqlMultiStageComprocessor, the thread pool needs to determine whether it has been shut down at stop. In the actual situation, there is a case where the thread pool shutdown is true and the terminal is always false. [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=1002) <br/>All committers have signed the CLA. tks
1001,use docker deploy canal-server Not available docker-restart command - phenomenon When used docker Deployment official canal-server Mirror canal canal server v1 1 0 and call after successful startup ```shell docker restart ``` There will be the following error message ```log mv: cannot stat `/home/admin/canal-server/conf/example': No such file or directory ``` - the reason an examination docker Startup script discovery https://github.com/alibaba/canal/blob/master/docker/image/admin/app.sh of 84-92 Line thrown error ```shell destination=`perl -le 'print $ENV{"canal.destinations"}'` if [[ "$destination" =~ ' ' ]]; then echo "multi destination:$destination is not support" exit 1; else if [ "$destination" != "" ] && [ "$destination" != "example" ] ; then mv /home/admin/canal-server/conf/example /home/admin/canal-server/conf/$destination fi fi ``` The behavior here is if it is specified each time it is started destination And destination Value is not Example will put ```path /home/admin/canal-server/conf/example ``` Rename the directory to the specified destination value Since I specified destination And the value is not Example so this behavior leads to execution ```shell docker restart ``` When there is no inside the container example Folder so container can&#39;t start - problem How do you solve the problem by yourself, how do you mirror yourself or have other postures? You can write one Docker compose yml mapping can refer to the docker compose yml I wrote https github com CanalSharp CanalSharp blob 692672b5e7ecec9c91019cd254a1e73adfea7289 docker docker compose yml The mv operation does the judgment to determine the existence of the directory. > You can write one Docker compose yml mapping can refer to the docker compose yml I wrote https github com CanalSharp CanalSharp blob 692672b5e7ecec9c91019cd254a1e73adfea7289 docker docker compose yml @WithLin I tried this method but reported other errors when I restarted. ```log server_1 | mv: cannot move `/home/admin/canal-server/conf/example' to `/home/admin/canal-server/conf/uc': Device or resource busy ```
1000,Parsing site records can&#39;t be started when using FileMixedLogPositionManager Version 1 1 0 mysql：5.6.36 All configurations are only modified in two places. The ip port of the mysql configured in the example is another spring file instance xml. as follows <!-- Parsing site record --> <property name="logPositionManager"> <bean class="com.alibaba.otter.canal.parse.index.FailbackLogPositionManager"> <constructor-arg> <bean class="com.alibaba.otter.canal.parse.index.FileMixedLogPositionManager" > <constructor-arg index="0" value="/tmp/parse.dat" /> <constructor-arg index="1" value="2000" /> <constructor-arg index="2" ref="memoryLogPositionManager"/> </bean> </constructor-arg> <constructor-arg> <bean class="com.alibaba.otter.canal.parse.index.MetaLogPositionManager"> <constructor-arg ref="metaManager"/> </bean> </constructor-arg> </bean> </property> memoryLogPositionManager definition `<bean id="memoryLogPositionManager" class="com.alibaba.otter.canal.parse.index.MemoryLogPositionManager" />` The error is as follows 2018-10-12 15:46:57.245 [destination = example address = /127.0.0.1:3307 EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address /127.0.0.1:3307 has an error retrying. caused by java.lang.NullPointerException: null at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPositionInternal(MysqlEventParser.java:480) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPosition(MysqlEventParser.java:362) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:182) ~[canal.parse-1.1.0.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171] 2018-10-12 15:46:57.258 [destination = example address = /127.0.0.1:3307 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.lang.NullPointerException at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPositionInternal(MysqlEventParser.java:480) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPosition(MysqlEventParser.java:362) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:182) at java.lang.Thread.run(Thread.java:748) ] The original configuration of spring keeps the original configuration and starts everything normally. <!-- Parsing site record --> <property name="logPositionManager"> <bean class="com.alibaba.otter.canal.parse.index.FailbackLogPositionManager"> <constructor-arg> <bean class="com.alibaba.otter.canal.parse.index.MemoryLogPositionManager" /> </constructor-arg> <constructor-arg> <bean class="com.alibaba.otter.canal.parse.index.MetaLogPositionManager"> <constructor-arg ref="metaManager"/> </bean> </constructor-arg> </bean> </property> Com alibaba otter canal parse index FileMixedLogPositionManager here requires dataDir to be a root path to find a path when looking for a dat file dataDir + /$destination$/ + Parse dat your path is constructed incorrectly @agapple thx
999,ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.lang.IllegalArgumentException Reference Canal Kafka The instructions in QuickStart are configured with canal to kafka data synchronization start canal kafka post-report `2018-10-12 15:26:32.189 [destination = example address = /192.168.206.136:3306 EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address /192.168.206.136:3306 has an error retrying. caused by java.lang.IllegalArgumentException: null at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1310) ~[na:1.7.0_79] at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1233) ~[na:1.7.0_79] at java.util.concurrent.Executors.newFixedThreadPool(Executors.java:114) ~[na:1.7.0_79] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor.start(MysqlMultiStageCoprocessor.java:84) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:238) ~[canal.parse-1.1.0.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.7.0_79] 2018-10-12 15:26:32.189 [destination = example address = /192.168.206.136:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.lang.IllegalArgumentException at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1310) at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1233) at java.util.concurrent.Executors.newFixedThreadPool(Executors.java:114) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor.start(MysqlMultiStageCoprocessor.java:84) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:238) at java.lang.Thread.run(Thread.java:745) ]` Have you encountered this problem? Use environment stand-alone zookeeper 3 4 12 kafka_2 10 0 10 0 1 canal kafka 1 1 0 ； I also encountered the same problem, have you solved it? This issue has been resolved with the server side profile canal properties The number of parallel threads in the middle can be turned off and the singleton can be run in a single instance.
998,RowChange resolves Chinese garbled CanalEntry.RowChange rowChange = CanalEntry.RowChange.parseFrom(canalEntry.getStoreValue()); The data obtained is Chinese garbled and the ddl sql contains rn The database and canal configuration are utf8, why not go back to the big answer ![image](https://user-images.githubusercontent.com/26699764/46853678-60f21f80-ce31-11e8-8201-c7e87c996bfe.png) ![image](https://user-images.githubusercontent.com/26699764/46853718-77987680-ce31-11e8-8f17-e21e953c837a.png) Sorry toString to kill the light to see the information printed toString is not directly obtained value stupid
996,RDS deployment in the quarry tower is invalid Problem Description I wget Project code Compiled successfully and Changed ```conf/example/instance.properties``` content Error after running bin startup sh ``` 2018-10-11 20:12:49.383 [Thread-5] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - stop CannalInstance for null-example 2018-10-11 20:12:49.391 [Thread-5] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - stop successful.... 2018-10-11 20:12:57.090 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2018-10-11 20:12:57.095 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties] 2018-10-11 20:12:57.289 [main] WARN o.s.beans.GenericTypeAwarePropertyDescriptor - Invalid JavaBean property 'connectionCharset' being accessed! Ambiguous write methods found next to actually used [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.nio.charset.Charset)]: [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.lang.String)] 2018-10-11 20:12:57.343 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2018-10-11 20:12:57.343 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties] 2018-10-11 20:12:57.553 [main] ERROR com.alibaba.druid.pool.DruidDataSource - testWhileIdle is true validationQuery not set 2018-10-11 20:12:57.865 [main] INFO c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-example 2018-10-11 20:12:57.881 [main] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - start successful.... 2018-10-11 20:12:57.918 [destination = example address = xxxxxxxx:3333 EventParser] WARN c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - prepare to find start position just show master status 2018-10-11 20:12:59.013 [destination = example address = xxxxxxxx:3333 EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address xxxxxxxx:3333 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: ErrorPacket [errorNumber=1142 fieldCount=-1 message=SHOW command denied to user 'xxxxxx' for table 'slow_log' sqlState=42000 sqlStateMarker=#] with command: show create table `mysql`.`event`;show create table `mysql`.`func`;show create table `mysql`.`general_log`;show create table `mysql`.`help_category`;show create table `mysql`.`help_keyword`;show create table `mysql`.`help_relation`;show create table `mysql`.`help_topic`;show create table `mysql`.`proc`;show create table `mysql`.`slow_log`;show create table `mysql`.`time_zone`;show create table `mysql`.`time_zone_leap_second`;show create table `mysql`.`time_zone_name`;show create table `mysql`.`time_zone_transition`;show create table `mysql`.`time_zone_transition_type`; Caused by: java.io.IOException: ErrorPacket [errorNumber=1142 fieldCount=-1 message=SHOW command denied to user 'xxxxxxx' for table 'slow_log' sqlState=42000 sqlStateMarker=#] with command: show create table `mysql`.`event`;show create table `mysql`.`func`;show create table `mysql`.`general_log`;show create table `mysql`.`help_category`;show create table `mysql`.`help_keyword`;show create table `mysql`.`help_relation`;show create table `mysql`.`help_topic`;show create table `mysql`.`proc`;show create table `mysql`.`slow_log`;show create table `mysql`.`time_zone`;show create table `mysql`.`time_zone_leap_second`;show create table `mysql`.`time_zone_name`;show create table `mysql`.`time_zone_transition`;show create table `mysql`.`time_zone_transition_type`; at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.queryMulti(MysqlQueryExecutor.java:109) ~[canal.parse.driver-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.queryMulti(MysqlConnection.java:107) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:175) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:129) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:91) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:188) ~[canal.parse-1.1.0.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171 ``` Remove the mysql library subscription
995,How can deal with the data of the source table delete Such as title Delete in the binlog is only one before Image&#39;s event and insert update have no difference. What do you mean by processing? > Delete in the binlog is only one before Image&#39;s event and insert update have no difference. What do you mean by processing? Ok, actually, my expression is unclear. It should be said that canal can resolve the binlog and can definitely get the delete statement. My problem should be to extract the data from the canal after obtaining the delete data. How does the hive repository handle the deleted data? note 1 hive does not enable transactions, cannot delete operations This is what you hive to deal with and what does it have to do with canal? You try apache orc > This is what you hive to deal with and what does it have to do with canal? You try apache orc Ok, I will try thank you.
994,Whether can support multiple instances pointing to the same database Just the corresponding whitelist is different Tried to set up multiple instances The database address of the instance properties is the same Just a whitelist pointing to a different table is mainly for a table corresponding to a kafka topic But the error is instantiated when the data source is initialized. Can be set But there is no update for the position of the cursor on zookeeper.
993,Specify xxx instance xml for a specific instance Excuse me If there are now two instances `conf/instance1/instance.properties` `conf/instance2/instance.properties ` So how do you specify conf spring for each of them? Different xml Configuration There should be no way to open only two canal @littleneko Open two instances in one server Specify a configuration for an instance Give the answer in the application extension https://github.com/alibaba/canal/wiki/DevGuide#%E5%BA%94%E7%94%A8%E6%89%A9%E5%B1%95 1. In conf Create a new instance property file in `new-instance/instance.properties` 2. New xml Configuration file `spring/custom-instance.xml` 3. canal.properties Medium specification `canal.instance.new-instance.spring.xml = classpath:spring/custom-instance.xml` in case canal.properties No automatic scan canal auto scan = true Turn on automatic scanning Need to be in canal.properties Medium specification canal.destinations= example new-instance --------------------------------------- The channel name here Is the name of the instance what is this else But canal instance destination What is the role of this placeholder? how to use Thank you, Da Yu @yudianer This method can&#39;t specify two different xml xml is configured in canal properties The channel name is new instance. Each instance needs to be created in the canal conf dir default conf directory in the canal properties configuration in the new directory to write the instance properties configuration file Canal instance destination is actually the directory of each instance 在 https github com alibaba canal blob master deployer src main java com alibaba otter canal deployer CanalController java 中初始化  ```java // This variable is used when setting the currently loaded channel to load the spring lookup file. System.setProperty(CanalConstants.CANAL_DESTINATION_PROPERTY destination); ``` @littleneko I feel the meaning of the trick. :smile: ![image](https://user-images.githubusercontent.com/12033023/46842989-50c34b80-ce03-11e8-8224-bf0a5aa593e0.png) > // This variable is used when setting the currently loaded channel to load the spring lookup file. System.setProperty(CanalConstants.CANAL_DESTINATION_PROPERTY destination); The sentence you said reminded me. `canal.instance.destination` Not inclined to user settings But when loading the configuration file Set to each instance name Then on properties Placeholders in the configuration file are replaced Thank you
992,Use CanalKafkaClientExample storeValue is garbled 2018-10-10 17:25:19.304 [Thread-2] INFO c.a.o.canal.client.running.kafka.CanalKafkaClientExample - Message[id=16 entries=[header { version: 1 logfileName: "mysql-bin.000156" logfileOffset: 298158292 serverId: 1 serverenCode: "UTF-8" executeTime: 1539163519000 sourceType: MYSQL schemaName: "payment" tableName: "tmp_canal_test" eventLength: 69 eventType: UPDATE props { key: "rowsCount" value: "1" } } entryType: ROWDATA storeValue: "\b\242\006\020\002P\000b\341\001\n\033\b\000\020\004\032\002id \001(\0000\000B\00245R\aint(11)\n\'\b\001\020\f\032\busername \000(\0000\000B\003111R\fvarchar(255)\n*\b\002\020\f\032\bpassword \000(\0000\000B\006123123R\fvarchar(255)\022\033\b\000\020\004\032\002id \001(\0000\000B\00245R\aint(11)\022\'\b\001\020\f\032\busername \000(\0000\000B\003111R\fvarchar(255)\022\'\b\002\020\f\032\bpassword \000(\0010\000B\003123R\fvarchar(255)" ] raw=true rawEntries=[]] How can I handle it? storeValue is com alibaba otter canal protocol RowChange type needs further analysis. See the definition in EntryProtocol proto. LS Correct Answer
991,Canal achieves HA And send a message to the kafka cluster The active/standby switch causes the message to be sent repeatedly. After canal achieve HA The position of binlog is recorded in zookeeper But after the main app is paused The backup will re-deliver the binlog that has not been processed through it for a while before to Kafka. In HA mode Stop A Switch to B Restart A Then stop the position of the cursor on the zk of the B operation by A is not updated. I don&#39;t know if it is integrated with Kafka or other reasons. Known issues are recommended to upgrade 1 1 1 new version
990,increase Target database password encryption and decryption using druid mode for encryption and decryption and setting canal instance enableDruid true, please make increase Target database password encryption and decryption using druid mode for encryption and decryption and setting canal instance enableDruid true, please make [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=990) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=990) before we can accept your contribution.<br/><hr/>**shichengming** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=990) it.</sub> [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=990) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=990) before we can accept your contribution.<br/><hr/>**shichengming** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=990) it.</sub> tks
989,Modify canal The code of the dbsync module is not valid. The installation and deployment of the canal has been completed and is running normally during the process of customizing the canal. Now need to code the dbsync module of canal com alibaba otter canal parse inbound mysql dbsync package below the class LogEventConvert method parseOneRow rewrite download canal Source and in intellij Opened in idea but never worked, don&#39;t know why Which god can help me with your thank you <img width="1232" alt="dbsync" src="https://user-images.githubusercontent.com/28953872/46657304-89380f00-cbe2-11e8-973f-3c13658b8e04.png"> @agapple @lcybo Ask God to have time to give directions Can the debug breakpoint not come into effect? Setting breakpoints in the idea has not been done here. Is it necessary to change the lib canal parse in the canal installation package? dbsync 1 0 24 jar The breakpoint can go to other places. First make sure that the debug in IDEA can go to the code block you added. The test is no problem. Put the project into a jar and deploy it. thanks for the reply Sorry to write the wrong before should be modified parser module is not dbsync module LogEventConvert code does not take effect In addition, I tried to set breakpoint debugging in other code of the parser module but did not run to the breakpoint. ********************** The specific situation is such that I am customizing the canal to get the mysql incremental data and after conversion to get a custom json String contains 1 modified record json format as follows insert 1 record { "table":"mysql.mysql.all_types" "transactionID":"29" "scn":"29.0" "ts":"1539333745000" "data":[{"opType":"i" "after":{"smallint1":"111" "timestamp1":"2018-10-12 16:42:25.000000" "float1":"345.102" "inta":"108" "tinyint1":"11" "varchar1":"a" "decimal1":"-1235666.666668899987000000000000000000" "double1":"123.134577501986" "datetime1":"2018-10-12 16:42:25.000000"}}]} But the nativeCanalClientTest java of the native canal 1 0 24 client-side example module uses double and decimal data obtained by column getvalue. with “mysql The precision of the query data inserted in the shell and the number of decimal places precision Inconsistent scale In order to solve this data consistency problem, I tried to rewrite the parse module&#39;s code com alibaba otter canal parse inbound mysql dbsync package below the class LogEventConvert method parseOneRow but it does not work, so I am in intellij Breakpoint debugging is turned on in idea but the program display will not be executed to parseOneRow I think the reason may be that I am in intellij Modified canal in idea Parser module canal Source code but canal server That is, the canal homepage download Deployer binary and decompressed to get lib bin conf and logs folder where lib contains various jars responsible for parsing mysql binary files so the real need to change is canal server The jar below the lib So I need to be in intellij After modifying the parser source code in the idea, you need to re-create the new parser. Jar package and replace canal with this new jar package server Parser below lib jar。 Guess to be verified Test results show that need to be in intellij After modifying the parser source code in the idea, you need to re-create the new parser. Jar package and replace canal with this new jar package server Parser below lib The jar modification has taken effect Thank you @theonesmx
988,Canal instance filter regex does not take effect Tried two mysql servers with the same configuration one takes effect and the other does not take effect Already checked bin The log format is ROW format Using KafkaCanalConnector No parameters in the subscribe method May I ask what is the reason Server 1 failed ![image](https://user-images.githubusercontent.com/24933564/46660266-44fc3d00-cbe9-11e8-93f3-2f7218564f10.png) ![image](https://user-images.githubusercontent.com/24933564/46660815-79bcc400-cbea-11e8-9085-ee90f1eb5933.png) Server 2 Effective ![image](https://user-images.githubusercontent.com/24933564/46660299-534a5900-cbe9-11e8-9aca-ae68e98b27ed.png) ![image](https://user-images.githubusercontent.com/24933564/46660833-82ad9580-cbea-11e8-80d8-9ce49dbbf46b.png) Will the differences in the configuration of the two server binlogs affect? Take a look at the FAQ in the wiki. There are several common cases where the filter is not in effect.
987,Parse create After the schema log is called, the constant throwing exception > Question 1 Scene start canal execute create Schema statement passed Message getWithoutAck(int batchSize gets the log data and throws the following exception when the method is called again next time If canal parsing create The script that parses the other statements crud before the schema log does not have such an error. ### Exception information stack 2018-10-09 10:19:20.755 [main] ERROR syncLogger - Incremental synchronization exception com alibaba otter canal protocol exception CanalClientException something goes wrong with reason: something goes wrong with channel:[id: 0xcf561b9e /192.168.1.179:50803 => /192.168.1.179:2017] exception=java.lang.NullPointerException at com.alibaba.otter.canal.store.helper.CanalEventUtils.createPosition(CanalEventUtils.java:69) at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:375) at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:36) at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:307) at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:273) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:124) at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88) at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791) at org.jboss.netty.channel.SimpleChannelHandler.messageReceived(SimpleChannelHandler.java:142) at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:36) at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:294) at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70) at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296) at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:536) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:435) at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70) at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255) at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88) at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:109) at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312) at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90) at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178) at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.receiveMessages(SimpleCanalConnector.java:344) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:315) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:287) > Question 2 Also create The event corresponding to the schema statement Type is Query isDdl false doesn&#39;t feel right ![image](https://user-images.githubusercontent.com/22972651/46643311-2167d100-cbae-11e8-8695-a9a2559f92b3.png) 1. Upgrade 1 1 11 version 2. create schema The syntax does not recognize the ddl syntax. It mainly recognizes the create alter drop. Table and other operations
986,idle timeout exceeds close channel to save server resources Canal1 1 1 version error 2018-10-08 18:09:07.144 [Hashed wheel timer #1] WARN c.a.o.c.server.netty.handler.ClientAuthenticationHandler - channel:[id: 0x2d3c7336 /10.8.32.241:54621 => /10.8.32.241:11111] idle timeout exceeds close channel to save server resources... 2018-10-08 18:09:13.279 [New I/O server worker #1-14] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x4863b38b /10.8.32.241:54622 :> /10.8.32.241:11111] exception=java.nio.channels.ClosedChannelException at org.jboss.netty.channel.socket.nio.NioWorker.cleanUpWriteBuffer(NioWorker.java:629) at org.jboss.netty.channel.socket.nio.NioWorker.close(NioWorker.java:605) at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:356) at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) Client has not been with canal for a long time The server has an interaction that causes the default timeout of 1 hour.
985,The server is configured with multiple instances. How to recover after an instance is reported incorrectly? The server listens to multiple libraries and has a library. Because of network problems, it has been interrupted for a while and has not been reported for timeout. If you want to reconnect, can you only restart the server or you can only restart the wrong instance? ![default](https://user-images.githubusercontent.com/22339074/46599888-0e57f100-cb1b-11e8-81e4-38203ac9e231.PNG) It is recommended to test the latest 1 1 1 version directly.
984,Ask a question about the canal code data structure RowChange getRowDatasList in rowChange may Will return more than one if it will Under what circumstances will Batch situation
983,1 1 0 Whether to support MariaDB 10 3 6 Support mariadb 5 x and 10 x
982,How can the client get it? TableMeta Thank you If you want to get it on the client side Primary key or unique key in the table But I don&#39;t know where to start. Now we use canal Is 1 0 22 It is said that after 1 0 26 canal increase tsdb to store table structure information in real time, how to obtain the client? Direct connect to druid or in the batch pull from the returned message or other objects can be obtained Please let me know thank you all. The primary key of the table already has the identifier isKey in the returned protobuf object. If you want to get a unique key, you need to get the table name to check the database table structure. Thank you, I&#39;ve now understood
981,Rocketmq client package scope in client module Provide external use alone tks
980,table meta error ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - pls submit github issue show create table ddl:CREATE TABLE `partner_organization_data` ( `id` int(11) NOT NULL AUTO_INCREMENT `organizationName` varchar(250) DEFAULT NULL `organizationId` varchar(250) NOT NULL `organizationLogo` varchar(500) DEFAULT NULL COMMENT Institutional logo `organizationUrl` varchar(500) DEFAULT NULL COMMENT Institution address `isDelete` tinyint(4) NOT NULL DEFAULT '0' COMMENT Whether to delete 0 is to delete 1 is not to delete `isOnline` tinyint(4) NOT NULL DEFAULT '0' COMMENT Whether on line 0 is not on line 1 is on line `organizationLogoh5_2` varchar(500) DEFAULT NULL COMMENT 'h5 Second logo `organizationLogoh5` varchar(500) DEFAULT NULL COMMENT H5 agency logo `organizationUrlh5` varchar(500) DEFAULT NULL COMMENT H5 agency url PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=11 DEFAULT CHARSET=utf8 compare failed . db : TableMeta [schema=operation table=partner_organization_data fileds= FieldMeta [columnName=id columnType=int(11) nullable=false key=true defaultValue=null extra=null unique=false] FieldMeta [columnName=organizationName columnType=varchar(250) nullable=true key=false defaultValue=null extra=null unique=false] FieldMeta [columnName=organizationId columnType=varchar(250) nullable=false key=false defaultValue=null extra=null unique=false] FieldMeta [columnName=organizationLogo columnType=varchar(500) nullable=true key=false defaultValue=null extra=null unique=false] FieldMeta [columnName=organizationUrl columnType=varchar(500) nullable=true key=false defaultValue=null extra=null unique=false] FieldMeta [columnName=isDelete columnType=tinyint(4) nullable=false key=false defaultValue=0 extra=null unique=false] FieldMeta [columnName=isOnline columnType=tinyint(4) nullable=false key=false defaultValue=0 extra=null unique=false] FieldMeta [columnName=organizationLogoh5_2 columnType=varchar(500) nullable=true key=false defaultValue=null extra=null unique=false] FieldMeta [columnName=organizationLogoh5 columnType=varchar(500) nullable=true key=false defaultValue=null extra=null unique=false] FieldMeta [columnName=organizationUrlh5 columnType=varchar(500) nullable=true key=false defaultValue=null extra=null unique=false] ] mem : TableMeta [schema=operation table=partner_organization_data fileds= FieldMeta [columnName=id columnType=int(11) nullable=false key=true defaultValue=null extra=null unique=false] FieldMeta [columnName=organizationName columnType=varchar(250) nullable=true key=false defaultValue=null extra=null unique=false] FieldMeta [columnName=organizationId columnType=varchar(250) nullable=false key=false defaultValue=null extra=null unique=false] FieldMeta [columnName=organizationLogo columnType=varchar(500) nullable=true key=false defaultValue=null extra=null unique=false] FieldMeta [columnName=organizationUrl columnType=varchar(500) nullable=true key=false defaultValue=null extra=null unique=false] FieldMeta [columnName=isDelete columnType=tinyint(4) nullable=false key=false defaultValue=0 extra=null unique=false] FieldMeta [columnName=isOnline columnType=tinyint(4) nullable=false key=false defaultValue=0 extra=null unique=false] FieldMeta [columnName=organizationLogoh5_2 columnType=varchar(500) nullable=true key=false defaultValue=null extra=null unique=false] FieldMeta [columnName=organizationLogoh5_2 columnType=varchar(500) nullable=true key=false defaultValue=null extra=null unique=false] FieldMeta [columnName=organizationLogoh5 columnType=varchar(500) nullable=true key=false defaultValue=null extra=null unique=false] FieldMeta [columnName=organizationUrlh5 columnType=varchar(500) nullable=true key=false defaultValue=null extra=null unique=false] ] 2018-09-30 15:53:41.988 [[scheduler-table-meta-snapshot]] ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - compare failed check log The above is the error log Added a data column two days ago organizationLogoh5_2 Two organizationLogoh5_2 appeared in the error mem that happened today. 1. The corresponding version is 2. You can find the DDL corresponding to the operation on the table from the local h2 file. I will reproduce it locally. The memory table structure is not subject to weight. The main reason is that meta_history will perform deduplication based on the serverId bit. If it is normal, the active and standby devices will retain the serverId when copying, but the corresponding sites will be different. Meta_history will have the main The respective DDLs of the standby database will have duplicate memory columns if they are backtracked again. After the National Day, support the idempotent processing of the tsdb memory table structure to avoid the backtracking of the active and standby switching. Master version 2 days ago a canal instance The instance configuration is connected to the main library. Standby not set select * from PUBLIC.meta_snapshot The following is the result of the query Only one 1 2018-09-29 15:53:47.086000000 2018-09-29 15:53:47.086000000 partner 0 0 -1 -2 {"operation":"CREATE TABLE `application` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT \n\t`name` varchar(100) NOT NULL DEFAULT '' \n\t`type` varchar(50) NOT NULL DEFAULT '' \n\tPRIMARY KEY (`id`) \n\tUNIQUE Application noun index (`name`)\n) ENGINE = InnoDB AUTO_INCREMENT = 3 CHARSET = utf8; \nCREATE TABLE `op_account_history` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT \n\t`hexun_user_id` int(11) NOT NULL \n\t`hexun_user_nick` varchar(255) NOT NULL \n\t`add_time` datetime NOT NULL \n\t`type` varchar(255) NOT NULL \n\t`content` varchar(255) NOT NULL \n\t`success` int(1) NOT NULL \n\t`params` text \n\t`access_type` varchar(45) DEFAULT NULL \n\t`access_log_param` text \n\t`access_log_detail` text \n\t`ip` varchar(45) DEFAULT NULL \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 217066 CHARSET = utf8 COMMENT Administrator account access record \nCREATE TABLE `op_account_url_permission` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT COMMENT Self-increasing permission id\n' \n\t`permission_url` varchar(255) NOT NULL COMMENT Permissions are controlled according to the url \n\t`permission_desc` varchar(255) DEFAULT NULL COMMENT Description of permissions such as audit management \n\t`allowed_roles` varchar(512) NOT NULL COMMENT Allowed role ID \n\t`log_message_format` varchar(6000) DEFAULT NULL \n\t`access_type` varchar(45) DEFAULT NULL \n\t`permission_api_id` varchar(128) DEFAULT NULL \n\tPRIMARY KEY (`id`) \n\tUNIQUE `id_UNIQUE` (`id`) \n\tUNIQUE `permission_url_UNIQUE` (`permission_url`)\n) ENGINE = InnoDB AUTO_INCREMENT = 353 CHARSET = utf8 COMMENT The role that the role can exercise \nCREATE TABLE `op_account_user` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT \n\t`hexun_user_id` int(11) NOT NULL \n\t`hexun_user_name` varchar(255) NOT NULL \n\t`hexun_user_nick` varchar(255) NOT NULL \n\t`email_notification` varchar(255) NOT NULL \n\t`user_role_id` int(11) NOT NULL \n\t`create_time` datetime NOT NULL \n\t`creator_user_id` int(11) NOT NULL \n\t`creator_user_name` varchar(255) NOT NULL \n\t`creator_user_nick` varchar(255) NOT NULL \n\t`true_name` varchar(255) DEFAULT NULL \n\tPRIMARY KEY (`id`) \n\tUNIQUE `id_UNIQUE` (`id`) \n\tUNIQUE `hexun_user_id_UNIQUE` (`hexun_user_id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 85 CHARSET = utf8 COMMENT Operational background administrator table \nCREATE TABLE `op_account_user_role` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT \n\t`role_id` tinyint(4) NOT NULL \n\t`role_name` varchar(255) NOT NULL \n\t`role_desc` text \n\tPRIMARY KEY (`id`) \n\tUNIQUE `id_UNIQUE` (`id`) \n\tUNIQUE `roleName_UNIQUE` (`role_name`) \n\tUNIQUE `roleId_UNIQUE` (`role_id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 26 CHARSET = utf8; \nCREATE TABLE `op_check_flow_right` (\n\t`id` tinyint(8) NOT NULL \n\t`check_level` tinyint(4) DEFAULT NULL \n\t`role_id` varchar(16) DEFAULT NULL \n\t`modular_id` tinyint(4) DEFAULT NULL \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB CHARSET = utf8; \nCREATE TABLE `op_check_partner_eventflow` (\n\t`id` int(8) NOT NULL AUTO_INCREMENT \n\t`partner_id` int(8) DEFAULT NULL \n\t`check_level` tinyint(2) DEFAULT '1' COMMENT Divided into Level 1 and Level 2 Audit Level 3 is dismissed \n\t`check_status` tinyint(2) DEFAULT '1' COMMENT 1 pending review 2 approved \n\t`check_time` datetime DEFAULT NULL \n\t`check_reason` varchar(512) DEFAULT NULL \n\t`partner_submit_time` datetime DEFAULT NULL \n\t`adviser_status` tinyint(2) DEFAULT '0' COMMENT Investigate identity review 0 live broadcast audit 1 investment review 2 investment review completed 3 vote to downgrade to live broadcasters \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 7034 CHARSET = utf8; \nCREATE TABLE `op_check_partner_log` (\n\t`id` int(8) NOT NULL AUTO_INCREMENT \n\t`operateName` varchar(32) DEFAULT '' COMMENT Operator name \n\t`operateType` varchar(32) DEFAULT '' COMMENT Operator role \n\t`partnerId` int(8) DEFAULT NULL COMMENT Event ID \n\t`operateTime` datetime DEFAULT NULL COMMENT Operating time \n\t`operateContent` varchar(256) DEFAULT '' COMMENT Operational content \n\t`checkType` varchar(64) DEFAULT '' COMMENT Operation type application rejected for review \n\t`checkUserId` int(11) DEFAULT NULL \n\t`reason` mediumtext \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 2534 CHARSET = utf8; \nCREATE TABLE `op_check_partner_situation_detail` (\n\t`id` int(8) NOT NULL AUTO_INCREMENT \n\t`check_user_id` int(11) DEFAULT NULL \n\t`partner_id` int(11) DEFAULT NULL \n\t`check_time` datetime DEFAULT NULL \n\t`check_status` tinyint(2) DEFAULT NULL \n\t`check_level` tinyint(2) DEFAULT NULL \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 104 CHARSET = utf8; \nCREATE TABLE `op_menu_management` (\n\t`id` int(8) NOT NULL AUTO_INCREMENT \n\t`parent_id` int(8) DEFAULT '0' COMMENT Menu parent id \n\t`menu_name` varchar(64) NOT NULL COMMENT Menu name \n\t`menu_flag` varchar(32) NOT NULL COMMENT Menu identifier \n\t`menu_link_address` varchar(128) DEFAULT NULL COMMENT Secondary menu link address \n\t`menu_permission` varchar(32) DEFAULT NULL COMMENT Menu permissions \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 104 CHARSET = utf8; \nCREATE TABLE `op_operate_log` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT \n\t`operaterId` int(11) DEFAULT NULL \n\t`operaterName` varchar(64) DEFAULT NULL \n\t`model` varchar(64) DEFAULT NULL \n\t`userId` int(11) DEFAULT NULL \n\t`operateContent` varchar(512) DEFAULT NULL \n\t`timeStamp` datetime DEFAULT NULL \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 1949 CHARSET = utf8; \nCREATE TABLE `op_partner_comment_statistics` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT \n\t`dataDate` datetime(4) DEFAULT NULL COMMENT date \n\t`articleCommentCount` int(4) DEFAULT '0' COMMENT Article comments \n\t`videoCommentCount` int(4) DEFAULT '0' COMMENT Live video commentary \n\t`systemClassCommentCount` int(4) DEFAULT '0' COMMENT System class comments \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 422 CHARSET = utf8; \nCREATE TABLE `op_partner_data_statistics` (\n\t`id` int(8) NOT NULL AUTO_INCREMENT \n\t`dataDate` datetime DEFAULT NULL \n\t`partnerId` int(11) DEFAULT NULL \n\t`caiquanCount` int(4) DEFAULT '0' COMMENT Number of financial circles \n\t`articleCount` int(4) DEFAULT '0' COMMENT Total number of articles \n\t`articleMessageCount` int(4) DEFAULT '0' COMMENT Article number of comments \n\t`videoDemandClassCount` int(4) DEFAULT '0' COMMENT Video on demand lesson \n\t`liveClassCount` int(4) DEFAULT '0' COMMENT Video live lessons \n\t`caiquanReplyCount` int(4) DEFAULT '0' COMMENT Financial replies \n\t`newAttentionCount` int(4) DEFAULT '0' COMMENT Add attention \n\t`cancelAttentionCount` int(4) DEFAULT '0' COMMENT Unfollow the number \n\t`increaseAttentionCount` int(4) DEFAULT '0' COMMENT Net increase in attention \n\t`totalAttentionCount` int(4) DEFAULT '0' COMMENT Cumulative number of followers \n\t`caiboCount` int(4) DEFAULT '0' COMMENT Number of financial broadcasts \n\t`sales` int(8) DEFAULT '0' COMMENT Sales \n\t`aSharesCount` int(4) DEFAULT '0' COMMENT A-share radar number \n\t`tuguReplyCount` int(4) DEFAULT '0' COMMENT Answers \n\t`remarkCount` int(4) DEFAULT '0' COMMENT Total number of teachers in the live room \n\t`studentSpeakCount` int(4) DEFAULT '0' COMMENT Total number of participants in the live room \n\t`openReplyCount` int(4) DEFAULT '0' COMMENT The total number of public replies from the teacher in the live room \n\t`studentSecretAskCount` int(4) DEFAULT '0' COMMENT The total number of questions asked by the students \n\t`secretReplyCount` int(4) DEFAULT '0' COMMENT Teacher private letter reply total \n\t`freeStudentSpeaks` int(4) DEFAULT '0' COMMENT Students speak for free \n\t`nonFreeStudentSpeaks` int(4) DEFAULT '0' COMMENT Student speaking fee \n\t`freeStudentAsks` int(4) DEFAULT '0' COMMENT Student private letter asking for free \n\t`nonFreeStudentAsks` int(4) DEFAULT '0' COMMENT Student private letter questioning fee \n\t`freeTeacherSpeaks` int(4) DEFAULT '0' COMMENT Teacher speaks free of charge \n\t`nonFreeTeacherSpeaks` int(4) DEFAULT '0' COMMENT Teacher public speaking fee \n\t`freeTeacherRepliesPublic` int(4) DEFAULT '0' COMMENT Teacher public reply free \n\t`nonFreeTeacherRepliesPublic` int(4) DEFAULT '0' COMMENT Teacher publicly responded to the charge \n\t`freeTeacherRepliesPrivate` int(4) DEFAULT '0' COMMENT Teacher private reply free \n\t`nonFreeTeacherRepliesPrivate` int(4) DEFAULT '0' COMMENT Teacher private reply fee \n\t`freeArticleCount` int(4) DEFAULT '0' COMMENT Free articles \n\t`nonfreeArticleCount` int(4) DEFAULT '0' COMMENT Number of articles charged \n\t`onlineTime` int(4) DEFAULT '0' COMMENT Teacher live room online time \n\t`freeStudentCount` int(4) DEFAULT '0' COMMENT Free live room online number \n\t`nonFreeStudentCount` int(4) DEFAULT '0' COMMENT Charged live room online number \n\t`freeTeacherSecretSpeaks` int(4) DEFAULT '0' COMMENT Free viewing room with privacy views \n\t`nonFreeTeacherSecretSpeaks` int(4) DEFAULT '0' COMMENT Privacy view number of live broadcast room no data \n\t`totalTeacherSecretSpeaks` int(4) DEFAULT '0' COMMENT Teacher privacy view all \n\tPRIMARY KEY (`id`) \n\tUNIQUE `index_data_id` USING BTREE (`dataDate` `partnerId`) \n\tKEY `index_date` USING BTREE (`dataDate`)\n) ENGINE = InnoDB AUTO_INCREMENT = 1412753 CHARSET = utf8; \nCREATE TABLE `op_partner_data_statistics_log` (\n\t`id` int(8) NOT NULL AUTO_INCREMENT \n\t`partnerId` int(11) DEFAULT NULL \n\t`dataDate` datetime DEFAULT NULL \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 943575 CHARSET = utf8; \nCREATE TABLE `op_product_data` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT \n\t`productId` int(11) DEFAULT NULL \n\t`productName` varchar(100) DEFAULT NULL \n\t`partnerId` int(11) DEFAULT NULL \n\t`checkType` varchar(20) DEFAULT NULL COMMENT Audit type \n\t`productType` varchar(30) DEFAULT NULL \n\t`createTime` datetime DEFAULT NULL \n\t`bussinessProId` varchar(50) DEFAULT NULL \n\t`bussinessCheck` varchar(10) DEFAULT NULL \n\t`bussinessInfo` varchar(1000) DEFAULT NULL \n\t`bussinessLog` varchar(1000) DEFAULT NULL \n\t`modifiTime` datetime DEFAULT NULL \n\t`isClose` int(8) DEFAULT '0' \n\t`verifyCode` varchar(100) DEFAULT NULL \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 137 CHARSET = utf8; \nCREATE TABLE `op_req_external_service_log` (\n\t`id` int(8) NOT NULL AUTO_INCREMENT \n\t`partner_id` int(11) DEFAULT NULL COMMENT Partner id \n\t`service_name` varchar(64) DEFAULT NULL COMMENT Request interface name \n\t`req_param` text COMMENT Request parameter \n\t`req_time` datetime DEFAULT NULL COMMENT Request time \n\t`resp_content` text COMMENT Interface return content \n\t`resp_status` tinyint(2) DEFAULT NULL COMMENT Whether the request was successful 0 successful 1 failure \n\t`manual_sync_status` tinyint(2) DEFAULT '0' COMMENT '1 This record representing failure has been manually synchronized \n\tPRIMARY KEY (`id`) \n\tKEY `index_partner_id` USING BTREE (`partner_id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 1484 CHARSET = utf8; \nCREATE TABLE `op_violation_forbidden_words` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT \n\t`forbidden_word` varchar(255) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL DEFAULT '' \n\t`add_time` datetime NOT NULL \n\t`creator_user_id` int(11) NOT NULL \n\t`creator_user_nick` varchar(255) NOT NULL \n\tPRIMARY KEY (`id`) \n\tUNIQUE `id_UNIQUE` (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 503987 CHARSET = utf8 COMMENT Sensitive word \nCREATE TABLE `partner_advertising_data` (\n\t`id` int(8) NOT NULL AUTO_INCREMENT \n\t`partnerId` int(11) DEFAULT NULL COMMENT Teacher id \n\t`content` text COMMENT Advertising content \n\t`content_h` text COMMENT Extended field \n\t`createTime` datetime DEFAULT NULL COMMENT Creation time \n\t`createUserId` int(11) DEFAULT NULL COMMENT Create User ID \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 5224 CHARSET = utf8; \nCREATE TABLE `partner_circle_data` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT \n\t`partnerId` int(11) NOT NULL \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 3 CHARSET = utf8; \nCREATE TABLE `partner_config` (\n\t`id` int(4) NOT NULL AUTO_INCREMENT \n\t`partnerId` bigint(20) NOT NULL COMMENT Partner id \n\t`isCombined` tinyint(4) DEFAULT '0' COMMENT Whether to hide 1 is a combination account 0 is not a combination account \n\t`isHide` tinyint(4) NOT NULL DEFAULT '0' COMMENT Whether to hide 1 is hidden 0 is not hidden \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 12 CHARSET = utf8; \nCREATE TABLE `partner_data` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT COMMENT Self-added id \n\t`partnerId` int(11) NOT NULL COMMENT User id \n\t`truename` varchar(25) DEFAULT NULL COMMENT actual name \n\t`sex` varchar(5) DEFAULT NULL COMMENT gender \n\t`province` varchar(25) DEFAULT NULL COMMENT province \n\t`city` varchar(25) DEFAULT NULL COMMENT city \n\t`qq` varchar(25) DEFAULT NULL COMMENT Qq number \n\t`blog` varchar(250) DEFAULT NULL COMMENT Blog \n\t`weibo` varchar(250) DEFAULT NULL COMMENT Maiho \n\t`orgname` varchar(100) DEFAULT NULL COMMENT name of association \n\t`intro` text COMMENT Introduction \n\t`noticeType` varchar(10) DEFAULT NULL COMMENT Label type \n\t`noticeInfo` text COMMENT content \n\t`level` varchar(32) DEFAULT NULL COMMENT content \n\t`modifitime` datetime DEFAULT NULL COMMENT Last Modified \n\t`createtime` datetime DEFAULT NULL COMMENT Creation time \n\t`cooperationState` tinyint(2) DEFAULT '0' COMMENT 0 not cooperation 1 cooperation 2 suspension 3 pull black \n\t`aplanstate` tinyint(2) DEFAULT '0' COMMENT '0 Deactivate 1 open \n\t`zhibostate` tinyint(2) DEFAULT '1' COMMENT Text live service 0 Deactivate 1 open \n\t`busDepartment` varchar(500) DEFAULT NULL \n\t`jobCode` varchar(250) DEFAULT NULL \n\t`adviserCooperationState` tinyint(2) DEFAULT '0' COMMENT 0 not cooperation 1 cooperation 2 suspension 3 pull black \n\t`openWeChatPlatform` tinyint(2) DEFAULT '0' COMMENT 0 has not opened WeChat platform 1 to open WeChat platform \n\t`adviserHide` tinyint(2) DEFAULT '0' COMMENT 1 hidden \n\t`openEntrust` tinyint(2) DEFAULT '0' COMMENT 0 is no permission 1 is authorized \n\tPRIMARY KEY (`id`) \n\tUNIQUE `partnerId` (`partnerId`) \n\tKEY `level` USING BTREE (`level`)\n) ENGINE = InnoDB AUTO_INCREMENT = 7171 CHARSET = utf8; \nCREATE TABLE `partner_navigation` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT \n\t`partnerId` int(11) NOT NULL COMMENT Partner id \n\t`navigation` varchar(250) DEFAULT NULL \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 882 CHARSET = utf8; \nCREATE TABLE `partner_notice` (\n\t`id` int(8) NOT NULL AUTO_INCREMENT \n\t`title` varchar(64) DEFAULT NULL \n\t`content` varchar(512) DEFAULT NULL \n\t`createTime` datetime DEFAULT NULL \n\t`createUserName` varchar(64) DEFAULT NULL \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 68 CHARSET = utf8; \nCREATE TABLE `partner_notice_number` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT \n\t`partnerId` int(11) DEFAULT NULL \n\t`readNumber` int(11) DEFAULT NULL \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 464 CHARSET = utf8; \nCREATE TABLE `partner_organization` (\n\t`id` int(4) NOT NULL AUTO_INCREMENT \n\t`partnerId` int(8) DEFAULT NULL COMMENT Partner id \n\t`organization` varchar(25) DEFAULT NULL \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 95 CHARSET = utf8; \nCREATE TABLE `partner_organization_data` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT \n\t`organizationName` varchar(250) DEFAULT NULL \n\t`organizationId` varchar(250) NOT NULL \n\t`organizationLogo` varchar(500) DEFAULT NULL COMMENT Institutional logo \n\t`organizationUrl` varchar(500) DEFAULT NULL COMMENT Institution address \n\t`isDelete` tinyint(4) NOT NULL DEFAULT '0' COMMENT Whether to delete 0 is to delete 1 is not to delete \n\t`isOnline` tinyint(4) NOT NULL DEFAULT '0' COMMENT Whether on line 0 is not on line 1 is on line \n\t`organizationLogoh5_2` varchar(500) DEFAULT NULL COMMENT 'h5 Second logo \n\t`organizationLogoh5` varchar(500) DEFAULT NULL COMMENT H5 agency logo \n\t`organizationUrlh5` varchar(500) DEFAULT NULL COMMENT H5 agency url \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 11 CHARSET = utf8; \nCREATE TABLE `partner_recommend` (\n\t`partner_id` int(11) NOT NULL COMMENT Teacher id \n\t`partner_order` int(11) NOT NULL COMMENT Sort \n\t`open_recommend` tinyint(2) DEFAULT NULL COMMENT Whether to open the recommendation \n\tPRIMARY KEY (`partner_id`) \n\tUNIQUE `index_partner_id` USING BTREE (`partner_id`)\n) ENGINE = InnoDB CHARSET = utf8; \nCREATE TABLE `partner_recommend_copy` (\n\t`partner_id` int(11) NOT NULL COMMENT Teacher id \n\t`partner_order` int(11) NOT NULL COMMENT Sort \n\t`open_recommend` tinyint(2) DEFAULT NULL COMMENT Whether to open the recommendation \n\tPRIMARY KEY (`partner_id`) \n\tUNIQUE `index_partner_id` USING BTREE (`partner_id`)\n) ENGINE = InnoDB CHARSET = utf8; \nCREATE TABLE `partner_service_data` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT \n\t`partnerId` int(11) NOT NULL COMMENT Partner id \n\t`serviceStatus` tinyint(4) NOT NULL COMMENT Whether to display the service button \n\t`aplanStatus` tinyint(4) NOT NULL COMMENT Whether to show the A plan \n\t`articleStatus` tinyint(4) NOT NULL COMMENT Whether to display articles \n\t`circleStatus` tinyint(4) NOT NULL COMMENT Whether to show circles \n\t`classesStatus` tinyint(4) NOT NULL COMMENT Whether to display the course \n\t`blogStatus` tinyint(4) DEFAULT NULL COMMENT Whether to display the blog \n\t`aplanUrl` varchar(500) DEFAULT NULL COMMENT a plan link \n\t`articleUrl` varchar(500) DEFAULT NULL COMMENT Article url \n\t`circleUrl` varchar(500) DEFAULT NULL COMMENT Circle url \n\t`classUrl` varchar(500) DEFAULT NULL COMMENT Course url \n\t`blogUrl` varchar(500) DEFAULT NULL COMMENT Blog url \n\tPRIMARY KEY (`id`) \n\tKEY `partnerId` USING BTREE (`partnerId`)\n) ENGINE = InnoDB AUTO_INCREMENT = 15 CHARSET = utf8; \nCREATE TABLE `partner_signelectric` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT COMMENT Self-incrementing primary key \n\t`partnerId` int(11) NOT NULL COMMENT Partner id \n\t`partnerName` varchar(50) DEFAULT NULL COMMENT Partner name \n\t`trueName` varchar(20) DEFAULT NULL COMMENT actual name \n\t`idCard` varchar(50) DEFAULT NULL COMMENT identity number \n\t`address` varchar(200) DEFAULT NULL COMMENT address \n\t`createtime` datetime DEFAULT NULL COMMENT Creation time \n\t`modifitime` datetime DEFAULT NULL COMMENT Last Modified \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 1279 CHARSET = utf8; \nCREATE TABLE `partner_tag_category` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT COMMENT Self-incrementing primary key \n\t`categoryId` int(11) NOT NULL DEFAULT '0' COMMENT Column id \n\t`parentId` int(11) DEFAULT NULL COMMENT Parent column id \n\t`categoryName` varchar(50) DEFAULT NULL COMMENT program name \n\t`categoryType` int(11) DEFAULT NULL COMMENT Column type \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 16 CHARSET = utf8; \nCREATE TABLE `partner_tag_data` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT COMMENT Self-incrementing primary key \n\t`tagId` int(11) NOT NULL DEFAULT '0' COMMENT Subject id \n\t`tagName` varchar(50) DEFAULT NULL COMMENT Subject name \n\t`tagType` int(11) DEFAULT NULL COMMENT Type of target \n\t`categoryId` int(11) DEFAULT NULL COMMENT Subject column \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 49 CHARSET = utf8; \nCREATE TABLE `partner_user_tag` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT COMMENT Self-incrementing primary key \n\t`partnerId` int(11) DEFAULT NULL COMMENT Partner id \n\t`tag` varchar(200) DEFAULT NULL COMMENT Subject \n\t`tagType` int(4) DEFAULT NULL COMMENT Types of \n\t`createtime` datetime DEFAULT NULL COMMENT time \n\t`userDefined` tinyint(1) DEFAULT NULL \n\tPRIMARY KEY (`id`)\n) ENGINE = InnoDB AUTO_INCREMENT = 64455 CHARSET = utf8; \nCREATE TABLE `statistics_px-operation-app` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT \n\t`timestamp` bigint(1) NOT NULL DEFAULT '0' COMMENT Timestamp \n\t`serviceInterface` varchar(255) NOT NULL DEFAULT '' COMMENT Interface name \n\t`method` varchar(255) NOT NULL DEFAULT '' COMMENT Method name \n\t`type` varchar(10) DEFAULT NULL COMMENT Currently called application type \n\t`tps` float(11 2) NOT NULL DEFAULT '0.00' COMMENT TPS value \n\t`kbps` float(11 2) DEFAULT NULL COMMENT flow \n\t`host` varchar(50) DEFAULT NULL COMMENT Ip address \n\t`elapsed` int(11) DEFAULT NULL COMMENT time consuming \n\t`concurrent` int(11) DEFAULT NULL COMMENT Concurrent number \n\t`input` int(11) DEFAULT NULL COMMENT input value \n\t`output` int(11) DEFAULT NULL COMMENT Output size \n\t`successCount` int(11) DEFAULT NULL COMMENT Number of successes \n\t`failureCount` int(11) DEFAULT NULL COMMENT number of failures \n\t`remoteAddress` varchar(50) DEFAULT NULL COMMENT Remote address \n\t`remoteType` varchar(20) DEFAULT NULL COMMENT Remote application type \n\tPRIMARY KEY (`id`) \n\tKEY `time-index` (`timestamp`) \n\tKEY `method-index` (`method`) \n\tKEY `service-index` (`serviceInterface`)\n) ENGINE = InnoDB AUTO_INCREMENT = 168704 CHARSET = utf8; \nCREATE TABLE `statistics_zhibo-app` (\n\t`id` int(11) NOT NULL AUTO_INCREMENT \n\t`timestamp` bigint(1) NOT NULL DEFAULT '0' COMMENT Timestamp \n\t`serviceInterface` varchar(255) NOT NULL DEFAULT '' COMMENT Interface name \n\t`method` varchar(255) NOT NULL DEFAULT '' COMMENT Method name \n\t`type` varchar(10) DEFAULT NULL COMMENT Currently called application type \n\t`tps` float(11 2) NOT NULL DEFAULT '0.00' COMMENT TPS value \n\t`kbps` float(11 2) DEFAULT NULL COMMENT flow \n\t`host` varchar(50) DEFAULT NULL COMMENT Ip address \n\t`elapsed` int(11) DEFAULT NULL COMMENT time consuming \n\t`concurrent` int(11) DEFAULT NULL COMMENT Concurrent number \n\t`input` int(11) DEFAULT NULL COMMENT input value \n\t`output` int(11) DEFAULT NULL COMMENT Output size \n\t`successCount` int(11) DEFAULT NULL COMMENT Number of successes \n\t`failureCount` int(11) DEFAULT NULL COMMENT number of failures \n\t`remoteAddress` varchar(50) DEFAULT NULL COMMENT Remote address \n\t`remoteType` varchar(20) DEFAULT NULL COMMENT Remote application type \n\tPRIMARY KEY (`id`) \n\tKEY `time-index` (`timestamp`) \n\tKEY `method-index` (`method`) \n\tKEY `service-index` (`serviceInterface`)\n) ENGINE = InnoDB AUTO_INCREMENT = 1657961 CHARSET = utf8; \n"}
979,reset last position when apply snapshot to database The meaning of reset in applySnapshotToDB is ![image](https://user-images.githubusercontent.com/8461826/46252914-816dc180-c4a2-11e8-8eef-39f8a859b2aa.png) If not reset Even without ddl changes The position here still points to the last ddl The position in the applySnapshotToDB is passed to the location where the last DDL change is applied to the memory structure. I still don&#39;t understand why I need to reset.
978,Solve kafka Client message cannot be ack bug Solve kafka Client message cannot be ack bug
977,Synchronization site problem canal Version 1 0 24 Source instance RDS Problem canal The server is configured with multiple intancaes after startup. If the rds active/standby switchover or instance migration occurs during the subscription process, after modifying the configuration file, the intances are only reloaded. The modification information will not be synchronized to zk. Then what should I do at this time? Stop canal server 删除zookeeper中 otter canal destinations your_instance 1001 Restart the canal for the entire znode server @1241407808 If it is the corresponding multiple intenses, restart the server will affect other intenses? Of course, other instances will stop. canal Server will put other instances The site is recorded to the next start in zk and will continue according to the corresponding site. Thank you very much.
976,Canal kafka 1 1 0 version binlog does not change when the cpu takes up a core 99 99 cpu (50.00% of 2 core) Use the canal kafka 1 1 0 version of the canal properties to use the default configuration after the startup cpu occupancy is very high binlog no change Server CPU information [cpuinfo.txt](https://github.com/alibaba/canal/files/2430253/cpuinfo.txt) Thread call information is ![123](https://user-images.githubusercontent.com/16751056/46240746-a862bf80-c3de-11e8-9707-8a6bb6771a6e.png) Flame map ![image](https://user-images.githubusercontent.com/16751056/46240795-5bcbb400-c3df-11e8-9e51-874821bc76fa.png) Is it because the machine configuration is low or other reasons? Please follow the latest version #904
975,Data is always syncing but the amount of data is wrong ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x127a1b89 /5.39.221.52:60106 => My canal ip I’m going to report this mistake every few seconds. Is someone stealing my canal? This ip seems to be Dutch 5.39.221.52:60106 Don&#39;t expose yourself to the public website. -_-#
973,[ISSUE 800 provides RocketMQ native access canal Rocketmq Connector docking canal but currently only supports single-partition ordering to the upper canal The message can be split according to the business field hash, and then the multi-partition is ordered, but the change to the canal is larger. Subsequent follow-ups indicate that there is a problem with multi-partition order processing but there is an association between the tables. [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=973) <br/>All committers have signed the CLA. [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=973) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=973) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=973) it.</sub> The client&#39;s canal client yml configuration format is a bit problematic should be changed to ``` mqTopics: - mqMode: kafka topic: example groups: - groupId: example_g1 outAdapters: - name: logger ``` rocketMQ can consider adding support for FlatMessage to do data hash fragmentation
972,canal1.1.0 Stop the consumption of ClosedChannelException after running for a while in ZK mode Continue to consume after restarting the client but the location is not updated. The following is the error log that appears on the canal server. 2018-09-26 17:14:32.336 [New I/O server worker #1-16] ERROR c.a.otter.canal.server.netty.handler.Ses sionHandler - something goes wrong with channel:[id: 0x4b49367f /172.16.40.202:46512 :> /172.16.40. 200:11111] exception=java.nio.channels.ClosedChannelException at org.jboss.netty.channel.socket.nio.NioWorker.cleanUpWriteBuffer(NioWorker.java:629) at org.jboss.netty.channel.socket.nio.NioWorker.close(NioWorker.java:605) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioSe rverSocketPipelineSink.java:119) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketP ipelineSink.java:76) at org.jboss.netty.channel.Channels.close(Channels.java:720) at org.jboss.netty.channel.AbstractChannel.close(AbstractChannel.java:208) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.exceptionCaught(SessionHandle r.java:272) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwar eChannelHandler.java:48) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.exceptionCaught(ReplayingDecoder.ja va:462) at org.jboss.netty.channel.Channels.fireExceptionCaught(Channels.java:432) at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:332) at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) 2018-09-26 17:15:30.046 [New I/O server worker #1-1] ERROR c.a.otter.canal.server.netty.handler.Sess ionHandler - something goes wrong with channel:[id: 0x7ac76894 /172.16.40.202:51872 => /172.16.40.2 00:11111] exception=java.io.IOException: Connection reset by peer at sun.nio.ch.FileDispatcherImpl.read0(Native Method) at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39) at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223) at sun.nio.ch.IOUtil.read(IOUtil.java:192) at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380) at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:322) at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Restart the server site and start updating.
971,Question: canal v1.1.0 Why is there a comparison process like DatabaseTableMeta compareTableMetaDbAndMemory? Such as the title In order to ensure the correctness of the table structure maintained based on the memory tsdb, compare it with the current database before doing the snapshot. In the case that the table structure maintained by the memory tsdb is correct, the following two cases will still cause the DatabaseTableMeta compareTableMetaDbAndMemory to return false. 1. The consumption of canal has a relatively large delay. At this time, there is a difference between the table structure in memory and the real table structure of mysql. 2. Regression site repeated consumption binlog consumption is very slow, not catching up to the latest site within 24 hours The snapshot will be started every 24 hours. If the comparison fails in one time, it can be recorded after the next comparison. 3Q I know the design intent of this piece.
970,fix bug #968 bug Reproduce 1. Enable parallel parsing 2. Normal connection source database - Number of print threads 3. Close the source database - Number of print threads By frequently printing the current number of threads, the number of threads is constantly increasing until OOM give it a like
969,Fix link error as title tks
968,Parallel parsing of the database has not been able to connect to cause OOM exceptions version v1.1.1-alpha 1 heapdump: ![image](https://user-images.githubusercontent.com/33280738/46004767-435d5000-c0e6-11e8-942d-1b283879a799.png)
967,Canal Running for a while Server error [New I/O server worker #1-2] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x5c5493e4 /10.152.0.121:18964 => /10.193. 16.48:11111] exception=java.io.IOException: Connection reset by peer canal version 1.1.0 See other issues Said to be idleTimeout This idle timeout problem I looked at the error. It seems that I was given an error after 1 hour. I want to ask if I can set this idleTimeout. What time is the parameter? Thank you. SimpleCanalConnector.idleTimeout
966,Source code questions about MemoryEventStoreWithBuffer checkFreeSlotAt implementation The put operation of MemoryEventStoreWithBuffer will call checkFreeSlotAt to check if there is any space source as follows `private boolean checkFreeSlotAt(final long sequence) { final long wrapPoint = sequence - bufferSize; final long minPoint = getMinimumGetOrAck(); if (wrapPoint > minPoint) { // Just catching up with the round return false; } else { //... } }` I want to know why the getMinimumGetOrAck method is called to get the smaller value of getSequence or ackSequence. ackSequence should not always be less than or equal to getSequence. It is not possible to compare directly with ackSequence. Yes ack < get < Put basically satisfies this relationship
965,Tsdb persistent storage scheme can use mysql to monitor the data and change the data to h2. The version is 1 1 0 H2 configuration in canal properties #table meta tsdb info canal.instance.tsdb.enable=true canal.instance.tsdb.dir=${canal.file.data.dir:../conf}/${canal.instance.destination:} canal.instance.tsdb.url=jdbc:h2:${canal.instance.tsdb.dir}/h2;CACHE_SIZE=1000;MODE=MYSQL; canal.instance.tsdb.dbUsername=canal canal.instance.tsdb.dbPassword=canal instance.properties H2 configuration #table meta tsdb info Recording table structure by time series canal.instance.tsdb.enable=true Save the table structure data in h2 Embedded database canal.instance.tsdb.url=jdbc:h2:${canal.instance.tsdb.dir}/h2;CACHE_SIZE=1000;MODE=MYSQL; canal.instance.tsdb.dbUsername=canal canal.instance.tsdb.dbPassword=canal Open h2 Database file found meta_history with meta_snapshot No data in the middle ![h2](https://user-images.githubusercontent.com/10652165/45876725-0cc5c380-bdce-11e8-8d64-dd5814762051.png) The problem is too general attention to the server exception log
964,The field in the table creation statement of the meta_history table of tsdb is different from the field of the query statement in sqlmap_history, which causes an error when changing the table structure. The version used is 1 1 0 conf\spring\tsdb\sql\create_table.sql The table construction statement for creating the meta_history table is as follows ``` CREATE TABLE IF NOT EXISTS `meta_history` ( `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT COMMENT Primary key `gmt_create` datetime NOT NULL COMMENT Creation time `gmt_modified` datetime NOT NULL COMMENT Change the time `destination` varchar(128) DEFAULT NULL COMMENT Channel name `binlog_file` varchar(64) DEFAULT NULL COMMENT Binlog file name `binlog_offest` bigint(20) DEFAULT NULL COMMENT Binlog offset `binlog_master_id` varchar(64) DEFAULT NULL COMMENT Binlog node id `binlog_timestamp` bigint(20) DEFAULT NULL COMMENT Timestamp of the binlog application `use_schema` varchar(1024) DEFAULT NULL COMMENT The corresponding schema when executing sql `schema` varchar(1024) DEFAULT NULL COMMENT Corresponding schema `table` varchar(1024) DEFAULT NULL COMMENT Corresponding table `sql` longtext DEFAULT NULL COMMENT Executing sql `type` varchar(256) DEFAULT NULL COMMENT Sql type `extra` text DEFAULT NULL COMMENT Additional extended information PRIMARY KEY (`id`) UNIQUE KEY binlog_file_offest(`destination` `binlog_master_id` `binlog_file` `binlog_offest`) KEY `destination` (`destination`) KEY `destination_timestamp` (`destination` `binlog_timestamp`) KEY `gmt_modified` (`gmt_modified`) ) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8 COMMENT table structure change schedule ``` conf\spring\tsdb\sql-map\sqlmap_history.xml The fields in the query are as follows ```xml <sql id="allVOColumns"> <![CDATA[ a.id as id a.gmt_create as gmtCreate a.gmt_modified as gmtModified a.destination as destination a.binlog_file as binlogFile a.binlog_offest as binlogOffest a.binlog_master_id as binlogMasterId a.binlog_timestamp as binlogTimestamp a.use_schema as useSchema a.sql_schema as sqlSchema a.sql_table as sqlTable a.sql_text as sqlText a.sql_type as sqlType a.extra as extra ]]> </sql> ``` Inconsistent field shema -> sql_shema table ->sql_table sql ->sql_text type ->sql_type The latest backbone has been fixed
963,Canal ClientIdentity Destination with clientId What is the meaning? Please help explain Read more wiki
962,canal How does the client customize the data type of the field content obtained by the client? canal The content of the field obtained by the client is defaulted to the string type. Please advise me how to customize the data type of the field content obtained by the client. See more wikis to convert strings to specific types based on the type of business.
961,canal The precision of the float type field obtained by the client is inconsistent with that in mysql. phenomenon 1. Field type of source table in mysql float(36 6) 2. The value of this field queried in the shell 345.678894 Query after insert 3. The value obtained in the canal after the insert operation triggers the canal 345 6789 Get method client side column getValue problem canal The precision of the float type field obtained by the client is inconsistent with that in mysql. I have tried to rewrite Com alibaba otter canal common utils CanalToStringStyle appendDetail method is as follows <img width="1230" alt="mysql float" src="https://user-images.githubusercontent.com/28953872/45869265-98355980-bdba-11e8-900c-fd9bebe6fcf0.png"> But the result of the actual method rewriting does not work. How can God correct this problem? good luck Tell me about the way to reproduce the mysql version of the canal version of the table statement test SQL and so on The reason why the fix has not taken effect before it has taken effect is that I am not in intellij Rewrite the modified source code in the idea and replace the installed canal The repair method of the corresponding jar under the lib in the server is inconsistent. The parseOneRow method of the logeventconvert under the parser module case Types REAL converts float to double and then processes It is your own change of code that leads to phenomenon Field type of source table in mysql float(36 6) The value of this field queried in the shell 345.678894 Query after insert The value obtained in the canal after the insert operation triggers the canal 345 6789 Get method client side column getValue problem canal The precision of the float type field obtained by the client is inconsistent with that in mysql. This inconsistency in the accuracy problem is not due to changes to the source code. ******************************************** Mysql version Server version: 5.7.22-log MySQL Community Server (GPL) Canal version 1 0 24 Table statement CREATE TABLE `all_types` ( `varchar1` varchar(20) DEFAULT NULL `inta` int(10) unsigned NOT NULL AUTO_INCREMENT `tinyint1` tinyint(4) DEFAULT NULL `smallint1` smallint(6) DEFAULT NULL `bigint1` bigint(20) DEFAULT NULL `decimal1` decimal(65 30) DEFAULT NULL `double1` double DEFAULT NULL `date1` date DEFAULT NULL `time1` time DEFAULT NULL `year1` year(4) DEFAULT NULL `datetime1` datetime DEFAULT NULL `timestamp1` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP `float1` float(36 6) DEFAULT NULL PRIMARY KEY (`inta`) ) ENGINE=InnoDB AUTO_INCREMENT=111118 DEFAULT CHARSET=latin1 Test SQL insert into all_types values('a' 111112 11 111 1111111 -1235666.6666688999870 123.1345775019860 sysdate() sysdate() '1999' sysdate() sysdate() 345.67890110); After testing, it may be because it was done in the parseOneRow method of logeventconvert case Types.REAL: // The object is number type direct valueof columnBuilder.setValue(String.valueOf(value)); break; String valueOf value will cause loss of precision if value is float This is not necessarily a good change. 1. Mysql comes with the mysqlbinlog tool parsing 345 67890110 directly into 345 679 canal parsing to 345 6789 if the strong to double is 345 67889404296875 2. Inserted by yourself is 345 67890110 The actual mysql deposit is 345 678894 3. Mysql for the description of the float on the storage itself has inaccurate behavior https dev mysql com doc refman 5 7 en problems with float html consider the accuracy error of 0 0001 I don’t know if it’s handled correctly. Float first double mysql stored as 345 678894 double is 345 67889404296875 data type is float 36 6 Therefore scale is 6 double decimal point scale 1 bit 345 6788940 double the decimal point scale bit When the decimal point scales 1 digit 5 Then the decimal place of the scale 1 is not changed, so the 345 6788940 takes 6 is 345 678894 After several tests, it is found that the processed results are consistent with the results stored in mysql. There is no reliable case to rely on. mysql&#39;s own binlog parsing code is also directly float object processing. Insert statement directly into the 345 6789 mysql select results is also 345 678894 Yeah, this is really bad. The type of float itself determines Float is not recommended in mysql
960,Is there any tool to put Message Change to SQL statement I need to put Message Convert to a statement executable from the library Canal Whether the relevant interface is provided Look at the example in the client
959,Kafka message order problem Kafka can only guarantee partition Order of messages How to deal with a certain record of a table needs to guarantee a strict order canalDestinations Not for kafka Key what special treatment Default all send play partition 0 Configured pk The hash will be sent to the corresponding partition #958
958,Kafka production side increased by pk Hash to the corresponding partition function Kafka production side increased by pk Hash to the corresponding partition function does not set the partition hash or the default state is all sent to the partition 0 The server side kafka adds the following configuration ```` canalDestinations: - canalDestination: example topic: expample Corresponding number of topic partitions partitionsNum: 3 partitionHash: Library name table name Unique primary key Specify a unique primary key in the library table to use its value as a hash distribution partition mytest.person: id ```` This feature is only available in flatMessage mode. It is recommended to optimize the hash field definition of the table to facilitate configuration problems in a large number of tables. 1. Add a wildcard match table to support default keywords like pk 2. Missing wildcards schema.table : columngA Match Can&#39;t wait
957,Some related issues of canal kafka Currently using canal kafka to achieve a set of data increments to elasticsearch projects have some questions 1. When the client hangs up the incremental data during this time, after the client restarts, the incremental data will not notify the client that there will be data loss problem? 2. Is there any monitoring solution for cannal similar to jmxtrans InfluxDb Grafana? Monitoring for kafka Look at the documentation. The answers you want are all monitored. There is direct docking for prometheus.
956,Is there a roadmap for canal in the coming year? Can pay attention to issue and wiki
955,canal V1 1 0 will open GTID will appear CanalParseException: Java util ConcurrentModificationException Configure canal instance gtidon true as follows # enable gtid use true/false canal.instance.gtidon=true Caused by: java.util.ConcurrentModificationException at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:901) at java.util.ArrayList$Itr.next(ArrayList.java:851) at com.alibaba.otter.canal.parse.driver.mysql.packets.UUIDSet.toString(UUIDSet.java:125) at com.alibaba.otter.canal.parse.driver.mysql.packets.MysqlGTIDSet.toString(MysqlGTIDSet.java:111) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.createHeader(LogEventConvert.java:849) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.createHeader(LogEventConvert.java:823) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.buildQueryEntry(LogEventConvert.java:807) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsQueryEvent(LogEventConvert.java:394) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:137) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$DmlParserStage.onEvent(MysqlMultiStageCoprocessor.java:298) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$DmlParserStage.onEvent(MysqlMultiStageCoprocessor.java:288) at com.lmax.disruptor.WorkProcessor.run(WorkProcessor.java:143) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) Use the latest 1 1 1 alpha version
954,Canal1 1 1 encountered several problems on RDS Using RDS Mysql5 6 high-availability version 2 RDS have applied for a high-right account to configure an error according to the regular instance properties 2018-09-19 11:30:38.912 [destination = xxx address = xxx/xxx:3306 EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump addressxxx/xxx:3306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: ErrorPacket [errorNumber=1142 fieldCount=-1 message=SHOW VIEW command denied to user 'canal'@'xxx' for table 'db_view' sqlState=42000 sqlStateMarker=#] It seems that the high-rights account does not have the permissions of the db_view table of the Mysql library. Also does not have this permission but starts normally Consulted the next Ali cloud engineer said that the problem of canal instance filter regex needs to eliminate the mysql library listener and then start normal but the client does not receive data The second problem is to delete the node on the server side but every time you start the error, you can find destination:{} Tracking debugging with source code found that the node that was originally deleted is too strange. I deleted the entire deployer package and redeployed it or started the deleted node. I am using a stand-alone deployment without using zk. Honestly, I don’t understand your question too much. > Honestly, I don’t understand your question too much. The background is such that deploying a single machine on the ECS The server side then connected 4 RDS The first problem is that we can not access some of the table high-privileged accounts below mysql is not OK then this seems to only remove the mysql by configuring the filter The monitoring of the schema at that time does not receive the binlog data should be the problem of the filter regular configuration error. The second problem is in the deploy The server&#39;s conf folder deleted an RDS configuration but after rebooting canal The server is still loading this deleted configuration is very strange. I will delete the deploy project and redeploy it or will load it. This is what I saw and then only restart ECS. It will be fine after restarting. So I suspect that it is cached under conf. Node The cache should be no one in the conf directory > The cache should be no one in the conf directory In addition, if the log data of the server mode in stand-alone mode will not burst if it is not consumed, these days, ECS is inexplicably full of memory optimization. Is there any good suggestion? Or change zk See more wikis will not explode memory
953,Word spelling correction MemoryEventStoreWithBuffer Inside INIT_SQEUENCE Change to INIT_SEQUENCE [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=953) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=953) before we can accept your contribution.<br/><hr/>**zhikuodai** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=953) it.</sub> tks
952,canal 1.1.0 Do not print data rowdata information ![default](https://user-images.githubusercontent.com/16894071/45675324-b3595c80-bb61-11e8-9d31-103f0386ba22.png) Class information reference https://github.com/alibaba/canal/blob/master/example/src/main/java/com/alibaba/otter/canal/example/AbstractCanalClientTest.java written ` protected void printEntry(List<Entry> entrys) { for (Entry entry : entrys) { long executeTime = entry.getHeader().getExecuteTime(); long delayTime = System.currentTimeMillis() - executeTime; Date date = new Date(entry.getHeader().getExecuteTime()); SimpleDateFormat simpleDateFormat = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss"); if (entry.getEntryType() == EntryType.TRANSACTIONBEGIN || entry.getEntryType() == EntryType.TRANSACTIONEND) { if (entry.getEntryType() == EntryType.TRANSACTIONBEGIN) { TransactionBegin begin = null; try { begin = TransactionBegin.parseFrom(entry.getStoreValue()); } catch (InvalidProtocolBufferException e) { throw new RuntimeException("parse event has an error data:" + entry.toString() e); } // Print transaction header information execution thread id transaction time consuming logger.info(transaction_format new Object[] { entry.getHeader().getLogfileName() String.valueOf(entry.getHeader().getLogfileOffset()) String.valueOf(entry.getHeader().getExecuteTime()) simpleDateFormat.format(date) entry.getHeader().getGtid() String.valueOf(delayTime) }); logger.info(" BEGIN ----> Thread id: {}" begin.getThreadId()); printXAInfo(begin.getPropsList()); } else if (entry.getEntryType() == EntryType.TRANSACTIONEND) { TransactionEnd end = null; try { end = TransactionEnd.parseFrom(entry.getStoreValue()); } catch (InvalidProtocolBufferException e) { throw new RuntimeException("parse event has an error data:" + entry.toString() e); } // Print transaction commit information transaction id logger.info("----------------\n"); logger.info(" END ----> transaction id: {}" end.getTransactionId()); printXAInfo(end.getPropsList()); logger.info(transaction_format new Object[] { entry.getHeader().getLogfileName() String.valueOf(entry.getHeader().getLogfileOffset()) String.valueOf(entry.getHeader().getExecuteTime()) simpleDateFormat.format(date) entry.getHeader().getGtid() String.valueOf(delayTime) }); } continue; } if (entry.getEntryType() == EntryType.ROWDATA) { RowChange rowChage = null; try { rowChage = RowChange.parseFrom(entry.getStoreValue()); } catch (Exception e) { throw new RuntimeException("parse event has an error data:" + entry.toString() e); } EventType eventType = rowChage.getEventType(); String tableName = entry.getHeader().getTableName(); for (CanalEntry.RowData rowData : rowChage.getRowDatasList()) { Map map = new HashMap<>(); map.put("tableName" tableName); map.put("eventType" eventType); map.put("eventDate" org.apache.http.client.utils.DateUtils.formatDate(new Date() "yyyy-MM-dd")); if (eventType == CanalEntry.EventType.DELETE) { printColumn(map rowData.getBeforeColumnsList()); Send delete command } else if (eventType == CanalEntry.EventType.INSERT) { Send insert command printColumn(map rowData.getAfterColumnsList()); } else { Send modification command printColumn(map rowData.getAfterColumnsList()); } } if (eventType == EventType.QUERY || rowChage.getIsDdl()) { logger.info(" sql ----> " + rowChage.getSql() + SEP); continue; } // printXAInfo(rowChage.getPropsList()); } } } ` Check filter conditions
951,When there is no data on the source side of the V1 1 0 version, the socket2 5s of the dump log causes the timeout to throw an exception and causes reconnection. Start Canal If there is continuous data, no error, when there is no data, 2 5s, SocketTimeoutException will cause reconnection. The error is located in BioSocketChannel read BioSocketChannel java 123 Look at the SO_TIMEOUT 1000 in the code BioSocketChannel or 1s. If the read is less than the data 1s, it will throw a SocketTimeoutException. The 2 5s added to the input will throw an exception up and cause the reconnection. The first accumulated timeout is 25s Secondly, the canal opens the master. Heartbeat will get a heartbeat packet reset timeout every 15s when there is no data by default. You can start checking the system from here. @lcybo I described the wrong is 25s is not 2 5s The master is defined in the file DirectLogFetcher hearbeat 15 然后READ_TIMEOUT_MILLISECONDS MASTER_HEARTBEAT_PERIOD_SECONDS + 10) * 1000 is the timeout 25s when read The current phenomenon is that the timeout time of the read 25 method in BioSocketChannel is 25s. If there is no incremental data more than 25s, the SocketTimeoutException is thrown. as you said If the master is not turned on Heartbeat is not 25s will reconnect the sockettimeout exception Turn on the master Heartbeat is the canal instance detection enable in the configuration file. =true 和canal instance detecting heartbeatHaEnable = True, I have set it up now, but it’s not working. canal instance detecting enable和binlog master Heartbeat is not a concept Canal instance detecting enable is fork out another connection by canal Server-initiated heartbeat binlog master Heartbeat is MySQL Part of the dump protocol by MySQL Master initiated directly in dump Transmission canal in connection server No need to configure direct opening in order to solve the semi-join problem https://dev.mysql.com/doc/refman/8.0/en/change-master-to.html Can see MASTER_HEARTBEAT_PERIOD This section checks whether the master side takes effect. @lcybo Thanks to me for introducing this MASTER_HEARTBEAT_PERIOD Is it set on the slave side? canal Server as a slave directly open already should not need to set up on the Canal side You asked me to check if the master is effective. What is meant by the master? Execute CHANGE on the master side MASTER TO MASTER_HEARTBEAT_PERIOD 20, my command execution is successful but still reconnected in 25s My Canal The server is connected to the master Or canal In order to solve the semi-connection problem, it is normal for the 25s to automatically reconnect once. 2018-09-18 17:40:44.165 [destination = example address = /127.0.0.1:3306 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - Socket timeout expired closing connection java.net.SocketTimeoutException: Timeout occurred failed to read 4 bytes in 25000 milliseconds. at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.read(BioSocketChannel.java:123) ~[classes/:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:174) ~[classes/:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:77) ~[classes/:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:213) [classes/:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:248) [classes/:na] at java.lang.Thread.run(Unknown Source) [na:1.7.0_45] @lcybo Thanks, I tried it again. The master connected before is mysql5 1 It does not support MASTER_HEARTBEAT_PERIOD Now it is changed to mysql5 7. So that MASTER_HEARTBEAT_PERIOD does not take effect in 25s. If the connection below 5 6 does not support MASTER_HEARTBEAT_PERIOD, it will be reconnected. Right @theonesmx Well, this feature is 5 5 joined
950,GTID mode synchronously reports gtid_purged error 1 1 0 version enables gtid mode source database gtid_purged has value after startup error can not be synchronized ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = The slave is connecting using CHANGE MASTER TO MASTER_AUTO_POSITION = 1 but the master has purged binary logs containing GTIDs that the slave requires. at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:102) see 902 If you are in a hurry, you can compile and package the main code first and then try it out. Google errmsg
949,Fix simpleCanalConnector bug as title tks
948,Serialization problem Deserialization error after updating version ERROR com.alibaba.otter.canal.client.kafka.MessageDeserializer - Error when deserializing byte[] to message
947,Fix group mode error repair https://github.com/alibaba/canal/issues/943 1. counter Unify to the abstractMySQLEventParser layer 2. Separate statistics for each parser in gourp tks
945,Received a DML statement for a non-subscribed table Mysql 5.7 binlog-rows-query-log-events ON Canal 1.1.0 canal.instance.filter.query.dml false DML statements that receive non-subscription tables are not normal and will not receive non-subscription table data. Setting canal.instance.filter.query.dml = True, currently not going to 5 7 DML The query statement is filtered because the corresponding dbname cannot be obtained. @agapple Ok originally wanted to record the SQL statement can not be filtered, then the binlog rows query log events also turned off tks
944,Is there a problem with the ack of CanalAdapterKafkaWorker? Com alibaba otter canal client adapter loader CanalAdapterKafkaWorker class The following code means that only the poll to null or execution time is more than one minute to execute the ack operation. The normal consumption data will never execute the connector ack. Is there a problem? Why do you want to? Write like this while (running) { try { // switcher.get(); Waiting switch on final Message message = connector.getWithoutAck(100L TimeUnit.MILLISECONDS timeFlag startTime); timeFlag = false; executing.set(true); if (message != null) { ....... // Ack once every interval Prevent switching to another client due to timeout not responding long currentTS = System.currentTimeMillis(); while (executing.get()) { // Less than 1 minute has not consumed ack once keep alive if (System.currentTimeMillis() - currentTS > 60000) { connector.ack(); currentTS = System.currentTimeMillis(); } } } else { connector.ack(); } } catch (CommitFailedException e) { logger.warn(e.getMessage()); } catch (Exception e) { logger.error(e.getMessage() e); TimeUnit.SECONDS.sleep(1L); } } There are indeed bugs in the latest pr will fix the interval here for a while ack is to prevent consumption timeout kafka Consumer will switch to another one @rewerma Okay thank you
943,V1 1 0 version goup mode problem At startup An instance of goup will report an error Other examples will not but consume normally Error is as follows com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.lang.String)]: [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.nio.charset.Charset)] 2018-09-12 14:51:57.847 [main] WARN com.alibaba.otter.canal.prometheus.PrometheusService - Unable to register instance exports for coupon. java.lang.IllegalArgumentException: CanalEventParser must be MysqlEventParser at com.alibaba.otter.canal.prometheus.impl.ParserCollector.register(ParserCollector.java:86) ~[canal.prometheus-1.1.0.jar:na] at com.alibaba.otter.canal.prometheus.CanalInstanceExports.register(CanalInstanceExports.java:65) ~[canal.prometheus-1.1.0.jar:na] at com.alibaba.otter.canal.prometheus.PrometheusService.register(PrometheusService.java:96) ~[canal.prometheus-1.1.0.jar:na] at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.start(CanalServerWithEmbedded.java:109) [canal.server-1.1.0.jar:na] at com.alibaba.otter.canal.deployer.CanalController$2$1.processActiveEnter(CanalController.java:140) [canal.deployer-1.1.0.jar:na] at com.alibaba.otter.canal.common.zookeeper.running.ServerRunningMonitor.processActiveEnter(ServerRunningMonitor.java:244) [canal.common-1.1.0.jar:na] at com.alibaba.otter.canal.common.zookeeper.running.ServerRunningMonitor.initRunning(ServerRunningMonitor.java:149) [canal.common-1.1.0.jar:na] at com.alibaba.otter.canal.common.zookeeper.running.ServerRunningMonitor.start(ServerRunningMonitor.java:103) [canal.common-1.1.0.jar:na] at com.alibaba.otter.canal.deployer.CanalController.start(CanalController.java:438) [canal.deployer-1.1.0.jar:na] at com.alibaba.otter.canal.deployer.CanalLauncher.main(CanalLauncher.java:38) [canal.deployer-1.1.0.jar:na] 2018-09-12 14:51:57.847 [main] INFO c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-coupon 2018-09-12 14:51:57.858 [main] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - start successful.... It seems to be the real use of setConnectionCharset Charset connectionCharset this method But it is set to use setConnectionCharset(String connectionCharset this method but why other modes do not report an error https://github.com/alibaba/canal/pull/947 @shizhengchao If you can, please patch and test it. Thank you very much. > #947 > @shizhengchao If you can, please patch and test it. Thank you very much. @lcybo Well, now I have not reported an error. @shizhengchao thx
942,Add the current gtid related information to the Entry Header 1. Will be the current gtid sequence no And last Committed information is added to Entry trx begin/end rowdata) Header in The client can get the relevant value from the property 2. example Increase the sample of the corresponding gtid related information protected Map<String String> gtidMap = new HashMap<>(); There will be a concurrency when put and get LogDecoder Will be for each event Event instantiation LogHeader Object to logHeader when GTID event arrives The subsequent map put operation, whether serial or parallel parsing, is a get operation on the map in the private header of this event event. My understanding ``` Serial parsing order should be put |-> get-> get -> get Parallel parsing order should be put |->get |-> get |-> get ``` tks
941,Increase flat message for kafka message delivery Adding the FlatMessage class for kafka&#39;s message delivery Message Each Entry will be split into corresponding FlatMessages using json serialization sent to kafka for consumption. Follow-up will be implemented on FlatMessage by pk hash with Partition for splitting needs Increase configuration Server side conf/kafka.yml Add new attribute flatMessage: true True means that the delivery of false messages using the FlatMessage message means that the message is delivered using the native message. Client side canal_client/conf/canal-client.yml Add attribute flatMessage: true True means that the delivery of false messages using the FlatMessage message means that the message is delivered using the native message. Note FlatMessage only delivers native message messages that do not affect tcp for kafka message delivery. ![image](https://user-images.githubusercontent.com/33280738/45535386-e08bcf00-b830-11e8-8349-37378d5470bb.png) I feel that this biggest gap should be One is to submit to Kafka synchronously using asynchronous commit. Otherwise, from the perspective of your package, this difference should not be so big. Have you tried to change it to asynchronous test? Change to asynchronous tps and there is no big difference OK
940,1 1 0 production environment 4 destination points information 2 have not written zk2 not updated Seen from zk 01 and 04 destinaton no cursor directory 02 and 03 have the cursor directory but do not update and restart 2 times. I deploy another canal When the server instance is not reported TableIdNotFoundException point information is all normal Canal canal log error message when starting 2018-09-13 23:46:59.156 [destination = ordercanalprd02 address = /***:3321 EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address /***:3321 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.TableIdNotFoundException: not found tableId:110 Caused by: com.alibaba.otter.canal.parse.exception.TableIdNotFoundException: not found tableId:110 2018-09-13 23:46:59.109 [destination = ordercanalprd01 address = /***:3320 EventParser] WARN c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - prepare to find start position ::1536827400000 **2018-09-13 23:46:59.151 [destination = ordercanalprd03 address = /***:3322 EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address /***:3322 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.TableIdNotFoundException: not found tableId:118 Caused by: com.alibaba.otter.canal.parse.exception.TableIdNotFoundException: not found tableId:118** 2018-09-13 23:46:59.156 [destination = ordercanalprd02 address = /***:3321 EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address /***:3321 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.TableIdNotFoundException: not found tableId:110 Caused by: com.alibaba.otter.canal.parse.exception.TableIdNotFoundException: not found tableId:110 2018-09-13 23:46:59.203 [destination = ordercanalprd03 address = /---:3322 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:ordercanalprd03[com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.TableIdNotFoundException: not found tableId:118 Caused by: com.alibaba.otter.canal.parse.exception.TableIdNotFoundException: not found tableId:118 ] 2018-09-13 23:46:59.204 [destination = ordercanalprd02 address = /---:3321 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:ordercanalprd02[com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.TableIdNotFoundException: not found tableId:110 Caused by: com.alibaba.otter.canal.parse.exception.TableIdNotFoundException: not found tableId:110 ] Configuration information # canal.properties binlog filter config canal.instance.filter.druid.ddl = true canal.instance.filter.query.dcl = true canal.instance.filter.query.dml = true canal.instance.filter.query.ddl = true canal.instance.filter.table.error = true canal.instance.filter.rows = false canal.instance.filter.transaction.entry = false table meta tsdb info **canal.instance.tsdb.enable=false** canal.instance.tsdb.dir=${canal.file.data.dir:../conf}/${canal.instance.destination:} canal.instance.tsdb.url=jdbc:h2:${canal.instance.tsdb.dir}/h2;CACHE_SIZE=1000;MODE=MYSQL; canal.instance.tsdb.dbUsername=canal canal.instance.tsdb.dbPassword=canal canal.instance.global.spring.xml = classpath:spring/default-instance.xml # instance.properties canal.instance.gtidon=false position info canal.instance.master.address=***:3322 canal.instance.master.journal.name= canal.instance.master.position= canal.instance.master.timestamp=1536827400000 canal.instance.master.gtid= rds oss binlog canal.instance.rds.accesskey= canal.instance.rds.secretkey= canal.instance.rds.instanceId= table meta tsdb info **canal.instance.tsdb.enable=false** # Kafka yml configuration information servers: *** retries: 0 batchSize: 16384 lingerMs: 1 bufferMemory: 33554432 Lot size unit of canal The amount of k is recommended to be changed to 1M canalBatchSize: 50 filterTransactionEntry: true canalDestinations: - canalDestination: ordercanalprd01 topic: ordercanalprd01 partition: - canalDestination: ordercanalprd02 topic: ordercanalprd01 partition: - canalDestination: ordercanalprd03 topic: ordercanalprd01 partition: - canalDestination: ordercanalprd04 topic: ordercanalprd01 partition: I checked it again. Estimated that this error is happening canal.destinations= This configuration is directly empty. I put this place on it and then slaveid deploys the other 28 destinations by instance. No problem occurs. I guess it should be a problem caused by the sequence of scanning destinations and other code during startup.
939,Canal1 1 0 The amount of data sent to kafka is inconsistent with the amount generated by mysql My kafka yml is configured like this retries: 0 batchSize: 1024 lingerMs: 0 bufferMemory: 33554432 # Lot size unit of canal The amount of k is recommended to be changed to 1M canalBatchSize: 50 filterTransactionEntry: true canalDestinations: - canalDestination: elample topic: elample partition: Mysql uses a stored procedure to insert 10,000 pieces of data each time can only consume more than 6,000 pieces in kafka batchSize can not be changed too much kafka message body default maximum can not exceed 1M data so canalBatchSize must 1M suggest 500K batchSize can be modified by default @rewerma Is this unit not a byte? 1024 is less than 1M in bytes. I changed to the default and the same situation. retries: 0 batchSize: 16384 lingerMs: 1 bufferMemory: 33554432 # Lot size unit of canal k canalBatchSize: 50 filterTransactionEntry: true I have tried these parameters and they are all the same. I can only synchronize more than 6,600 data at a time.
938,Fix upgrade proto2 to proto3 Fix upgrade proto2 to proto3 And that test is already compatible with previous versions of the client.
937,Canal instance filter transaction entry does not work Mysql version 5.6.26 Canalserver version 1.0.24 Question 1 在canal properties Configuration canal.instance.filter.transaction.entry = true after that In canal client You can still see the entryType Have transactionbegin with transactionend How can I handle it can be removed on the server side? transcation Question two Canal instance filter regex is configured in instance properties after that In canal client You can still see the table that is not in focus. transactionbegin with transactionend It’s just less than paying attention to the watch. Query eventType with Dml eventType Can canalcan only do the synchronization of the binlog of my attention table To reduce the load of the canalserver Question 2 is to upgrade from 1 to 1 1 caused by the failure of the problem 1 to try again ps: 1. Open canal instance filter transaction entry After true, it will lead to failure to ack. Once the synchronization occurs, it may lead to the possibility of losing data or even batch data retransmission. 2. canal server Before the performance optimization of 1 1 0 is done, the performance problem of the version should be greatly alleviated or even solved on 1 1 0. Thanks for answering the current application scenario, it is not allowed to leave ack ; Then try 1 1 version
936,canal1.1.0 Concurrent analysis of multi-threaded GTID update operations Cause java util ConcurrentModificatioinException The specific error is as follows 2018-09-12 18:38:25 776||destination = 1002 address = /*********:3306 EventParser|?|ERROR|c.a.o.c.p.i.mysql.MysqlEventParser - dump address ************:3306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: java.util.ConcurrentModificationException: null at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:901) ~[na:1.8.0_121] at java.util.ArrayList$Itr.next(ArrayList.java:851) ~[na:1.8.0_121] at com.alibaba.otter.canal.parse.driver.mysql.packets.UUIDSet.toString(UUIDSet.java:125) ~[canal.parse.driver-1.1.0-20180912.102450-4.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.packets.MysqlGTIDSet.toString(MysqlGTIDSet.java:111) ~[canal.parse.driver-1.1.0-20180912.102450-4.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.createHeader(LogEventConvert.java:849) ~[canal.parse-1.1.0-20180912.102500-4.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:561) ~[canal.parse-1.1.0-20180912.102500-4.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$DmlParserStage.onEvent(MysqlMultiStageCoprocessor.java:302) ~[canal.parse-1.1.0-20180912.102500-4.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$DmlParserStage.onEvent(MysqlMultiStageCoprocessor.java:288) ~[canal.parse-1.1.0-20180912.102500-4.jar:na] at com.lmax.disruptor.WorkProcessor.run(WorkProcessor.java:143) ~[disruptor-3.4.2.jar:na] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_121] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_121] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_121] 2018-09-12 18:38:25 776||destination = 1002 address = /172.16.2.71:3306 EventParser|?|ERROR|c.a.o.c.common.alarm.LogAlarmHandler - destination:1002[com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: java.util.ConcurrentModificationException at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:901) at java.util.ArrayList$Itr.next(ArrayList.java:851) at com.alibaba.otter.canal.parse.driver.mysql.packets.UUIDSet.toString(UUIDSet.java:125) at com.alibaba.otter.canal.parse.driver.mysql.packets.MysqlGTIDSet.toString(MysqlGTIDSet.java:111) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.createHeader(LogEventConvert.java:849) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:561) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$DmlParserStage.onEvent(MysqlMultiStageCoprocessor.java:302) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$DmlParserStage.onEvent(MysqlMultiStageCoprocessor.java:288) at com.lmax.disruptor.WorkProcessor.run(WorkProcessor.java:143) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) ]st.java:901) at java.util.ArrayList$Itr.next(ArrayList.java:851) at com.alibaba.otter.canal.parse.driver.mysql.packets.UUIDSet.toString(UUIDSet.java:125) at com.alibaba.otter.canal.parse.driver.mysql.packets.MysqlGTIDSet.toString(MysqlGTIDSet.java:111) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.createHeader(LogEventConvert.java:849) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:561) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$DmlParserStage.onEvent(MysqlMu JDK: 1.8.0_21 Configuration information preallell=true parallelthreadSize 4 calculation result Analysis reasons are not necessarily correct 180 lines in LogEventConvert Updated gtid_set In fact, it is updated 34 lines of MysqlGTIDSet sets.get(sid).intervals.addAll(us.intervals) This place updated the modCount of the list In the toString method of UUIDSet Traversing the interval The result triggers the checkForComodification method of 901 rows of ArrayList. Since addAll adds modCount But execptedModCount has not increased Run out of the exception Solution Method 1 preallell false Method 2 in the case of multithreading Is it necessary to lock? https://github.com/alibaba/canal/pull/902 You can try the alpha version of 1 1 1 first. I encountered the same problem analysis reason and mycat lulin is consistent. I use the method of preallell false to test the subsequent consideration of locking.
935,How to specify the data of a table to a partition instead of being scattered into multiple partitions Or how to confirm When Kafka creates a topic, the partition only gives one not to ok. #958
934,How to set a table to a topic specified partition This will be the name of the table as the key of the message. #958
933,canal Modify the instance configuration file to restart the instance. Will this information be synchronized to zk? The version is 1 0 24 will not
932,Can I get a single piece of data? Otherwise, sending to kafka can only be a single partition. I would like to ask 1 10 to send to kafka now can only be a single partition to send multiple partition order can not guarantee that we want to be able to send to different partitions and hash partition according to the key. My approach is to take the entries out and then take the rowchange and traverse the last to get a Rowdata uses the name of the library plus the value of the primary key to spell out the key. By sending this data according to the key, you can ensure that a piece of data will always go to a partition. However, it is very inefficient. Will there be a way to get a single piece of data directly? There is The modified code is as follows public void send(KafkaProperties.Topic topic Message message) throws IOException { Get entries List<ByteString> rawEntries = message.getRawEntries(); for (ByteString rawEntry : rawEntries) { message.addEntry(CanalEntry.Entry.parseFrom(rawEntry)); } Traversing entries for (CanalEntry.Entry entry : message.getEntries()) { String tableName = entry.getHeader().getTableName(); CanalEntry.RowChange rowChage = CanalEntry.RowChange.parseFrom(entry.getStoreValue()); CanalEntry.EventType eventType = rowChage.getEventType(); // Traversing rowchange for (CanalEntry.RowData rowData : rowChage.getRowDatasList()) { String key = ""; List<CanalEntry.Column> list = null; if (eventType == CanalEntry.EventType.DELETE) { list = rowData.getBeforeColumnsList(); } else { list = rowData.getAfterColumnsList(); } Repackage CanalEntry.RowData.Builder rowDataBuilder = CanalEntry.RowData.newBuilder(); for (CanalEntry.Column column : list) { if (column.getIsKey()) { key = tableName + "_" + column.getValue(); } if (eventType == CanalEntry.EventType.DELETE) { rowDataBuilder.addBeforeColumns(column); } else { rowDataBuilder.addAfterColumns(column); } } CanalEntry.RowChange.Builder rowChangeBuilder = CanalEntry.RowChange.newBuilder(); rowChangeBuilder.addRowDatas(rowDataBuilder); CanalEntry.RowChange rowChangeNew = rowChangeBuilder.build(); CanalEntry.Entry.Builder entryBuilder = CanalEntry.Entry.newBuilder(); entryBuilder.setHeader(entry.getHeader()) .setEntryType(entry.getEntryType()) .setStoreValue(rowChangeNew.toByteString()); List<CanalEntry.Entry> listEntry = new ArrayList<>(); listEntry.add(entryBuilder.build()); Message messageNew = new Message(message.getId()); messageNew.setEntries(listEntry); Send hash partition according to rowkey ProducerRecord<String Message> record; record = new ProducerRecord<String Message>(topic.getTopic() key messageNew); producer.send(record); if (logger.isDebugEnabled()) { logger.debug("send message to kafka topic: {} \n {}" topic.getTopic() message.toString()); } } } } For data hashes to different partitions, if you need to do the calculation before building protobuf, you will have the problem of decompression deserialization. Thanks for replying, but looking at the code can only get data by batch. It is not deserialized and can&#39;t be directly split into a single bar. In fact, you can handle canal like this. The data written to kafka as a paste source exists in Kafka and can be used as a sink for pre-processing such as distribution and merge. Because there must be a processor to parse the packet anyway, this part of the source data is always In the form of Message packet, it exists in kafka. No matter how the subsequent data needs to process the data of the paste source, it will serve as the basic data store and serve the different data stream. There are also several scenarios that need to be considered and addressed. 1. Multiple key combination keys 2. Isolation of package data after idempotent processing @wingerx Thanks for providing a layering solution to better decoupling is a good method, but the source layer can only be a single partition pre-processing distribution part can only be single-threaded overall performance is the same as it is now, in fact, the amount is not up to now To the extent that it is not enough, just want to see if there is a better way, so let&#39;s talk to you. @undeadwing I tried to open the concurrent test. I don’t know how much delay is the tps of your current database. @wingerx I originally used the group mode to pick up 5 mysql. It is very difficult to find out that each mysql is connected to a default mode instance. After filtering a single instance, the tps is normally tens to more than 100. There is no delay. Sometimes the whole table will be updated. The field will be stuck for a long time. The tps is also very high. Before using kafka, the data is wrong. The server needs to be rolled back. Then the client can only chase the number of single-threaded chasing. So now I want to use kafka multi-partition after the problem or the whole table update. Multi-threaded consumption is fast Client if the consumption logic assembly data is very slow, the consumption performance is too low, the client can completely record the offset to zk after the multi-threaded consumption transaction is submitted. If kill 9 restarts the consumer consumption output from the offset next time, it is necessary to ensure that the idempotent actually consumes the thread pool. In practice, all are blocked by the hunger state. There is no task waiting for the line to start. You can ignore this scenario. If you need to ensure the order problem, you can initialize the N single-threaded threads after the thread is bound by the client. I am now dealing with 8 pools according to this idea. canal Server backtracking history binlog 25 minutes pull 1100W message consumption 8 nodes each node 50 thread pool takes 40 minutes to consume if it is single-threaded consumption encounters a lot of updates is dumbfounded If you can enter the kafka at the source, you can do the data distribution client without considering the order of concurrency. @yin007008 Thanks for providing ideas before I have thought about thread pool binding. My current practice is to split the data into a single message according to the key sent to kafka multi-partition when sending kafka. The advantage is that the order can be guaranteed when the data is backed up. The end does not need to operate the client side to set the kafka consumption start time. The third is that the consumer can be single instance multi-threaded or multi-instance multi-threaded consumption performance and availability on different machines. The shortcoming is the data splitting and recombination part. Need to deserialize and resequence may become a performance bottleneck. Currently access is normal. The maximum number of concurrent tests is not tested. The biggest problem with multiple partition partitions is the change of pk uk If the business scenario can be circumvented, it is more perfect. ps. Someone has submitted a multi-partition to write to kafka&#39;s next version of the program will be brought #958
931,fix #904: Sleep when empty 1s prevents looping get data too fast 1000ms is a little longer If controlling binlog low latency Can consider 10 100ms tks
930,Canal service exception [New I/O server worker #1-1] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x6b1bba32 /192.10.0.46:57388 => /192.10.16.48:11111] exception=java.io.IOException: Connection reset by peer Consider upgrading the canal version It is estimated that the idle timed out.
929,Message Always return RawEntries How to analyze How can I ask `RawEntries` ByteString in Switch to a normal business object Make it clear that it is first parsed out `Entry` Parse out `RowChange`。 Entry changeEntry = Entry.parseFrom(rawEntry); RowChange.parseFrom(changeEntry.getStoreValue());
928,Consider add filed executeTime at canal-adapter-common class Dml? Consider add filed executeTime? That's sql exactly execute time more useful sometimes. solved on v1.1.1
927,fix #912 Solve guava conflicts Package the guava modification package path into the canal client via the maven plugin tks
926,In version 1 1 0, tsdb error message is enabled by default as follows 2018-09-11 11:10:08.724 [destination = example address = /127.0.0.1:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: apply failed Caused by: org.springframework.jdbc.BadSqlGrammarException: SqlMapClient operation; bad SQL grammar []; nested exception is com.ibatis.common.jdbc.exception.NestedSQLException: --- The error occurred in spring/tsdb/sql-map/sqlmap_history.xml. --- The error occurred while executing query. --- Check the select a.id as id a.gmt_create as gmtCreate a.gmt_modified as gmtModified a.destination as destination a.binlog_file as binlogFile a.binlog_offest as binlogOffest a.binlog_master_id as binlogMasterId a.binlog_timestamp as binlogTimestamp a.use_schema as useSchema a.sql_schema as sqlSchema a.sql_table as sqlTable a.sql_text as sqlText a.sql_type as sqlType a.extra as extra from meta_history a where destination = ? and binlog_timestamp >= ? and binlog_timestamp <= ? order by binlog_timestamp asc id asc . --- Check the SQL Statement (preparation failed). --- Cause: org.h2.jdbc.JdbcSQLException: Table "META_HISTORY" not found; SQL statement: select a.id as id a.gmt_create as gmtCreate a.gmt_modified as gmtModified a.destination as destination a.binlog_file as binlogFile a.binlog_offest as binlogOffest a.binlog_master_id as binlogMasterId a.binlog_timestamp as binlogTimestamp a.use_schema as useSchema a.sql_schema as sqlSchema a.sql_table as sqlTable a.sql_text as sqlText a.sql_type as sqlType a.extra as extra from meta_history a where destination = ? and binlog_timestamp >= ? and binlog_timestamp <= ? order by binlog_timestamp asc id asc [42102-196] at org.springframework.jdbc.support.SQLErrorCodeSQLExceptionTranslator.doTranslate(SQLErrorCodeSQLExceptionTranslator.java:237) at org.springframework.jdbc.support.AbstractFallbackSQLExceptionTranslator.translate(AbstractFallbackSQLExceptionTranslator.java:72) at org.springframework.orm.ibatis.SqlMapClientTemplate.execute(SqlMapClientTemplate.java:206) at org.springframework.orm.ibatis.SqlMapClientTemplate.queryForList(SqlMapClientTemplate.java:296) at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.dao.MetaHistoryDAO.findByTimestamp(MetaHistoryDAO.java:28) at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.applyHistoryOnMemory(DatabaseTableMeta.java:367) at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:123) at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:91) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:188) at java.lang.Thread.run(Thread.java:748) Caused by: com.ibatis.common.jdbc.exception.NestedSQLException: --- The error occurred in spring/tsdb/sql-map/sqlmap_history.xml. --- The error occurred while executing query. --- Check the select a.id as id a.gmt_create as gmtCreate a.gmt_modified as gmtModified a.destination as destination a.binlog_file as binlogFile a.binlog_offest as binlogOffest a.binlog_master_id as binlogMasterId a.binlog_timestamp as binlogTimestamp a.use_schema as useSchema a.sql_schema as sqlSchema a.sql_table as sqlTable a.sql_text as sqlText a.sql_type as sqlType a.extra as extra from meta_history a where destination = ? and binlog_timestamp >= ? and binlog_timestamp <= ? order by binlog_timestamp asc id asc . --- Check the SQL Statement (preparation failed). --- Cause: org.h2.jdbc.JdbcSQLException: Table "META_HISTORY" not found; SQL statement: select a.id as id a.gmt_create as gmtCreate a.gmt_modified as gmtModified a.destination as destination a.binlog_file as binlogFile a.binlog_offest as binlogOffest a.binlog_master_id as binlogMasterId a.binlog_timestamp as binlogTimestamp a.use_schema as useSchema a.sql_schema as sqlSchema a.sql_table as sqlTable a.sql_text as sqlText a.sql_type as sqlType a.extra as extra from meta_history a where destination = ? and binlog_timestamp >= ? and binlog_timestamp <= ? order by binlog_timestamp asc id asc [42102-196] at com.ibatis.sqlmap.engine.mapping.statement.MappedStatement.executeQueryWithCallback(MappedStatement.java:201) at com.ibatis.sqlmap.engine.mapping.statement.MappedStatement.executeQueryForList(MappedStatement.java:139) at com.ibatis.sqlmap.engine.impl.SqlMapExecutorDelegate.queryForList(SqlMapExecutorDelegate.java:567) at com.ibatis.sqlmap.engine.impl.SqlMapExecutorDelegate.queryForList(SqlMapExecutorDelegate.java:541) at com.ibatis.sqlmap.engine.impl.SqlMapSessionImpl.queryForList(SqlMapSessionImpl.java:118) at org.springframework.orm.ibatis.SqlMapClientTemplate$3.doInSqlMapClient(SqlMapClientTemplate.java:298) at org.springframework.orm.ibatis.SqlMapClientTemplate$3.doInSqlMapClient(SqlMapClientTemplate.java:296) at org.springframework.orm.ibatis.SqlMapClientTemplate.execute(SqlMapClientTemplate.java:203) ... 7 more Caused by: org.h2.jdbc.JdbcSQLException: Table "META_HISTORY" not found; SQL statement: select a.id as id a.gmt_create as gmtCreate a.gmt_modified as gmtModified a.destination as destination a.binlog_file as binlogFile a.binlog_offest as binlogOffest a.binlog_master_id as binlogMasterId a.binlog_timestamp as binlogTimestamp a.use_schema as useSchema a.sql_schema as sqlSchema a.sql_table as sqlTable a.sql_text as sqlText a.sql_type as sqlType a.extra as extra from meta_history a where destination = ? and binlog_timestamp >= ? and binlog_timestamp <= ? order by binlog_timestamp asc id asc [42102-196] at org.h2.message.DbException.getJdbcSQLException(DbException.java:345) at org.h2.message.DbException.get(DbException.java:179) at org.h2.message.DbException.get(DbException.java:155) at org.h2.command.Parser.readTableOrView(Parser.java:5552) at org.h2.command.Parser.readTableFilter(Parser.java:1266) at org.h2.command.Parser.parseSelectSimpleFromPart(Parser.java:1946) at org.h2.command.Parser.parseSelectSimple(Parser.java:2095) at org.h2.command.Parser.parseSelectSub(Parser.java:1940) at org.h2.command.Parser.parseSelectUnion(Parser.java:1755) at org.h2.command.Parser.parseSelect(Parser.java:1743) at org.h2.command.Parser.parsePrepared(Parser.java:449) at org.h2.command.Parser.parse(Parser.java:321) at org.h2.command.Parser.parse(Parser.java:293) at org.h2.command.Parser.prepareCommand(Parser.java:258) at org.h2.engine.Session.prepareLocal(Session.java:578) at org.h2.engine.Session.prepareCommand(Session.java:519) at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1204) at org.h2.jdbc.JdbcPreparedStatement.<init>(JdbcPreparedStatement.java:73) at org.h2.jdbc.JdbcConnection.prepareStatement(JdbcConnection.java:288) at com.alibaba.druid.pool.DruidPooledConnection.prepareStatement(DruidPooledConnection.java:349) at com.ibatis.sqlmap.engine.execution.SqlExecutor.prepareStatement(SqlExecutor.java:497) at com.ibatis.sqlmap.engine.execution.SqlExecutor.executeQuery(SqlExecutor.java:175) at com.ibatis.sqlmap.engine.mapping.statement.MappedStatement.sqlExecuteQuery(MappedStatement.java:221) at com.ibatis.sqlmap.engine.mapping.statement.MappedStatement.executeQueryWithCallback(MappedStatement.java:189) Table "META_HISTORY" not found There are some initialized ddl files under conf to check @agapple 在canal 1 1 0 conf spring tsdb sql create_table sql Have this file Try to debug it. The operation for 啥initTable does not take effect initTable in MetaHistoryDAO Okay thank you @agapple
925,How to determine synchronization completion Using the example program example, it will continue to obtain the send dump protocol to get the binlog data of the master and then update it. However, it seems that the program does not end or is in the standby state after the data is synchronized, but continuously repeats the last synchronization data, for example, to the main library. Add 10 records according to the processing flow. The library is increased by 10 according to the general logic. At this time, the client has completed synchronization and should be in standby until the main library has new updates and then synchronize. The sample program shows that it has been synchronized before and after repeated. The data is asked what parameter settings are needed? In addition, the 1 1 0 version of the canal example only outputs the eventype and sql statements for the DDL operation. It seems that there is no corresponding DDL operation for the standby database. I don&#39;t know if the situation is true. Thank you for your answer. 2018-09-10 17:06:07.896 [Thread-2] WARN com.alibaba.otter.canal.example.db.CanalConnectorClient - parse event : QUERY sql: create table xgeom3(id int(20) not null auto_increment location point not null x timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP primary key(id) spatial key sp_index(location)) ENGINE=MyISAM AUTO_INCREMENT=3 DEFAULT CHARSET=utf8 . ignored! 2018-09-10 17:06:07.899 [Thread-2] INFO com.alibaba.otter.canal.example.db.CanalConnectorClient - ===========Transaction begin : =======>>>binlog[mysql-bin.000001:8519] executeTime : 1536567819000 delay : 2548899ms close this issue.
924,update Infinite loop error after the entire table no match ack positionLogPosition The error message is as follows 2018-09-10 12:09:57.669 [pool-7-thread-1] ERROR com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - no match ack positionLogPosition[identity=LogIdentity[sourceAddress=/47.97.169.27:3317 slaveId=-1] postion=EntryPosition[included=false journalName=mysql-bin.000003 position=7080270 serverId=20183317 gtid= timestamp=1536552101000]] sql: update t_order_shop_0024 set modify_time = now() ; # binlog filter config canal.instance.filter.druid.ddl = true canal.instance.filter.query.dcl = true canal.instance.filter.query.dml = true canal.instance.filter.query.ddl = true canal.instance.filter.table.error = true canal.instance.filter.rows = false canal.instance.filter.transaction.entry = true Parsed binlog information * Batch Id: [149136] count : [1] memsize : [8064] Time : 2018-09-10 12:30:11 * Start : [mysql-bin.000003:7080270:1536552101000(2018-09-10 12:01:41)] * End : [mysql-bin.000003:7080270:1536552101000(2018-09-10 12:01:41)] Kafka&#39;s message has been growing to see the log binlog all above is this mysql bin 000003 7080270 1536552101000 The problem is basically identified canal.instance.filter.transaction.entry = true After the filter transaction header is set to true, if there is no point information on the zk, I will give a timestamp 5 days ago. The key point of the problem should be canal instance filter transaction entry = true This setting is true Cannot use canal instance filter transaction entry to true if a site update is required LS Correct Answer
923,How to solve the host access to docker Bind ip Using docker When the mode is canal registered with zookeeper, the ip is the network of the docker container ip 172 17 0 3 For example, the address of the canal server binding in docker is 172 17 0 3 11111 Then my client can&#39;t access it. com.alibaba.otter.canal.protocol.exception.CanalClientException: java.net.ConnectException: Connection timed out: connect at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.doConnect(SimpleCanalConnector.java:171) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.access$000(SimpleCanalConnector.java:48) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector$1.processActiveEnter(SimpleCanalConnector.java:396) at com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor.processActiveEnter(ClientRunningMonitor.java:207) at com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor.initRunning(ClientRunningMonitor.java:118) at com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor.start(ClientRunningMonitor.java:92) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.connect(SimpleCanalConnector.java:93) at com.alibaba.otter.canal.client.impl.ClusterCanalConnector.connect(ClusterCanalConnector.java:63) at com.fcbox.canal.scheduling.SchedulerTask.run(SchedulerTask.java:39) Caused by: java.net.ConnectException: Connection timed out: connect at sun.nio.ch.Net.connect0(Native Method) at sun.nio.ch.Net.connect(Net.java:454) at sun.nio.ch.Net.connect(Net.java:446) at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.doConnect(SimpleCanalConnector.java:132) ... 8 common frames omitted 2018-09-10 11:03:05 454 WARN (ClusterCanalConnector.java:66)- failed to connect to:**### /172.17.0.3:11111** after retry 0 times And canal ip can not be directly written as the host&#39;s ip will report error Can consider docker&#39;s host mode
922,on canal.instance.filter.regex Set problem version 1.0.24 @agapple Problem phenomenon In the same mysql My canal instance filter regex cms rc_ means that only the table starting with rc_ under the cms database is no problem. But when I debug the code, I modify the tables in other databases. such as I modified The table under the test database, my client can still receive the message entryType Only TRANSACTIONBEGIN and TRANSACTIONEND Although these two messages are not what I want, I still hope not to receive the modified spam from other databases. Spam is as follows header { version: 1 logfileName: "mysql-bin.000001" logfileOffset: 6882 serverId: 1 serverenCode: "UTF-8" executeTime: 1536320325000 sourceType: MYSQL schemaName: "" tableName: "" eventLength: 75 } entryType: TRANSACTIONBEGIN storeValue: " \036" I want to express My question 1 canal instance filter regex cms rc_ does achieve my desired results 2 But other database schema messages I have received, add, delete, change, will receive the message entryType Although there are no additions and deletions, only TRANSACTIONBEGIN and TRANSACTIONEND, but I don&#39;t want to receive these two messages. 3 Is there any way to solve it? There are parameters to close Filter out the head and tail of an empty transaction In canal properteis Only one inside is true All others can be changed to false Https github com alibaba canal wiki AdminGuide can be seen here canal.instance.filter.transaction.entry = true This means to filter the head and tail of the transaction. canal.instance.filter.query.dcl = true canal.instance.filter.query.dml = true canal.instance.filter.query.ddl = true canal.instance.filter.table.error = true This configuration is to filter out the default sql statement, that is, after the binlog log is changed to row mode, there will still be a SQL statement with EntryType QUERY. @yin007008 @agapple Thank you
921,canal.instance.filter.transaction.entry After setting true zk Cusor is not updated canal.instance.filter.transaction.entry = true {"@type":"com.alibaba.otter.canal.protocol.position.LogPosition" "identity":{"slaveId":-1 "sourceAddress":{"address":"10.35.165.15" "port":3306}} "postion":{"gtid":"" "included":false "journalName":"mysql-bin.000023" "**position**":43893 "serverId":1 "timestamp":1536312619000}} Canal instance filter transaction entry Will cause the zk state not to be updated
920,canan kafka Data filtering problem Configured whitelist canal.instance.filter.regex=easy4ip.civil_user_phone Still able to receive information from other tables See the print below ``` table:sys_user logfile:mysql-bin.000023 entry:ROWDATA eventType:QUERY sql:UPDATE `sys_user` SET `LOGIN_DATE`='2018-09-08 15:36:02' WHERE (`ID`='21') ``` See FAQ
919,Fix upgrade protobuf version upgrade protobuf version [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=919) <br/>All committers have signed the CLA. [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=919) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=919) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=919) it.</sub> Upgraded protobuf After 3 6 1 Use the old version of protobuf Can the client of 2 6 1 read the data? It is estimated that this upgrade is not compatible. I can read the data. I used the 1 1 0 cliet test to connect. tks
918,Whether the new and old versions are compatible After v1 1 0 comes out, I plan to upgrade the old version 1 0 26 snapshot. Because of the high real-time requirements, I can&#39;t stop thinking about it and I want to upgrade smoothly. 1 The new version is configured with the same config and the old version is co-registered on the same zk 2 Turn off the old version of the node to let the new version preemptive processing rights 3 Then a new version of the server Is there a question in the first step when the new and old versions are compatible? The document says it is fully compatible
917,canal.instance.dbPassword = Database password supports encryption Can be considered to expand according to their own needs propertiesResolver is overwritten
916,Zookeeper cluster build canal startup error - Session 0x0 for server null unexpected error closing socket connection and attempting reconnect java.net.ConnectException: Connection refused at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[na:1.8.0_181] at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) ~[na:1.8.0_181] at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350) ~[zookeeper-3.4.5.jar:3.4.5-1392090] at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1068) ~[zookeeper-3.4.5.jar:3.4.5-1392090] `canal.properties:` canal.id= 1 canal.zkServers=10.153.1.208:2888 10.153.0.183:2888 10.153.0.38:2888 canal.instance.global.spring.xml = classpath:spring/default-instance.xml Is the port number of my canal connection zookeeper connection is wrong
915,About mysql active and standby switching Two questions hope to teach 1 canal set the alternate library main library after the machine cuts to the standby library, the main library restores the standby library, can the machine back to the main library? 2 canal support vip without paying attention to physical node switching 1. Will not take the initiative to the main library 2. Support rds Vip mode @agapple tks After switching to the standby database, the 60s will be rolled back. How to solve the problem? How to solve the problem? If the primary key is modified, for example, the source operation 1、insert id=1; 2、update id=2 where id=1; The source target database is a data id 2. At this time, the main library switches to the standby database to roll back the 60s log and redo the steps 1 and 2. 1 Success 2 fails. The primary key conflicts. The target library becomes id 1 and id. 2 two data data are inconsistent and source 1. Insert can consider using merge sql 2. Update encountered a primary key conflict split into delete before pk + merge insert new pk
914,canal.instance.dbPassword = Database password supports encrypted password This password supports encrypted passwords Currently not supported Consider submitting a PR to me
913,V1 1 0 abnormal scenario test causes operating system not allocate Memory leak Simulate the contents of the MySQL binlong file to drop the part of the data in the binlog file. The canal is abnormal when the send dump is parsed into this binlog file for a long time. Operating system log Fatal error : pthread_create() failed Eventually the operating system cannot allocate memory Finally only restart the system @agapple You check the memory parameters required by jvm and the memory of your machine. @agapple Machine memory 16G Canal uses default memory @agapple Canal will continue to create threads up to 30,000 when the system can not allocate memory. Canal will throw an exception java lang OutOfMemoryError unable to create new native The thread eventually causes the entire system to be unavailable. The jvm parameters are created with the default values ​​of a large number of threads as follows MultiStageCoprocessor-other-example-0 .... MultiStageCoprocessor-other-example-8 MultiStageCoprocessor-Parse-example-0 .... MultiStageCoprocessor-Parse-example-8 The latest master has been fixed https://github.com/alibaba/canal/pull/1002
912,1 1 0 canal and springboot 2 0 4 integration anomaly guava 18 Version is too low springboot Integrated guava 20 ` java.lang.NoSuchMethodError: com.google.common.collect.MapMaker.makeComputingMap(Lcom/google/common/base/Function;)Ljava/util/concurrent/ConcurrentMap; at com.google.common.collect.MigrateMap.makeComputingMap(MigrateMap.java:17) ~[canal.common-1.1.0.jar:na] at com.alibaba.otter.canal.common.zookeeper.ZkClientx.<clinit>(ZkClientx.java:26) ~[canal.common-1.1.0.jar:na] at com.alibaba.otter.canal.client.kafka.KafkaCanalConnector.<init>(KafkaCanalConnector.java:52) ~[classes/:na] at com.alibaba.otter.canal.client.kafka.KafkaCanalConnectors.newKafkaConnector(KafkaCanalConnectors.java:47) ~[classes/:na] at com.louxun.search.service.SyncDataToESJob.syncHouseDataToES(SyncDataToESJob.java:41) ~[classes/:na] at com.louxun.search.listener.StartupListener.onApplicationEvent(StartupListener.java:18) ~[classes/:na] at com.louxun.search.listener.StartupListener.onApplicationEvent(StartupListener.java:10) ~[classes/:na] at org.springframework.context.event.SimpleApplicationEventMulticaster.doInvokeListener(SimpleApplicationEventMulticaster.java:172) ~[spring-context-5.0.8.RELEASE.jar:5.0.8.RELEASE] at org.springframework.context.event.SimpleApplicationEventMulticaster.invokeListener(SimpleApplicationEventMulticaster.java:165) ~[spring-context-5.0.8.RELEASE.jar:5.0.8.RELEASE] at org.springframework.context.event.SimpleApplicationEventMulticaster.multicastEvent(SimpleApplicationEventMulticaster.java:139) ~[spring-context-5.0.8.RELEASE.jar:5.0.8.RELEASE] at org.springframework.context.support.AbstractApplicationContext.publishEvent(AbstractApplicationContext.java:400) ~[spring-context-5.0.8.RELEASE.jar:5.0.8.RELEASE] at org.springframework.context.support.AbstractApplicationContext.publishEvent(AbstractApplicationContext.java:354) ~[spring-context-5.0.8.RELEASE.jar:5.0.8.RELEASE] at org.springframework.context.support.AbstractApplicationContext.finishRefresh(AbstractApplicationContext.java:888) ~[spring-context-5.0.8.RELEASE.jar:5.0.8.RELEASE] at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.finishRefresh(ServletWebServerApplicationContext.java:161) ~[spring-boot-2.0.4.RELEASE.jar:2.0.4.RELEASE] at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:553) ~[spring-context-5.0.8.RELEASE.jar:5.0.8.RELEASE] at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:140) ~[spring-boot-2.0.4.RELEASE.jar:2.0.4.RELEASE] at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:762) [spring-boot-2.0.4.RELEASE.jar:2.0.4.RELEASE] at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:398) [spring-boot-2.0.4.RELEASE.jar:2.0.4.RELEASE] at org.springframework.boot.SpringApplication.run(SpringApplication.java:330) [spring-boot-2.0.4.RELEASE.jar:2.0.4.RELEASE] at org.springframework.boot.SpringApplication.run(SpringApplication.java:1258) [spring-boot-2.0.4.RELEASE.jar:2.0.4.RELEASE] at org.springframework.boot.SpringApplication.run(SpringApplication.java:1246) [spring-boot-2.0.4.RELEASE.jar:2.0.4.RELEASE] at com.louxun.search.SearchApplication.main(SearchApplication.java:10) [classes/:na] ` Exclusion seems to solve I modified the original code to solve ` package com.alibaba.otter.canal.common.zookeeper; import java.util.Map; import com.google.common.cache.CacheBuilder; import com.google.common.cache.CacheLoader; import com.google.common.cache.LoadingCache; import org.I0Itec.zkclient.IZkConnection; import org.I0Itec.zkclient.ZkClient; import org.I0Itec.zkclient.exception.ZkException; import org.I0Itec.zkclient.exception.ZkInterruptedException; import org.I0Itec.zkclient.exception.ZkNoNodeException; import org.I0Itec.zkclient.exception.ZkNodeExistsException; import org.I0Itec.zkclient.serialize.ZkSerializer; import org.apache.zookeeper.CreateMode; import com.google.common.base.Function; import com.google.common.collect.MigrateMap; /** * Use a custom ZooKeeperx for zk connection * * @author jianghang 2012-7-10 02 31 15 PM * @version 1.0.0 */ public class ZkClientx extends ZkClient { /* BUG Cache once for zkclient to avoid using multiple zk inside a jvm connection private static Map<String ZkClientx> clients = MigrateMap.makeComputingMap(new Function<String ZkClientx>() { public ZkClientx apply(String servers) { return new ZkClientx(servers); } }); */ use guava New way Replace the old old one with the springboot2 0 integration problem private static LoadingCache<String ZkClientx> clients = CacheBuilder.newBuilder().build( new CacheLoader<String ZkClientx>() { public ZkClientx load(String servers) { return new ZkClientx(servers); } }); // Other original code omitted without modification } ` I am talking about you in the pom. Where to use guava One of the old dependencies plus the exclusion tag to see if it can be solved No, ah canal depends on the old version of guava 18 with the new guava 20. The outdated method in one of the classes has been removed. I want to unify the new version. So I am now modifying the guava method in which the source code in the canal will expire. Replace with new way Canal uses the spring version for 3 2 6 and spring boot2 0 4 depends on the spring version for 5 x The difference in the version is relatively cautious If you have a successful replacement, you can submit a PR to me. @panjianping Brother, which version of canal you used later? And which version of springboot @gezhiwei8899 I used canal 1 1 0 with springboot-2.0.4 Modified canal Source code ZkClientx.java I also introduced dubbo, I can’t get up. @panjianping @panjianping Can you re-canal a method? Avoid modifying the source code @gezhiwei8899 The main problem now is that the high version of spring boot uses guava 20 and the current canal ZkClientx java relies on the method in guava 18 but this method has been removed in guava 20, so you don&#39;t want to modify the original code, you need to use spring. The 3 x version I probably thought of was the solution. Directly in your pom display dependent guava 18 is not it? I just solved this. <dependency> <groupId>com.google.guava</groupId> <artifactId>guava</artifactId> <version>18.0</version> </dependency>
911,The latest canal kafka 1 1 0 tar deployment can not start debugging source code is also the same exception to pay attention 2018-09-04 16:10:09.223 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2018-09-04 16:10:09.226 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties] 2018-09-04 16:10:09.400 [main] WARN o.s.beans.GenericTypeAwarePropertyDescriptor - Invalid JavaBean property 'connectionCharset' being accessed! Ambiguous write methods found next to actually use d [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.lang.String)]: [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlE ventParser.setConnectionCharset(java.nio.charset.Charset)] 2018-09-04 16:10:09.435 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2018-09-04 16:10:09.435 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties] 2018-09-04 16:10:09.667 [main] ERROR com.alibaba.druid.pool.DruidDataSource - testWhileIdle is true validationQuery not set 2018-09-04 16:10:09.891 [main] INFO c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-example 2018-09-04 16:10:09.905 [main] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - start successful.... 2018-09-04 16:10:09.948 [destination = example address = /192.168.200.30:3306 EventParser] WARN c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - prepare to find start position just las t position {"identity":{"slaveId":-1 "sourceAddress":{"address":"mySlave" "port":3306}} "postion":{"gtid":"" "included":false "journalName":"mysql-bin.000029" "position":132895982 "serverId":2 "timestamp":1 536044055000}} 2018-09-04 16:10:09.993 [Thread-7] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - stop CannalInstance for null-example 2018-09-04 16:10:09.996 [destination = example address = /192.168.200.30:3306 EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - parse events has an error com.alibaba.otter.canal.parse.exception.CanalParseException: dump address /192.168.200.30:3306 has an error retrying. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: should execute connector.connect() first Caused by: java.io.IOException: should execute connector.connect() first at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.<init>(MysqlQueryExecutor.java:30) ~[canal.parse.driver-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.queryMulti(MysqlConnection.java:106) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:175) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:129) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:91) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:188) ~[canal.parse-1.1.0.jar:na] at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_171] 2018-09-04 16:10:09.997 [Thread-7] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - stop successful.... Can&#39;t retry in our environment, you try to debug and see what MysqlConnection is disconnected under what conditions.
910,How can import some data of mysql into redis cluster There are already 6 redis clusters. Canal client In case the data is not updated How can I import the data of some existing tables in mysql into the redis cluster? Thank you In case the data is not updated This premise does not meet the working principle of canal may not meet your needs. Can you see if yugong or dataX can be satisfied? Thank you, I&#39;ve now understood
909,V1 1 0 How can deal with cloud server IP problem size network IP problem in Zookeeper Canal registers the intranet IP of the cloud server in Zookeeper such as 192 168 4 x and the corresponding large network server address is other IP such as 10 100 xx As a result, the canal and the Zookeeper client must be deployed to the same cloud server in a small network segment. @agapple This special network is not in the scope of design considerations. @agapple Can you plan to support Zuul and have similar situations for network segment filtering? Each canal Server own human flesh setting corresponding ip You can consider submitting a pr to me.
908,Meta bat problem Why do I start the service is not the generated meta bat file but the generated h2 mv db file has no big god to explain H2 mv db file will have canal instance global spring xml by default = Classpath spring default instance xml is written to zk on canal instance global spring xml = Classpath spring file instance xml This is the local meta bat LS Correct Answer
907,Error in example 2018-09-04 11:39:04.244 [WrapperSimpleAppMain] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2018-09-04 11:39:04.244 [WrapperSimpleAppMain] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties] 2018-09-04 11:39:04.400 [WrapperSimpleAppMain] WARN o.s.beans.GenericTypeAwarePropertyDescriptor - Invalid JavaBean property 'connectionCharset' being accessed! Ambiguous write methods found next to actually used [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.nio.charset.Charset)]: [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.lang.String)] 2018-09-04 11:39:04.400 [WrapperSimpleAppMain] INFO c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-example 2018-09-04 11:39:04.416 [WrapperSimpleAppMain] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - start successful.... 2018-09-04 11:39:04.416 [canal-instance-scan-0] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - stop CannalInstance for null-example 2018-09-04 11:39:04.432 [destination = example address = /127.0.0.1:3308 EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - parse events has an error com.alibaba.otter.canal.parse.exception.CanalParseException: dump address /127.0.0.1:3308 has an error retrying. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: java.net.SocketException: Socket Closed Caused by: java.net.SocketException: Socket Closed at java.net.SocketInputStream.socketRead0(Native Method) ~[na:1.8.0_77] at java.net.SocketInputStream.socketRead(SocketInputStream.java:116) ~[na:1.8.0_77] at java.net.SocketInputStream.read(SocketInputStream.java:170) ~[na:1.8.0_77] at java.net.SocketInputStream.read(SocketInputStream.java:141) ~[na:1.8.0_77] at java.io.BufferedInputStream.fill(BufferedInputStream.java:246) ~[na:1.8.0_77] at java.io.BufferedInputStream.read1(BufferedInputStream.java:286) ~[na:1.8.0_77] at java.io.BufferedInputStream.read(BufferedInputStream.java:345) ~[na:1.8.0_77] at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.read(BioSocketChannel.java:52) ~[canal.parse.driver-1.1.0.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.readHeader(PacketManager.java:12) ~[canal.parse.driver-1.1.0.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.readNextPacket(MysqlQueryExecutor.java:159) ~[canal.parse.driver-1.1.0.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.query(MysqlQueryExecutor.java:56) ~[canal.parse.driver-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:102) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.loadBinlogFormat(MysqlConnection.java:433) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.getBinlogFormat(MysqlConnection.java:582) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:95) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) ~[canal.parse-1.1.0.jar:na] at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_77] 2018-09-04 11:39:04.432 [canal-instance-scan-0] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - stop successful.... Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: java.net.SocketException: Socket Closed Caused by: java.net.SocketException: Socket Closed Feeling active off
906,1 1 0 version startup exception Unable to detect database changes [destination = example address = /10.19.1.80:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: EOF encountered. Caused by: java.io.IOException: EOF encountered. at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.read(BioSocketChannel.java:56) at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.readHeader(PacketManager.java:12) at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.readNextPacket(MysqlQueryExecutor.java:159) at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.query(MysqlQueryExecutor.java:56) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:102) at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:148) at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:129) at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:91) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:188) at java.lang.Thread.run(Thread.java:748) ] Caused by: java.io.IOException: EOF encountered. Check the database
905,Whether to support multi-subscriber mode Does canal support multi-subscriber mode to maintain different mark for each subscriber? with ack? Does not support the need for multiple subscribers to write data to MQ
904,1.1.0 Version canal server 8 destinations after startup Cpu has been 800 dead loop new Events object 1.1.0 Version canal server After starting, the CPU has been 100 or even 160. This is a problem with this version. Canal instance global spring xml for configuration = classpath:spring/default-instance.xml Using kafka integration Regardless of whether there is binlog down canal server The CPU consumption of the process is basically 100. top -Hp Pid to see which thread accounts for cpu high and then use jstack to see which class and method is causing Now deployed to the pressure test environment, without any binlog download, deploy 4 cloud 8 core machines, 8 destination cpu800, solve the problem. "pool-12-thread-1" #29 prio=5 os_prio=0 tid=0x00007fdee4afa000 nid=0x2394 runnable [0x00007fde6cce4000] java.lang.Thread.State: RUNNABLE at com.alibaba.otter.canal.store.model.Events.<init>(Events.java:23) at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.doGet(MemoryEventStoreWithBuffer.java:280) at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.tryGet(MemoryEventStoreWithBuffer.java:261) at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getEvents(CanalServerWithEmbedded.java:478) at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:310) - locked <0x0000000740492388> (a com.alibaba.otter.canal.instance.spring.CanalInstanceWithSpring) at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:273) at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.worker(CanalKafkaStarter.java:121) at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.access$000(CanalKafkaStarter.java:26) at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter$1.run(CanalKafkaStarter.java:70) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) From the thread dump, it is to enter the infinite loop new Events object From jstat 500M 15 minutes in the eden area YGC 500 times At 10 o&#39;clock last night, I started to the current YGC64000 times. FGC only started initialization because STW&#39;s 2 records means that there is no YGC in operation. # binlog filter config canal.instance.filter.druid.ddl = true canal.instance.filter.query.dcl = true canal.instance.filter.query.dml = true canal.instance.filter.query.ddl = true canal.instance.filter.table.error = true canal.instance.filter.rows = false canal.instance.filter.transaction.entry = false canal.instance.global.spring.xml = classpath:spring/default-instance.xml The thread stack information when the binlog is pulled normally is as follows # "pool-11-thread-3" #73 prio=5 os_prio=0 tid=0x00007f7ae8d88800 nid=0x3e5e runnable [0x00007f7a70927000] java.lang.Thread.State: RUNNABLE at com.alibaba.otter.canal.store.helper.CanalEventUtils.createPosition(CanalEventUtils.java:76) at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:375) at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:36) at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:307) - locked <0x00000007406361f0> (a com.alibaba.otter.canal.instance.spring.CanalInstanceWithSpring) at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:273) at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.worker(CanalKafkaStarter.java:121) at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.access$000(CanalKafkaStarter.java:26) at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter$1.run(CanalKafkaStarter.java:70) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) # "pool-11-thread-2" #72 prio=5 os_prio=0 tid=0x00007f7ae8d86800 nid=0x3e5d runnable [0x00007f7a70968000] java.lang.Thread.State: RUNNABLE at com.google.common.collect.MapMakerInternalMap$Segment.getEntry(MapMakerInternalMap.java:2402) at com.google.common.collect.ComputingConcurrentHashMap$ComputingSegment.getOrCompute(ComputingConcurrentHashMap.java:81) at com.google.common.collect.ComputingConcurrentHashMap.getOrCompute(ComputingConcurrentHashMap.java:67) at com.google.common.collect.MapMaker$ComputingMapAdapter.get(MapMaker.java:885) at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:296) at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:273) at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.worker(CanalKafkaStarter.java:121) at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.access$000(CanalKafkaStarter.java:26) at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter$1.run(CanalKafkaStarter.java:70) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) # "pool-11-thread-1" #71 prio=5 os_prio=0 tid=0x00007f7ae8d85800 nid=0x3e5c runnable [0x00007f7a709a9000] java.lang.Thread.State: RUNNABLE at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:317) - locked <0x000000074004a4d0> (a com.alibaba.otter.canal.instance.spring.CanalInstanceWithSpring) at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:273) at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.worker(CanalKafkaStarter.java:121) at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.access$000(CanalKafkaStarter.java:26) at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter$1.run(CanalKafkaStarter.java:70) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) @rewerma Let me see this question. CanalKafkaStarter worker estimates that sleep time is not set It is equivalent to an infinite loop running against the empty result. Can optimize Modified commit is a bit strange, I don&#39;t have sleep here, I have been wirelessly looping to take data, no problem, the CPU is normal. I am using a 1.8 jdk G1 collector here.
903,Could not find first log file name in binary log index file Error message 2018-09-03 17:06:05.707 [main] INFO c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-example 2018-09-03 17:06:05.709 [main] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - start successful.... 2018-09-03 17:06:05.781 [destination = example address = xxxxx:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position {"identity":{"slaveId":-1 "sourceAddress":{"address":"xxxx" "port":3306}} "postion":{"included":false "journalName":"mysql-bin.002736" "position":30948432 "serverId":2118896143 "timestamp":1535632015000}} 2018-09-03 17:06:05.886 [destination = example address = xxxx:3306 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) ~[canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122) [canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) [canal.parse-1.0.24.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.7.0_141] 2018-09-03 17:06:05.887 [destination = example address = xxxx:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address xxxxx:3306 has an error retrying. caused by java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) ~[canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122) ~[canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) ~[canal.parse-1.0.24.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.7.0_141] 2018-09-03 17:06:05.893 [destination = example address = xxxx:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) at java.lang.Thread.run(Thread.java:748) ] The problem should be that the binlog file in the meta dat can&#39;t keep up with the show. master Log file in STATUS The file in the meta dat is mysql bin 002736 {"clientDatas":[{"clientIdentity":{"clientId":1001 "destination":"example" "filter":""} "cursor":{"identity":{"slaveId":-1 "sourceAddress":{"address":"":"xxxx" "p" "port":3306}} "postion":{"included":false "journalName":"mysql-bin.002736" "position":30948432 "serverId":2118896143 "timestamp":1535632015000}}}] "destination":"example"} show master STATUS latest file mysql-bin.002752 24891270 The previous practice is to delete the meta dat file directly and start syncing again, but the data will be lost. After running for a while, the problem will occur again. How do you solve the problem? mysql Master will sweep based on certain strategies Binlog according to size or time if the log in the slow meta dat is cleaned up Consider this situation to improve overall throughput. 2018-09-04 09:35:33.645 [destination = example address = xxx:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: column size is not match for table:`skstandard`.`tbl_20180824090548lyqbbcqqjdt68u5m` 103 vs 15 Should be due to changes in the table structure, the service has been stuck here. Is there any good way to jump over? How to deal with the situation of this table structure change? I am manually modifying the position skip in the meta dat. Upgrade the new version https://github.com/alibaba/canal/wiki/TableMetaTSDB Oh, you are this column. size is not match for How is the table problem solved? Hope to enlighten me The local version is 1 1 0 but the column will appear. size is not match for Table Daxie enlightenment solution
902,fix bug #890: Parallel analysis + Gtid Did not initialize the gtidSet in the LogContext tks
901,Using the kafka client consumption binlog appears garbled *B> 0mysql-bin.000102�0 UTF 80踬 8B I also encountered the same problem Kafka sends a message packet that is not visible. The client needs to deserialize the package after receiving it. client kafka
900,kafka Server-side code adjustment
899,After the data is consumed for a while, the canal client cannot pull new data to determine that the db is constantly updated. ![image](https://user-images.githubusercontent.com/5965173/44947782-0e424280-ae45-11e8-973a-b2c102d7d704.png) Use the latest version 1 10 Check if there is an error on the server side. ![image](https://user-images.githubusercontent.com/5965173/45035342-57d09e80-b08c-11e8-9bf8-bba18de5ae43.png) After an exception occurs on the server side, the client automatically resubscribes the information and then the client I can&#39;t read the data. Consumption stopped after another abnormality ![image](https://user-images.githubusercontent.com/5965173/45036036-fad5e800-b08d-11e8-8b33-a956e293438d.png) Mysql version 5.6.28 getWithoutAck The above problem occurs with a mode that does not require confirmation. Use get Ack&#39;s model is no problem. First consider upgrading the canal version
898,Canal kafka output garbled CentOS Linux release 7.3.1611 (Core) 3.10.0-514.el7.x86_64 JDK：jdk-8u161-linux-x64.tar.gz zookeeper：zookeeper-3.4.13.tar.gz kafka：kafka_2.11-2.0.0.tgz canal.kafka：canal.kafka-1.1.0.tar.gz MySQL：5.7.22-log Follow https github com alibaba canal wiki Canal Kafka QuickStart configuration Kafka is all garbled I need to adjust there. bin/kafka-console-consumer.sh --bootstrap-server 192.168.10.110:9092 --topic example *D mysql-binlog.000012*UTF-80BJP401663 *? mysql-binlog.000012*UTF-80BJPH  mysql-binlog.000012*UTF-80BtestJtP+Xb _-+_C-+++1Pb? id (0B1Ri++(11)  +e_+ (0B1R +a_cha_(255)D +y_-+-bi++-g.000012 *UTF-80 8BJP401877 *? +y_-+-bi++-g.000012 *UTF-80 8BJPI > +y_-+-bi++-g.000012 *UTF-80 8Beca_dJ+_c+a___ca_d__e-+e_+_+-gPTXb _-+_C-+++1Pb  Kafka data delivery is the data packet. After receiving the data, it must be unpacked into the corresponding message. Refer to the kafka implementation in the client. I saw the AbstractKafkaTest java in the kafka client. After changing the corresponding configuration parameters, insert the new record into mysql and use the command bin kafka console consumer sh --zookeeper 192.168.206.128:2181 --topic canal1 From beginning, you can see that the packet being delivered is garbled but running KafkaClientRunningTest java does not receive the packet.
897,Canal 1 1 0 source canal kafka entry start has been reported abnormal 2018-08-31 09:05:51.141 [destination = example address = /192.168.200.42:3306 EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address /192.168.200.42:3306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: java.net.SocketException: Socket closed Caused by: java.net.SocketException: Socket closed at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:116) ~[na:1.8.0_20] at java.net.SocketOutputStream.write(SocketOutputStream.java:141) ~[na:1.8.0_20] at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.write(BioSocketChannel.java:36) ~[classes/:na] at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.writeBody0(PacketManager.java:42) ~[classes/:na] at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.writeBody(PacketManager.java:35) ~[classes/:na] at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.query(MysqlQueryExecutor.java:55) ~[classes/:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:102) ~[classes/:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.loadBinlogFormat(MysqlConnection.java:433) ~[classes/:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.getBinlogFormat(MysqlConnection.java:582) ~[classes/:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:95) ~[classes/:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) ~[classes/:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_20] 2018-08-31 09:05:51.146 [destination = example address = /192.168.200.42:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: java.net.SocketException: Socket closed Caused by: java.net.SocketException: Socket closed at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:116) at java.net.SocketOutputStream.write(SocketOutputStream.java:141) at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.write(BioSocketChannel.java:36) at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.writeBody0(PacketManager.java:42) at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.writeBody(PacketManager.java:35) at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.query(MysqlQueryExecutor.java:55) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:102) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.loadBinlogFormat(MysqlConnection.java:433) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.getBinlogFormat(MysqlConnection.java:582) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:95) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) at java.lang.Thread.run(Thread.java:745) ] Mysql connection is no problem the same mysql configuration with canal entry to start no problem Socket closed Caused by: java.net.SocketException: Socket closed
896,Fix execTime faster than canal current time delay no data points @wingerx No data from the code The situation of points is basically mysql Host and canal server Ntp does not synchronize mysql time faster Now modify it to be 0 in this case. The wiki will also focus on modifying the description. tks
895,canal To kafka Data consistency problem How to ensure the consistency of data if I set multiple partition canal Whether kafka or rocketmq can only be single-segment global order or single-partition order If there is no strong association between the tables, it is possible to hash the single table to the same partition to ensure that the order table is ordered. It seems that I heard that the project author of canal may be implementing multiple partitions and one table and guarantee that the order does not know whether it is true. In theory, I think that if there is an index table sorted by time, the index table is read first and then the real data is read. Take the binglog data on multiple partitions #958
894,fix #893 index name contains "on" keyword [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=894) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=894) before we can accept your Contribution br hr Zhang Xin seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=894) it.</sub> [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=894) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=894) before we can accept your Contribution br hr Zhang Xin seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=894) it.</sub> tks
893,When creating or deleting an index, if the index name contains an on string, then the name of the table will be incorrect. such as CREATE INDEX `schema_new_index_version_s_idx` ON `tb_email_auth` (`status`) Since version contains on, the table name parsed at this time is _s_idx Correction method com alibaba otter canal parse inbound mysql ddl SimpleDdlParser中的 public static final String CREATE_INDEX_PATTERN = "^\\s*CREATE\\s*(UNIQUE)?(FULLTEXT)?(SPATIAL)?\\s*INDEX\\s*(.*?)\\s+ON\\s+(.*?)$"; public static final String DROP_INDEX_PATTERN = "^\\s*DROP\\s*INDEX\\s*(.*?)\\s+ON\\s+(.*?)$"; The s before and after ON must appear 1 time to multiple times in the source code. Change to In fact, simpparser should be removed from deprecate. This can&#39;t support other complicated scenes.
892,kafka client keep Alive interval
891,canal.deployer-1.1.0 Cursor persistence frequency problem When the test 1 client reconnects, the modification time of the meta dat is updated but the data remains unchanged. Test 2 Due to the problem of test 1, the position is not updated, causing the old persistent cursor client to consume at the old position if the server restarts reading at this time. issue Canal version 1 0 26 alpha5，1.1.0 1.0.26 Alpha2 does not exist this problem every time the client consumes update curosr [#882](https://github.com/alibaba/canal/issues/882)
890,Parallel analysis + Gtid Did not initialize the gtidSet in the LogContext Did not initialize the gtidSet in the LogContext public final void putGtid(GtidLogEvent logEvent) { if (logEvent != null) { String gtid = logEvent.getSid().toString() + ":" + logEvent.getGno(); if (gtidSet == null) { gtid = logEvent.getSid().toString() + ":1-" + logEvent.getGno(); gtidSet = MysqlGTIDSet.parse(gtid); } gtidSet.update(gtid); } } The current server&#39;s gtid in the event will override zk Historical gtidSet in the cursor several server start end combination Next time restart the dump error Is it based on the current main code? Based on 1 1 0 release This commit . commit dd8b1ce9551b59b719516615e43193c467214ade (tag: canal-1.1.0) Author: Seven fronts <jianghang.loujh@alibaba-inc.com> Date: Mon Aug 20 13:28:57 2018 +0800 [maven-release-plugin] prepare release canal-1.1.0 Then cherry pick Parallel parsing of gtid Two bugs fix commit 0ca9fa129975ccb4884d8ca9c02c63b066461e6a Author: winger <winger2049@gmail.com> Date: Sat Aug 25 09:17:13 2018 +0800 fix bug: Open gtid in parallel parsing mode Will cause parsing errors commit 186b58396862cbea82781458625608400fe3e5c5 Author: winger <winger2049@gmail.com> Date: Sat Aug 25 03:54:38 2018 +0800 fix bug: Open gtid in parallel parsing mode Will cause parsing errors If the position stored on the previous zk has multiple gtid collections and some have been mysql master Purged off Very easy to reproduce
889,Canal1 1 0 start example log error 2018-08-27 17:22:51.560 [destination = example address = /192.168.155.35:3306 EventParser] WARN c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - prepare to find start position just show master status 2018-08-27 17:22:51.562 [destination = example address = /192.168.155.35:3306 EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address /192.168.100.249:3306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: command : 'show master status' has an error! pls check. you need (at least one of) the SUPER REPLICATION CLIENT privilege(s) for this operation 2018-08-27 17:22:51.562 [destination = example address = /192.168.155.35:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: command : 'show master status' has an error! pls check. you need (at least one of) the SUPER REPLICATION CLIENT privilege(s) for this operation ] Permissions issue To solve the problem From NetEase Mailbox Master On August 28, 2018 17:47，agapple<notifications@github.com> Write Closed #889. — You are receiving this because you authored the thread. Reply to this email directly view it on GitHub or mute the thread. Daxie An abnormal column appears after the canal modify the table structure size is not match table xx Will ddl open or not? Give trouble to enlighten me Take a look at the document TableMetaTSDB
888,connector.subscribe("xxx") Does not work canal.instance.filter.regex=fid_standard_product\\..* I am on the server side to define this entire library in a client connector subscribe fid_standard_product fid_stock_report One of the tables but the other tables in the library will still be synchronized. Why does the connector subscribe not filter? See more FAQ
887,canal Running an ack error Canal run ack error ack error clientId:1001 batchId:261365 is not exist please check。 This should be a server-side instance restart that causes the server-side configuration to be canal.instance.get.ddl.isolation = false ################################################# ######### destinations ############# ################################################# canal.destinations= db9002 # conf root dir canal.conf.dir = ../conf # auto scan instance dir add/remove and start/stop instance canal.auto.scan = false canal.auto.scan.interval = 5 canal.instance.global.mode = spring canal.instance.global.lazy = false Why does the instance restart? How to control the instance of the server does not restart When you encounter an ack exception, you can roll back again and you can see the example project of example.
886,canal
885,About kafka Topic settings Does it support setting a corresponding kafka for each mysql table? topic？ At first glance, the code seems to only support all topics that correspond to a destination setting multiple topic data will be sent to the settings. Currently only supports one destination for one destination.
884,By canal instance filter regex filter And through the java program connector subscribe Is there anything different? In actual use, only canal can monitor a certain number of tables in a library, and how to match the performance will be better. Look at the wiki
883,Canal1 1 0, when listening to the first change, it will be fine after reporting an error. logEventParserProxy - dump address /106.12.14.74:3306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: java.net.SocketException: Connection reset Caused by: java.net.SocketException: Connection reset at java.net.SocketInputStream.read(SocketInputStream.java:210) ~[na:1.8.0_181] at java.net.SocketInputStream.read(SocketInputStream.java:141) ~[na:1.8.0_181] at java.io.BufferedInputStream.fill(BufferedInputStream.java:246) ~[na:1.8.0_181] at java.io.BufferedInputStream.read1(BufferedInputStream.java:286) ~[na:1.8.0_181] at java.io.BufferedInputStream.read(BufferedInputStream.java:345) ~[na:1.8.0_181] at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.read(BioSocketChannel.java:52) ~[canal.parse.driver-1.1.0.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.readHeader(PacketManager.java:12) ~[canal.parse.driver-1.1.0.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.readNextPacket(MysqlQueryExecutor.java:159) ~[canal.parse.driver-1.1.0.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.query(MysqlQueryExecutor.java:56) ~[canal.parse.driver-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:102) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:148) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:129) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:91) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:188) ~[canal.parse-1.1.0.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_181] 2018-08-26 19:08:07.309 [destination = example address = /106.12.14.74:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: java.net.SocketException: Connection reset Caused by: java.net.SocketException: Connection reset network anomaly Will automatically retry
882,1.0.26-alpha 2: canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield canal.instance.tsdb.enable=false canal.instance.gtidon=false Error log 1 ``` 2018-08-25 10:04:02.917 [destination = platform_data address = ip:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTa bleMeta - parse faield : CREATE TABLE `t_bill_details` ( `bill_id` bigint(20) NOT NULL .... KEY `IDX_BD_CITY` (`store_city`) KEY_BLOCK_SIZE=8 KEY `IDX_BD_DETAIL_ID` (`detail_id`) KEY_BLOCK_SIZE=8 KEY `IDX_BD_SD` (`bill_sale_date`) KEY_BLOCK_SIZE=8 KEY `IDX_BD_SSS` (`bill_status` `store_city` `bill_sale_date` `store_id`) KEY_BLOCK_SIZE=8 KEY `IDX_STORE_ID` (`store_id`) KEY_BLOCK_SIZE=8 KEY `IDX_BD_STATUS` (`bill_status` `bill_sale_date`) KEY_BLOCK_SIZE=8 ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 ROW_FORMAT=COMPRESSED KEY_BLOCK_SIZE=4 com.alibaba.fastsql.sql.parser.ParserException: syntax error error in :'id`) KEY_BLOCK_SIZE=8 KEY `IDX_B' expect RPAREN actual IDENTIFIER pos 1834 line 43 column 32 token IDENTIFIER KEY_BLOCK_SIZE at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:305) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] at com.alibaba.fastsql.sql.parser.SQLParser.accept(SQLParser.java:313) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] at com.alibaba.fastsql.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:260) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] at com.alibaba.fastsql.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:239) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:165) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:76) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:469) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:331) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:71) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.parseTableMeta(TableMetaCache.java:101) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMetaByDB(TableMetaCache.java:87) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:32) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:57) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:52) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) [guava-18.0.jar:na] at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) [guava-18.0.jar:na] at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) [guava-18.0.jar:na] at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) [guava-18.0.jar:na] at com.google.common.cache.LocalCache.get(LocalCache.java:3937) [guava-18.0.jar:na] at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941) [guava-18.0.jar:na] at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824) [guava-18.0.jar:na] at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830) [guava-18.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:185) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:793) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:457) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:133) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:68) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:345) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:187) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:154) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_60] ``` Error log 2 ``` 2018-08-25 09:59:04.733 [destination = platform_data address = ip:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHand ler - destination:platform_data[com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.util.concurrent.UncheckedExecutionException: com.alibaba.otter.canal.parse.exception.CanalParseException: fetch failed by table meta:`platform_data`.`tmp_presto_0e13f1bc14574ec4ae4950cade1bc360` Caused by: com.google.common.util.concurrent.UncheckedExecutionException: com.alibaba.otter.canal.parse.exception.CanalParseException: fetch failed by table meta:`platform_data`.`tmp_presto_0e13f1bc145 74ec4ae4950cade1bc360` at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2203) at com.google.common.cache.LocalCache.get(LocalCache.java:3937) at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941) at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824) at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:185) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:793) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:457) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:127) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:68) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:345) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:187) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:154) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) at java.lang.Thread.run(Thread.java:745) Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: fetch failed by table meta:`platform_data`.`tmp_presto_0e13f1bc14574ec4ae4950cade1bc360` Caused by: java.io.IOException: ErrorPacket [errorNumber=1146 fieldCount=-1 message=Table 'platform_data.tmp_presto_0e13f1bc14574ec4ae4950cade1bc360' doesn't exist sqlState=42S02 sqlStateMarker=#] with command: desc `platform_data`.`tmp_presto_0e13f1bc14574ec4ae4950cade1bc360` at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.query(MysqlQueryExecutor.java:61) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:96) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMetaByDB(TableMetaCache.java:89) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:32) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:62) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:52) at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) at com.google.common.cache.LocalCache.get(LocalCache.java:3937) at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941) at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824) at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:185) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:793) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:457) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:127) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:68) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:345) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:187) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:154) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) at java.lang.Thread.run(Thread.java:745) ``` In fact, I only need to monitor dml. If I configure filtering, how do I configure the following? ``` canal.instance.filter.druid.ddl = true canal.instance.filter.query.dcl = true canal.instance.filter.query.dml = false canal.instance.filter.query.ddl = true canal.instance.filter.table.error = true canal.instance.filter.rows = false ``` Canal instance filter rows specifically do what the wiki did not understand Upgrade the latest 1 1 0 to test first
881,fix bug: Open gtid in parallel parsing mode Will cause parsing errors tks
880,aliyun rds Log parsing failed Could not find first log file name in binary log index file Isn&#39;t that full support for rds? Why is this wrong with canal instance rds accesskey? Those parameters are also matched, but it seems to have no effect. This is the binlog is deleted and cleared off. Use the binlog timestamp to locate
879,Why do I specify a table but I can&#39;t see the schema name and table name from kafka? I set this up canal.instance.filter.regex=db_shopdog_test.deli_order But kafka does not resolve schema names and table names. First look at the wiki to confirm whether the filter is in effect.
878,Instance properties filter the table canal instance filter regex Does not work how to configure changes in a specified table in the monitoring database See more FAQs on the wiki I also have this requirement. I don&#39;t know how to do it. I need to monitor three tables in a library. The FAQ in the wiki inside github can&#39;t be opened. > See more FAQs on the wiki I also have this requirement. I don&#39;t know how to do it. I need to monitor three tables in a library. The FAQ in the wiki inside github can&#39;t be opened.
877,Canal how to configure only listen for update status insert delete does not listen Can filter itself according to the CanalEntry EventType Thank you for your guidance. From NetEase Mailbox Master On August 24, 2018 11:28，xiaokangzhao<notifications@github.com> Write Can filter itself according to the CanalEntry EventType — You are receiving this because you authored the thread. Reply to this email directly view it on GitHub or mute the thread.
876,Integration kafka To server with client tks
875,Canal log start canal error 2018-08-24 06:25:04.409 [New I/O server worker #1-1] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x599715d9 /192.168.254.1:57576 => /192.168.254.128:11111] exception=java.io.IOException: Connection reset by peer at sun.nio.ch.FileDispatcherImpl.read0(Native Method) at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39) at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223) at sun.nio.ch.IOUtil.read(IOUtil.java:192) at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380) at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:322) at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) Looking at the big guys Upgrade to the latest 1 1 0
874,How long is the master-slave synchronization delay? Is there any relevant performance test report? For example, when the amount of data is large, the delay is relatively large. See more wiki perfomance and monitoring
873,Creating a thread pool in a single core environment will cause an error as title [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=873) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=873) before we can accept your contribution.<br/><hr/>**Chuanyi Li L** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=873) it.</sub> [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=873) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=873) before we can accept your contribution.<br/><hr/>**Chuanyi Li L** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=873) it.</sub> Tks It’s no wonder that there have been a lot of feedback thread pool creation failures recently. So many small-size virtual machines are used.
872,canal server、client with canal-kafka server、canal-kafka client merge canal server、client with canal-kafka server、canal-kafka client merge
871,Why is it that I am transmitting to kafka and it is all garbled? The configuration is based on the documentation of the mysql character set is utf8 mysql-bin.000002« *UTF-80¨£¯Ԭ8BJP58025 XshellXshellXshellXshell8 mysql-bin.000002 *UTF-80򯰖 8BJPK w mysql-bin.000002 *UTF-80򯰖 8Btest_heJbP'Xb rowsCount1򂎁Pa (0BdR char(200)= mysql-bin.000002® *UTF-80򯰖 8BJP58052 XshellXshellXshellXshell8 mysql-bin.000002 *UTF-80ȹ񱔬8BJPK w mysql-bin.000002 *UTF-80ȹ񱔬8Btest_heJbP'Xb rowsCount1򂎁Pa (0BdR char(200)= mysql-bin.000002± *UTF-80ȹ񱔬8BJP58481 XshellXshellXshellXshell8 mysql-bin.000002 *UTF-80񺷰Ԭ8BJPK w mysql-bin.000002 *UTF-80񺷰Ԭ8Btest_heJbP'Xb rowsCount1򂎁Pa (0BdR char(200)= mysql-bin.000002´ *UTF-80񺷰Ԭ8BJP58486 XshellXshellXshellXshellXshell8 mysql-bin.000002 *UTF-80º񔪸BJPK w mysql-bin.000002 *UTF-80º񔪸Btest_heJbP'Xb rowsCount1򂎁Pa (0BdR char(200)= mysql-bin.000002· *UTF-80º񔪸BJP58491 XshellXshellXshellXshell8 mysql-bin.000002 *UTF-80񼱖 8BJPK w mysql-bin.000002 *UTF-80񼱖 8Btest_heJbP'Xb rowsCount1򂎁Pa (0BdR char(200)= mysql-bin.000002º *UTF-80񼱖 8BJP58498 XshellXshellXshellXshel8* mysql-bin.000002 *UTF-80тÿ°Ԭ8BJPK w mysql-bin.000002 *UTF-80тÿ°Ԭ8Btest_heJbP'Xb rowsCount1򂎁Pa (0BdR char(200)= mysql-bin.000002½ *UTF-80тÿ°Ԭ8BJP58507 XshellXshellXshellXshell* 8 mysql-bin.000002 *UTF-80Ȇ±Ԭ8BJPK w mysql-bin.000002 *UTF-80Ȇ±Ԭ8Btest_heJbP'Xb rowsCount1򂎁Pa (0BdR char(200)= mysql-bin.000002 *UTF-80Ȇ±Ԭ8BJP58520 XshellXshellXshellXshell* 8 mysql-bin.000002 ¡ *UTF-80¸²±Ԭ8BJPK w mysql-bin.000002¢ *UTF-80¸²±Ԭ8Btest_heJbP'X rowsCount1򂎁Pa (0BdR char(200)= mysql-bin.000002â *UTF-80¸²±Ԭ8BJP58525 XshellXshellXshellXshell* 8 mysql-bin.000002££ *UTF-80׍±Ԭ8BJPK w mysql-bin.000002¤ *UTF-80׍±Ԭ8Btest_heJbP'Xb rowsCount1򂎁Pa (0BdR char(200)= mysql-bin.000002Ƥ *UTF-80׍±Ԭ8BJP58538 Passed to kafka, the serialized data client of the message is deserialized to message after receiving it.
870,Canal monitor how multiple mysql databases should be configured Read more wiki
869,canal-kafka-1.0.26-preview-4: CanalKafkaStarter High CPU usage CanalKafkaStarter.worker Calling getWithoutAck in thread does not use timeout parameter ``` "pool-8-thread-1" #21 prio=5 os_prio=0 tid=0x00007facb47b1000 nid=0x5fe runnable [0x00007fac8d16b000] java.lang.Thread.State: RUNNABLE at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.isStart(CanalServerWithEmbedded.java:126) at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.checkStart(CanalServerWithEmbedded.java:484) at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:279) at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:259) at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.worker(CanalKafkaStarter.java:118) at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.access$000(CanalKafkaStarter.java:26) at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter$1.run(CanalKafkaStarter.java:67) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) ``` The latest 1 1 1 has been fixed
868,canal-kafka-1.1.0: Infinite loop CanalKafkaStarter - process error! kafka: n1:9092 n2:9092 zk: n3:2181 n4:2181 n5:2181 canal: n1 Error log ``` 2018-08-23 16:25:08.274 [destination = example address = n1/192.168.4.11:3306 EventParser] WARN c.a.o.c.p.inboun d.mysql.rds.RdsBinlogEventParserProxy - find start position : EntryPosition[included=false journalName=mysql-bin.0000 01 position=3024 serverId=<null> gtid=<null> timestamp=<null>] 2018-08-23 16:25:08.299 [main] INFO com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - ## start the kafka wo rkers. 2018-08-23 16:25:08.300 [main] INFO com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - ## the kafka workers is running now ...... 2018-08-23 16:25:08.301 [pool-4-thread-1] INFO com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - ## start t he canal consumer: example. 2018-08-23 16:25:08.305 [pool-4-thread-1] INFO com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - ## the can al consumer example is running now ...... 2018-08-23 16:25:15.105 [pool-4-thread-1] ERROR com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - process er ror! java.lang.NullPointerException: null at com.alibaba.otter.canal.store.helper.CanalEventUtils.createPosition(CanalEventUtils.java:69) ~[canal.store -1.1.0.jar:na] at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffe r.java:375) ~[canal.store-1.1.0.jar:na] at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffe r.java:36) ~[canal.store-1.1.0.jar:na] at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java :307) ~[canal.server-1.1.0.jar:na] at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java :273) ~[canal.server-1.1.0.jar:na] at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.worker(CanalKafkaStarter.java:121) [canal.kafka-1 .1.0.jar:na] at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.access$000(CanalKafkaStarter.java:26) [canal.kafk a-1.1.0.jar:na] at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter$1.run(CanalKafkaStarter.java:70) [canal.kafka-1.1 .0.jar:na] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_161] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_161] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-08-23 16:25:15.106 [pool-4-thread-1] ERROR com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - process er ror! java.lang.NullPointerException: null at com.alibaba.otter.canal.store.helper.CanalEventUtils.createPosition(CanalEventUtils.java:69) ~[canal.store -1.1.0.jar:na] at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffe : ``` Configuration canal.properties ```################################################# ######### common argument ############# ################################################# canal.id= 1 canal.ip= canal.port=11111 canal.metrics.pull.port=11112 canal.zkServers=n3:2181 n4:2181 n5:2181 # flush data to zk canal.zookeeper.flush.period = 1000 canal.withoutNetty = true # flush meta cursor/parse position to file canal.file.data.dir = ${canal.conf.dir} canal.file.flush.period = 1000 ## memory store RingBuffer size should be Math.pow(2 n) canal.instance.memory.buffer.size = 16384 ## memory store RingBuffer used memory unit size default 1kb canal.instance.memory.buffer.memunit = 1024 ## meory store gets mode used MEMSIZE or ITEMSIZE canal.instance.memory.batch.mode = MEMSIZE ## detecing config canal.instance.detecting.enable = false #canal.instance.detecting.sql = insert into retl.xdual values(1 now()) on duplicate key update x=now() canal.instance.detecting.sql = select 1 canal.instance.detecting.interval.time = 3 canal.instance.detecting.retry.threshold = 3 canal.instance.detecting.heartbeatHaEnable = false # support maximum transaction size more than the size of the transaction will be cut into multiple transactions delivery canal.instance.transaction.size = 1024 # mysql fallback connected to new master should fallback times canal.instance.fallbackIntervalInSeconds = 60 # network config canal.instance.network.receiveBufferSize = 16384 canal.instance.network.sendBufferSize = 16384 canal.instance.network.soTimeout = 30 # binlog filter config canal.instance.filter.druid.ddl = true canal.instance.filter.query.dcl = true canal.instance.filter.query.dml = false canal.instance.filter.query.ddl = true canal.instance.filter.table.error = false canal.instance.filter.rows = false canal.instance.filter.transaction.entry = true # binlog format/image check canal.instance.binlog.format = ROW STATEMENT MIXED canal.instance.binlog.image = FULL MINIMAL NOBLOB # binlog ddl isolation canal.instance.get.ddl.isolation = false # parallel parser config canal.instance.parser.parallel = true ## concurrent thread number default 60% available processors suggest not to exceed Runtime.getRuntime().availableProcessors() #canal.instance.parser.parallelThreadSize = 16 ## disruptor ringbuffer size must be power of 2 canal.instance.parser.parallelBufferSize = 256 # table meta tsdb info canal.instance.tsdb.enable=true canal.instance.tsdb.dir=${canal.file.data.dir:../conf}/${canal.instance.destination:} canal.instance.tsdb.url=jdbc:h2:${canal.instance.tsdb.dir}/h2;CACHE_SIZE=1000;MODE=MYSQL; canal.instance.tsdb.dbUsername=canal canal.instance.tsdb.dbPassword=canal # rds oss binlog account canal.instance.rds.accesskey = canal.instance.rds.secretkey = ################################################# ######### destinations ############# ################################################# canal.destinations= example # conf root dir canal.conf.dir = ../conf # auto scan instance dir add/remove and start/stop instance canal.auto.scan = true canal.auto.scan.interval = 5 canal.instance.tsdb.spring.xml=classpath:spring/tsdb/h2-tsdb.xml #canal.instance.tsdb.spring.xml=classpath:spring/tsdb/mysql-tsdb.xml canal.instance.global.mode = spring canal.instance.global.lazy = false #canal.instance.global.manager.address = 127.0.0.1:1099 #canal.instance.global.spring.xml = classpath:spring/memory-instance.xml #canal.instance.global.spring.xml = classpath:spring/file-instance.xml canal.instance.global.spring.xml = classpath:spring/default-instance.xml ``` kafka.yml ```servers: n1:9092 n2:9092 retries: 0 batchSize: 16384 lingerMs: 1 bufferMemory: 33554432 # Lot size unit of canal The amount of k is recommended to be changed to 1M canalBatchSize: 50 filterTransactionEntry: true canalDestinations: - canalDestination: example topic: example partition: ``` example/instance.properties ```################################################# ## mysql serverId v1.0.26+ will autoGen # canal.instance.mysql.slaveId=0 # enable gtid use true/false canal.instance.gtidon=false # position info canal.instance.master.address=n1:3306 canal.instance.master.journal.name= canal.instance.master.position= canal.instance.master.timestamp= canal.instance.master.gtid= # rds oss binlog canal.instance.rds.accesskey= canal.instance.rds.secretkey= canal.instance.rds.instanceId= # table meta tsdb info canal.instance.tsdb.enable=false #canal.instance.tsdb.url=jdbc:mysql://127.0.0.1:3306/canal_tsdb #canal.instance.tsdb.dbUsername=canal #canal.instance.tsdb.dbPassword=canal #canal.instance.standby.address = #canal.instance.standby.journal.name = #canal.instance.standby.position = #canal.instance.standby.timestamp = #canal.instance.standby.gtid= # username/password canal.instance.dbUsername=root canal.instance.dbPassword=root canal.instance.connectionCharset=UTF-8 # table regex canal.instance.filter.regex=test\\..* # table black regex canal.instance.filter.black.regex= ################################################# ``` First insert into table Successfully parsing binlog and then this problem occurs Use the SimpleCanalClientTest in the official canal example to read the first inserted data can be parsed after the error log is as follows Always changing ports ``` **************************************************** * Batch Id: [1] count : [1] memsize : [55] Time : 2018-08-25 15:56:55 * Start : [mysql-bin.000005:642:1535212613000(2018-08-25 23:56:53)] * End : [mysql-bin.000005:642:1535212613000(2018-08-25 23:56:53)] **************************************************** ----------------> binlog[mysql-bin.000005:642] name[test ar_tmp] eventType : INSERT executeTime : 1535212613000(2018-08-25 23:56:53) gtid : () delay : -28797618 ms id : 1 type=int(11) update=true name : a type=varchar(32) update=true price : 1.1 type=double update=true time : 2018-08-25 23:56:53 type=datetime update=true process error!com.alibaba.otter.canal.protocol.exception.CanalClientException: something goes wrong with reason: something goes wrong with channel:[id: 0x12e25627 /192.168.4.1:9230 => /192.168.4.21:11111] exception=java.lang.NullPointerException at com.alibaba.otter.canal.store.helper.CanalEventUtils.createPosition(CanalEventUtils.java:69) at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:375) at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:36) at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:307) at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:273) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:124) at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:100) at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754) at org.jboss.netty.channel.SimpleChannelHandler.messageReceived(SimpleChannelHandler.java:154) at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:100) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80) at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:444) at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80) at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:540) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261) at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:350) at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.receiveMessages(SimpleCanalConnector.java:344) ~[classes/:na] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:315) ~[classes/:na] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:287) ~[classes/:na] at com.alibaba.otter.canal.example.AbstractCanalClientTest.process(AbstractCanalClientTest.java:110) ~[classes/:na] at com.alibaba.otter.canal.example.AbstractCanalClientTest$2.run(AbstractCanalClientTest.java:76) [classes/:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171] process error!com.alibaba.otter.canal.protocol.exception.CanalClientException: failed to subscribe with reason: something goes wrong with channel:[id: 0x1fda2da3 /192.168.4.1:9232 => /192.168.4.21:11111] exception=java.lang.NullPointerException at com.alibaba.otter.canal.store.helper.CanalEventUtils.createPosition(CanalEventUtils.java:69) at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:375) at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:36) at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.subscribe(CanalServerWithEmbedded.java:157) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:77) at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:100) at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754) at org.jboss.netty.channel.SimpleChannelHandler.messageReceived(SimpleChannelHandler.java:154) at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:100) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80) at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:444) at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80) at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:540) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261) at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:350) at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.subscribe(SimpleCanalConnector.java:241) ~[classes/:na] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.subscribe(SimpleCanalConnector.java:218) ~[classes/:na] at com.alibaba.otter.canal.example.AbstractCanalClientTest.process(AbstractCanalClientTest.java:108) ~[classes/:na] at com.alibaba.otter.canal.example.AbstractCanalClientTest$2.run(AbstractCanalClientTest.java:76) [classes/:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171] process error!com.alibaba.otter.canal.protocol.exception.CanalClientException: failed to subscribe with reason: something goes wrong with channel:[id: 0x1c0b056f /192.168.4.1:9233 => /192.168.4.21:11111] exception=java.lang.NullPointerException at com.alibaba.otter.canal.store.helper.CanalEventUtils.createPosition(CanalEventUtils.java:69) at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:375) at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:36) at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.subscribe(CanalServerWithEmbedded.java:157) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:77) at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:100) at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754) at org.jboss.netty.channel.SimpleChannelHandler.messageReceived(SimpleChannelHandler.java:154) at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:100) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80) at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:444) at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80) at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:540) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261) at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:350) at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.subscribe(SimpleCanalConnector.java:241) ~[classes/:na] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.subscribe(SimpleCanalConnector.java:218) ~[classes/:na] at com.alibaba.otter.canal.example.AbstractCanalClientTest.process(AbstractCanalClientTest.java:108) ~[classes/:na] at com.alibaba.otter.canal.example.AbstractCanalClientTest$2.run(AbstractCanalClientTest.java:76) [classes/:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171] process error!com.alibaba.otter.canal.protocol.exception.CanalClientException: failed to subscribe with reason: something goes wrong with channel:[id: 0x6db99e95 /192.168.4.1:9234 => /192.168.4.21:11111] exception=java.lang.NullPointerException at com.alibaba.otter.canal.store.helper.CanalEventUtils.createPosition(CanalEventUtils.java:69) at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:375) at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.getFirstPosition(MemoryEventStoreWithBuffer.java:36) at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.subscribe(CanalServerWithEmbedded.java:157) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:77) ``` I am using the spring file instance xml preliminary estimate should be the destination_dir meta dat problem Meta dat written in alpha5 version ``` {"clientDatas":[{"clientIdentity":{"clientId":1001 "destination":"local_mysql" "filter":""} "cursor":{"identity":{"slaveId":-1 "sourceAddress":{"address":"192.168.4.101" "port":3306}} "postion":{"gtid":"" "included":false "journalName":"mysql-bin.000005" "position":14016 "serverId":1 "timestamp":1535189721000}}}] "destination":"local_mysql"} ``` And the meta dat written by 1 1 0 ``` {"clientDatas":[{"clientIdentity":{"clientId":1001 "destination":"local_mysql" "filter":""}}] "destination":"local_mysql"} ``` If I put the alpha5 meta dat over the 1 1 0 1 1 0, there is no upstairs problem. NPE problem We pay attention to it 2018-09-03 17:21:00.046 [main] INFO com.alibaba.otter.canal.kafka.CanalServerStarter - ## the canal server is running now ...... 2018-09-03 17:21:00.048 [main] INFO com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - ## load kafka configurations 2018-09-03 17:21:00.169 [main] INFO com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - ## start the kafka workers. 2018-09-03 17:21:00.169 [main] INFO com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - ## the kafka workers is running now ...... 2018-09-03 17:21:00.169 [pool-5-thread-1] INFO com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - ## start the canal consumer: ordertest3317. 2018-09-03 17:21:53.135 [canal-instance-scan-0] WARN o.s.beans.GenericTypeAwarePropertyDescriptor - Invalid JavaBean property 'connectionCharset' being accessed! Ambiguous write methods found next to actually used [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.lang.String)]: [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.nio.charset.Charset)] 2018-09-03 17:21:53.157 [canal-instance-scan-0] INFO c.a.o.canal.deployer.monitor.SpringInstanceConfigMonitor - auto notify reload ordertest3317 successful. 2018-09-03 17:21:54.161 [destination = ordertest3317 address = yunjitest.mysql.rds.aliyuncs.com/47.98.70.247:3306 EventParser] WARN c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - prepare to find start position just show master status 2018-09-03 17:21:54.204 [destination = ordertest3317 address = yunjitest.mysql.rds.aliyuncs.com/47.98.70.247:3306 EventParser] WARN c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - find start position : EntryPosition[included=false journalName=mysql-bin.001277 position=264457500 serverId=<null> gtid=<null> timestamp=<null>] 2018-09-03 17:21:54.230 [pool-5-thread-1] INFO com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - ## the canal consumer ordertest3317 is running now ...... **2018-09-03 17:21:55.892 [pool-5-thread-1] ERROR com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - process error! java.lang.NullPointerException: null at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.calculateSize(MemoryEventStoreWithBuffer.java:555) ~[canal.store-1.1.0.jar:na] at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.doGet(MemoryEventStoreWithBuffer.java:322) ~[canal.store-1.1.0.jar:na] at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.tryGet(MemoryEventStoreWithBuffer.java:261) ~[canal.store-1.1.0.jar:na] at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getEvents(CanalServerWithEmbedded.java:478) ~[canal.server-1.1.0.jar:na] at** com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:310) ~[canal.server-1.1.0.jar:na] at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:273) ~[canal.server-1.1.0.jar:na] at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.worker(CanalKafkaStarter.java:121) [canal.kafka-1.1.0.jar:na] at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.access$000(CanalKafkaStarter.java:26) [canal.kafka-1.1.0.jar:na] at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter$1.run(CanalKafkaStarter.java:70) [canal.kafka-1.1.0.jar:na] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_181] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_181] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_181] 2018-09-03 17:21:55.896 [pool-5-thread-1] INFO com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - ## the canal consumer ordertest3317 is running now ...... 2018-09-03 17:21:55.896 [pool-5-thread-1] ERROR com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - process error! java.lang.NullPointerException: null at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.calculateSize(MemoryEventStoreWithBuffer.java:555) ~[canal.store-1.1.0.jar:na] at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.doGet(MemoryEventStoreWithBuffer.java:322) ~[canal.store-1.1.0.jar:na] at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.tryGet(MemoryEventStoreWithBuffer.java:261) ~[canal.store-1.1.0.jar:na] at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getEvents(CanalServerWithEmbedded.java:478) ~[canal.server-1.1.0.jar:na] at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:310) ~[canal.server-1.1.0.jar:na] at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:273) ~[canal.server-1.1.0.jar:na] at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.worker(CanalKafkaStarter.java:121) [canal.kafka-1.1.0.jar:na] at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter.access$000(CanalKafkaStarter.java:26) [canal.kafka-1.1.0.jar:na] at com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter$1.run(CanalKafkaStarter.java:70) [canal.kafka-1.1.0.jar:na] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_181] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_181] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_181] Version 1 1 0 spring.xml /default-instance.xml I also encountered an infinite NPE error here. The key log of the error is shown in the bold section above. server After the last debugging, there was more than 1 week without using this startup. I reported the error. I cleared the zk and restarted. at com.alibaba.otter.canal.store.helper.CanalEventUtils.createPosition(CanalEventUtils.java:69) I have seen this error but I can&#39;t find the corresponding log. I changed the database instance to another test library to clear the zk startup. This error did not occur, then I re-exchange the database instance to the original problem. Set timestamp to the current time. I suspect that there is a problem with the binlog. The current time is set to skip the problematic binlog. The startup result is not reported. I want the error to reappear and change the timestamp back to the time before the problem. Clear zk but the error Did not appear again I want to locate the cause of this NPE error. After that, I can avoid or solve it in production. Please enlighten me this question. What is the problem? I am very worried that there will be problems after going online, and I will not be able to hold it. 2018-10-19 03:08:14.957 [pool-4-thread-1] INFO com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - ## the canal consumer example is running now ...... 2018-10-19 03:08:14.957 [pool-4-thread-1] ERROR com.alibaba.otter.canal.kafka.producer.CanalKafkaStarter - process error! java.lang.NullPointerException: null ![image](https://user-images.githubusercontent.com/14846522/47178034-9a082380-d34c-11e8-9b2a-d9079e6c5485.png) My version is canal.kafka-1.1.0 This is the error above. You try cat canal.properties Do not consult the wiki The default use on the top is my solution #canal.instance.global.spring.xml = classpath:spring/memory-instance.xml canal.instance.global.spring.xml = classpath:spring/file-instance.xml #canal.instance.global.spring.xml = classpath:spring/default-instance.xml ![image](https://user-images.githubusercontent.com/14846522/47182391-e311a500-d357-11e8-8d21-929ddd83e007.png) reopened but closed again just because Github was down yesterday which I think is MS 'stealling user's data
867,I encountered mysql The client will report an error when doing large-scale operations. com.alibaba.otter.canal.protocol.exception.CanalClientException: java.io.IOException: Broken pipe at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.ack(SimpleCanalConnector.java:339) at com.adups.canal.CanalHandler.handler(CanalHandler.java:72) at com.adups.canal.CanalClient$2.run(CanalClient.java:52) at java.lang.Thread.run(Thread.java:748) Caused by: java.io.IOException: Broken pipe at sun.nio.ch.FileDispatcherImpl.write0(Native Method) at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47) at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93) at sun.nio.ch.IOUtil.write(IOUtil.java:65) at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:471) at java.nio.channels.Channels.writeFullyImpl(Channels.java:78) at java.nio.channels.Channels.writeFully(Channels.java:98) at java.nio.channels.Channels.access$000(Channels.java:61) at java.nio.channels.Channels$1.write(Channels.java:174) at java.nio.channels.Channels$WritableByteChannelImpl.write(Channels.java:458) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.writeWithHeader(SimpleCanalConnector.java:382) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.writeWithHeader(SimpleCanalConnector.java:369) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.ack(SimpleCanalConnector.java:333) ... 3 common frames omitted The client processing speed is slower than the timeout period. Upgrade 1 1 0 Is the client ack too slow to cause a timeout error?
866,Canal deployer 1 1 0 version when listening to database changes, the server end reported abnormally 2018-08-23 22:52:32.366 [destination = example address = /106.12.14.74:3306 EventParser] WARN c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - find start position : EntryPosition[included=false journalName=mysql-bin.000004 position=5377 serverId=1 gtid=<null> timestamp=1535008711000] 2018-08-23 22:52:32.582 [destination = example address = /106.12.14.74:3306 EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address /106.12.14.74:3306 has an error retrying. caused by java.lang.IllegalArgumentException: null at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1314) ~[na:1.8.0_181] at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1237) ~[na:1.8.0_181] at java.util.concurrent.Executors.newFixedThreadPool(Executors.java:151) ~[na:1.8.0_181] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor.start(MysqlMultiStageCoprocessor.java:84) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:238) ~[canal.parse-1.1.0.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_181] 2018-08-23 22:52:32.582 [destination = example address = /106.12.14.74:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.lang.IllegalArgumentException at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1314) at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1237) at java.util.concurrent.Executors.newFixedThreadPool(Executors.java:151) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor.start(MysqlMultiStageCoprocessor.java:84) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:238) at java.lang.Thread.run(Thread.java:748) ] I have encountered the same problem, I don’t know if the landlord has solved it. I was still sending emails to the development team. He said to see if the parameter configuration has been modified in the canal properties and I have not modified it. Still waiting for him to reply --- from Dcein. On August 23, 2018 17:23，platypus0127<notifications@github.com> Write I have encountered the same problem, I don’t know if the landlord has solved it. — You are receiving this because you authored the thread. Reply to this email directly view it on GitHub or mute the thread. Asking if the problem is running in a single-core environment, there may be a bug. Change the canal instance parser parallel to false. @lcybo Thank you for your success. If you have this in a multi-core environment, you don’t have to set it up. Multi-core Thanks for yelling if you can monitor multiple mysql server changes, you need to configure multiple canal instance standby address = Is the monitored mysql address still configured with multiple canal instance master address? On August 23, 2018 18:20，lcybo<notifications@github.com> Write Multi-core — You are receiving this because you authored the thread. Reply to this email directly view it on GitHub or mute the thread. Daxie Hello, I really bothered you. I have time to help me solve the problem. I have opened the row mode in our linux mysql configuration file and let the operation manager open the canal related operation permission. However, after checking, the new example log still reports an error. Give trouble and give pointers 2018-08-27 18:27:16.849 [destination = example address = /192.168.100.249:3306 EventParser] WARN c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - prepare to find start position just show master status 2018-08-27 18:27:16.851 [destination = example address = /192.168.100.249:3306 EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address /192.168.100.249:3306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: command : 'show master status' has an error! pls check. you need (at least one of) the SUPER REPLICATION CLIENT privilege(s) for this operation 2018-08-27 18:27:16.851 [destination = example address = /192.168.100.249:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: command : 'show master status' has an error! pls check. you need (at least one of) the SUPER REPLICATION CLIENT privilege(s) for this operation ] Thank you for your enlightenment. | | Dcein520 | | Mailbox Dcein520 163 com | Signature by NetEase Mailbox Master custom made On August 23, 2018 18:26，Dcein520 Write Thanks for yelling if you can monitor multiple mysql server changes, you need to configure multiple canal instance standby address = Is the monitored mysql address still configured with multiple canal instance master address? On August 23, 2018 18:20，lcybo<notifications@github.com> Write Multi-core — You are receiving this because you authored the thread. Reply to this email directly view it on GitHub or mute the thread. you need (at least one of) the SUPER REPLICATION CLIENT privilege(s) for this Check mysql grant > Daxie Hello, I really bothered you. I have time to help me solve the problem. I have opened the row mode in our linux mysql configuration file and let the operation manager open the canal related operation permission. However, after checking, the new example log still reports an error. Give trouble and give pointers 2018-08-27 18:27:16.849 [destination = example address = /192.168.100.249:3306 EventParser] WARN c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - prepare to find start position just show master status 2018-08-27 18:27:16.851 [destination = example address = /192.168.100.249:3306 EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address /192.168.100.249:3306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: command : 'show master status' has an error! pls check. you need (at least one of) the SUPER REPLICATION CLIENT privilege(s) for this operation 2018-08-27 18:27:16.851 [destination = example address = /192.168.100.249:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: command : 'show master status' has an error! pls check. you need (at least one of) the SUPER REPLICATION CLIENT privilege(s) for this operation ] Thank you for your enlightenment. | | Dcein520 | | Mailbox Dcein520 163 com | Signature by NetEase Mailbox Master custom made On August 23, 2018 18:26，Dcein520 Write Thanks for yelling if you can monitor multiple mysql server changes, you need to configure multiple canal instance standby address = Is the monitored mysql address still configured with multiple canal instance master address? On August 23, 2018 18:20，lcybo<notifications@github.com> Write Multi-core — You are receiving this because you authored the thread. Reply to this email directly view it on GitHub or mute the thread. Recommended in example instance properties Modify inside canal.instance.filter.regex=.*\\..* Only monitor the database of your own instance I found that even when the rds is used, the superuser will prompt insufficient permissions. Estimated insufficient built-in library permissions Rds super account still has no permissions on the mysql library part of the table need to filter out the mysql library
865,fix #849: HBase data synchronization adaptation HBase data synchronization external adapter will be packaged under client launcher tks
864,canal-1.1.0 BioSocketChannel Timeout `2018-08-21 05:51:48.895 [destination = zaful address = /127.0.0.1:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position {"identity":{"slaveId":-1 "sourceAddress":{"address":"127.0.0.1" "port":3306}} "postion":{"gtid":"" "included":false "journalName":"binlog.002535" "position":157229530 "serverId":97153 "timestamp":1534833018000}} 2018-08-21 05:51:48.895 [destination = zaful address = /127.0.0.1:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=binlog.002535 position=157229530 serverId=97153 gtid= timestamp=1534833018000] 2018-08-21 05:52:36.469 [destination = zaful address = /127.0.0.1:3306 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - Socket timeout expired closing connection java.net.SocketTimeoutException: Timeout occurred failed to read 9471 bytes in 25000 milliseconds. at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.read(BioSocketChannel.java:123) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:174) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:85) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:206) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:240) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_91] 2018-08-21 05:52:36.470 [destination = zaful address = /127.0.0.1:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address 127.0.0.1/127.0.0.1:3306 has an error retrying. **caused by _### **### > java.net.SocketTimeoutException: Timeout occurred failed to read 9471 bytes in 25000 milliseconds **_.** at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.read(BioSocketChannel.java:123) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:174) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:85) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:206) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:240) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_91] 2018-08-21 05:52:36.470 [destination = zaful address = /127.0.0.1:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:zaful[java.net.SocketTimeoutException: Timeout occurred failed to read 9471 bytes in 25000 milliseconds. at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.read(BioSocketChannel.java:123) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:174) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:85) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:206) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:240) at java.lang.Thread.run(Thread.java:745) ]` Program startup After running for a period of time, I have been reporting this error. After each fetch timeout, I will kill the connection with Mysql and then try again and then timeout. This is the endless loop restart. It is useless, but after deleting the meta dat, it can run normally for a while. Will report a timeout error I really can&#39;t understand why reading a few kb of data can&#39;t be read for 25s without a lot of binlog files being consumed. Do you have colleagues who have the same problem? Try 1 1 0 version @fangchunsheng If your environment can often reproduce, it is recommended BioSocketChannel.read(BioSocketChannel.java:123) Is there a debug inside? input.read(data off + n len - n); This call is really timeout PS: Looked at the openjdk implementation timeout through poll or select to see if the return value is 0, so even if only 1 byte is read, it does not count timeout @agapple Sorry The typo was written as 2 1 1 The actual use is 1 1 0, but I will not have this situation when I switch to 1 0 25 @lcybo ok let me try However, we are not using openjdk online. Thank you I read 4 bytes here will timeout start example will also report this error java.net.SocketTimeoutException: Timeout occurred failed to read 4 bytes in 25000 milliseconds. at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.read(BioSocketChannel.java:123) ~[canal.parse.driver-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:174) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:77) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:213) [canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:248) [canal.parse-1.1.0.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.7.0_191] 2018-08-22 05:24:35.190 [destination = centos1 address = /127.0.0.1:3306 EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address /127.0.0.1:3306 has an error retrying. caused by java.net.SocketTimeoutException: Timeout occurred failed to read 4 bytes in 25000 milliseconds. at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.read(BioSocketChannel.java:123) ~[canal.parse.driver-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:174) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:77) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:213) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:248) ~[canal.parse-1.1.0.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.7.0_191] 2018-08-22 05:24:35.194 [destination = centos1 address = /127.0.0.1:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:centos1[java.net.SocketTimeoutException: Timeout occurred failed to read 4 bytes in 25000 milliseconds. at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.read(BioSocketChannel.java:123) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:174) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:77) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:213) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:248) at java.lang.Thread.run(Thread.java:748) @nerverbe Yes I will have this situation on my side. That is, 4 bytes will also report an error. I looked at the code and should read the header of his protocol. It will be 4 bytes. I also tried to change 25000ms to 75000ms but the same error. If you confirm that the socketInputStream read call has a timeout, you can consider the network problem. View dump Whether the network card used for connection is occupied or not @lcybo I use canal 1 1 0 The test case of the dump inside goes to the line and runs through the same binglog file and then the same position. There is no problem when running, so it should not be a network cause. @fangchunsheng Is the test case you said is MysqlDumpTest? The MysqlEventParser in the test case is the socketChannel used by the parser on the line. It is also consistent. It seems that we should also consider the factors outside of MysqlEventParser. @agapple 。
863,Set the master in the program Position error I am direct extends MysqlEventParser ```java /** * Query the current binlog location * @param mysqlConnection * @return */ private EntryPosition findEndPosition(MysqlConnection mysqlConnection) { try { // ... EntryPosition endPosition = new EntryPosition( "my-bin.000001" Long.valueOf(1L)); return endPosition; ``` Direct error reporting ```cmd 18:06:43.222 [destination = example address = /117.107.241.79:3306 EventParser] INFO c.a.o.c.p.d.mysql.MysqlConnector - KILL DUMP 1110 failure java.io.IOException: ErrorPacket [errorNumber=1094 fieldCount=-1 message=Unknown thread id: 1110 sqlState=HY000 sqlStateMarker=#] with command: KILL CONNECTION 1110 at com.alibaba.otter.canal.parse.driver.mysql.MysqlUpdateExecutor.update(MysqlUpdateExecutor.java:49) ~[classes/:na] at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.disconnect(MysqlConnector.java:107) ~[classes/:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.disconnect(MysqlConnection.java:93) [classes/:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:309) [classes/:na] ``` I am too low, it should start from 4L
862,kafka Version wiki configuration instructions are unknown The deploy version of canal_kafka configures kafka xml according to the wiki configuration method. Other reference canal independent configuration kafka Console consumption does not output canal here and there is no log feedback. Do you need special configuration? Canal_server is not directly started from the target canal bin under the deploy module, but from the target canal bin under the kafka module, after starting the server, observe whether there is data sent to kafka. The file directory in the release is just the same. When the canal is started, there is kafka. workers is running now Just don&#39;t output to kafka Client can&#39;t see the data The same question asks how to solve it or not, and there is no log output. Kafka is not able to consume data. Reference documentation https github com alibaba canal wiki Canal Kafka RocketMQ QuickStart Modify and feedback if the document is not clear
861,Client The datasource configuration item of the QPS indicator is changed from Prometheus to the datasource parameter. After the upgrade of Canal to 1 1 0, the monitoring is found in the monitoring page of Grafana. The reason why QPS&#39;s panel is not available is because of Canal The profile panel&#39;s configuration file Canal_instances_tmpl json is named Client The datasource configuration item of the QPS panel is written as a fixed value. Prometheus is modified to take the datasource parameter and the panel can be displayed normally. [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=861) <br/>All committers have signed the CLA. [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=861) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=861) before we can accept your contribution.<br/><hr/>**lixiang** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=861) it.</sub> Thank you
860,How to share zk canal in cluster mode Client side How to specify zk? An article is provided with the following method Production scene HA mode, such as using ZK as a service management, here at least specify the majority ZK Node&#39;s IP list If you have multiple Canals Cluster shares ZK, then each Canal also needs to use a unique rootpath canal.zkServers = 10.0.1.21:2818 10.0.1.22 10.0.2.21:2818/canal/g1 My own machine test canal.zkServers = localhost:2181/canal/g1 Start directly and report an error Something goes wrong when starting up the canal Server: java.lang.IllegalArgumentException: Path length must be > 0 If native support is not supported, it is only possible to isolate multiple canals by deploying multiple sets of zk. server Cluster? /canal/g1 Do not bring this I also encountered this problem. The current design looks like only one set of zk supports a set of canal server HA cluster Same set of canal Can the server&#39;s registration path in zk be modified? Canal itself supports cluster mode in A canal server After the startup is completed, there will be information about the node B in the running of the zk destination. canal If the server is started with A canal The destination of the server has the same name. There will be information about the node in the cluster of the zk destination directory. Since A is already running, this node will not pull the binlog only when A canal server After hanging, ZK will notify B to obtain the binlog running information and change to B. canal The information of A in server cluster will be gone. The client side is directly connected to the zk and destination of the corresponding destination. Name can be used to implement HA through zk scheduling when there are multiple clients. The problem has been solved before asking this question is unfamiliar with canal
859,Can you provide some related internal design documents for 1 1 0? Can you provide some related internal design documents for 1 1 0? Are in the wiki Looks like the old wiki No 1 1 0 Such as prometheus https://github.com/alibaba/canal/wiki Here Thank you very much
858,The tableMetaCache property in LogEventConvert java is null in 1 1 0 When is tableMetaCache initialized? Specific reproduction method And have you modified your code yourself? Changed to the code Direct inheritance ```java extends MysqlEventParser ``` Thanks so that tableMetaCache is initialized.
857,Add the total entrance of the client launcher module external data landing adapter The total entry of the external data landing adapter can be adapted to the data synchronization of HBase and ES.
856,ErrotCode:400 canal 1.0.26-SNAPSHOT-2 2018-08-20 09:46:27.050 [New I/O server worker #1-1] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400 Caused by : something goes wrong with channel:[id: 0x2ae8ccbc /10.31.152.38:34114 => /10.31.152.38:11111] exception=com.alibaba.otter.canal.server.exception.CanalServerException: ack error clientId:1001 batchId:36171 is not exist please check 2018-08-20 09:48:12.626 [New I/O server worker #1-3] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400 Caused by : something goes wrong with channel:[id: 0x568191e2 /10.31.152.38:34184 => /10.31.152.38:11111] exception=com.alibaba.otter.canal.server.exception.CanalServerException: ack error clientId:1001 batchId:36172 is not exist please check 2018-08-20 09:50:13.754 [New I/O server worker #1-1] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400 Caused by : something goes wrong with channel:[id: 0x4176eecb /10.31.152.38:34258 => /10.31.152.38:11111] exception=com.alibaba.otter.canal.server.exception.CanalServerException: ack error clientId:1001 batchId:36173 is not exist please check 2018-08-20 09:52:11.962 [New I/O server worker #1-3] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400 Caused by : something goes wrong with channel:[id: 0x49e5e3d0 /10.31.152.38:34332 => /10.31.152.38:11111] exception=com.alibaba.otter.canal.server.exception.CanalServerException: ack error clientId:1001 batchId:36174 is not exist please check 2018-08-20 09:54:03.646 [New I/O server worker #1-1] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400 Caused by : something goes wrong with channel:[id: 0x74b1d81d /10.31.152.38:34400 => /10.31.152.38:11111] exception=com.alibaba.otter.canal.server.exception.CanalServerException: ack error clientId:1001 batchId:36175 is not exist please check 2018-08-20 09:55:57.774 [New I/O server worker #1-3] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400 Caused by : something goes wrong with channel:[id: 0x2b72e577 /10.31.152.38:34460 => /10.31.152.38:11111] exception=com.alibaba.otter.canal.server.exception.CanalServerException: ack error clientId:1001 batchId:36176 is not exist please check 2018-08-20 09:57:52.922 [New I/O server worker #1-1] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400 Caused by : something goes wrong with channel:[id: 0x61134480 /10.31.152.38:34530 => /10.31.152.38:11111] exception=com.alibaba.otter.canal.server.exception.CanalServerException: ack error clientId:1001 batchId:36177 is not exist please check The server-side instance must have a rollback or restart behavior. Re-subscibe get ack according to the example project.
855,Remove TPS(events) in tmpl. The display layer first removes TPS events
854,Improvement based on comments This is basically the same as the comments. Delay is divided into master put get Ack carefully considered or retained the master server and mysql master 的delay  master Delay and put get The main difference between ack is that master Delay focuses on server and mysql when mysql is in idle state without delta Binlog can also be refreshed by heartbeat and put get Ic, especially the latter two are biased towards the client delay。 Documentation updated later I understand the master delay Mainly refers to the difference between the last time the binlog was received and the current time. For no binlog Update the timestamp of the last received data via heartbeat ? "expr": "rate(canal_instance_store_consume_seq{destination=~\"$destination\"}[2m])" This binlog Events TPS calculation In fact, we count the binlog that enters the eventStore. Not the original mysql binlog count For example Mysql DML will have a TableLogEvent WriteLogEvent two But this consume_seq will only record a WriteLogEvent 1. master Delay pair of masters passing the current 15s heartbeat。 2. I understand that TableLogEvent is a table Map metadata, that TPS events doesn’t seem to make much sense. I took him to thx.
853,Fixed 48-year delay when reading without execTime at startup 1. Fix bug 2. Add template 3. Update some maps
852,The fields and values ​​that canal resolve to do not match Daxie to see 呗 ![image](https://user-images.githubusercontent.com/18712087/44260217-51e25d00-a246-11e8-9622-32655791da82.png) Rowchange taken in this row Inside you can see the parsed fields Value Does not match the original library or even the type is not the original library int After parsing is date Did the ddl change? @wingerx For example Original table There is an insert but the canal is not consumed. If the original table has done ddl, then the data before canal consumption will happen. Yes, this situation can be turned on to avoid tsdb to avoid similar problems. . What does tsdb mean? Don&#39;t understand @wingerx I remember this before, and I haven’t seen this problem since I changed the table structure.
851,canal sever-client heartbeat The Idle detection on the server side only detects the blocking time of the Socket read/write channel. After the client and server subscription relationship is established, CanalConnector does not provide a pure heartbeat detection method. Only the get request can be sent to the server to prove that it is alive. If the consumption speed is too slow, the server will close the Socket connection. Can I add a simple heartbeat detection implementation? Let&#39;s adjust the timeout first in the short term.
850,The alter statement cannot be parsed Normal mysql Alter statement server side parsing error Can&#39;t skip the temporary solution that I think of now is to manually execute in the target library and then move the offset back 2018-08-14 16:53:22.500 [destination =xxxx address = /xxxx EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : ALTER TABLE `loan_withdraw_record` ADD COLUMN `remark` varchar(255) DEFAULT NULL COMMENT Remarks AFTER `is_remind_limit` ALGORITHM=inplace LOCK=NONE com.alibaba.fastsql.sql.parser.ParserException: syntax error expect TABLES or TABLE actual EQ pos 143 line 1 column 143 token = at com.alibaba.fastsql.sql.dialect.mysql.parser.MySqlStatementParser.parseStatementListDialect(MySqlStatementParser.java:863) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:483) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.apply(DatabaseTableMeta.java:104) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.apply(TableMetaCache.java:228) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseQueryEvent(LogEventConvert.java:265) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:126) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:68) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:345) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:187) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:154) [canal.parse-1.0.26-SNAPSHOT.jar:na] SHTERM: session timeoutotter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_51] Fastsql parsing problem
849,Canal support sync to hbase Hbase is currently one of the most widely used solutions in the big data field. It is expected to provide an ability to synchronize to hbase based on canal increments.
847,I reported a message after opening the server. dump address /127.0.0.1:3306 has an error What is the reason for this? [destination = example address = /127.0.0.1:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /127.0.0.1:3306 has an error retrying. caused by java.lang.IllegalArgumentException: null at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1314) ~[na:1.8.0_144] at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1237) ~[na:1.8.0_144] at java.util.concurrent.Executors.newFixedThreadPool(Executors.java:151) ~[na:1.8.0_144] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor.start(MysqlMultiStageCoprocessor.java:84) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:230) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144] 2018-08-16 12:21:45.933 [destination = example address = /127.0.0.1:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.lang.IllegalArgumentException at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1314) at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1237) at java.util.concurrent.Executors.newFixedThreadPool(Executors.java:151) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor.start(MysqlMultiStageCoprocessor.java:84) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:230) at java.lang.Thread.run(Thread.java:748) Is 1 0 26 The version of alpha is wrong Is this version Alpha several versions of alpha3 or alpha5 don&#39;t use alpha4 alpha4 problem alpha5 This version reported an error alpha5 Encountered the same problem canal.instance.parser.parallelThreadSize Need to set You were at the time of the canal instance parser parallelThreadSize This parameter is how much the configuration is. I commented out this line with the default value of the original configuration document. Also encountered this problem for help The latest 1 1 1 version has been resolved
846,V1 0 26 alpha5 client occasionally frequently reports error no alive canal server com.alibaba.otter.canal.protocol.exception.CanalClientException:no alive canal server at com.alibaba.otter.canal.client.impl.ClusterNodeAccessStrategy.nextNode(ClusterNodeAccessStrategy.java:76)~ The client occasionally reports the above error frequently. Check in Zookeeper get /otter/canal/destinations/example/running {"active":true "address":"xxx.xxx.xxx.xxx" "cid":1} It is normal for the server to report no error, but the client will report an error frequently. If the client self-recovers Check out the canal at the time Whether the server has switched or exited @agapple How to check canal Server? @agapple Check in Zookeeper get /otter/canal/destinations/example/running result Node does not exist : /otter/canal/destinations/example/running get /otter/canal/destinations/sample/running result Node does not exist : /otter/canal/destinations/sample/running This situation is a bit more frequent, although you can retry successfully, but this problem is not canal The server has a problem. Current scene Two instances of example sample are exactly the same in both instances. There is no error in the log log in the canal deployer, but the node information is not detected in the Zookeeper. Appears no alive canal Server must be canal The server has an exit behavior that needs to check the server&#39;s log.
845,Add header size for each packet. Client traffic is added to the header length MD directly to the wiki
844,Plus if (logger.isDebugEnabled()) Prevent message toString from being run every time. tks Mail can contact me jianghang115@gmail.com Invite you to join canal Collaborators Co-constructed together @rewerma
843,canal + rocksmq Can you ensure that the data is not lost? If canal pushes data to rocksmq, rmq crashes and mq does not flush and lose data in time. canal + Does rocksmq have the possibility of losing binlog? If there is no flush before the rmq crash, then the MQ design problem will definitely lose data. You need to manually return the data to supplement the data. ok @agapple
842,When will the official version of v1 0 26 be released? The v1 1 0 alapa version is still in the release version of v1 0 26 1 0 26 directly converted to 1 1 0 It is expected that this two weeks will be officially released.
841,Create canal prometheus docs. The initial document and so on will be put into the wiki. Every night, I will update part of it to make sure I can get it this week. As the current work and canal have been decoupled, I can only go home at night and take time to make progress. Please forgive me. Thank you. You can write directly to the wiki. Here https github com alibaba canal wiki
840,Upgrade Kafka Version to 2.11_1.1.1 awesome
839,Debug Memory Leak [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=839) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=839) before we can accept your contribution.<br/>**1** out of **2** committers have signed the CLA.<br/><br/>:white_check_mark: payonxp<br/>:x: yfpeng<br/><hr/>**yfpeng** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=839) it.</sub>
838,Canal 1 0 26 preview 5 Embedded Server cannot get Entries The previous 1 0 25 version is ok https://github.com/alibaba/canal/releases/tag/canal-1.0.26-preview-5 1.0.26 Optimized for performance on Entries Modifications have been made Embedded server 可以参考https github com alibaba canal blob master client src main java com alibaba otter canal client impl SimpleCanalConnector java L332 How data is processed Enn is the new version of the Message information is placed in the List ByteString rawEntries = Message getRawEntries and then get the complete information through result addEntry Entry parseFrom byteString Thank you
837,MemoryEventStoreWithBuffer fixes memory leak increase Max batch size Fixed a memory leak in MemoryEventStoreWithBuffer public void cleanUntil(Position position) throws CanalStoreException line 430: for (long next = sequence + 1; next <= maxSequence; next++) The maximum Batch length parameter is added to the CanalEventStore interface so that the otter can control the maximum length of the acquired Batch. The Dubbo request exceeds Max in the RPC usage scenario. Payload problem [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=837) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=837) before we can accept your contribution.<br/>**1** out of **2** committers have signed the CLA.<br/><br/>:white_check_mark: payonxp<br/>:x: yfpeng<br/><hr/>**yfpeng** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=837) it.</sub> [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=837) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=837) before we can accept your contribution.<br/>**1** out of **2** committers have signed the CLA.<br/><br/>:white_check_mark: payonxp<br/>:x: yfpeng<br/><hr/>**yfpeng** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=837) it.</sub> 1. The code first merges the master. Invalid branch changes are a bit more 2. otter Payload problem canal The master code can accurately control the memory size of the batch and does not require an additional batchSize to control the payload of the next service. The auto mode can determine whether to take the rpc or HTTP file download according to the data size. #839 Our business is not suitable for HTTP file downloads, so it is based on RPC. PayLoad oversized problem has been remodeled Let&#39;s take a look at the memory leak.
836,merge [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=836) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=836) before we can accept your contribution.<br/>**1** out of **2** committers have signed the CLA.<br/><br/>:white_check_mark: payonxp<br/>:x: yfpeng<br/><hr/>**yfpeng** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=836) it.</sub>
835,MemoryEventStoreWithBuffer fixes memory leak increase Max batch size Fixed a memory leak in MemoryEventStoreWithBuffer public void cleanUntil(Position position) throws CanalStoreException line 430: for (long next = sequence + 1; next <= maxSequence; next++) The maximum Batch length parameter is added to the CanalEventStore interface so that the otter can control the maximum length of the acquired Batch. The Dubbo request exceeds Max in the RPC usage scenario. Payload problem [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=835) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=835) before we can accept your contribution.<br/>**1** out of **2** committers have signed the CLA.<br/><br/>:white_check_mark: payonxp<br/>:x: yfpeng<br/><hr/>**yfpeng** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=835) it.</sub> [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=835) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=835) before we can accept your contribution.<br/>**1** out of **2** committers have signed the CLA.<br/><br/>:white_check_mark: payonxp<br/>:x: yfpeng<br/><hr/>**yfpeng** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=835) it.</sub>
834,About master - slave of bin-log problem The document states that Canal Is by simulating yourself as a “slave” Go and master Interactive bin-log Subscription and resolution If slave Also opened the bin log Canal Can it be slave Interact to achieve related functions The reason for this is to consider the current US master The physical machine performance is poor and then there is no previous bin-log Mode is not ROW, I want to ask if I can slave open Bin log then Canal Pass directly slave Subscribe and parse Thank you Yes, for the canal, your slave is the master. @lcybo thx
833,Store adds bufferSize metrics Server port configurable Such as the title
832,Will Daxie support conditional screening later? For example, to synchronize the data of a table, but I don&#39;t want to synchronize all the past. I want to do some filtering similar to where After defining some conditions that meet the conditions, we will synchronize the past. Write code in the client to do data filtering
831,Is there any plan to upgrade Google? Protocol Buffer? Want to be a client connection to c CanalServer。 You can submit a PR Can do upgrades got it
830,Entry header in executeTime Fields can achieve millisecond precision Binst inside timestamp 4 bytes precision is seconds
829,I would like to ask the example of the instance properties and rds_properties inside the database configuration is different. I remember that 1 0 24 is the rds_properties configured in the instance properties. Why is this canal instance tsdb dbUsername? Oh, help me, my brother. This new version will be replaced by a new program. : https://github.com/alibaba/canal/issues/727
828,canal Performance index collection List of indicators canal_instance_traffic_delay Delay with the master unit millisecond precision milliseconds canal_instance_transactions Number of transactions received canal_instance_row_events Received rowdata Number of entries canal_instance_rows_counter Number of rows changed canal_instance Instance list canal_instance_subscriptions Instance subscription number canal_instance_publish_blocking_time Pushlish blocking time unit millisecond precision nanosecond in parallel mode canal_instance_received_binlog_bytes Number of binlog bytes received canal_instance_parser_mode Whether parser is parallel mode canal_instance_client_packets client Instance request packets canal_instance_client_bytes To the client Instances send bytes canal_instance_client_empty_batches To the client Instances send empty packets canal_instance_client_request_error client Number of array error requests canal_instance_client_request_latency Request latency canal_instance_sink_blocking_time Sink thread put Store blocking time unit millisecond precision nanoseconds canal_instance_store_produce_seq Production serial number canal_instance_store_consume_seq The serial number of the consumption entry is used in conjunction with canal_instance_store_produce_seq canal_instance_store Store basic information canal_instance_store_produce_mem The production entry takes up the total amount of mem canal_instance_store_consume_mem The consumption entry accounts for the total amount of mem To be added The httpserver port is currently 11112 At night and the rds commit crashed so some conflict files merge 2 6 1 version for protoc Follow-up will add Prometheus expression visualization related documents test Cases, parameters such as ports can be configured, etc. Please review it. Thank you. Great ask a question METRICS_OPTS This comment Is the current monitoring information obtained by JMX or HTTP? ? I suggest you to monitor the concept of this indicator. And the case based on the indicator to locate the problem can list Afterwards, everyone can share it. The submission of rds is relatively large There are also crashes with some of my submissions. Need to pay attention to see if it is covered Submission of this RDS Mainly to solve the RDS on Alibaba Cloud Binlog deleted question Will automatically connect to the oss binlog with mysql binlog. With this mechanism In the future, docking all kinds of cloud RDS is basically barrier-free. METRICS_OPTS Originally used for javaagent is now removed Currently, the monitoring information is obtained by http. - job_name: 'canal' static_configs: - targets: ['localhost:11112'] The port or other configuration will be configured into the file later. The concept of the indicator and the case will be written together with the expression an MD and then commit including some develop Guide and visual content Like praise
827,Merge master updates [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=827) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=827) before we can accept your contribution.<br/>**2** out of **3** committers have signed the CLA.<br/><br/>:white_check_mark: agapple<br/>:white_check_mark: lcybo<br/>:x: lin848497337<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=827) it.</sub>
826,Metrics support Increase heartbeat refresh delay increase MHEARTBEAT event type。 Use version 2.6.1 Protoc compile CanalEntry Change the number of rows using the rowsCount in the pair [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=826) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=826) before we can accept your contribution.<br/>**4** out of **5** committers have signed the CLA.<br/><br/>:white_check_mark: payonxp<br/>:white_check_mark: agapple<br/>:white_check_mark: wingerx<br/>:white_check_mark: lcybo<br/>:x: yfpeng<br/><hr/>**yfpeng** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=826) it.</sub>
825,Remove dirty test case Source below com alibaba otter canal parse inbound mysql tablemeta has deleted the test The case is still having problems compiling tks
824,How does the client control the insert in the same thing? Update consumption order How does the client control the insert in the same thing? Update consumption order Sometimes the client often gets the update event and gets the insert event. The order that the client gets is the order of the binlog records in the database.
823,TableMetaCache.parseTableMetaByDesc Parsing desc table the result of NullPointer Mysql version 5 6 desc Table error in packet resolution public static final String COLUMN_NAME = "COLUMN_NAME"; public static final String COLUMN_TYPE = "COLUMN_TYPE"; public static final String IS_NULLABLE = "IS_NULLABLE"; public static final String COLUMN_KEY = "COLUMN_KEY"; public static final String COLUMN_DEFAULT = "COLUMN_DEFAULT"; public static final String EXTRA = "EXTRA"; Should be changed public static final String COLUMN_NAME = "Field"; public static final String COLUMN_TYPE = "Type"; public static final String IS_NULLABLE = "Null"; public static final String COLUMN_KEY = "Key"; public static final String COLUMN_DEFAULT = "Default"; public static final String EXTRA = "Extra"; Otherwise, I will report NullPointer in this line. meta.setColumnName(packet.getFieldValues().get(nameMaps.get(COLUMN_NAME) + i * size).intern()); mysql> select @@version ; +--------------------+ | @@version | +--------------------+ | 5.6.28-cdb2016-log | +--------------------+ 1 row in set (0.01 sec) mysql> desc test.test ; +-------+---------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +-------+---------+------+-----+---------+-------+ | id | int(11) | YES | | NULL | | +-------+---------+------+-----+---------+-------+ 1 row in set (0.01 sec) ![image](https://user-images.githubusercontent.com/33280738/43878223-88545b3a-9bd0-11e8-9f31-6dccbc75e37a.png) Take the originalName instead of the name Is NPE the result of the test? ![image](https://user-images.githubusercontent.com/7187362/43878339-234ac804-9bd1-11e8-99de-a4646bdb9c61.png) Buddy, I am here to put the packet out. Is this the inconsistency between my mysql version and your side? System.out.println(packet); UT can be measured possible https://dev.mysql.com/doc/refman/8.0/en/columns-table.html ![image](https://user-images.githubusercontent.com/33280738/43881977-9a81a8ac-9be0-11e8-9126-c6711b98eb16.png) ![image](https://user-images.githubusercontent.com/33280738/43881992-ad3b1fbe-9be0-11e8-97e7-11bad3d560c9.png) I used 5 6 28 to measure the same. The version of your side is obtained from where. Brother, I got the wrong mysqlConnection and connected to another database. The amount is a version of 8 0 11 I am embarrassed to take up your time. ok. The 80 version is currently not supported. Excuse me, the issue is closed.
822,fix tableMetaStorage NPE
821,NPE：MysqlEventParser TableMetaCacheWithStorage When the tableMetaStorageFactory is NULL, the storage value is null. Subsequent to the NullPointerException at com.alibaba.otter.canal.parse.inbound.mysql.tablemeta.TableMetaCacheWithStorage.<init>(TableMetaCacheWithStorage.java:21) ![image](https://user-images.githubusercontent.com/5847660/43820893-1ccadbde-9b1a-11e8-9979-2ab94346dfdb.png) #822 See my latest master commit has removed the tableMetaStorageFactory ok
820,Canal is supported to MySQL5 7 18? 5 7 18 or higher version is completely unsupported or there is a problem with the support of some features, there are no children&#39;s shoes used in the higher version. We are now 5.7.20 No problems found on the use
819,canal Performance monitoring changes Code I first merge to the development branch metrics_support Recently, there are some other things to be busy. The progress is a bit slow. Sorry. The current progress is a simple debug. After going home for a few days, I will try my best to test it. TOTO TODO List memo 1. Extend the EntryProtocol to increase the master&#39;s heartbeat to refresh the delay in the idle state. 2. Extend the EntryProtocol to increase the rowData count in the header header, otherwise you can only get it by deserialization. @agapple The comments have been crossed out in Pair. [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=819) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=819) before we can accept your contribution.<br/>**4** out of **5** committers have signed the CLA.<br/><br/>:white_check_mark: wingerx<br/>:white_check_mark: lcybo<br/>:white_check_mark: agapple<br/>:white_check_mark: qmz<br/>:x: Chuanyi Li L<br/><hr/>**Chuanyi Li L** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=819) it.</sub>
818,Canal instance tsdb enable in HA mode. The master and slave of the database after the master-slave switch After switching, I will report the following exception. The current solution I use is to use canal instance tsdb enable false to switch normally. Canal instance tsdb enable in HA mode. The master and slave of the database after the master-slave switch After switching, I will report the following exception. The current solution I use is to use canal instance tsdb enable false to switch normally. 2018-08-07 10:44:48.772 [destination = orderfailover address = /10.8.132.135:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position by switch ::1532919362000 2018-08-07 10:44:48.780 [destination = orderfailover address = /10.8.132.135:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - Didn't find the corresponding binlog files from mysql-bin.000020 to mysql-bin.000021 2018-08-07 10:44:48.782 [destination = orderfailover address = /10.8.132.135:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /10.8.132.135:3306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: can't find start position for orderfailover 2018-08-07 10:44:48.782 [Druid-ConnectionPool-Create-1877078260] ERROR com.alibaba.druid.pool.DruidDataSource - create connection SQLException url: errorCode 0 state null java.sql.SQLException: connect error url driverClass org.h2.Driver at com.alibaba.druid.pool.DruidAbstractDataSource.createPhysicalConnection(DruidAbstractDataSource.java:1582) ~[druid-1.1.9.jar:1.1.9] at com.alibaba.druid.pool.DruidDataSource$CreateConnectionThread.run(DruidDataSource.java:2466) ~[druid-1.1.9.jar:1.1.9] 2018-08-07 10:44:48.782 [destination = orderfailover address = /10.8.132.135:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:orderfailover[com.alibaba.otter.canal.parse.exception.CanalParseException: can't find start position for orderfailover ] 2018-08-07 10:44:49.282 [Druid-ConnectionPool-Create-1877078260] ERROR com.alibaba.druid.pool.DruidDataSource - create connection SQLException url: errorCode 0 state null java.sql.SQLException: connect error url driverClass org.h2.Driver at com.alibaba.druid.pool.DruidAbstractDataSource.createPhysicalConnection(DruidAbstractDataSource.java:1582) ~[druid-1.1.9.jar:1.1.9] at com.alibaba.druid.pool.DruidDataSource$CreateConnectionThread.run(DruidDataSource.java:2466) ~[druid-1.1.9.jar:1.1.9] 2018-08-07 10:44:49.783 [Druid-ConnectionPool-Create-1877078260] ERROR com.alibaba.druid.pool.DruidDataSource - create connection SQLException url: errorCode 0 state null java.sql.SQLException: connect error url driverClass org.h2.Driver at com.alibaba.druid.pool.DruidAbstractDataSource.createPhysicalConnection(DruidAbstractDataSource.java:1582) ~[druid-1.1.9.jar:1.1.9] at com.alibaba.druid.pool.DruidDataSource$CreateConnectionThread.run(DruidDataSource.java:2466) ~[druid-1.1.9.jar:1.1.9] 2018-08-07 10:44:50.283 [Druid-ConnectionPool-Create-1877078260] ERROR com.alibaba.druid.pool.DruidDataSource - create connection SQLException url: errorCode 0 state null java.sql.SQLException: connect error url driverClass org.h2.Driver at com.alibaba.druid.pool.DruidAbstractDataSource.createPhysicalConnection(DruidAbstractDataSource.java:1582) ~[druid-1.1.9.jar:1.1.9] at com.alibaba.druid.pool.DruidDataSource$CreateConnectionThread.run(DruidDataSource.java:2466) ~[druid-1.1.9.jar:1.1.9] 2018-08-07 10:44:50.783 [Druid-ConnectionPool-Create-1877078260] ERROR com.alibaba.druid.pool.DruidDataSource - create connection SQLException url: errorCode 0 state null java.sql.SQLException: connect error url driverClass org.h2.Driver at com.alibaba.druid.pool.DruidAbstractDataSource.createPhysicalConnection(DruidAbstractDataSource.java:1582) ~[druid-1.1.9.jar:1.1.9] at com.alibaba.druid.pool.DruidDataSource$CreateConnectionThread.run(DruidDataSource.java:2466) ~[druid-1.1.9.jar:1.1.9] 2018-08-07 10:44:51.283 [Druid-ConnectionPool-Create-1877078260] ERROR com.alibaba.druid.pool.DruidDataSource - create connection SQLException url: errorCode 0 state null The main reason is that the h2 tsdb xml depends on the canal instance in the System. The destination variable defines that there is an asynchronous field operation when the active/standby switchover is empty.
817,canal-1.026-alpha4 Startup failed OS: CentOS Linux release 7.4.1708 canal : canal-1.026-alpha4 Verify no problem in the mac environment linux environment Startup failed java stack: `Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: java.lang.IllegalArgumentException: limit excceed: 44 Caused by: java.lang.IllegalArgumentException: limit excceed: 44 at com.taobao.tddl.dbsync.binlog.LogBuffer.getUint8(LogBuffer.java:235) ~[canal.parse.dbsync-1.0.26-SNAPSHOT.jar:na] at com.taobao.tddl.dbsync.binlog.event.GtidLogEvent.<init>(GtidLogEvent.java:48) ~[canal.parse.dbsync-1.0.26-SNAPSHOT.jar:na] at com.taobao.tddl.dbsync.binlog.LogDecoder.decode(LogDecoder.java:364) ~[canal.parse.dbsync-1.0.26-SNAPSHOT.jar:na] at com.taobao.tddl.dbsync.binlog.LogDecoder.decode(LogDecoder.java:110) ~[canal.parse.dbsync-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$SimpleParserStage.onEvent(MysqlMultiStageCoprocessor.java:210) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]` Known issues please download the latest alpha 5
816,fix ci: fixed compiler java 1.7 fix ci: fixed compiler java 1.7 1. Remove canal instance spring After the dependency, you need to add druid and mysql Connector dependency 2. PropertyPlaceholderConfigurer moved from canal instance spring to db tks
815,canal HA Mode view position canal server Opened two sets of HA high availability configuration as follows But zk was accidentally deleted now active canal Still receive the binlog parsing normally but can&#39;t update the position information in zk. I would like to ask zk to accidentally delete the position data of the current instance except zk. ` ################################################# ######### common argument ############# ################################################# canal.id= 1 canal.ip= canal.port= 11111 canal.zkServers=10.100.1.10:2181 10.100.1.11:2181 10.100.1.12:2181 canal.zookeeper.flush.period = 1000 canal.file.data.dir = ${canal.conf.dir} canal.file.flush.period = 1000 canal.instance.memory.buffer.size = 16384 canal.instance.memory.buffer.memunit = 1024 ` The meta log under logs will record every consumption Meta log under logs in HA mode No record is recorded when only non-HA is recorded. Is there a problem with my HA configuration?
814,schema history storage interface In order to solve Otter&#39;s backtracking data, Rowdata does not match the current schema. Provide a persistent interface to TableMeta based on timestamp matching history schema In our mysql storage implementation by Otter Manager performs database operations CanalEmbedSelector assembles Factory and puts into CanalProperties Degraded to the original TableMetaCache without the storage implementation [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=814) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=814) before we can accept your contribution.<br/>**1** out of **2** committers have signed the CLA.<br/><br/>:white_check_mark: payonxp<br/>:x: yfpeng<br/><hr/>**yfpeng** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=814) it.</sub> [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=814) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=814) before we can accept your contribution.<br/><hr/>**yfpeng** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=814) it.</sub> @payonxp Code conflicts, trouble, merge the main code first. This feature and tsdb What&#39;s the difference Looked at the next implementation is to resolve the database table structure when parsing the DDL log at that time, do a timestamp mapping record compared to TableMeta Tsdb has a problem is to get the binlog When the DDL is back-checked, the table structure at this time and the structure corresponding to the DDL at that time are inconsistent with time GAP. Can canal1 0 26 latest implementation https github com alibaba canal wiki TableMetaTSDB @agapple I didn&#39;t take a closer look at the implementation logic but from the description it feels like the function of TSDB implementation. Get the binlog When trying to call the parsing function of TSDB in DDL, it is not always going to the database every time. The main function and TSDB almost want to put the persistence function into the otter canal itself does not achieve persistence It is recommended to submit the storage capability implemented in the otter to a PR. I re-edited the support of Otter using TableMetaTSDB https://github.com/alibaba/canal/commit/9e816bc48f9955f9e2839873993cef9627c2f389 Corresponding otter commit : https://github.com/alibaba/otter/commit/7cc897131da65aab2c1aa31b7b0b8aa7a6f65745 Thx
813,fix #802: Strip MySQL from otter Related synchronization implementation as a reference for example Very much like
812,Link failure ### wiki-like home + Related information used with Alibaba&#39;s rocketMQ Connection failure ### wiki的Introduction + Binching of knowledge science mysql The second link introduced by Log is invalid + EventParser design binlog event Structure Please refer to the connection for details. Page Not Found Taobaodba&#39;s blog There are no other connections already fixed tks @wingerx It can be said that it is very efficient. give it a like Forgive my obsessive README.md The connection to RocketMQ has not changed. https://github.com/apache/rocketmq already fixed
811,change com.alibaba.fastsql version to the last on http://central.mave… Solve the bug of issue #808 [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=811) <br/>All committers have signed the CLA. [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=811) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=811) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=811) it.</sub> The latest version of fastsql will be submitted later. Ok, then deploy it as soon as possible. I will turn this off first.
810,fix bug: kafka get row data for performance tks
809,canal ack error 2018-08-03 15:17:45.414 [New I/O server worker #1-4] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x7cec5107 /10.111.61.32:47512 :> /10.111.61.32:11111] exception=java.nio.channels.ClosedChannelException at org.jboss.netty.channel.socket.nio.NioWorker.cleanUpWriteBuffer(NioWorker.java:649) at org.jboss.netty.channel.socket.nio.NioWorker.writeFromUserCode(NioWorker.java:370) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:137) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) at org.jboss.netty.channel.Channels.write(Channels.java:611) at org.jboss.netty.channel.Channels.write(Channels.java:578) at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:46) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:174) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.cleanup(ReplayingDecoder.java:542) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.channelDisconnected(ReplayingDecoder.java:450) at org.jboss.netty.channel.Channels.fireChannelDisconnected(Channels.java:360) at org.jboss.netty.channel.socket.nio.NioWorker.close(NioWorker.java:599) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:119) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) at org.jboss.netty.channel.Channels.close(Channels.java:720) at org.jboss.netty.channel.AbstractChannel.close(AbstractChannel.java:208) at org.jboss.netty.channel.ChannelFutureListener$1.operationComplete(ChannelFutureListener.java:46) at org.jboss.netty.channel.DefaultChannelFuture.notifyListener(DefaultChannelFuture.java:381) at org.jboss.netty.channel.DefaultChannelFuture.addListener(DefaultChannelFuture.java:148) at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:48) at com.alibaba.otter.canal.server.netty.NettyUtils.error(NettyUtils.java:69) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:253) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:444) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261) at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:350) at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:748) Do a retry according to the example code.
808,com.alibaba.fastsql:fastsql:jar:2.0.0_preview_520 mvn clean Error when installing [ERROR] Failed to execute goal on project canal.parse: Could not resolve dependencies for project com.alibaba.otter:canal.parse:jar:1.0.26-SNAPSHOT: Could not find artifact com.alibaba.fastsql:fastsql:jar:2.0.0_preview_520 in central (http://repo1.maven.org/maven2) -> [Help 1] How can I solve this problem? You use 2 0 0_preview_186 2 0 0_preview_520 Not open to the outside, the maven repository center is configured as http central maven org maven2 2.0.0_preview_520 In release There is a jar package inside
807,instance with Some fine-tuning of the manager mode 1 CanalParameter MetaMode adds local file mode adaptation manager mode is instance using local meta dat storage location information 2 typo correction 3 BatchMode default correction The parameter name adjustment of CanalParameter will have a history version compatibility problem. The problem of word spelling was previously found but can&#39;t be changed unless it is compatible with the old set get method. Understand that I reset the commit. The current pr is just adapted to the manager mode. You can use the FileMixedMetaManager local file to store the meta dat site data. To avoid the current manager mode, the production environment must use the zookeeper and file instance xml default configuration. The parameters correspond to the dataDir and metaFileFlushPeriod of the new parameters of CanalParameter. tks
806,Support GTID mode to get the current gtid value as the starting value by default tks
805,About position setting I would like to set the log read position is not only need to set the journal name and position in the instance proerties on the meda dat. Why is it different from the instance proerties? Which one takes effect? I am using default instance xml Zk cluster 1. After using default instance xml, the default is to manage the location information through zk. Meda dat is invalid here 2. The mode that meda dat targets is file instance xml 3. instance.properties In the locus and zk Site Relationship Default instance xml as an example * When the zk is not present in the locus, the canal is activated. The instance properties are subject to `show master status` Take the latest location information * when zk The location information in zk is subject to the record in zk So in the case of normal data synchronization, the location of zk and the location configured in instance properties are different canal It is also the ability to achieve breakpoints through it. @wingerx Hello, I tried to delete the location information on zk and let it regenerate. According to your statement, I should set the location information in Instance properties. I have set it up but there is no reason for this. 1. The configuration in Instance properties is only related to finding a location at startup. 2. The record in zk is the client side after the successful consumption of ack Time point It is not that the site information zk is configured in the instance properties to be updated to this site.
804,How can deal with blob data types in mysql Ask God to ask how to deal with the blob data type in mysql The information found on the Internet is as follows. I don&#39;t know if it is correct. Canal will binlog The value in the sequence is serialized String Format to the downstream program so Blob Formatted data is serialized into String Forced to save space IOS_8859_0 As a code, it will cause Chinese garbled in the following cases. Synchronization service JVM used UTF-8 coding BLOB Chinese characters are stored in the field AUTHOR haitaoyao Link https www jianshu com p be3f62d4dce0 Source book The copyright of the book is owned by the author. Please contact the author for authorization and indicate the source. Ask God to guide According to iso8859 1 reverse solution to bytes Thank you for guiding but I will use sqltype for longblob type fields using iso8859 1 inverse solution to bytes and then utf 8 encoding for string is still garbled This is the content of the longblob field <img width="1128" alt="canal-blob-getvalue" src="https://user-images.githubusercontent.com/28953872/43885058-677e5658-9bea-11e8-956c-edeb50a0da5b.png"> This is done using iso8859 1 Inverse solution <img width="1123" alt="iso" src="https://user-images.githubusercontent.com/28953872/43885109-8a92545a-9bea-11e8-9102-2b5a0c10ec0d.png"> My code is as follows if(column.getMysqlType().toUpperCase().equals("LONGBLOB")){ logger.info(">>>>>> is blob\n"); logger.info("\n>>>>>> column.getValue() is: " + column.getValue()); logger.info("\n\n\n\n\n\n>>>>>> new String(column.getValue().getBytes(\"iso8859-1\") \"UTF-8\") is: " + new String(column.getValue().getBytes("iso8859-1"))); } Ask the god how to fix use getValueBytes method takes the original bytes Still the claw machine used by getBytesValue can&#39;t confirm Can you tell me more about the gods? Column no getBytesValue method should be getBytesValue ``` try { if (StringUtils.containsIgnoreCase(column.getMysqlType() "BLOB") || StringUtils.containsIgnoreCase(column.getMysqlType() "BINARY")) { // get value bytes builder.append(column.getName() + " : " + new String(column.getValue().getBytes("ISO-8859-1") "UTF-8")); } else { builder.append(column.getName() + " : " + column.getValue()); } } catch (UnsupportedEncodingException e) { } ``` public java.lang.String getValue() { java.lang.Object ref = value_; if (!(ref instanceof java.lang.String)) { com.google.protobuf.ByteString bs = (com.google.protobuf.ByteString) ref; With getValue, here is already encoded with utf8 java.lang.String s = bs.toStringUtf8(); if (bs.isValidUtf8()) { value_ = s; } return s; } else { return (java.lang.String) ref; } } public com.google.protobuf.ByteString getValueBytes() { java.lang.Object ref = value_; if (ref instanceof String) { com.google.protobuf.ByteString b = com.google.protobuf.ByteString.copyFromUtf8((java.lang.String) ref); value_ = b; return b; } else { Get the original ByteString return (com.google.protobuf.ByteString) ref; } } @alexlgj Try first @agapple Right Impression in the 1 0 22 version of getValue Varbinary will lose information when you go directly to change the code @agapple @lcybo Ok, thank you for the guidance of the two great gods. I am not very good at this time. Did an experiment byte[] bs = {-1 -2}; The byte is a negative number. This depends on the encoding of your database. ByteString bstring = ByteString.copyFrom(bs); String utf = bstring.toStringUtf8(); System.out.println(utf); Lost data turned into two 65533 String iso = new String(utf.getBytes("ISO-8859-1") "UTF-8"); System.out.println(iso); There is no way to restore it into two 63 @alexlgj The coding problem is quite interesting According to the method of agapple, the content of the blob field can be successfully decoded into Chinese Beijing. Column getvalue get "image\\\":\\\"å\\\\u008C\\\\u0097äº¬\\\" \\\ new String(column.getValue().getBytes("ISO-8859-1") UTF 8 gets Image Beijing The canal I used is 1 0 24 version. In your example, there is no access to the database. Why is the byte negative? This depends on the encoding of your database. latin1 128 255 with a byte is a negative number indicates that there is a negative number in utf 8 which represents a multi-byte is a certain specification of 128 255 sequence is likely to violate these specifications become If there is a hard-coded thing, the unordered 128 255 non-utf 8 code still takes the original byteString. If the canal put in the string is the iso 8859 1 coded that I ignored, I ran away. @lcybo I understand very much, thank you very much. @lcybo Before the server-side blob type was strongly transferred to the string type, it is more reasonable to use ByteString, but now it can not be changed to the old user&#39;s blob parsing will be garbled. understanding I still have some questions about the two great gods. 1. Is it canal? Server side will mysql All fields of binlog do iso 8859 1 encoded into string 2. I don’t understand the coding at all. Can help explain why this change code can correctly parse the string stored in the blob type. String(column.getValue().getBytes("ISO-8859-1") "UTF-8")) What is the order of execution of the different encodings in the code? My understanding is canal Server does iso 8859 1 encoding on the byte array of the blob field, but iso 8859 1 encoding can not represent Chinese Beijing garbled. At this time, executing the column getValue method will enter if (!(ref instanceof java.lang.String)) { Utf8 string for ref God, I want to ask the column getValueBytes toByteArray returns the blob field original Byte array is still always utf 8 encoding or other encoded byte array. Why can&#39;t column in the canal getValueBytes toByteArray A Chinese character corresponds to 6 bytes? This is my test result Inserted blob field value ”0123abc" Print the binary array contents of the inserted string in a java program Arrays.toString(”0123abc".getBytes()) is： [48 49 50 51 97 98 99] Binary array content obtained in canal Arrays.toString(column.getValueBytes().toByteArray() is： [48 49 50 51 97 98 99] Blob field value Arrays toString 上 getBytes  is： [-28 -72 -118] Arrays.toString(column.getValueBytes().toByteArray() is: [-61 -92 -62 -72 -62 -118] Blob field value Shanghai Arrays toString 上海 getBytes  is： [-28 -72 -118 -26 -75 -73] Arrays.toString(column.getValueBytes().toByteArray() is： [-61 -92 -62 -72 -62 -118 -61 -90 -62 -75 -62 -73] 1 description from agapple god should be only blob and binary do Another problem if you use iso encoding into string then a char in the string is actually a single byte printed out is garbled but the actual data format is not lost What you mean is that although the printout is garbled, the binary data itself remains the same. Then why is new? String(column.getValue().getBytes("UTF-8") The result of UTF 8 printing is garbled. getBytes UTF 8 is equivalent to using utf 8 to encode that bunch of garbled prints, of course, garbled I just asked the students about some coding knowledge. Now I can understand the basics. Thank you, God’s patience and guidance.
803,canalConnector subscribe will block the thread and cause stop to stop When using zookeeper HA mode to start multiple Canals If the Client is a Canal Client has succeeded subsribe, currently started Canal Client will block on the canalConnector subscribe method At this point, calling the following stop method will block the stop at the join. public void stop() { if (!running) { return; } canalConnector stopRunning This version of the canal client does not support running = false; if (thread != null) { try { **thread.join();** } catch (InterruptedException e) { e.printStackTrace(); } } KafkaProducerUtil.stop(); MDC.remove("destination"); } I see the new version of the example stop method added the following code does not know if it is to solve the above problem connector.stopRunning(); If so, how should the previous version solve the blocking problem? Canal is through this mechanism to achieve HA on the client side. Blocking is to avoid having two identical clients at the same time
802,canal Example example code to increase data synchronization Application-rich at the client level expects to provide a standard implementation based on synchronization to the database
801,canal Dockerization package support 1. Increase Dockerfile support all in One packaging method centos + jdk + canal) 2. Parameter level allows docker e variable passing and modifying Read in java via System getenv 3. Delivery form - Allow users to package source code based on Dockerfile - Upload to Docker Hub allows users to download directly from the public network The corresponding docker uses the documentation https github com alibaba canal wiki Docker QuickStart
800,Canal native support for RocketMQ docking RocketMQ： https://rocketmq.apache.org/ Canal native support for data to be written to RocketMQ Code submitted Reference document https://github.com/alibaba/canal/wiki/Canal-Kafka-RocketMQ-QuickStart
799,About the sorting problem of canal received data When the same cust_id Add in order modify delete Add Canal receives data from two data Such as cust_id Types of Sql execution time 001 add 1533002422 001 update 1533002422 001 delete 1533002423 001 add 1533002423 There is a problem with this. When I made the zipper table of the data warehouse Unable to know 001 User&#39;s order of add and delete in the case of 1533002423 milliseconds my question is In this situation There are other fields that can provide the same sql execution time. Sort the data to distinguish the order Binlog is the order relationship cust_id Is the primary key? If it is the primary key, why is it added twice? mysql The binlog mechanism is more complicated. Many internal optimization transactions only guarantee the final result after the slave is executed.
798,Transaction in canal ID acquisition and entry getHeader getLogfileOffset offset understanding Question 1 A message of canal may correspond to multiple transactions in mysql. The list entry can be obtained by the method message getEntries. Entrys entry Divided into transanctionBegin transactionEnd and rowdata Now the problem is that I am > Get transaction id< Only the normal number is obtained through transactionEnd getTransactionId but the ones obtained through transactionbegin and rowdata are all null. Will there be other ways to get the transaction in canal? Id? Question 2 Entry getHeader getLogfileOffset The offset obtained by the current entry rowdata in the log file is the number of logs or bytes, that is, the number of bytes in front of the row. Ask God to enlighten me Thank you Do it yourself through mysqlbinlog Hexdump can view the commit to have xid Offset is a byte offset Thank you, great god
797,How can can distinguish multiple records in a transaction rowdata A transaction in mysql may contain multiple records of rowdata. So how does canal distinguish these records? Is there any scn system like oracle? change Number to uniquely identify changes made to a record #751 Look at this pr is not satisfied with the scene you said pr? Excuse me, is it convenient to explain in detail? Sorry, I just didn&#39;t see the link. Thank you.
796,Local debugging SimpleCanalClientTest throws an exception java net ConnectException Connection refused: Connect solution ![image](https://user-images.githubusercontent.com/16176283/43498536-f031ab84-9579-11e8-9bc6-09ca48546cbf.png) 将com alibaba otter canal example SimpleCanalClientTest The corresponding content in the class can be solved by changing the actual content.
794,Kafka integrated canal Event always receives no data, don&#39;t know why see PR #790
793,How to convert ByteString to CanalEntry Entry example In the code ``` ... CanalEntry.Entry.parseFrom(byteString) ``` OK
792,Received EOF packet from server ![image](https://user-images.githubusercontent.com/18360996/43445241-7a85f184-94d8-11e8-87f6-1bf1f0b8ddba.png) If slaveId If the setting does not repeat, please refer to the maximum possible. #777
791,Why the insert will be parsed into ddl RowChange object information is as follows unknownFields = {UnknownFieldSet@1470} "" bitField0_ = 30 tableId_ = 0 eventType_ = {CanalEntry$EventType@1609} "INSERT" isDdl_ = true sql_ = "insert into `test`(`school_student_id` `school_group_id` `like_user_id` `school_event_id`) values('114' '1' '66889278' '61')" rowDatas_ = {Collections$EmptyList@1473} size = 0 props_ = {Collections$EmptyList@1473} size = 0 ddlSchemaName_ = "test" memoizedIsInitialized = 1 memoizedSerializedSize = -1 memoizedSize = -1 memoizedHashCode = 0 1. Check if binlog is in row mode 2. RowsQueryLogEvent event in row mode 1 0 26 has solved this problem https://github.com/alibaba/canal/wiki/FAQ
790,kafka producer adaptation row data for performance #726 tks
789,Can&#39;t start ![image](https://user-images.githubusercontent.com/18360996/43390651-4090cdb2-9421-11e8-9e18-4b1d13f8e758.png) reference #777
788,Canal docking performance sampling first edition Hi Some personal ideas are as follows 1. Plugable In order to facilitate the docking of programs other than prometheus and to use the service in the relevant code The provider mechanism CanalServerWithEmbedded loads the service with the runtime Scope specified implementation in the deployer&#39;s pom <!-- Specify the metrics for the runtime here. provider--> <dependency> <groupId>com.alibaba.otter</groupId> <artifactId>canal.prometheus</artifactId> <version>${project.version}</version> <scope>runtime</scope> </dependency> 2. Try not to change the logic code of the canal as much as possible. The current commit changes to the original code as follows a. CanalServerWithEmbedded ServiceProvider logic b. startup.sh Load a javaagent for LTW load time Weaving is currently out of the stable version c. Deployer&#39;s pom specifies that the metrics implementation is currently also marked out. Therefore, the way to start the metrics of the logical logic of the canal is not to add the mvn configuration. install。 3. Currently implemented metrics have - jvm Family bucket - canal_net_inbound_bytes The amount of data received from mysql unit byte can be calculated using rate - canal_net_outbound_bytes The amount of data sent to the client in bytes can be calculated using rate - canal_instance instance List - canal_instance_traffic_delay The delay of each instance currently has a limitation in mysql When the binlog is not updated, the delay will always increase. The current idea is to use the mysql master. heart beat Packet refresh needs to parse this package TODO - eventstore Produce and ack index canal_instance_store_produce_seq with canal_instance_store_consume_seq Can be used to calculate TPS rate canal_instance_store_consume_seq and ringbuffer remain evnets cound(canal_instance_store_produce_seq - canal_instance_store_consume_seq) Other metrics please add 4. Some TODO to be perfected - Does HttpServer need to support https? - This machine is tested with the debug UT. - Parameter configurable - ... I would like to add and comment on the comments. Best regards To debug new features in the IDE, vmarg should be added -javaagent:/pathto/aspectjweaver-${version}.jar Looked at the structure is quite good, there are several suggestions 1. It is also necessary to change the code of the main server to do some burying for monitoring. Not specific advice through aspect 2. Differentiated between the instance level and the destination level for monitoring, such as network traffic reading and writing out, can distinguish different destinations Because the business often finds out who is the trickster, basically the individual destination affects the whole @agapple Thanks for comments, I will think about how to refactor BTW ask about the disadvantages of aspects in doing burying The main reason is that some maintenance monitoring should belong to the built-in capability of the canal. Ideally, the whole binlog parser sink store client can be fully buried and sampled to obtain performance data. The peripheral is very flexible and pluggable is not very meaningful. @agapple Well, understand that simply using canal The existing mechanism of the server to do some neutral burying points has some ideas and has time to sort out
787,Use show processlist The command does not see the dump process [v1.0.26.alpha4] After the canal starts up, it starts dumping. Why use show on mysql side processlist The command does not see the dump process Confirm if you are starting to consume data. There is no find in the instance log start position : EntryPosition[included=false journalName=xxxx position=xxxx serverId=<null> gtid=<null> timestamp=<null>]. Then the meta log has no continuous refresh show slave hosts Can see the dump process of canal
786,Ask Canal to subscribe to Alibaba Cloud RDS MySql case Business scene Existing Alibaba Cloud RDS MySql database wants to install binar for RDS data by installing canal Please provide a case for God Thank you, thank you. 参考 https github com alibaba canal wiki FAQ
785,no meda.dat I accidentally deleted the file：meta.dat（Below each instance） what should I do？ set position info in instance.properties which you could find in meta.log.However there might be an overlap. the latest one in meta.log @lcybo Do I need to manually create meta.dat and copy the contents of meta.log into newly meta.dat? It would be created and maintained by metamanager automatically. @lcybo Unfortunately it doesn't automatically generate in the document. Is it because I didn't configure position and what should position fill in? @lcybo I know why I did not generate meta.dat files because I did not add the following sentence to my code: connector.subscribe (* * \ \ \ * *); @lcybo But is this not dispensable as long as the canal.instance.filter.regex in the Instance.properties should not be configured with connector.subscribe (". * \ \.. *"); position is selected by following priority: 1.meta.dat 2.instance.properties 3.the position via 'show master status'(probably events lost) the meta.dat would be created after successful subscription if not exist. @lcybo Now you can automatically generate meta.dat but now I have to write connector.subscribe () in the program; connector is the CanalConnector type. If I write this the filter attribute in Instance.properties will be rewritten into an empty string and I can't let canal.instance.filter.regex in Instance.properties now. What do I do to make it work rather than using connector.subscribe () in the program? Yes filter is not necessary. Canal server work with canal client together there is a example client in canal project. Here is the code slice: protected void process() { int batchSize = 5 * 1024; while (running) { try { MDC.put("destination" destination); connector.connect(); connector.subscribe(); while (running) { Message message = connector.getWithoutAck(batchSize); // Get the specified amount of data long batchId = message.getId(); int size = message.getEntries().size(); if (batchId == -1 || size == 0) { // try { // Thread.sleep(1000); // } catch (InterruptedException e) { // } } else { printSummary(message batchId size); printEntry(message.getEntries()); } connector.ack(batchId); // Submit confirmation // connector.rollback(batchId); // Processing failure Rollback data } } catch (Exception e) { logger.error("process error!" e); } finally { connector.disconnect(); MDC.remove("destination"); } } } @lcybo If I write connector.subscribe () in the program again the regex on the server side will be covered. How can I make regex not cover? a. If you call subscribe() then canal.instance.filter.regex is used ignore the empty string in meta.dat. b.Otherwise(subscribe(regex)) use the parameter 'regex' instead of canal.instance.filter.regex. @lcybo It's like you said：you call subscribe() then canal.instance.filter.regex is used ignore the empty string in meta.dat.The problem has been solved，Thank you very much。 :)
784,Can canalserver support an instance instance to extract multiple libraries? Canalserver uses a single library instance configuration to not group and then extract the binlog logs of multiple libraries in one instance. Is this configuration method supported? Found configuration method canal instance defaultDatabaseName db1 Db2 This configuration is applicable to the same ip port instance under the different library library names separated by commas I think you understand that there is a small problem. You are configuring multiple libraries for the default listener. But it is better to do a regular match in the following configuration item. All libraries canal.instance.filter.regex = .*\\..* db1 db2 canal.instance.filter.regex = db1\\..* db2\\..* LS Correct Answer
783,Canal seeks to explain the message entries transanctionID batchid Event and rowchange relationship Learning the use of canal, asking God to sort out the relationship between several concepts in canal At present, my understanding is that a message contains multiple entries, which together constitute an entries. An entry corresponds to an eventtype. It can be an insert delete. An entry can be parsed to get a rowchange. A rowchange can be parsed to get multiple rowdata. A rowdata can be parsed to get the before column method getBeforeColumnsList. And after the columns - Question 1 What is the internal relationship between them? - Question 2 The difference between transactionID and batchID - Question 3 Entry getHeader getSchemaName can get the schema of mysql but how to get mysql Of instance Thank you Seek the guidance of the great God A message is the batch to get to Packet batchId is its identity is canal concept transactionId is mysql transaction related mysql binlog Will be the same type of operation insert in the same table Change line of update etc. merge to an event to save meta overhead entry is canal Proto type corresponds to mysql The event rowdata corresponds to each row of data before and after the change before and after the update, both of which have both inserts only after Delete only before In some special cases, such as opening ndb some parameters update will become insert type Hello, thank you for your advice. I have a problem. I have only inserted a row in mysql but the entries size displayed in the canal. Yes 3 Then how do I know which entry corresponds to the row I just inserted? It should be there transactionBegin with transactionEnd Event can be pressed event Type filter On Jul 26 2018 18:20 +0800 alexlgj <notifications@github.com> wrote: > Hello, thank you for your advice. I have a problem. I have only inserted a row in mysql but the entries size displayed in the canal. Yes 3 Then how do I know which entry corresponds to the row I just inserted? > — > You are receiving this because you are subscribed to this thread. > Reply to this email directly view it on GitHub or mute the thread. ls Correct Answer Thanks to the reply of the Great God, I now understand the test several times and the results are as follows An insert or delete or update operation entrys.size() is: 3 Perform three consecutive insert data operations and then start the canal client entrys.size() is: 9 When the entry is transactionbegin or transactionend, the eventtype is update. When the entry is rowdata, the corresponding eventtype is the corresponding operation, ie insert delete update Also encountered a problem how to get mysql through canal The instance name of database is only found in the header of entry. By entry getHeader getSchemaName but how to get mysql Of instance Does instance refer to the mysql instance of the port distinction? Consider using serverId to distinguish Ok, thank you very much.
782,MysqlMultiStageCoprocessor robustness enhancement Hi Wenge&#39;s commit solves the problem caused by the EOF package. However, MysqlMultiStageCoprocessor still has problems in other situations. If the exception is not onEvent What happens in the body is the InterruptedException of 771. The stop in onShutdown and the reset of the dump thread will compete. If MysqlMultiStageCoprocessor is only safely called serially within the dump thread So change to accident exception. Also check by publish check by dump thread Best regard [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=782) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=782) before we can accept your contribution.<br/><hr/>**Chuanyi Li L** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=782) it.</sub> [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=782) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=782) before we can accept your contribution.<br/><hr/>**Chuanyi Li L** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=782) it.</sub> tks @lcybo Your implicit definition and the previous exception are duplicated and merged directly into one Ok, I thought that canalParseException might have a special deal. The second commit deliberately added the incident distinction. It seems that I want more. @lcybo Invite you to join the project administrator trouble private letter my mailbox contact jianghang115 gmail com @agapple Has accpet mailbox chuanyili0625 gmail com Please take care
781,The latest version of fastsql preview 520 ： com.alibaba.fastsql.sql.parser.ParserException Fastsql version Latest 520 Sql statement and Exception case 1 : sql_6 = "CREATE INDEX `idx_t_uid` on stu_score (`uid`) COMMENT '' ALGORITHM DEFAULT LOCK DEFAULT "; Exception in thread "main" com.alibaba.fastsql.sql.parser.ParserException: syntax error error in :'ITHM DEFAULT LOCK DEFAULT pos 134 line 1 column 136 token IDENTIFIER null at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:363) at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:520) at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:92) at com.test.HelloWorld.main(HelloWorld.java:266) case 2: sql = "alter table test COLLATE utf8mb4_unicode_ci" ; Exception in thread "main" com.alibaba.fastsql.sql.parser.ParserException: syntax error error in :'LATE utf8mb4_unicode_ci' expect = actual null pos 44 line 1 column 27 token IDENTIFIER utf8mb4_unicode_ci at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:363) at com.alibaba.fastsql.sql.parser.SQLParser.accept(SQLParser.java:371) at com.alibaba.fastsql.sql.dialect.mysql.parser.MySqlStatementParser.parseAlterTable(MySqlStatementParser.java:4818) at com.alibaba.fastsql.sql.dialect.mysql.parser.MySqlStatementParser.parseAlter(MySqlStatementParser.java:3544) at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:264) at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:92) at com.test.HelloWorld.main(HelloWorld.java:266) case 3： sql_8 = "alter table task AUTO_INCREMENT = 20000000 COMMENT Self-incrementing start value ; Exception in thread "main" com.alibaba.fastsql.sql.parser.ParserException: syntax error error in :'0000 COMMENT Self-incrementing start value expect ON actual = pos 52 line 1 column 52 token = at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:363) at com.alibaba.fastsql.sql.parser.SQLParser.accept(SQLParser.java:371) at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseComment(SQLStatementParser.java:3488) at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:354) at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:92) at com.test.HelloWorld.main(HelloWorld.java:266) These are some problems that have been feedback and suggested to be modified to bypass alter table test COLLATE = utf8mb4_unicode_ci alter table task AUTO_INCREMENT = 20000000 COMMENT Self-incrementing start value Add another ddl CREATE TABLE `app_info` (`id` bigint(20) NOT NULL `app_name` varchar(255) NOT NULL PRIMARY KEY (`id`) INDEX `idx` USING BTREE (`app_name`) comment '') ;
780,fix bug #776 [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=780) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=780) before we can accept your contribution.<br/><hr/>**winger** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=780) it.</sub> [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=780) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=780) before we can accept your contribution.<br/><hr/>**winger** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=780) it.</sub> @agapple Gmail mailbox jianghang115 gmail com have my message thank you tks
779,Long comments can cause data confusion The note was originally Directional setting 32 characters Each character has 3 values ​​0 Empty 1 Unlimited 2 Determined value specified No specified exclusion Excluded as specified 3 Determined value Excluded 1st character Application channel 2nd character Network environment 3rd character Mobile operating system 4th character platform version number 5th character geographical orientation 6th character gender 7th character age segment 8th character keyword 0 no 1 no limit 2 designation 3 exclusion 4 smart 9th character commercial interest Label 10th character AppList label 11th character device price label 12th character video type 13th character operator 14th character custom crowd package Change to Directional setting 32 characters Each character has 3 values ​​0 Empty 1 Unlimited 2 Determined value specified No specified exclusion Excluded as specified 3 Determined value Excluded 1st character Application channel 2nd character Network environment 3rd character Mobile operating system 4th character platform version number 5th character geographical orientation 6th character gender 7th character age segment 8th character keyword 0 no 1 no limit 2 designation 3 exclusion 4 smart 9th character commercial interest Label 10th character AppList label 11th character device price label 12th character video type 13th character operator 14th character custom crowd package 15th character application channel information version 16th character platform version number information Version There is a field with data that is garbled in the future. Give me a complete ddl sql alter table tb_ad_group modify `target_setting` varchar(64) NOT NULL DEFAULT '00000000000000000000000000000000' COMMENT Directional setting 32 characters Each character has 3 values ​​0 Empty 1 Unlimited 2 Determined value specified No specified exclusion Excluded as specified 3 Determined value Excluded 1st character Application channel 2nd character Network environment 3rd character Mobile operating system 4th character platform version number 5th character geographical orientation 6th character gender 7th character age segment 8th character keyword 0 no 1 no limit 2 designation 3 exclusion 4 smart 9th character commercial interest Label 10th character AppList label 11th character device price label 12th character video type 13th character operator 14th character custom crowd package 15th character application channel information version 16th character platform version number information Version Latest alpha 4 analysis no problem I want to ask you about it. If there is any way to change the version, the version is not particularly convenient.
778,parse row data failed canal 1.0.26-SNAPSHOT 5.5.52-MariaDB GRANT SELECT REPLICATION SLAVE REPLICATION CLIENT ON *.* TO 'czbrepclient'@'%' IDENTIFIED BY PASSWORD '*A6A643DF20257C290F26693E1A59F69448428157' Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: java.lang.IllegalArgumentException: limit excceed: 148 at com.taobao.tddl.dbsync.binlog.LogBuffer.getFullString(LogBuffer.java:1123) Provide binlog files or test data to reproduce frequently Send you gmail, please check
777,fix issue #771 #776 #756 [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=777) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=777) before we can accept your contribution.<br/><hr/>**winger** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=777) it.</sub> tks
776,[v1.0.26.alpha4] decode Event time limit Exceed error ![image](https://user-images.githubusercontent.com/8179551/43183817-099b0e50-9019-11e8-8339-2d922e48b9ed.png) decode Event time limit Exceed error Should be related to this commit https://github.com/alibaba/canal/commit/89726a636530b73a6b97cecc2b5bcee4fb464f86 The mysql version is 啥 Sorry, gaoxiangyu I rely on MySQL 5 7 tested MySQL that can cause this error The version says it is convenient, I will follow up here. Thank you. 5.6.28 @gaoxiangyu Thank you for following me. @agapple I will follow up on this issue. BTW is convenient when you look at your gmail
775,Canal support spring4 or spring5? Currently conflicting with spring4 spring5 in the project when introducing the client Currently using spring3 by default @agapple Have plans to support spring4 or spring5? Spring4 is more common with spring3. You can submit a PR to my canal dependence on spring is an IOC can be upgraded Spring5 requires java8 to pay attention to
774,[v1.0.26.alpha4]connect timed Out error 018-07-25 10:50:58.209 [destination = example address = /127.0.0.1:3306 EventParser] WARN c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - Received EOF packet from server apparent master disconnected. It's may be duplicate slaveId check instance config 2018-07-25 10:51:17.034 [destination = example address = /127.0.0.1:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position {"identity":{"slaveId":-1 "sourceAddress":{"address":"127.0.0.1" "port":3306}} "postion":{"gtid":"" "included":false "journalName":"mysql-bin.000027" "position":299492902 "serverId":2 "timestamp":1532487056000}} 2018-07-25 10:51:17.086 [destination = example address = /127.0.0.1:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000027 position=299492902 serverId=2 gtid= timestamp=1532487056000] 2018-07-25 10:51:18.081 [destination = example address = /127.0.0.1:3306 EventParser] WARN c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - Received EOF packet from server apparent master disconnected. It's may be duplicate slaveId check instance config 2018-07-25 10:51:33.753 [destination = example address = /127.0.0.1:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position {"identity":{"slaveId":-1 "sourceAddress":{"address":"127.0.0.1" "port":3306}} "postion":{"gtid":"" "included":false "journalName":"mysql-bin.000027" "position":299516130 "serverId":2 "timestamp":1532487071000}} 2018-07-25 10:51:33.760 [destination = example address = /127.0.0.1:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000027 position=299516130 serverId=2 gtid= timestamp=1532487071000] 2018-07-25 10:51:43.767 [destination = example address = /127.0.0.1:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address 127.0.0.1/127.0.0.1:3306 has an error retrying. caused by java.io.IOException: connect /127.0.0.1:3306 failure at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:77) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.reconnect(MysqlConnector.java:86) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.reconnect(MysqlConnection.java:85) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:186) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Unknown Source) [na:1.8.0_152] Caused by: java.net.SocketTimeoutException: connect timed out at java.net.TwoStacksPlainSocketImpl.socketConnect(Native Method) ~[na:1.8.0_152] at java.net.AbstractPlainSocketImpl.doConnect(Unknown Source) ~[na:1.8.0_152] at java.net.AbstractPlainSocketImpl.connectToAddress(Unknown Source) ~[na:1.8.0_152] at java.net.AbstractPlainSocketImpl.connect(Unknown Source) ~[na:1.8.0_152] at java.net.PlainSocketImpl.connect(Unknown Source) ~[na:1.8.0_152] at java.net.SocksSocketImpl.connect(Unknown Source) ~[na:1.8.0_152] at java.net.Socket.connect(Unknown Source) ~[na:1.8.0_152] at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannelPool.open(BioSocketChannelPool.java:18) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannelPool.open(SocketChannelPool.java:18) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:72) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] ... 4 common frames omitted 2018-07-25 10:51:43.774 [destination = example address = /127.0.0.1:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: connect /127.0.0.1:3306 failure at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:77) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.reconnect(MysqlConnector.java:86) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.reconnect(MysqlConnection.java:85) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:186) at java.lang.Thread.run(Unknown Source) Caused by: java.net.SocketTimeoutException: connect timed out at java.net.TwoStacksPlainSocketImpl.socketConnect(Native Method) at java.net.AbstractPlainSocketImpl.doConnect(Unknown Source) at java.net.AbstractPlainSocketImpl.connectToAddress(Unknown Source) at java.net.AbstractPlainSocketImpl.connect(Unknown Source) at java.net.PlainSocketImpl.connect(Unknown Source) at java.net.SocksSocketImpl.connect(Unknown Source) at java.net.Socket.connect(Unknown Source) at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannelPool.open(BioSocketChannelPool.java:18) at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannelPool.open(SocketChannelPool.java:18) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:72) ... 4 more ] 2018-07-25 10:52:13.657 [destination = example address = /127.0.0.1:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address 127.0.0.1/127.0.0.1:3306 has an error retrying. caused by java.io.IOException: connect /127.0.0.1:3306 failure at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:77) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:81) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:172) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Unknown Source) [na:1.8.0_152] Caused by: java.net.SocketTimeoutException: connect timed out at java.net.TwoStacksPlainSocketImpl.socketConnect(Native Method) ~[na:1.8.0_152] at java.net.AbstractPlainSocketImpl.doConnect(Unknown Source) ~[na:1.8.0_152] at java.net.AbstractPlainSocketImpl.connectToAddress(Unknown Source) ~[na:1.8.0_152] at java.net.AbstractPlainSocketImpl.connect(Unknown Source) ~[na:1.8.0_152] at java.net.PlainSocketImpl.connect(Unknown Source) ~[na:1.8.0_152] at java.net.SocksSocketImpl.connect(Unknown Source) ~[na:1.8.0_152] at java.net.Socket.connect(Unknown Source) ~[na:1.8.0_152] at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannelPool.open(BioSocketChannelPool.java:18) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannelPool.open(SocketChannelPool.java:18) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:72) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] ... 3 common frames omitted 2018-07-25 10:52:13.658 [destination = example address = /127.0.0.1:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: connect /127.0.0.1:3306 failure at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:77) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:81) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:172) at java.lang.Thread.run(Unknown Source) Caused by: java.net.SocketTimeoutException: connect timed out at java.net.TwoStacksPlainSocketImpl.socketConnect(Native Method) at java.net.AbstractPlainSocketImpl.doConnect(Unknown Source) at java.net.AbstractPlainSocketImpl.connectToAddress(Unknown Source) at java.net.AbstractPlainSocketImpl.connect(Unknown Source) at java.net.PlainSocketImpl.connect(Unknown Source) at java.net.SocksSocketImpl.connect(Unknown Source) at java.net.Socket.connect(Unknown Source) at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannelPool.open(BioSocketChannelPool.java:18) at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannelPool.open(SocketChannelPool.java:18) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:72) ... 3 more ] @agapple Native MySQL pass 127.0.0.1:3306 Can&#39;t connect? @wingerx This 127 0 0 1 is another specific IP that was just replaced when posting the log.
773,[v1.0.26.alpha4]parse events has an Error error ![image](https://user-images.githubusercontent.com/9798724/43177235-d82ca3c0-8ff9-11e8-87f1-e43826acb891.png) 2018-07-25 10:52:13.662 [destination = example address = /127.0.0.1:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - parse events has an error java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.FutureTask@869cc59 rejected from java.util.concurrent.ThreadPoolExecutor@36f77020[Terminated pool size = 0 active threads = 0 queued tasks = 0 completed tasks = 0] at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(Unknown Source) ~[na:1.8.0_152] at java.util.concurrent.ThreadPoolExecutor.reject(Unknown Source) ~[na:1.8.0_152] at java.util.concurrent.ThreadPoolExecutor.execute(Unknown Source) ~[na:1.8.0_152] at java.util.concurrent.AbstractExecutorService.submit(Unknown Source) ~[na:1.8.0_152] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor.start(MysqlMultiStageCoprocessor.java:120) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor.reset(MysqlMultiStageCoprocessor.java:187) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:306) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Unknown Source) ~[na:1.8.0_152] The same problem is solved. @agapple This is not root Cause looking for the top log No problem with v1 0 25 Should be a single-core machine startup failure problem v1 1 1 fixed @agapple When will the official version of v1 1 1 be released?
772,fix issue #771 #756 [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=772) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=772) before we can accept your contribution.<br/><hr/>**winger** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=772) it.</sub> [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=772) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=772) before we can accept your contribution.<br/><hr/>**winger** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=772) it.</sub>
771,The thread pool is working abnormally when starting the canal 2018-07-24 15:44:43.247 [destination = example address = /22.5.229.239:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position mysql-bin.000001:558240:null 2018-07-24 15:44:43.271 [destination = example address = /22.5.229.239:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000001 position=558240 serverId=<null> gtid= timestamp=<null>] 2018-07-24 15:44:43.331 [destination = example address = /22.5.229.239:3306 EventParser] WARN c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - Received EOF packet from server apparent master disconnected. It's may be duplicate slaveId check instance config 2018-07-24 15:45:00.446 [destination = example address = /22.5.229.239:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position {"identity":{"slaveId":-1 "sourceAddress":{"address":"cbjup04" "port":3306}} "postion":{"gtid":"" "included":false "journalName":"mysql-bin.000001" "position":559146 "serverId":16782861 "timestamp":1532417891000}} 2018-07-24 15:45:00.447 [destination = example address = /22.5.229.239:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000001 position=559146 serverId=16782861 gtid= timestamp=1532417891000] 2018-07-24 15:45:00.453 [destination = example address = /22.5.229.239:3306 EventParser] WARN c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - Received EOF packet from server apparent master disconnected. It's may be duplicate slaveId check instance config 2018-07-24 15:45:18.422 [destination = example address = /22.5.229.239:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position {"identity":{"slaveId":-1 "sourceAddress":{"address":"cbjup04" "port":3306}} "postion":{"gtid":"" "included":false "journalName":"mysql-bin.000001" "position":559146 "serverId":16782861 "timestamp":1532417891000}} 2018-07-24 15:45:18.422 [destination = example address = /22.5.229.239:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000001 position=559146 serverId=16782861 gtid= timestamp=1532417891000] 2018-07-24 15:45:18.426 [destination = example address = /22.5.229.239:3306 EventParser] WARN c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - Received EOF packet from server apparent master disconnected. It's may be duplicate slaveId check instance config 2018-07-24 15:45:18.440 [destination = example address = /22.5.229.239:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - parse events has an error java.util.concurrent.RejectedExecutionException: Task com.lmax.disruptor.WorkProcessor@58d43d50 rejected from java.util.concurrent.ThreadPoolExecutor@56057cbf[Terminated pool size = 0 active threads = 0 queued tasks = 0 completed tasks = 0] at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2048) ~[na:1.7.0_45] at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:821) ~[na:1.7.0_45] at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1372) ~[na:1.7.0_45] at com.lmax.disruptor.WorkerPool.start(WorkerPool.java:140) ~[disruptor-3.4.2.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor.start(MysqlMultiStageCoprocessor.java:122) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor.reset(MysqlMultiStageCoprocessor.java:187) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:306) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:744) ~[na:1.7.0_45] #756 The same problem Found the problem @agapple in fix #726 Issue will binlog_flags Set in order to BINLOG_DUMP_NON_BLOCK master has no binlog to send After the event will return an EOF package. Usually for the slave, it’s probably better to keep the connection hanging so that you can receive the newly generated binlog in a timely manner. Event I don&#39;t know agapple Based on what considerations changed the settings BINLOG_DUMP_NON_BLOCK？ Or more elegant processing EOF package Instead of printing a misleading Received EOF packet from server apparent master disconnected. It's may be duplicate slaveId check instance config" Log information @wingerx Hello, my problem is mainly in the following when creating a parse thread pool will always report java util concurrent RejectedExecutionException and pool Size is 0 My server is 4 cores. According to the default value of parallelThreadSize, 60 poolSize of the total available physical core of the server should be 2 instead of 0. The same problem is solved by the landlord. @fanpeng1100 No problem with me is calling the thread pool error @sky-mariner The reason for the thread pool error is please see my submitted pr #777。 For the latest release v1.0.26 alpha 4 first change the configuration canal.instance.parser.parallel = false Use it @wingerx Okay thank you @wingerx Correct answer
770,Specify canal instance master timestamp timestamp consumption is invalid The timestamp is specified in the instance. The client will also get the log before this timestamp. If you want to consume only the log after this timestamp, which should be configured? Can you configure the canal instance master timestamp separately? Clean up the last recorded location information such as the zookeeper node and the meta dat file
769,1.0.26 Version single machine does not use h2 db Another sql in the modification of multiple table structures and the creation of a new table parsing the entire library of exception monitoring The exception information is as follows Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: fetch failed by table Meta database_table name_new Caused by: java.io.IOException: ErrorPacket [errorNumber=1146 fieldCount=-1 message=Table Database_table name_new doesn't exist sqlState=42S02 sqlStateMarker=#] at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.query(MysqlQueryExecutor.java:61) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:94) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMetaByDB(TableMetaCache.java:89) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:32) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:62) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:52) at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) at com.google.common.cache.LocalCache.get(LocalCache.java:3937) at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941) at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824) at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:185) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:759) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:428) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:114) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:66) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:337) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:184) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:152) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) at java.lang.Thread.run(Thread.java:748) message=Table Database_table name_new doesn't exist sqlState=42S02 sqlStateMarker=#] Table not found The error message that this table does not exist is the parsing exception caused by the temporary table when the index is added to the table and the other table operation is created. Open Table TSDB to test a lot of restrictions on the previous model of the database it is good I opened h2 Seeing that there will be such an exception afterwards
768,Modify Readme md new data subscription event announcement Modify Readme md new data subscription event announcement [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=768) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=768) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=768) it.</sub>
767,Canal client connection problem ZK at 47 96 186 32 2181 Canal at 47 96 186 32 11111 No problem with the port Canal server logs are normal When the client connects ZK can successfully connect But call **canalConnector.subscribe(canalClientProperties.getSubscribeFilter());** This method throws the following exception Canal autoscan is off something goes wrong when subscribing from server:null 2018-07-23 14:14:30.100 [main-SendThread(47.96.186.32:2181)] DEBUG org.apache.zookeeper.ClientCnxn.readResponse(815) - Reading reply sessionid:0x164b770991e0058 packet:: clientPath:null serverPath:null finished:false header:: 298 4 replyHeader:: 298 12176 0 request:: '/otter/canal/destinations/example/1001/running T response:: #7b22616374697665223a747275652c2261646472657373223a223132372e302e302e313a3536333633222c22636c69656e744964223a313030317d s{12174 12175 1532326470040 1532326470047 1 0 0 100406785942093912 59 0 12174} 2018-07-23 14:14:30.100 [taskExecutor-5] WARN c.a.o.c.c.i.r.ClientRunningMonitor.check(168) - canal is running in [127.0.0.1:56363] but not in [192.168.31.207] 2018-07-23 14:14:30.101 [taskExecutor-5] ERROR t.j.m.m.s.c.CanalIncrementSyncTask.handleException(25) - com.alibaba.otter.canal.protocol.exception.CanalClientException: failed to subscribe after 3 times retry. at com.alibaba.otter.canal.client.impl.ClusterCanalConnector.subscribe(ClusterCanalConnector.java:119) at tech.jiyu.micromagasin.mc.sync.canal.CanalIncrementSyncTask.sync(CanalIncrementSyncTask.java:75) at tech.jiyu.micromagasin.mc.sync.canal.CanalIncrementSyncTask$$FastClassBySpringCGLIB$$783fbb6f.invoke(<generated>) at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:204) at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:720) at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157) at org.springframework.aop.interceptor.AsyncExecutionInterceptor$1.call(AsyncExecutionInterceptor.java:115) at java.util.concurrent.FutureTask.run$$$capture(FutureTask.java:266) at java.util.concurrent.FutureTask.run(FutureTask.java) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) com.alibaba.otter.canal.protocol.exception.CanalClientException: failed to subscribe after 3 times retry. at com.alibaba.otter.canal.client.impl.ClusterCanalConnector.subscribe(ClusterCanalConnector.java:119) at tech.jiyu.micromagasin.mc.sync.canal.CanalIncrementSyncTask.sync(CanalIncrementSyncTask.java:75) at tech.jiyu.micromagasin.mc.sync.canal.CanalIncrementSyncTask$$FastClassBySpringCGLIB$$783fbb6f.invoke(<generated>) at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:204) at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:720) at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157) at org.springframework.aop.interceptor.AsyncExecutionInterceptor$1.call(AsyncExecutionInterceptor.java:115) at java.util.concurrent.FutureTask.run$$$capture(FutureTask.java:266) at java.util.concurrent.FutureTask.run(FutureTask.java) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) The reason for finding the problem right now ![image](https://user-images.githubusercontent.com/31718669/43062693-3e5187a2-8e8c-11e8-9c3a-beb631e3d868.png) canalServer 的canal properties Ip configuration is 127 0 0 1 When the client connects to ZK, the above data connection is obtained. The address used is also 127 0 0 1 to connect to Canal. Server This becomes the client machine connecting to the local CanalServer In the beginning, this was just a guess. But when I started a CanalServer locally, I no longer throw an exception. Can connect normally the question is that I tried to configure the remote CanalServer ip as a public IP. This should be able to connect properly. But changed to public network IP after CanalServer Startup failed Prompt error "**canal cannot assign requested address**"
766,Cancal starts normal java client can connect but the client can&#39;t get data change Cancal starts normal java client can connect on but there is data change after the client can not get the change below is the record Who has encountered this situation, please help, thank you 2018-07-23 12:29:37.519 [destination = example address = /127.0.01:3307 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: ErrorPacket [errorNumber=1046 fieldCount=-1 message=No database selected sqlState=3D000 sqlStateMarker=#] with command: show create table Sbed full speed stock show create table Sbed commodity filing information table show create table Sbed Condot stock show create table Sbed no GST merchandise show create table Sbed Australian cat commodity price show create table Sbed Australian cat commodity declaration price show create table Sbed Australian cat commodity sales price show create table Sbed online store housekeeper total inventory show create table Sbed online store housekeeper commodity data show create table Sbed goods price list show create table `sbed`.`QRTZ_BLOB_TRIGGERS`;show create table `sbed`.`QRTZ_CALENDARS`;show create table `sbed`.`QRTZ_CRON_TRIGGERS`;show create table `sbed`.`QRTZ_FIRED_TRIGGERS`;show create table `sbed`.`QRTZ_JOB_DETAILS`;show create table `sbed`.`QRTZ_LOCKS`;show create table `sbed`.`QRTZ_PAUSED_TRIGGER_GRPS`;show create table `sbed`.`QRTZ_SCHEDULER_STATE`;show create table `sbed`.`QRTZ_SIMPLE_TRIGGERS`;show create table `sbed`.`QRTZ_SIMPROP_TRIGGERS`;show create table `sbed`.`QRTZ_TRIGGERS`;show create table `sbed`.`sys_add_goods_advice`;show create table `sbed`.`sys_aomao_goods`;show create table `sbed`.`sys_aomao_goods_price`;show create table `sbed`.`sys_aomao_logistics`;show create table `sbed`.`sys_aomao_trade`;show create table `sbed`.`sys_aomao_trade_goods`;show create table `sbed`.`sys_aomao_user`;show create table `sbed`.`sys_aomao_user_group`;show create table `sbed`.`sys_aomao_user_inventory`;show create table `sbed`.`sys_attachment`;show create table `sbed`.`sys_au_data`;show create table `sbed`.`sys_auspost_base`;show create table `sbed`.`sys_auspost_log`;show create table `sbed`.`sys_auspost_order`;show create table `sbed`.`sys_common_map`;show create table `sbed`.`sys_config`;show create table `sbed`.`sys_content`;show create table `sbed`.`sys_content_taxonomy`;show create table `sbed`.`sys_currency`;show create table `sbed`.`sys_customer`;show create table `sbed`.`sys_customer_address`;show create table `sbed`.`sys_customer_category`;show create table `sbed`.`sys_customer_grade`;show create table `sbed`.`sys_dept`;show create table `sbed`.`sys_documents`;show create table `sbed`.`sys_eunionpay`;show create table `sbed`.`sys_excel_export`;show create table `sbed`.`sys_express`;show create table `sbed`.`sys_finance_log`;show create table `sbed`.`sys_finance_order`;show create table `sbed`.`sys_finance_order_goods`;show create table `sbed`.`sys_finance_refund`;show create table `sbed`.`sys_goods`;show create table `sbed`.`sys_goods_brand`;show create table `sbed`.`sys_goods_category`;show create table `sbed`.`sys_goods_price`;show create table `sbed`.`sys_goods_repertory`;show create table `sbed`.`sys_goods_return`;show create table `sbed`.`sys_goods_shop_price`;show create table `sbed`.`sys_goods_stock`;show create table `sbed`.`sys_goods_supplier`;show create table `sbed`.`sys_issue`;show create table `sbed`.`sys_issue_post`;show create table `sbed`.`sys_jingdong_refund`;show create table `sbed`.`sys_log`;show create table `sbed`.`sys_menu`;show create table `sbed`.`sys_order_detail`;show create table `sbed`.`sys_purchase`;show create table `sbed`.`sys_purchase_checkout`;show create table `sbed`.`sys_purchase_checkout_detail`;show create table `sbed`.`sys_purchase_detail`;show create table `sbed`.`sys_purchaseplan`;show create table `sbed`.`sys_purchaseplan_detail`;show create table `sbed`.`sys_quansutong_clearance`;show create table `sbed`.`sys_quansutong_express`;show create table `sbed`.`sys_quansutong_token`;show create table `sbed`.`sys_redbook_repertory`;show create table `sbed`.`sys_role`;show create table `sbed`.`sys_role_menu`;show create table `sbed`.`sys_sai_goods`;show create table `sbed`.`sys_sai_shipment_data`;show create table `sbed`.`sys_sai_stock`;show create table `sbed`.`sys_sale_num_goods`;show create table `sbed`.`sys_sales`;show create table `sbed`.`sys_sales_stock`;show create table `sbed`.`sys_schedule_job`;show create table `sbed`.`sys_schedule_job_log`;show create table `sbed`.`sys_shop`;show create table `sbed`.`sys_shop_delivery`;show create table `sbed`.`sys_shop_goods`;show create table `sbed`.`sys_shop_order`;show create table `sbed`.`sys_shop_order_goods`;show create table `sbed`.`sys_status`;show create table `sbed`.`sys_stock_all`;show create table `sbed`.`sys_stock_all_log`;show create table `sbed`.`sys_stock_all_num`;show create table `sbed`.`sys_stock_all_num_view`;show create table `sbed`.`sys_stock_allocation`;show create table `sbed`.`sys_stock_allocation_main`;show create table `sbed`.`sys_stock_goods`;show create table `sbed`.`sys_stock_store`;show create table `sbed`.`sys_stock_store_position`;show create table `sbed`.`sys_store`;show create table `sbed`.`sys_store_model`;show create table `sbed`.`sys_supplier`;show create table `sbed`.`sys_taobao_goods`;show create table `sbed`.`sys_taobao_goods_detail`;show create table `sbed`.`sys_taobao_goods_detail_item_imgs`;show create table `sbed`.`sys_taobao_goods_detail_location`;show create table `sbed`.`sys_taobao_goods_detail_prop_imgs`;show create table `sbed`.`sys_taobao_goods_detail_sku`;show create table `sbed`.`sys_taobao_goods_price`;show create table `sbed`.`sys_taobao_logistics_company`;show create table `sbed`.`sys_taobao_order`;show create table `sbed`.`sys_taobao_trade`;show create table `sbed`.`sys_taxonomy`;show create table `sbed`.`sys_third_nulaxgroup`;show create table `sbed`.`sys_tianmao_activity`;show create table `sbed`.`sys_tianmao_activity_goods`;show create table `sbed`.`sys_trade`;show create table `sbed`.`sys_trade_goods`;show create table `sbed`.`sys_trade_logistics`;show create table `sbed`.`sys_user`;show create table `sbed`.`sys_user_brand`;show create table `sbed`.`sys_user_formthead`;show create table `sbed`.`sys_user_role`;show create table `sbed`.`sys_user_store`;show create table `sbed`.`sys_user_token`;show create table `sbed`.`sys_value`;show create table `sbed`.`sys_wave_house_cost`;show create table `sbed`.`sys_yeebao_pay`;show create table `sbed`.`xhs_order`;show create table `sbed`.`xhs_order_detail`;show create table `sbed`.`xhs_stock`; Caused by: java.io.IOException: ErrorPacket [errorNumber=1046 fieldCount=-1 message=No database selected sqlState=3D000 sqlStateMarker=#] with command: show create table Sbed full speed stock show create table Sbed commodity filing information table show create table Sbed Condot stock show create table Sbed no GST merchandise show create table Sbed Australian cat commodity price show create table Sbed Australian cat commodity declaration price show create table Sbed Australian cat commodity sales price show create table Sbed online store housekeeper total inventory show create table Sbed online store housekeeper commodity data show create table Sbed goods price list show create table `sbed`.`QRTZ_BLOB_TRIGGERS`;show create table `sbed`.`QRTZ_CALENDARS`;show create table `sbed`.`QRTZ_CRON_TRIGGERS`;show create table `sbed`.`QRTZ_FIRED_TRIGGERS`;show create table `sbed`.`QRTZ_JOB_DETAILS`;show create table `sbed`.`QRTZ_LOCKS`;show create table `sbed`.`QRTZ_PAUSED_TRIGGER_GRPS`;show create table `sbed`.`QRTZ_SCHEDULER_STATE`;show create table `sbed`.`QRTZ_SIMPLE_TRIGGERS`;show create table `sbed`.`QRTZ_SIMPROP_TRIGGERS`;show create table `sbed`.`QRTZ_TRIGGERS`;show create table `sbed`.`sys_add_goods_advice`;show create table `sbed`.`sys_aomao_goods`;show create table `sbed`.`sys_aomao_goods_price`;show create table `sbed`.`sys_aomao_logistics`;show create table `sbed`.`sys_aomao_trade`;show create table `sbed`.`sys_aomao_trade_goods`;show create table `sbed`.`sys_aomao_user`;show create table `sbed`.`sys_aomao_user_group`;show create table `sbed`.`sys_aomao_user_inventory`;show create table `sbed`.`sys_attachment`;show create table `sbed`.`sys_au_data`;show create table `sbed`.`sys_auspost_base`;show create table `sbed`.`sys_auspost_log`;show create table `sbed`.`sys_auspost_order`;show create table `sbed`.`sys_common_map`;show create table `sbed`.`sys_config`;show create table `sbed`.`sys_content`;show create table `sbed`.`sys_content_taxonomy`;show create table `sbed`.`sys_currency`;show create table `sbed`.`sys_customer`;show create table `sbed`.`sys_customer_address`;show create table `sbed`.`sys_customer_category`;show create table `sbed`.`sys_customer_grade`;show create table `sbed`.`sys_dept`;show create table `sbed`.`sys_documents`;show create table `sbed`.`sys_eunionpay`;show create table `sbed`.`sys_excel_export`;show create table `sbed`.`sys_express`;show create table `sbed`.`sys_finance_log`;show create table `sbed`.`sys_finance_order`;show create table `sbed`.`sys_finance_order_goods`;show create table `sbed`.`sys_finance_refund`;show create table `sbed`.`sys_goods`;show create table `sbed`.`sys_goods_brand`;show create table `sbed`.`sys_goods_category`;show create table `sbed`.`sys_goods_price`;show create table `sbed`.`sys_goods_repertory`;show create table `sbed`.`sys_goods_return`;show create table `sbed`.`sys_goods_shop_price`;show create table `sbed`.`sys_goods_stock`;show create table `sbed`.`sys_goods_supplier`;show create table `sbed`.`sys_issue`;show create table `sbed`.`sys_issue_post`;show create table `sbed`.`sys_jingdong_refund`;show create table `sbed`.`sys_log`;show create table `sbed`.`sys_menu`;show create table `sbed`.`sys_order_detail`;show create table `sbed`.`sys_purchase`;show create table `sbed`.`sys_purchase_checkout`;show create table `sbed`.`sys_purchase_checkout_detail`;show create table `sbed`.`sys_purchase_detail`;show create table `sbed`.`sys_purchaseplan`;show create table `sbed`.`sys_purchaseplan_detail`;show create table `sbed`.`sys_quansutong_clearance`;show create table `sbed`.`sys_quansutong_express`;show create table `sbed`.`sys_quansutong_token`;show create table `sbed`.`sys_redbook_repertory`;show create table `sbed`.`sys_role`;show create table `sbed`.`sys_role_menu`;show create table `sbed`.`sys_sai_goods`;show create table `sbed`.`sys_sai_shipment_data`;show create table `sbed`.`sys_sai_stock`;show create table `sbed`.`sys_sale_num_goods`;show create table `sbed`.`sys_sales`;show create table `sbed`.`sys_sales_stock`;show create table `sbed`.`sys_schedule_job`;show create table `sbed`.`sys_schedule_job_log`;show create table `sbed`.`sys_shop`;show create table `sbed`.`sys_shop_delivery`;show create table `sbed`.`sys_shop_goods`;show create table `sbed`.`sys_shop_order`;show create table `sbed`.`sys_shop_order_goods`;show create table `sbed`.`sys_status`;show create table `sbed`.`sys_stock_all`;show create table `sbed`.`sys_stock_all_log`;show create table `sbed`.`sys_stock_all_num`;show create table `sbed`.`sys_stock_all_num_view`;show create table `sbed`.`sys_stock_allocation`;show create table `sbed`.`sys_stock_allocation_main`;show create table `sbed`.`sys_stock_goods`;show create table `sbed`.`sys_stock_store`;show create table `sbed`.`sys_stock_store_position`;show create table `sbed`.`sys_store`;show create table `sbed`.`sys_store_model`;show create table `sbed`.`sys_supplier`;show create table `sbed`.`sys_taobao_goods`;show create table `sbed`.`sys_taobao_goods_detail`;show create table `sbed`.`sys_taobao_goods_detail_item_imgs`;show create table `sbed`.`sys_taobao_goods_detail_location`;show create table `sbed`.`sys_taobao_goods_detail_prop_imgs`;show create table `sbed`.`sys_taobao_goods_detail_sku`;show create table `sbed`.`sys_taobao_goods_price`;show create table `sbed`.`sys_taobao_logistics_company`;show create table `sbed`.`sys_taobao_order`;show create table `sbed`.`sys_taobao_trade`;show create table `sbed`.`sys_taxonomy`;show create table `sbed`.`sys_third_nulaxgroup`;show create table `sbed`.`sys_tianmao_activity`;show create table `sbed`.`sys_tianmao_activity_goods`;show create table `sbed`.`sys_trade`;show create table `sbed`.`sys_trade_goods`;show create table `sbed`.`sys_trade_logistics`;show create table `sbed`.`sys_user`;show create table `sbed`.`sys_user_brand`;show create table `sbed`.`sys_user_formthead`;show create table `sbed`.`sys_user_role`;show create table `sbed`.`sys_user_store`;show create table `sbed`.`sys_user_token`;show create table `sbed`.`sys_value`;show create table `sbed`.`sys_wave_house_cost`;show create table `sbed`.`sys_yeebao_pay`;show create table `sbed`.`xhs_order`;show create table `sbed`.`xhs_order_detail`;show create table `sbed`.`xhs_stock`; at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.queryMulti(MysqlQueryExecutor.java:109) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.queryMulti(MysqlConnection.java:101) at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:173) at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:84) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) at java.lang.Thread.run(Thread.java:748) ] Execute that show command to see the error The same problem canal1 1 1 How to solve the help @agapple Execute the show command that gave the error @shuaicloud Thanks agapple Executed multiple shows create table XXXXX; No error 。 Mysql5 7 18 has multiple database permissions on it Newspaper ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address xxxxx/xxxxx:3306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: ErrorPacket [errorNumber=1046 fieldCount=-1 message=No database selected sqlState=3D000 sqlStateMarker=#] mysql5.6.40 The same set of canal service has no problem permission Execute multi-statement show on 5 7 create Table is not error? > Execute multi-statement show on 5 7 create Table is not error? Yes
765,Canal monitoring docking prometheus grafana Many people have feedback on how to effectively monitor canal The initial thinking of the server is to choose the current popular prometheus grafana Monitored indicator 1. Server level status (instance list / client list) + (cpu / jvm gc / net / io ...) 2. Instance level status (delay / tps / ringbuffer / heartbeat ... ) If you have already done a similar solution I welcome everyone to submit PR to me. cpu/jvm gc These can be used directly as exports provided by simpleclient_hotspot @agapple I’m just learning about the content recently. Can I mention pr? Very welcome to submit two PR according to @agapple The comments combine with some of the feelings of their own use to implement the canal Some ideas of the server itself monitoring First list some indicators to open parallel mode analysis as an example PS By with instance Tag can collect and summarize indicators in the instance level 1. From read to store put a. Canal_inbound_bytes representing inbound traffic b. Indicates the dump thread pushlish events -> disruptor Buffer blocking time slice canal_dump_publish_blocking_time c. Indicates sink thread put events -> Store blocking time slice canal_sink_put_blocking_time d. The delegate delay can be obtained by CanalEventDownStreamHandler Canal_instance_delay various events The classification statistics of type determine whether there is a batch SQL or the like in the near future by the number of rowData attached to rowchange. 2. netty The server side uses the SessionHandler to call write to increase the ChannelFutureListener callback to sample the client&#39;s request and completion information, including destination. PacketType write amount batchId only judges whether GET is empty or not error code response time Wait a. Canal_inbound_bytes representing outbound traffic b. Classification statistics of each packetType package c. Empty packet rate d. The error occurred is as error Code statistics e. Response time 3. event Store more natural buried point a. TPS is calculated according to ackSequence b. If it is memory mode MPS is calculated according to ackMemSize c. store remain events Quantity d. store remain memory Some scenes - 1-b No obvious blocking，1-d Delay increases by 1 a Small flow --> It can be roughly inferred whether the bottleneck is insufficient in the network or the traffic is crowded. - 1-b Significant blocking，1-c No obvious blocking client spending quickly Can try to increase the canal instance parser parallelThreadSize to speed up the parsing speed - 1-b with 1 c has significant blocking Delay is increasing --> The bottleneck lies in client read and write and consumption - 2-c High air frequency --> Consider adjusting the timeout time of the optimized get - 2-b && 2-d --> Whether the client&#39;s behavior is normal Welcome comments BTW New pulled a branch metrics_support @lcybo Considered a very comprehensive point of praise :) <img width="910" alt="grafana" src="https://user-images.githubusercontent.com/15042781/44220795-30379600-a1b2-11e8-9f56-24dd8395f0bf.PNG"> Sticking a preview image of the home environment traffic is not easy to simulate complex scenes awesome perfect! <img width="251" alt="down" src="https://user-images.githubusercontent.com/15042781/44293887-3f0d6e00-a2c2-11e8-92e7-da3ccae6c991.PNG"> Generate template to increase datasource convenience configuration instanes switch Submitted template canal conf metrics Canal_instances_tmpl json Re-typed the dashboard Instructions for use have also come to an end I will fix it as soon as possible if there is a problem. ![image](https://user-images.githubusercontent.com/834743/44321609-8ab74780-a47b-11e8-80c2-6d5baab962fa.png) The final rendering can be found in https github com alibaba canal wiki Prometheus QuickStart
764,canal 1.0.26 Occasionally disconnected and then connected from the new Partial synchronization stop scenes to be used 1 Synchronization task for connecting 20 databases to a single virtual machine A large number of logs are suddenly appearing at a certain moment. But after the new connection Part of the database is in sync Partial database synchronization stops Zk&#39;s cursor does not change Stay in the disconnected place log as follows MysqlConnector - |disConnect Mysql Connection to 10.x.x.x/10.x.x.x:3306..... MysqlConnector - |connect Mysql Connection to 10.x.x.x/10.x.x.x:3306..... MysqlConnector - |handshake initialization packet received prepare the client authentication packet to send MysqlConnector - |client authentication packet is sent out. There have been many such cases at present. Unknown reason Too little information can&#39;t be located, you can look at the jstack at the time. +1 Constantly kill dump以及disConnect connect The handshake intranet test is like this @kervin521 You have encountered this problem, or let me provide more information now in accordance with the meaning of the god Analysis of jstack Reopen after having a recurring method
763,Canal instance filter regex can only grab the first item after the first item data change can not get E.g ``` canal.instance.filter.regex=vip\\.crm.* .oms.* .ser.* .sku.* .vip.* .wfe.* ``` Can only get the data change oms of the crm prefix table and can not get the following take a look https://github.com/alibaba/canal/wiki/FAQ
762,Modify Readme md Add DTS instructions [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=762) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=762) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=762) it.</sub>
761,Add Canal commercial version update instructions Add Canal commercial version update instructions [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=761) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=761) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=761) it.</sub> repeated This follow-up will increase ，hdfs/hbase/ Other interface plugins are not
760,canal+OTTER Client side batchid 1 Canal version 1 0 22 After the configuration is started, the value obtained by the client using canalConnector getWithoutAck batchSize getId is always 1 and the cursor node has not generated the normal port log under 1001node on zk. Client side log ![image](https://user-images.githubusercontent.com/41414514/42923813-0c834b50-8b5a-11e8-83f4-bf00bdb56d34.png) Set the binlog name on the canal side and the position still can&#39;t create the cursor and the file exists. Has anyone encountered such a problem? Ask for advice. Look at canal Whether the log on the server is abnormal
759,append gtid lastCommitted and sequenceNumber support [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=759) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=759) before we can accept your contribution.<br/><hr/>**winger** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=759) it.</sub> tks
758,Client switch delay problem Deployed two client terminals to listen to the same destination through zk. Look at the document. It does not seem to describe the time interval for getting data when two clients switch. The two client terminals are kept through the while true mode. Subscribe connect getMessage It is reasonable to say that the time interval should be the time when the zk handles the state change. I set it. Canal zookeeper flush period 1000 From the results, the interval is about 30S each time and the configuration is quite different. Is there a way to shorten this interval? The business has high requirements for real-time performance. This is zookeeper&#39;s sessionTimeout. You can find the parameters on the Internet to lower it.
757,used 1.0.26 Why is the p3 version? EventType.QUERY Printed sql is insert or it could be create table？ > DEBUG com.pamirs.data.AbstractCanalClient - Event Type Is Query sql --> UPDATE T_PDC_HANDOVER_INDEX_OR code show as below ```java if (eventType == EventType.QUERY || rowChage.getIsDdl()) { if (rowChage.getIsDdl()) { StringBuilder exceptionMessage = new StringBuilder(); exceptionMessage.append("mysql has an ddl sql is [ ").append(rowChage.getSql()).append(" ]"); ExceptionHandle.handleException(new ExceptionMessageEntity(entityTask.getTableTask().getId() ExceptionMessageEntityDesc.PAMIRS_DATA_MYSQL_PULL_DDL_MESSAGE ExceptionMessageTypeEnum.pamirs_data exceptionMessage.toString())); } else { logger.debug("Event Type Is Query sql --> {}" rowChage.getSql()); } return; } ``` RowsQueryLogEvent ， After 5 6 the row mode binlog will be mixed with statement and row. For insert update delete, the corresponding original SQL will also be recorded. This sql is the goods. @agapple How should we distinguish?
756,com.lmax.disruptor.FatalExceptionHandler handleEventException Just understand canal directly with canal kafka 1 0 26 SNAPSHOT tar package will report a com alibaba druid pool DruidDataSource - testWhileIdle is true validationQuery not Set but can still be used but there is an error when running CanalLauncher java with kafka package Image https user images githubusercontent com 26325971 42809370 19b008e0 89e8 11e8 8ba0 bc520efcb083 png Is there anything I am doing wrong? Really can&#39;t find the error where the landlord helps me。 Image https user images githubusercontent com 26325971 42810158 d57bd4ae 89e9 11e8 95a9 97a38b5d6402 png Profile Information ![image](https://user-images.githubusercontent.com/26325971/42810224-04150a7e-89ea-11e8-954b-1855e2d8fe0b.png) ![image](https://user-images.githubusercontent.com/26325971/42813766-a1392646-89f4-11e8-82ea-3279b7b1e13a.png) How can I solve this problem? ![image](https://user-images.githubusercontent.com/834743/42819117-28463a7e-8a05-11e8-9058-64224e83e062.png) Duplicate slaveId Optimized the log output to remove the useless I also saw this duplicate slaveId error but I only have one in the Instance prorerties that only defines one Image https user images githubusercontent com 26325971 42854088 b68f65c8 8a6c 11e8 94c2 abc81424ca81 png Where else to change?
755,The configured filter regex does not take effect. The server does not filter out the sys library data. Sometimes the mysql library data is not filtered out. Version 1 0 26 Binlog mode ROW Instance properties configuration canal.instance.filter.regex=blacklist3\\..* canal.instance.filter.black.regex= Server log 2018-07-17 15:45:09.388 [destination = xxxx address = /xxxx:xxxx EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:xxxx[com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: ErrorPacket [errorNumber=1142 fieldCount=-1 message=SHOW VIEW command denied to user 'xxxx'@'xxxx for table 'host_summary' sqlState=42000 sqlStateMarker=#] with command: show create table `sys`.`host_summary`;show create table `sys`.`host_summary_by_file_io`;show create table `sys`.`host_summary_by_file_io_type`;show create table `sys`.`host_summary_by_stages`;show create table `sys`.`host_summary_by_statement_latency`;show create table `sys`.`host_summary_by_statement_type`;show create table `sys`.`innodb_buffer_stats_by_schema`;show create table `sys`.`innodb_buffer_stats_by_table`;show create table `sys`.`innodb_lock_waits`;show create table `sys`.`io_by_thread_by_latency`;show create table `sys`.`io_global_by_file_by_bytes`;show create table `sys`.`io_global_by_file_by_latency`;show create table `sys`.`io_global_by_wait_by_bytes`;show create table `sys`.`io_global_by_wait_by_latency`;show create table `sys`.`latest_file_io`;show create table `sys`.`memory_by_host_by_current_bytes`;show create table `sys`.`memory_by_thread_by_current_bytes`;show create table `sys`.`memory_by_user_by_current_bytes`;show create table `sys`.`memory_global_by_current_bytes`;show create table `sys`.`memory_global_total`;show create table `sys`.`metrics`;show create table `sys`.`processlist`;show create table `sys`.`ps_check_lost_instrumentation`;show create table `sys`.`schema_auto_increment_columns`;show create table `sys`.`schema_index_statistics`;show create table `sys`.`schema_object_overview`;show create table `sys`.`schema_redundant_indexes`;show create table `sys`.`schema_table_lock_waits`;show create table `sys`.`schema_table_statistics`;show create table `sys`.`schema_table_statistics_with_buffer`;show create table `sys`.`schema_tables_with_full_table_scans`;show create table `sys`.`schema_unused_indexes`;show create table `sys`.`session`;show create table `sys`.`session_ssl_status`;show create table `sys`.`statement_analysis`;show create table `sys`.`statements_with_errors_or_warnings`;show create table `sys`.`statements_with_full_table_scans`;show create table `sys`.`statements_with_runtimes_in_95th_percentile`;show create table `sys`.`statements_with_sorting`;show create table `sys`.`statements_with_temp_tables`;show create table `sys`.`sys_config`;show create table `sys`.`user_summary`;show create table `sys`.`user_summary_by_file_io`;show create table `sys`.`user_summary_by_file_io_type`;show create table `sys`.`user_summary_by_stages`;show create table `sys`.`user_summary_by_statement_latency`;show create table `sys`.`user_summary_by_statement_type`;show create table `sys`.`version`;show create table `sys`.`wait_classes_global_by_avg_latency`;show create table `sys`.`wait_classes_global_by_latency`;show create table `sys`.`waits_by_host_by_latency`;show create table `sys`.`waits_by_user_by_latency`;show create table `sys`.`waits_global_by_latency`;show create table `sys`.`x$host_summary`;show create table `sys`.`x$host_summary_by_file_io`;show create table `sys`.`x$host_summary_by_file_io_type`;show create table `sys`.`x$host_summary_by_stages`;show create table `sys`.`x$host_summary_by_statement_latency`;show create table `sys`.`x$host_summary_by_statement_type`;show create table `sys`.`x$innodb_buffer_stats_by_schema`;show create table `sys`.`x$innodb_buffer_stats_by_table`;show create table `sys`.`x$innodb_lock_waits`;show create table `sys`.`x$io_by_thread_by_latency`;show create table `sys`.`x$io_global_by_file_by_bytes`;show create table `sys`.`x$io_global_by_file_by_latency`;show create table `sys`.`x$io_global_by_wait_by_bytes`;show create table `sys`.`x$io_global_by_wait_by_latency`;show create table `sys`.`x$latest_file_io`;show create table `sys`.`x$memory_by_host_by_current_bytes`;show create table `sys`.`x$memory_by_thread_by_current_bytes`;show create table `sys`.`x$memory_by_user_by_current_bytes`;show create table `sys`.`x$memory_global_by_current_bytes`;show create table `sys`.`x$memory_global_total`;show create table `sys`.`x$processlist`;show create table `sys`.`x$ps_digest_95th_percentile_by_avg_us`;show create table `sys`.`x$ps_digest_avg_latency_distribution`;show create table `sys`.`x$ps_schema_table_statistics_io`;show create table `sys`.`x$schema_flattened_keys`;show create table `sys`.`x$schema_index_statistics`;show create table `sys`.`x$schema_table_lock_waits`;show create table `sys`.`x$schema_table_statistics`;show create table `sys`.`x$schema_table_statistics_with_buffer`;show create table `sys`.`x$schema_tables_with_full_table_scans`;show create table `sys`.`x$session`;show create table `sys`.`x$statement_analysis`;show create table `sys`.`x$statements_with_errors_or_warnings`;show create table `sys`.`x$statements_with_full_table_scans`;show create table `sys`.`x$statements_with_runtimes_in_95th_percentile`;show create table `sys`.`x$statements_with_sorting`;show create table `sys`.`x$statements_with_temp_tables`;show create table `sys`.`x$user_summary`;show create table `sys`.`x$user_summary_by_file_io`;show create table `sys`.`x$user_summary_by_file_io_type`;show create table `sys`.`x$user_summary_by_stages`;show create table `sys`.`x$user_summary_by_statement_latency`;show create table `sys`.`x$user_summary_by_statement_type`;show create table `sys`.`x$wait_classes_global_by_avg_latency`;show create table `sys`.`x$wait_classes_global_by_latency`;show create table `sys`.`x$waits_by_host_by_latency`;show create table `sys`.`x$waits_by_user_by_latency`;show create table `sys`.`x$waits_global_by_latency`; Caused by: java.io.IOException: ErrorPacket [errorNumber=1142 fieldCount=-1 message=SHOW VIEW command denied to user 'xxxx'@'xxxx' for table 'host_summary' sqlState=42000 sqlStateMarker=#] with command: show create table `sys`.`host_summary`;show create table `sys`.`host_summary_by_file_io`;show create table `sys`.`host_summary_by_file_io_type`;show create table `sys`.`host_summary_by_stages`;show create table `sys`.`host_summary_by_statement_latency`;show create table `sys`.`host_summary_by_statement_type`;show create table `sys`.`innodb_buffer_stats_by_schema`;show create table `sys`.`innodb_buffer_stats_by_table`;show create table `sys`.`innodb_lock_waits`;show create table `sys`.`io_by_thread_by_latency`;show create table `sys`.`io_global_by_file_by_bytes`;show create table `sys`.`io_global_by_file_by_latency`;show create table `sys`.`io_global_by_wait_by_bytes`;show create table `sys`.`io_global_by_wait_by_latency`;show create table `sys`.`latest_file_io`;show create table `sys`.`memory_by_host_by_current_bytes`;show create table `sys`.`memory_by_thread_by_current_bytes`;show create table `sys`.`memory_by_user_by_current_bytes`;show create table `sys`.`memory_global_by_current_bytes`;show create table `sys`.`memory_global_total`;show create table `sys`.`metrics`;show create table `sys`.`processlist`;show create table `sys`.`ps_check_lost_instrumentation`;show create table `sys`.`schema_auto_increment_columns`;show create table `sys`.`schema_index_statistics`;show create table `sys`.`schema_object_overview`;show create table `sys`.`schema_redundant_indexes`;show create table `sys`.`schema_table_lock_waits`;show create table `sys`.`schema_table_statistics`;show create table `sys`.`schema_table_statistics_with_buffer`;show create table `sys`.`schema_tables_with_full_table_scans`;show create table `sys`.`schema_unused_indexes`;show create table `sys`.`session`;show create table `sys`.`session_ssl_status`;show create table `sys`.`statement_analysis`;show create table `sys`.`statements_with_errors_or_warnings`;show create table `sys`.`statements_with_full_table_scans`;show create table `sys`.`statements_with_runtimes_in_95th_percentile`;show create table `sys`.`statements_with_sorting`;show create table `sys`.`statements_with_temp_tables`;show create table `sys`.`sys_config`;show create table `sys`.`user_summary`;show create table `sys`.`user_summary_by_file_io`;show create table `sys`.`user_summary_by_file_io_type`;show create table `sys`.`user_summary_by_stages`;show create table `sys`.`user_summary_by_statement_latency`;show create table `sys`.`user_summary_by_statement_type`;show create table `sys`.`version`;show create table `sys`.`wait_classes_global_by_avg_latency`;show create table `sys`.`wait_classes_global_by_latency`;show create table `sys`.`waits_by_host_by_latency`;show create table `sys`.`waits_by_user_by_latency`;show create table `sys`.`waits_global_by_latency`;show create table `sys`.`x$host_summary`;show create table `sys`.`x$host_summary_by_file_io`;show create table `sys`.`x$host_summary_by_file_io_type`;show create table `sys`.`x$host_summary_by_stages`;show create table `sys`.`x$host_summary_by_statement_latency`;show create table `sys`.`x$host_summary_by_statement_type`;show create table `sys`.`x$innodb_buffer_stats_by_schema`;show create table `sys`.`x$innodb_buffer_stats_by_table`;show create table `sys`.`x$innodb_lock_waits`;show create table `sys`.`x$io_by_thread_by_latency`;show create table `sys`.`x$io_global_by_file_by_bytes`;show create table `sys`.`x$io_global_by_file_by_latency`;show create table `sys`.`x$io_global_by_wait_by_bytes`;show create table `sys`.`x$io_global_by_wait_by_latency`;show create table `sys`.`x$latest_file_io`;show create table `sys`.`x$memory_by_host_by_current_bytes`;show create table `sys`.`x$memory_by_thread_by_current_bytes`;show create table `sys`.`x$memory_by_user_by_current_bytes`;show create table `sys`.`x$memory_global_by_current_bytes`;show create table `sys`.`x$memory_global_total`;show create table `sys`.`x$processlist`;show create table `sys`.`x$ps_digest_95th_percentile_by_avg_us`;show create table `sys`.`x$ps_digest_avg_latency_distribution`;show create table `sys`.`x$ps_schema_table_statistics_io`;show create table `sys`.`x$schema_flattened_keys`;show create table `sys`.`x$schema_index_statistics`;show create table `sys`.`x$schema_table_lock_waits`;show create table `sys`.`x$schema_table_statistics`;show create table `sys`.`x$schema_table_statistics_with_buffer`;show create table `sys`.`x$schema_tables_with_full_table_scans`;show create table `sys`.`x$session`;show create table `sys`.`x$statement_analysis`;show create table `sys`.`x$statements_with_errors_or_warnings`;show create table `sys`.`x$statements_with_full_table_scans`;show create table `sys`.`x$statements_with_runtimes_in_95th_percentile`;show create table `sys`.`x$statements_with_sorting`;show create table `sys`.`x$statements_with_temp_tables`;show create table `sys`.`x$user_summary`;show create table `sys`.`x$user_summary_by_file_io`;show create table `sys`.`x$user_summary_by_file_io_type`;show create table `sys`.`x$user_summary_by_stages`;show create table `sys`.`x$user_summary_by_statement_latency`;show create table `sys`.`x$user_summary_by_statement_type`;show create table `sys`.`x$wait_classes_global_by_avg_latency`;show create table `sys`.`x$wait_classes_global_by_latency`;show create table `sys`.`x$waits_by_host_by_latency`;show create table `sys`.`x$waits_by_user_by_latency`;show create table `sys`.`x$waits_global_by_latency`; at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.queryMulti(MysqlQueryExecutor.java:109) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.queryMulti(MysqlConnection.java:101) at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:173) at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:84) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) at java.lang.Thread.run(Thread.java:745) Debug look at the DatabaseTableMeta dumpTableMeta whether the filter condition is not correctly received by the client&#39;s subscribe condition Should be this problem Switching between two instances on the server side The client side configuration will affect the server side as follows. ``` String filter = ".*"; String filter = "\\.*"; String filter = "\\.\\*"; String filter = ".*\\\\..*"; ``` Looked at the source code If set to null, it will not affect String filter = null; The specific impact is that the server that is started later will subscribe to the table other than the server-side filter, resulting in no permission to report the error. The client getMessage is always empty. Two server instance configurations canal.properties In addition to canal id Canal port is the same instance.properties apart from Canal instance mysql slaveId is the same `canal.instance.filter.regex=blacklist3\\..*`
754,fix issue #751 [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=754) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=754) before we can accept your contribution.<br/><hr/>**winger** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=754) it.</sub> [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=754) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=754) before we can accept your contribution.<br/><hr/>**winger** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=754) it.</sub> tks
753,com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed Running for a while Always report this error Version 1 0 26 com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4832) ~[guava-18.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:160) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:759) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:428) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:114) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:66) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:337) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:184) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:145) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] Caused by: java.io.IOException: should execute connector.connect() first at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.<init>(MysqlQueryExecutor.java:30) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:87) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMetaByDB(TableMetaCache.java:80) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:30) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:55) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:50) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache.get(LocalCache.java:3937) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830) ~[guava-18.0.jar:na] ... 10 common frames omitted 2018-07-17 10:48:26.646 [destination = example address = /172.16.203.11:3309 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address 172.16.203.11/172.16.203.11:3309 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4832) ~[guava-18.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:160) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:759) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:428) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:114) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:66) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:337) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:184) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:145) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] Caused by: java.io.IOException: should execute connector.connect() first at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.<init>(MysqlQueryExecutor.java:30) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:87) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMetaByDB(TableMetaCache.java:80) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:30) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:55) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:50) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache.get(LocalCache.java:3937) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830) ~[guava-18.0.jar:na] ... 10 common frames omitted 2018-07-17 10:48:26.646 [destination = example address = /172.16.203.11:3309 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4832) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:160) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:759) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:428) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:114) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:66) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:337) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:184) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:145) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) at java.lang.Thread.run(Thread.java:748) Caused by: java.io.IOException: should execute connector.connect() first at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.<init>(MysqlQueryExecutor.java:30) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:87) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMetaByDB(TableMetaCache.java:80) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:30) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:55) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:50) at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) at com.google.common.cache.LocalCache.get(LocalCache.java:3937) at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941) at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824) at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830) ... 10 more ] You can&#39;t download the latest 26 version of the code line.
752,GTID cannot be obtained ![6b06917b225fc61a76d0b901b656ecc9](https://user-images.githubusercontent.com/5210147/42793757-40d64d60-89ae-11e8-8090-f69238c5ed5a.jpg) My master and slave are both open gtid synchronization is normal only when there is transaction id in transactionend, other times can not get gtid and transaction id How did you get the gtid method? Have you seen the example code in the extended attribute field? Brother example entry.getHeader().getGtid() Printed out as an empty string Need to open canal Gtid&#39;s subscription instance.property canal.instance.gtidon=true Open this ok thanks！！！
751,canal Increase the tag within a transaction row data count Hi @agapple For large transaction scenarios, since the canal exceeds a certain number in the delivery process, the default 1024 will be sent in batches. For some business scenarios, it is necessary to know the row within a large transaction. The amount of data is then merged
750,Canal deployer 1 0 26 SNAPSHOT version after running the report point error The details of the false information are as follows: something goes wrong with channel:[id: 0x1d0b2315 /xx.xx.xx.xx:xxxx => /xx.xx.xx.xx:xxxx] exception=com.alibaba.otter.canal.server.exception.CanalServerException: ack error clientId:1001 batchId:261365 is not exist please check [New I/O server worker #1-12] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x1d0b2315 /xx.xx.xx.xx:xxxx :> /xx.xx.xx.xx:xxxx] exception=java.nio.channels.ClosedChannelException at org.jboss.netty.channel.socket.nio.NioWorker.cleanUpWriteBuffer(NioWorker.java:649) at org.jboss.netty.channel.socket.nio.NioWorker.writeFromUserCode(NioWorker.java:370) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:137) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) at org.jboss.netty.channel.Channels.write(Channels.java:611) at org.jboss.netty.channel.Channels.write(Channels.java:578) at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:28) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:144) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.cleanup(ReplayingDecoder.java:542) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.channelDisconnected(ReplayingDecoder.java:450) at org.jboss.netty.channel.Channels.fireChannelDisconnected(Channels.java:360) at org.jboss.netty.channel.socket.nio.NioWorker.close(NioWorker.java:599) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:119) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) at org.jboss.netty.channel.Channels.close(Channels.java:720) at org.jboss.netty.channel.AbstractChannel.close(AbstractChannel.java:208) at org.jboss.netty.channel.ChannelFutureListener$1.operationComplete(ChannelFutureListener.java:46) at org.jboss.netty.channel.DefaultChannelFuture.notifyListener(DefaultChannelFuture.java:381) at org.jboss.netty.channel.DefaultChannelFuture.addListener(DefaultChannelFuture.java:148) at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:30) at com.alibaba.otter.canal.server.netty.NettyUtils.error(NettyUtils.java:51) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:200) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:444) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261) at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:350) at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) clientId:1001 batchId:261365 is not exist please check The server estimates that an instance restart has occurred. Why does the restart occur and how to avoid this restart?
749,Fastsql 2 0 0_preview_371 in which maven download ```xml <dependency> <groupId>com.alibaba.fastsql</groupId> <artifactId>fastsql</artifactId> <version>2.0.0_preview_371</version> </dependency> ``` modify pom.xml replace fastsql Node content <dependency> <groupId>com.alibaba.fastsql</groupId> <artifactId>fastsql</artifactId> <version>2.0.0_preview_186</version> </dependency> 371 is in my binary In the master branch, is it necessary to change the fastsql dependency from 371 to a usable version? Otherwise, the import project can not find the jar. Missing artifact com.alibaba.fastsql:fastsql:jar:2.0.0_preview_520 Now I am missing this package, and I will pass it to maven.
748,How can the deal of the canal be too slow? MySQL show master status mysql-bin.000093 Canal&#39;s dump in zk parse mysql-bin.000064 Each binlog size is 100M dump entry: 30~40 event/second Show in MySQL full processlist: Show canal dump for # writing to net Are you a new version? @WithLin V1 0 25 v1 0 26 There is a problem with non-stop disconnecting the database and database connection timeout problem @agapple @kervin521 Specifically describe the problem of v1 0 26 You are too general
747,add restart.sh restart canal server [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=747) <br/>All committers have signed the CLA. [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=747) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=747) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=747) it.</sub> tks
746,canal server Connection database timeout Server connection database timeout and then no longer reconnected after about two hours Automatically reconnected. Which parameter can be set? How long does it take to reconnect after a timeout? You changed to v1 0 25 version, v1 0 26 has a problem v1.0.25 Is there any parameter setting? I have no problem, look at the log, is there an error? The 25 version has been marked as not recommending the use of nio @WithLin The 26 version of this non-stop disconnection is too big. After 7 pm, this situation is more changed to 25 version, no problem to see if there is a problem with the timer. @WithLin Reporting is a simple connection timeout connect timeout Feeling may be caused by network fluctuations but It took 2 hours to regain the connection. Is it too long after the timeout? How long does it take to re-acquire the connection without parameters that can be set by myself? It’s okay to retry the network. while(true) catch How long does it take to rest and try again?
745,canal Service deletes the temp log under the log and the service does not restart, causing disk space to be occupied. Such as canal The server found that the disk was occupied but did not find the specific occupation problem through lsof |grep Delete found a large amount of the following information java 37924 37925 appops 2w REG 254 1 50965216675 136005 /home/appops/canal/logs/canal/canal.log6671827459413464.tmp (deleted) Checked that the file is deleted and the process is still alive and thus the space is occupied. It is necessary to kill the process to recover. But the service is always restored by killing. It is definitely not convenient. Is there any good solution?
744,1 0 26 server is always repeated something goes wrong with channel Server 1 0 26 2018-07-10 15:50:20.197 [destination = example address = /110.110.10.27:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position {"identity":{"slaveId":-1 "sourceAddress":{"address":"10.110.10.27" "port":3306}} "postion":{"gtid":"" "included":false "journalName":"mysql-bin.000007" "position":313931 "serverId":27 "timestamp":1531208924000}} 2018-07-10 15:50:20.203 [destination = example address = /10.110.10.27:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000007 position=313931 serverId=27 gtid= timestamp=1531208924000] 2018-07-10 15:50:21.402 [New I/O server worker #1-1] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400 Caused by : something goes wrong with channel:[id: 0x65065848 /192.168.37.1:60600 => /192.168.37.129:11111] exception=com.alibaba.otter.canal.store.CanalStoreException: no match ack positionLogPosition[identity=LogIdentity[sourceAddress=10.110.10.27/10.110.10.27:3306 slaveId=-1] postion=EntryPosition[included=false journalName=mysql-bin.000007 position=313931 serverId=27 gtid= timestamp=1531208924000]] 2018-07-10 15:50:31.406 [New I/O server worker #1-1] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x65065848 /192.168.37.1:60600 :> /192.168.37.129:11111] exception=java.nio.channels.ClosedChannelException at org.jboss.netty.channel.socket.nio.NioWorker.cleanUpWriteBuffer(NioWorker.java:649) at org.jboss.netty.channel.socket.nio.NioWorker.writeFromUserCode(NioWorker.java:370) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:137) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) at org.jboss.netty.channel.Channels.write(Channels.java:611) at org.jboss.netty.channel.Channels.write(Channels.java:578) at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:28) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:144) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.cleanup(ReplayingDecoder.java:542) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.channelDisconnected(ReplayingDecoder.java:450) at org.jboss.netty.channel.Channels.fireChannelDisconnected(Channels.java:360) at org.jboss.netty.channel.socket.nio.NioWorker.close(NioWorker.java:599) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:119) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) at org.jboss.netty.channel.Channels.close(Channels.java:720) at org.jboss.netty.channel.AbstractChannel.close(AbstractChannel.java:208) at org.jboss.netty.channel.ChannelFutureListener$1.operationComplete(ChannelFutureListener.java:46) at org.jboss.netty.channel.DefaultChannelFuture.notifyListener(DefaultChannelFuture.java:381) at org.jboss.netty.channel.DefaultChannelFuture.addListener(DefaultChannelFuture.java:148) at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:30) at com.alibaba.otter.canal.server.netty.NettyUtils.error(NettyUtils.java:51) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:200) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:444) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261) at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:350) at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) 2018-07-10 15:50:32.315 [destination = example address = /10.110.10.27:3306 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 4052 sqlstate = HY000 errmsg = A slave with the same server_uuid/server_id as this slave has connected to the master; the first event 'mysql-bin.000007' at 313931 the last event read from 'mysql-bin.000007' at 313931 the last byte read from 'mysql-bin.000007' at 313962. at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_102] 2018-07-10 15:50:32.317 [destination = example address = /10.110.10.27:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address 10.110.10.27/10.110.10.27:3306 has an error retrying. caused by java.io.IOException: Received error packet: errno = 4052 sqlstate = HY000 errmsg = A slave with the same server_uuid/server_id as this slave has connected to the master; the first event 'mysql-bin.000007' at 313931 the last event read from 'mysql-bin.000007' at 313931 the last byte read from 'mysql-bin.000007' at 313962. at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_102] 2018-07-10 15:50:32.318 [destination = example address = /10.110.10.27:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: Received error packet: errno = 4052 sqlstate = HY000 errmsg = A slave with the same server_uuid/server_id as this slave has connected to the master; the first event 'mysql-bin.000007' at 313931 the last event read from 'mysql-bin.000007' at 313931 the last byte read from 'mysql-bin.000007' at 313962. at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) at java.lang.Thread.run(Thread.java:745) ] 2018-07-10 15:50:41.415 [New I/O server worker #1-2] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400 Caused by : something goes wrong with channel:[id: 0x650a9f4a /192.168.37.1:60616 => /192.168.37.129:11111] exception=com.alibaba.otter.canal.store.CanalStoreException: no match ack positionLogPosition[identity=LogIdentity[sourceAddress=10.110.10.27/10.110.10.27:3306 slaveId=-1] postion=EntryPosition[included=false journalName=mysql-bin.000007 position=313931 serverId=27 gtid= timestamp=1531208924000]] 2018-07-10 15:50:46.048 [destination = example address = /10.110.10.27:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position {"identity":{"slaveId":-1 "sourceAddress":{"address":"10.110.10.27" "port":3306}} "postion":{"gtid":"" "included":false "journalName":"mysql-bin.000007" "position":313931 "serverId":27 "timestamp":1531208924000}} 2018-07-10 15:50:46.052 [destination = example address = /10.110.10.27:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000007 position=313931 serverId=27 gtid= timestamp=1531208924000] 2018-07-10 15:50:51.418 [New I/O server worker #1-2] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x650a9f4a /192.168.37.1:60616 :> /192.168.37.129:11111] exception=java.nio.channels.ClosedChannelException at org.jboss.netty.channel.socket.nio.NioWorker.cleanUpWriteBuffer(NioWorker.java:649) at org.jboss.netty.channel.socket.nio.NioWorker.writeFromUserCode(NioWorker.java:370) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:137) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) at org.jboss.netty.channel.Channels.write(Channels.java:611) at org.jboss.netty.channel.Channels.write(Channels.java:578) at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:28) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:144) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.cleanup(ReplayingDecoder.java:542) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.channelDisconnected(ReplayingDecoder.java:450) at org.jboss.netty.channel.Channels.fireChannelDisconnected(Channels.java:360) at org.jboss.netty.channel.socket.nio.NioWorker.close(NioWorker.java:599) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:119) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) at org.jboss.netty.channel.Channels.close(Channels.java:720) at org.jboss.netty.channel.AbstractChannel.close(AbstractChannel.java:208) at org.jboss.netty.channel.ChannelFutureListener$1.operationComplete(ChannelFutureListener.java:46) at org.jboss.netty.channel.DefaultChannelFuture.notifyListener(DefaultChannelFuture.java:381) at org.jboss.netty.channel.DefaultChannelFuture.addListener(DefaultChannelFuture.java:148) at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:30) at com.alibaba.otter.canal.server.netty.NettyUtils.error(NettyUtils.java:51) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:200) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:444) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261) at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:350) at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) 2018-07-10 15:51:00.260 [destination = example address = /10.110.10.27:3306 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 4052 sqlstate = HY000 errmsg = A slave with the same server_uuid/server_id as this slave has connected to the master; the first event 'mysql-bin.000007' at 313931 the last event read from 'mysql-bin.000007' at 313931 the last byte read from 'mysql-bin.000007' at 313962. at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_102] 2018-07-10 15:51:00.261 [destination = example address = /10.110.10.27:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address 10.110.10.27/10.110.10.27:3306 has an error retrying. caused by java.io.IOException: Received error packet: errno = 4052 sqlstate = HY000 errmsg = A slave with the same server_uuid/server_id as this slave has connected to the master; the first event 'mysql-bin.000007' at 313931 the last event read from 'mysql-bin.000007' at 313931 the last byte read from 'mysql-bin.000007' at 313962. at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_102] 2018-07-10 15:51:00.261 [destination = example address = /10.110.10.27:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: Received error packet: errno = 4052 sqlstate = HY000 errmsg = A slave with the same server_uuid/server_id as this slave has connected to the master; the first event 'mysql-bin.000007' at 313931 the last event read from 'mysql-bin.000007' at 313931 the last byte read from 'mysql-bin.000007' at 313962. at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) at java.lang.Thread.run(Thread.java:745) ] 2018-07-10 15:51:01.425 [New I/O server worker #1-1] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400 Caused by : something goes wrong with channel:[id: 0x3f8a301b /192.168.37.1:60627 => /192.168.37.129:11111] exception=com.alibaba.otter.canal.store.CanalStoreException: no match ack positionLogPosition[identity=LogIdentity[sourceAddress=10.110.10.27/10.110.10.27:3306 slaveId=-1] postion=EntryPosition[included=false journalName=mysql-bin.000007 position=313931 serverId=27 gtid= timestamp=1531208924000]] 2018-07-10 15:51:11.430 [New I/O server worker #1-1] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x3f8a301b /192.168.37.1:60627 :> /192.168.37.129:11111] exception=java.nio.channels.ClosedChannelException at org.jboss.netty.channel.socket.nio.NioWorker.cleanUpWriteBuffer(NioWorker.java:649) at org.jboss.netty.channel.socket.nio.NioWorker.writeFromUserCode(NioWorker.java:370) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:137) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) at org.jboss.netty.channel.Channels.write(Channels.java:611) at org.jboss.netty.channel.Channels.write(Channels.java:578) at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:28) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:144) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.cleanup(ReplayingDecoder.java:542) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.channelDisconnected(ReplayingDecoder.java:450) at org.jboss.netty.channel.Channels.fireChannelDisconnected(Channels.java:360) at org.jboss.netty.channel.socket.nio.NioWorker.close(NioWorker.java:599) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:119) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) at org.jboss.netty.channel.Channels.close(Channels.java:720) at org.jboss.netty.channel.AbstractChannel.close(AbstractChannel.java:208) at org.jboss.netty.channel.ChannelFutureListener$1.operationComplete(ChannelFutureListener.java:46) at org.jboss.netty.channel.DefaultChannelFuture.notifyListener(DefaultChannelFuture.java:381) at org.jboss.netty.channel.DefaultChannelFuture.addListener(DefaultChannelFuture.java:148) at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:30) at com.alibaba.otter.canal.server.netty.NettyUtils.error(NettyUtils.java:51) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:200) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:444) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261) at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:350) at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) Client official exampledemo ---------------- END ----> transaction id: 17715 ================> binlog[mysql-bin.00000732Ï:313931] executeTime : 1531208924000(2018-07-10 15:48:44) gtid : () delay : 317551ms ---------------- END ----> transaction id: 17715 ================> binlog[mysql-bin.000007:313931] executeTime : 1531208924000(2018-07-10 15:48:44) gtid : () delay : 317552ms process error!com.alibaba.otter.canal.protocol.exception.CanalClientException: something goes wrong with reason: something goes wrong with channel:[id: 0x092dc999 /192.168.37.1:60768 => /192.168.37.129:11111] exception=com.alibaba.otter.canal.store.CanalStoreException: no match ack positionLogPosition[identity=LogIdentity[sourceAddress=10.110.10.27/10.110.10.27:3306 slaveId=-1] postion=EntryPosition[included=false journalName=mysql-bin.000007 position=313931 serverId=27 gtid= timestamp=1531208924000]] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.receiveMessages(SimpleCanalConnector.java:338) ~[classes/:na] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:315) ~[classes/:na] at com.alibaba.otter.canal.example.AbstractCanalClientTest.process(AbstractCanalClientTest.java:123) ~[classes/:na] at com.alibaba.otter.canal.example.AbstractCanalClientTest$2.run(AbstractCanalClientTest.java:87) [classes/:na] at java.lang.Thread.run(Thread.java:722) [na:1.7.0_17] Database 10 2 12 MariaDB What problem is the big god to solve Received error packet: errno = 4052 sqlstate = HY000 errmsg = A slave with the same server_uuid/server_id as this slave has connected to the master; the first event 'mysql-bin.000007' at 313931 the last event read from 'mysql-bin.000007' at 313931 the last byte read from 'mysql-bin.000007' at 313962.
743,About MySQL8 support Consult us this project has MySQL8 Is there a plan to provide support? Will support You have started using mysql8. @agapple Ok Already started to replace 8
742,Mysql and canal deployed problems on Tencent Cloud just now The mysql canal service deployed on Tencent Cloud is also a startup canal on Tencent Cloud. The error message after the server is as follows Also let the operation of the home and the canal account 2018-07-10 11:03:29.165 [Druid-ConnectionPool-Create-525230202] ERROR com.alibaba.druid.pool.DruidDataSource - create connection SQLException url: jdbc:h2:../conf/yunjiradartrace899/h2;CACHE_SIZE=1000;MODE=MYSQL; errorCode 28000 state 28000 org.h2.jdbc.JdbcSQLException: Wrong user name or password [28000-196] at org.h2.message.DbException.getJdbcSQLException(DbException.java:345) ~[h2-1.4.196.jar:1.4.196] at org.h2.message.DbException.get(DbException.java:179) ~[h2-1.4.196.jar:1.4.196] at org.h2.message.DbException.get(DbException.java:155) ~[h2-1.4.196.jar:1.4.196] at org.h2.message.DbException.get(DbException.java:144) ~[h2-1.4.196.jar:1.4.196] at org.h2.engine.Engine.validateUserAndPassword(Engine.java:336) ~[h2-1.4.196.jar:1.4.196] at org.h2.engine.Engine.createSessionAndValidate(Engine.java:162) ~[h2-1.4.196.jar:1.4.196] at org.h2.engine.Engine.createSession(Engine.java:137) ~[h2-1.4.196.jar:1.4.196] at org.h2.engine.Engine.createSession(Engine.java:27) ~[h2-1.4.196.jar:1.4.196] at org.h2.engine.SessionRemote.connectEmbeddedOrServer(SessionRemote.java:354) ~[h2-1.4.196.jar:1.4.196] at org.h2.jdbc.JdbcConnection.<init>(JdbcConnection.java:116) ~[h2-1.4.196.jar:1.4.196] at org.h2.jdbc.JdbcConnection.<init>(JdbcConnection.java:100) ~[h2-1.4.196.jar:1.4.196] at org.h2.Driver.connect(Driver.java:69) ~[h2-1.4.196.jar:1.4.196] at com.alibaba.druid.pool.DruidAbstractDataSource.createPhysicalConnection(DruidAbstractDataSource.java:1513) ~[druid-1.1.9.jar:1.1.9] at com.alibaba.druid.pool.DruidAbstractDataSource.createPhysicalConnection(DruidAbstractDataSource.java:1578) ~[druid-1.1.9.jar:1.1.9] at com.alibaba.druid.pool.DruidDataSource$CreateConnectionThread.run(DruidDataSource.java:2466) ~[druid-1.1.9.jar:1.1.9] determine Account password is not wrong Close tablemeta first Tsdb&#39;s ability This error is mainly caused by the local H2 error. If you have a better understanding of H2, you can try to locate or fix it. H2 as a table The historical version of ddl will be stored based on the default jdbc configuration of H2. ``` canal.instance.tsdb.dir=${canal.file.data.dir:../conf}/${canal.instance.destination:} canal.instance.tsdb.url=jdbc:h2:${canal.instance.tsdb.dir}/h2;CACHE_SIZE=1000;MODE=MYSQL; canal.instance.tsdb.dbUsername=canal canal.instance.tsdb.dbPassword=canal ``` Interpretation is the first time you will create conf instance instance mv db and set the access password to canal canal. If the second reopen will verify xx mv db whether there are multiple processes at the same time will appear java lang IllegalStateException The file is Locked will also verify that the access password for this time is the same as when it was first created. If you really encounter some inexplicable problems, the omnipotent solution removes the conf corresponding xx mv db will reinitialize an h2 local file
741,Canal throughput is reduced sharply 2018-07-09 08:10:22.493 - clientId:1001 cursor:[mysql-bin.000274 21149132 1530956958000] address[/10.132.27.103:3306] 2018-07-09 10:11:27.493 - clientId:1001 cursor:[mysql-bin.000274 25141059 1530957528000] address[/10.132.27.103:3306] Found canal log error 2018-07-09 13:39:29.043 [destination = cube address = /10.132.27.103:3306 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Unexpected End Stream at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) ~[canal.parse-1.0.21.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:78) ~[canal.parse-1.0.21.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) [canal.parse-1.0.21.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) [canal.parse-1.0.21.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.7.0_55] 2018-07-09 13:39:29.044 [destination = cube address = /10.132.27.103:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /10.132.27.103:3306 has an error retrying. caused by java.io.IOException: Unexpected End Stream at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) ~[canal.parse-1.0.21.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:78) ~[canal.parse-1.0.21.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) ~[canal.parse-1.0.21.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) ~[canal.parse-1.0.21.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.7.0_55] 2018-07-09 13:39:29.047 [destination = cube address = /10.132.27.103:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:cube[java.io.IOException: Unexpected End Stream at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:78) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) at java.lang.Thread.run(Thread.java:745) Today, on the 9th, I found that the synchronous super slow is going to update dozens of times per hour. It takes 1 2 hours for today&#39;s synchronization. Can the normal producers of the normal environment of the producers also have the guidance of the experts to check the problem? Can test 1 0 26 SNAPSHOT Synchronized for high volume data DML It is recommended to increase the jvm memory From my test, as long as the memory resources are large enough The resolution speed is still very stable Refer to the performance in the FAQ https github com alibaba canal wiki FAQ
740,Some questions in the wiki AdminGuide https://github.com/alibaba/canal/wiki/AdminGuide Right Is there a slight blur in the introduction of the difference in spring assembly? ![image](https://user-images.githubusercontent.com/13183268/42429351-1335062e-836b-11e8-9d1e-d0566544cbf8.png) Store is not only memory mode, the store storage here should be only memory It won&#39;t be stored in zookeeper and file. Your understanding is correct Store is only the ability of memory
739,Canal client 1 0 26 Which maven is SNAPSHOT pulled? ```xml <dependency> <groupId>com.alibaba.otter</groupId> <artifactId>canal.client</artifactId> <version>1.0.26-SNAPSHOT</version> </dependency> ``` The client version of 1 0 26 is not available in which maven library. mvn clean install My client program should use 1 0 26 SNAPSHOT Is it possible to package the project and then pull it 1 0 26 SNAPSHOT jar package to join its own project? Can not directly pull from the maven library? 1 0 25 SNAPSHOT in the public network maven Have
738,How does Alibaba Cloud RDS use GTID The RDS version used on Alibaba Cloud is 5 6 before using binlog position to synchronize data and try to set it. After the canal instance gtidon true ``` java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = The slave is connecting using CHANGE MASTER TO MASTER_AUTO_POSITION = 1 but the master has purged binary logs containing GTIDs that the slave requires. ``` Is it necessary to modify the configuration of the RDS? The version of my canal is 1 0 26 This requires mysql to set the gtid ability You can search for errmsg. Or look at my gtid issue I also encountered a similar problem. Checked that gtid has been enabled. There is a search because the GIT_PURGED value on the main library is causing the slave library to be unable to synchronize. Other posts do not know the reason. @agapple
737,Perfection of Parallel parameter details Some small details 60%processor Anyway will be :16 Overwrite Second 60 processor * 16 Most likely not meeting the ringbuffer limit I am very careful to see it.
736,com.alibaba.fastsql.sql.parser.ParserException Scene TSDB is turned on Fastsql version 371 I have encountered two types of exceptions after upgrading to the 371 version of fastsql. One Student-service 2018-07-05 17:10:13.017 WARN ???? [6 EventParser] c.a.o.c.p.i.m.t.MemoryTableMeta : [] parse faield : ALTER TABLE `platform`.`notice` CHANGE COLUMN `content` `content` VARCHAR(3000) CHARACTER SET 'utf8mb4' COLLATE 'utf8mb4_unicode_ci' NOT NULL DEFAULT unfilled COMMENT default com.alibaba.fastsql.sql.parser.ParserException at com.alibaba.fastsql.sql.parser.SQLExprParser.parseCharTypeRest(SQLExprParser.java:2910) ~[Student-service.jar!/:?] at com.alibaba.fastsql.sql.parser.SQLExprParser.parseDataType(SQLExprParser.java:2785) ~[Student-service.jar!/:?] at com.alibaba.fastsql.sql.parser.SQLExprParser.parseDataType(SQLExprParser.java:2666) ~[Student-service.jar!/:?] at com.alibaba.fastsql.sql.dialect.mysql.parser.MySqlExprParser.parseColumn(MySqlExprParser.java:463) ~[Student-service.jar!/:?] at com.alibaba.fastsql.sql.dialect.mysql.parser.MySqlStatementParser.parseAlterTable(MySqlStatementParser.java:4315) ~[Student-service.jar!/:?] at com.alibaba.fastsql.sql.dialect.mysql.parser.MySqlStatementParser.parseAlter(MySqlStatementParser.java:3307) ~[Student-service.jar!/:?] at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:248) ~[Student-service.jar!/:?] at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[Student-service.jar!/:?] at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[Student-service.jar!/:?] two Student-service 2018-07-05 17:32:42.210 WARN ???? [6 EventParser] c.a.o.c.p.i.m.t.MemoryTableMeta : [] parse faield : ALTER TABLE `student` DROP COLUMN `display_name` DROP COLUMN `system` MODIFY COLUMN `id` bigint(20) UNSIGNED NOT NULL AUTO_INCREMENT FIRST CHANGE COLUMN `name` `student_name` varchar(255) CHARACTER SET utf8 COLLATE utf8_unicode_ci NOT NULL DEFAULT '' COMMENT Student name AFTER `id` MODIFY COLUMN `description` varchar(255) CHARACTER SET utf8 COLLATE utf8_unicode_ci NOT NULL DEFAULT '' COMMENT School description AFTER `student_name` MODIFY COLUMN `created_at` timestamp NOT NULL AFTER `description` MODIFY COLUMN `updated_at` timestamp NOT NULL AFTER `created_at` CHANGE COLUMN `creator` `created_by` varchar(50) CHARACTER SET utf8 COLLATE utf8_unicode_ci NOT NULL AFTER `updated_at` CHANGE COLUMN `updated_name` `updated_by` varchar(50) CHARACTER SET utf8 COLLATE utf8_unicode_ci NOT NULL AFTER `created_by` ADD COLUMN `is_deleted` tinyint(1) NOT NULL DEFAULT 0 COMMENT Delete tag 0 not deleted 1 delete AFTER `description` com.alibaba.fastsql.sql.parser.ParserException: syntax error error in :'MODIFY COLUMN `id` bigint(20) UNSIGN pos 86 line 4 column 9 token COLUMN at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:361) ~[Student-service.jar!/:?] at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:498) ~[Student-service.jar!/:?] at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[Student-service.jar!/:?] at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[Student-service.jar!/:?] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:72) [Student-service.jar!/:?] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.apply(DatabaseTableMeta.java:104) [Student-service.jar!/:?] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.apply(TableMetaCache.java:228) [Student-service.jar!/:?] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseQueryEvent(LogEventConvert.java:265) [Student-service.jar!/:?] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:126) [Student-service.jar!/:?] Your second SQL is also complicated enough. The exception caused by the first parseCharTypeRest is more preferential than the second common trouble. Test found Alter In Table COLLATE Will cause parsing to fail The 2 0 0_preview_186 version also has this problem and is not related to TSDB. This is the problem of fastsql&#39;s parsing support for DDL. I will record and feedback The latest fastsql has been fixed Will the latest sql version be ？
735,High CPU usage after starting the client Start client connection canal The server&#39;s corresponding java process usage rate in the cpu instantly increased to 40 - 50 does not seem normal between the two, I see the client The inside of the demo uses while Infinite loops may be the reason why cpu is high. Does the client obtain data through continuous training of the server? Client code is as follows CanalConnector connector = CanalConnectors.newSingleConnector(new InetSocketAddress(canalServerIp canalServerPort) serverDestination canalUserName canalPassword); new Thread( () -> { while(true){ try{ connector.connect(); connector.subscribe(monitorDatabase+"\\..*"); while(true){ Message message = connector.getWithoutAck(batchSize); // Get the specified amount of data long batchId = message.getId(); int size = message.getEntries().size(); if (batchId != -1 && size != 0) { try{ processData(message.getEntries()); connector.ack(batchId); // Submit confirmation //Thread.sleep(500); }catch (Exception e){ e.printStackTrace(); Connector rollback If an exception occurs, you need to roll back the last read binlog location } } } }catch (Exception e){ e.printStackTrace(); Logger error and canal Server interaction exception }finally { connector.disconnect(); } } } ).start(); The following is the performance parameters of my machine, please pay attention to the corresponding java process Processes: 346 total 6 running 340 sleeping 2038 threads 15:09:09 Load Avg: 5.62 5.26 5.03 CPU usage: 58.75% user 19.6% sys 22.18% idle SharedLibs: 185M resident 57M data 27M linkedit. MemRegions: 78596 total 7644M resident 207M private 1706M shared. PhysMem: 16G used (2031M wired) 136M unused. VM: 1565G vsize 1110M framework vsize 0(0) swapins 0(0) swapouts. Networks: packets: 38214974/2633M in 38184606/2541M out. Disks: 284609/7890M read 251982/9950M written. PID COMMAND %CPU TIME #TH #WQ #PORT MEM PURG CMPRS PGRP 3399 mdworker 0.1 00:00.06 4 2 44 3252K+ 0B 0B 3399 3398 mdworker 0.0 00:00.06 4 2 44 3272K 0B 0B 3398 3397 mdworker 0.1 00:00.06 4 2 44 3260K+ 0B 0B 3397 3391 com.apple.au 0.0 00:00.01 2 2 16 972K 0B 0B 3391 3353 Google Chrom 0.0 00:00.10 15 1 113 13M 0B 0B 526 **3347 java 25.9 01:28.67 144/2 1 326 1327M+ 0B 0B 665** 3344 top 4.1 00:23.44 1/1 0 30 3668K+ 0B 0B 3344 **3333 java 38.9 03:29.43 30 1 98 260M 0B 0B 665 3322 java 48.9 04:08.49 33/1 1 104 606M 0B 0B 3312** 3295 tail 0.0 00:00.14 1 0 10 360K 0B 0B 3295 Also my version is v1.0.24 After the client starts, the server will start parsing the work.
734,add create database support Increase create database Statement support [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=734) <br/>All committers have signed the CLA. tks
733,v1.0.24 Canal instance filter regex is invalid Use version 1 0 24 Just need to listen to the binlog of two tables Set in the instance properties file canal.instance.defaultDatabaseName = mall canal.instance.filter.regex = mall.users mall.users_extra_info The client code did not write a connector subscribe statement But still received the binlog of all the tables in the mall database. Solve Regular should only be hardcoded to be ok. The other regular does not support complex cautiousness.
732,When is the version of oracle supported open source? Such as the title Oracle this piece will not open source Can refer to https github com alibaba yugong based on materialized view to make increments The rest can refer to Oracle&#39;s LogMiner tool or CDC to achieve or
731,Canal shows that the connection is successful but the database message is changed and the subscription message is not received. Canal start does not report an error, but no matter how the database is operated, there is no database change log printed out to ask God to advise ![image](https://user-images.githubusercontent.com/8399936/42198827-071f8a10-7ebd-11e8-95ee-d0438707b334.png) I also encountered a similar problem with the 26 version of the client version 24 server zk cluster connection startup application can be consumed but after a period of time through the stack found that the client program is blocked in the semaphore acquisition where is not handled well Deadlocked 2 client instance consumption solution already settled It is recommended to use the new version of v1 1 1
730,canal increase create Database support Pulled out 1 0 26 preview3 branch test currently canal in processing Query event create Database does not parser the build statement Call logic LogEventConvert#parseQueryEvent -> DruidDdlParser.parse(queryString event.getDbName()); -> fastsql_preview366#SQLCreateDatabaseStatement At present, there is no code for fastsql, which is roughly modified as follows. ![qq 20180702202624](https://user-images.githubusercontent.com/5847660/42163743-3e061f4e-7e36-11e8-8126-24af0d1386cf.jpg) The latest trunk Master also did not deal with this problem fastsql when open source _ You submit a MR to me. #734 Raised a PR GitHub MR how to operate
729,update Why is the statement ddl ？ ![image](https://user-images.githubusercontent.com/5965173/42151092-603695a8-7e0e-11e8-8386-8920b4bc4d70.png) As shown mysql 5.5.24 Is there any problem with this version support? Found that there will be problems with updated data loss and parsing exceptions. ![image](https://user-images.githubusercontent.com/5965173/42151207-cd8a93f2-7e0e-11e8-8250-6ac2445ef3a3.png) DDL syntax parsing errors do not affect DML data Manually update the database to observe that some data can be in canal client Successfully received some conclusions that did not result in data loss The current canal open source version supports 5 7 and below versions of Ali internal mysql 5.7.13 5.6.10 mysql 5 5 18 and 5 1 40 48 Therefore 5 5 24 The official version does not guarantee that the reliability of the data is lost. Currently canal test has supported mysql 5 7 13 5 6 10 and below mariab 5 5 35 and 10 0 7 theoretically support the following versions But here&#39;s a look at 5 5 24 It should be supported, so I don’t know what the reason is. Please answer it. Thank you D. This error is not related to mariadb. It is a simple SQL syntax parsing error that cannot accurately filter data and is not tight. Using Mysql5 5 24 ， How to locate the situation when data update is lost? PS AC can&#39;t be added, can you pull it? 469673467 Thank you ：D
728,Jar package conflict spring code和canal common Reference canal client 1 0 12 after the code is reported org.springframework.core.MethodParameter（Cannot resolve method 'hasParameterAnnotation(java.lang.Class<>） spring-code 4.3.14.RELEASE operating <dependency> <groupId>com.alibaba.otter</groupId> <artifactId>canal.client</artifactId> <version>1.0.12</version> <exclusions> ...... <exclusion> <artifactId>canal.common</artifactId> <groupId>com.alibaba.otter</groupId> </exclusion> </exclusions> Normal after dependency Run code error java lang NoClassDefFoundError com/alibaba/otter/canal/common/utils/BooleanMutex How to solve You are also a bit low in the canal version. Upgrade to 1 0 26
727,Canal support aliyun Rds binlog subscription Aliyun is a great company and is currently the highest proportion of the domestic cloud market. There are too many small partners in front and I have feedback to expect more comprehensive support for RDS. Binlog subscription aliyun RDS mainly meets the user&#39;s simple and convenient use of MySQL. For the rapid development of user services, it is expected to use the aliyun for future storage computing scalability. [DRDS（ Distributed Relational Database Service https www aliyun com product drds one based on MySQL Sharding&#39;s distributed database solution can also be used for data subscription based on canal because the storage itself is MySQL. First, let&#39;s clear a few questions that are usually faced by binlog subscriptions for this type of cloud MySQL. 1. Account permission problem solved * Canal&#39;s strategy is to simulate MySQL Slave&#39;s behavior therefore needs to have a SELECT REPLICATION SLAVE REPLICATION CLIENT permissions * Solution The current account created by RDS on aliyun has its own permissions for RDS. 5 6 5 7 high-privilege instance can use the root account for additional authorization and authorization operations. Refer to QuickStart. 2. Binlog deleted problem solved * Corresponding binlog cleanup strategy https help aliyun com knowledge_detail 41815 html After more than 18 hours, it will be deleted and backed up to oss. If the canal task stops for more than 18 hours, it will encounter xx similar error. * Solution RDS provides oss by default for a while Binlog downloadability reference documentation https help aliyun com document_detail 26291 html canal can identify the timestamp in the site to compare RDS show binary The earliest binlog in the logs will be downloaded to the machine through the oss interface for parsing and smoothing the historical binlog before switching to RDS. Continue to consume in binlog Canal code will support ![image](https://user-images.githubusercontent.com/834743/43991319-e6de1772-9d9c-11e8-9407-fe74e48da1ce.png) 3. Problems caused by active/standby switchover solved * The general cloud MySQL master and backup solution uses the vip mode to shield the active/standby switch between the backend physical nodes. That is, for the canal, only the single node MySQL is seen. Ip will encounter xx similar errors when holding the old main library location for physical and standby switching. * Solutions Canal can be used for MySQL5 6 The gtid subscription method is not supported by local binlog parsing when there is a problem 2. Or it is recommended to automatically identify the active/standby switch based on the serverId. Check the serverId and the current database node in the bin each time the binlog subscription is performed. Whether the serverId is consistent. If the inconsistency indicates that the server generates the active/standby switchover, you can relocate the corresponding binlog in the new main library based on the timestamp and continue the subsequent consumption. Here you need to consider the case where the binlog is deleted when locating the site. Code submission https github com alibaba canal commit ea6391d1c3231406e241ef09217ea1c78f885373 Specific aliyun Rds using the documentation https github com alibaba canal wiki aliyun RDS QuickStart
726,Canal overall performance optimization There are more friends before Feedback can&#39;t keep up with the speed of large-scale data DML changes Feedback list of questions 1. https://github.com/alibaba/canal/issues/672 2. https://github.com/alibaba/canal/issues/547 3. https://github.com/alibaba/canal/issues/355 4. https://github.com/alibaba/canal/issues/267 Here will be a relatively complete test for targeted optimization and also welcome everyone&#39;s participation and code MR to work together to solve the problem of performance and stability. ps. 1 0 26 will be a milestone version ---- Final optimization results https github com alibaba canal wiki Performance ![image](https://user-images.githubusercontent.com/834743/42986955-1f40b0d6-8c2a-11e8-8eb5-c5b885d8040a.png) Like praise Looking forward to ing looking forward to ing awesome Let me first throw a brick to talk about the issue and some ideas I have encountered before. The scene is using load backup File way to fill data while using canal for synchronization One or more files per table because the rows in each file belong to the same event belonging to a table. INSERT mysql binlog will merge a certain amount of rows depending on binlog row event max size to the same event Single sentence DML affects multiple rows of data should also apply The compressed event is parsed into a protobuf object. The memory usage is exploding and the fgc is frequent and even oom. In MySQL The official version of 5 6 defaults the binlog row event max size from 1024 to 8192. At that time we were bypassed by setting the binlog row event max size back to 1024. If the server client Netty3 upgrade to 4 with PooledByteBuf should be able to ease a small part but big head or protobuf this memory hogs. Add ringbuffer Size is set to 4096 memory is 4g due to load backup After file is smooth normal traffic, so I don&#39;t want to adjust the memory or further reduce the ringbuffer The first step of network optimization is only to do header analysis to identify the packet size and other information without specific record level resolution. The result is the 32k socket set by default. Buffer for large throughput transmission is a bit too small to remove the default settings for socket self-coordination by default on the 24c96g physical machine test receiveBuffer coordination result is 180k Performance throughput comparison before and after adjustment 18MB VS 117MB provides 6 times more sockets Buffer optimization can basically run full network card The second step of optimizing the parsing ability ran a simple object parsing without protobuf object construction. The initial speed is 20MB. The main optimization time field is improved to 45MB. At present, there is a bottleneck in the observation system. The load cpu is up to 1 5 cores and jvm. Gc is mainly concentrated in the next optimization of the young area. Multi-thread parallel analysis maximizes the use of system load for optimization. The third step of the overall concurrency model design according to the previous optimization network and object analysis bottlenecks are in the depth analysis of the object. If you want to maximize the performance, you must introduce the concurrent design. ![image](https://user-images.githubusercontent.com/834743/42207319-3e58ac2c-7edc-11e8-966e-33f46c57e4bc.png) The concurrency model based on ringbuffer will be divided into four phases. The phases are separated from each other and depend on the analysis of the most consumable bottleneck. This multi-threaded model is used to accelerate the entire optimization. The entire analysis can run through the entire Gigabit network card. ps. If you have a 10G network card model, you can also help run it. @lcybo What optimization suggestions do you have for memory usage? Replace protobuf Personal immature thoughts Protostuff based on protobuf and java can be used without cross-language Relative advantage - Faster than the protobuf takes up less memory size。 - Can use POJO without proto file generated by protobuf Code is too unfriendly to people Insufficient region - Community is not very active - To be added haha Post some details // Re-use (manage) this buffer to avoid allocating on every serialization LinkedBuffer buffer = LinkedBuffer.allocate(512); Contrast protobuf byte[] result = new byte[this.getSerializedSize()]; In addition, POJO can also refer to netty&#39;s Recycler or other object pool theory. The peak of the Entry object in the ringbuffer event is the number of objects that the ObjectPool needs to cache. ProtostuffIOUtil.mergeFrom(protostuff **fooParsed** schema); fooParsed is available for deserialization that can benefit from ObjectPool Of course, doing this code will be more complicated. The multiplexing of byte is The idea based on ObjectPool has never been considered before. The various nested structures in the object are also different. Do not understand the principle of reuse @lcybo Parallel parsing how to guarantee binlog Event ordering agapple The method is quite a chestnut to put the result as satellite data countdownlatch or other blocking or parallelstream, and finally no reorder Agapple is not convenient to go back to the codeword on the outside claw machine @lan1994 The serial and parallel combination of serials completes the basic object parsing and mainly processes some time-consuming long-term parsing of specific field parsing protobuf objects such as DML. At present, the preliminary test results of 24 concurrent can basically put a 24core physical machine cpu to run full throughput can run to 80MB s From the binlog received to generate CanalEntry overall optimization before about 7MB s to increase an order of magnitude the current bottleneck is basically on the cpu string serialization ratio is very high afternoon will continue to test from mysql -> canal server -> canal Overall throughput of the client About nested structures Suppose Row nests Column&#39;s list Both classes use ObjectPool to implement the Recycable interface and the recycle method. Clear data fields when calling recycle on Row. Call recycle for each Column nest and then list clear It should be noted that the embedded Column life cycle should not be longer than Row. About Netty&#39;s Recycler Every object new will be bind ThreadLocal&#39;s reclaim stack If there is a recycle get in the same thread then there will be no competition at all. If the two are separated, it will involve the WeakOrderQueue shared by the potential threads. According to the serial shallow parsing concurrent parsing proto object ringbuffer serial get The processing logic serializes the point in the SessionHandler&#39;s get will cause the depth to resolve the competition between the threads. When the thread stack is empty, they will go to the WeakOrderQueue to steal some objects. It looks like at least the netty&#39;s Recycler is not suitable for this application unless the serialization is early. Put the result into the ringbuffer reference io.netty.util.Recycler<T> Object reference io.netty.util.internal.RecyclableArrayList Serialization in BTW sessionhandler NettyUtils.write(ctx.getChannel() packetBuilder.build().toByteArray() null);// Output Data packetBuilder build toByteArray every time new Byte should have optimized space The third step optimization has completed the analysis. This introduces the ringbufer model and is divided into multiple stages. The network receives the simple parsing DML depth parsing and delivery to the store. The most time-consuming DML depth parsing in the middle is replaced by the concurrent parsing model. Received a test from the binlog to generate the Entry object. 1. About 5MB s before parallelization Single thread 2. Do parallelization transformation 16 concurrent parsing is probably 80MB S Basically positive correlation linear expansion Basically run the cpu bottleneck. If you want to further optimize, you can only optimize the construction protocol of the Entry object. The fourth step optimization has begun to focus on the throughput of the binlog parsing to the client receiving data. The current step-by-step pressure measurement is probably binlog download throughput in the 8MB s Entry object throughput is about 55MB s is about 1 7 data expansion rate At present, the bottleneck seen by the profile is mainly when the server serializes the Entry object. Compared with the network packet, the record of 50 tests is about 100 bytes. The converted tps is about 20w. Record s My test data is more compact in batch insert and update binlog ---- If you optimize this theoretical Entry object, you can run full network bandwidth and expect to increase the overall 150. Compared to the unoptimized because the network transmission bottleneck at the last end is relatively large, so I have eaten the improvement brought by the previous optimizations. If we want to further optimize the design of the protobuf protocol. ---- Preliminary optimization 1. Multithreading constructs the serialization result of the Entry in advance to avoid the client. Temporary serialization when get operation Because there is a bottleneck in single thread 2. If you change the protobuf protocol, you can compress some of the data storage by designing some dictionary tables. ![image](https://user-images.githubusercontent.com/834743/42286259-d9dede60-7fe4-11e8-9220-4ae70469bd12.png) Read the code parallel parameter some details Pr 737 https github com alibaba canal pull 737 awesome I welcome everyone to submit PR. I am thinking about some optimization details of protobuf. The main bottleneck is the serialization and byte amplification of protobuf. I will try to ensure compatibility, but it does not rule out the incompatibility of protocol design changes in extreme cases. so. If you have a good idea, you can give me feedback as soon as possible. enum Compression { NONE = 1; ZLIB = 2; GZIP = 3; LZF = 4; } The compression is reserved in proto. If it is concurrently parsed, this step is additionally serialized and concurrently compressed. SessionHandler there _packetBuilder setBody compressed data _ Then the client serially receives the parallel decompression and deserialization Parallel compression network transmission VS Uncompressed network transmission does not know how to perform 1. Single-threaded compression and network transmission will be a balance point 2. Data compression ratio after protobuf serialization also needs to be evaluated The fourth step of optimization has been carried out for more than half of the approximate effect compared to the total unoptimized before the increase of 35 throughput can go to 8900 TPS This pressure measurement data and the previous batch of inserts have chosen a random library of the business to run a 19MB binlog. The network transmission is about 35MB. Optimization point 1. The serialization of the CanalEntry object is completed before entering the ringbuffer. The final sessionHandler only makes a ByteString copy. 2. SimpleCanalConnector adds lazyParseEntry parameter to support lazy parsing of CanalEntry object to reduce the cost of the entire get ack serial operation to maximize serial throughput The biggest bottleneck in the current profile analysis is that there are multiple array copies when constructing network transmissions. Current code ``` Packet.Builder packetBuilder = CanalPacket.Packet.newBuilder(); packetBuilder.setType(PacketType.MESSAGES); Messages.Builder messageBuilder = CanalPacket.Messages.newBuilder(); messageBuilder.setBatchId(message.getId()); if (message.getId() != -1) { if (message.isRaw()) { // for performance if (!CollectionUtils.isEmpty(message.getRawEntries())) { messageBuilder.addAllMessages(message.getRawEntries()); } } else { if (!CollectionUtils.isEmpty(message.getEntries())) { for (Entry entry : message.getEntries()) { messageBuilder.addMessages(entry.toByteString()); } } } } packetBuilder.setBody(messageBuilder.build().toByteString()); NettyUtils.write(ctx.getChannel() packetBuilder.build().toByteArray() null);// Output Data ``` 1. messageBuilder build toByteString will copy all the data of message getRawEntries to a byte. 2. packetBuilder build toByteArray will copy the entire data of the message to a packet byte. 3. NettyUtils.write Network sending The original 10MB record will be at least the original 3x targeted optimization is straight code one-time construction of the protobuf data format rather than through the builder + The way toByteString is expected to increase by about 10 Try to bypass the multiple copies of protobuf with a byte array. The next step is to optimize how to reuse the byte array to avoid opening a new byte array for each request. Open a 10MB memory block at a time or have some overhead including the client level. Test conclusion https://github.com/alibaba/canal/wiki/Performance ![image](https://user-images.githubusercontent.com/834743/42986934-0bae041a-8c2a-11e8-8f31-f3f8e016bc8d.png)
725,EventParser] ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta EventParser] ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - pls submit github issue show create table ddl:CREATE TABLE `batch_job_seq` ( `ID` bigint(20) NOT NULL `UNIQUE_KEY` char(1) NOT NULL UNIQUE KEY `UNIQUE_KEY_UN` (`UNIQUE_KEY`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 compare failed . db : TableMeta [schema=test table=batch_job_seq fileds= FieldMeta [columnName=ID columnType=bigint(20) defaultValue=null nullable=false key=false] FieldMeta [columnName=UNIQUE_KEY columnType=char(1) defaultValue=null nullable=false key=true] ] mem : TableMeta [schema=test table=batch_job_seq fileds= FieldMeta [columnName=ID columnType=bigint(20) defaultValue=null nullable=false key=false] FieldMeta [columnName=UNIQUE_KEY columnType=char(1) defaultValue=null nullable=false key=false] ] You are the old version 25. Upgrade to 26 already fixed @qingmu2017 Will you solve this problem after upgrading to 26? I am using 26 SNAPSHOT or have this problem.
724,scheudle applySnapshotToDB faield ![image](https://user-images.githubusercontent.com/9798724/42080735-3392c10c-7bb6-11e8-8f56-23829972429c.png) 2018-06-27 22:09:34.820 [[scheduler-table-meta-snapshot]] ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - scheudle applySnapshotToDB faield com.alibaba.otter.canal.parse.exception.CanalParseException: java.net.SocketException: Broken pipe (Write failed) Caused by: java.net.SocketException: Broken pipe (Write failed) at java.net.SocketOutputStream.socketWrite0(Native Method) ~[na:1.8.0_144] at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111) ~[na:1.8.0_144] at java.net.SocketOutputStream.write(SocketOutputStream.java:143) ~[na:1.8.0_144] at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.write(BioSocketChannel.java:35) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.writeBody0(PacketManager.java:42) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.writeBody(PacketManager.java:35) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.query(MysqlQueryExecutor.java:55) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:94) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.compareTableMetaDbAndMemory(DatabaseTableMeta.java:296) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.applySnapshotToDB(DatabaseTableMeta.java:251) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.access$100(DatabaseTableMeta.java:45) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta$2.run(DatabaseTableMeta.java:84) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_144] at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) [na:1.8.0_144] at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) [na:1.8.0_144] at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) [na:1.8.0_144] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_144] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_144] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144] This is a scheduled schedule task Tomorrow will be the table in memory Meta data to do checkpoint In the process of the process will compare the memory value and the table structure of the database This exception is generated when the database table structure is obtained. pipe I will add a retry to ensure the validity of the network. My solution at the time was commented out. Canal properties about tsdb Comments such as canal instance tsdb spring xml and setting canal instance tsdb enable false This is not to understand the complete turn off the table Meta time series versioning function @agapple Is there any solution to this problem? @qingmu2017 Just set canal instance tsdb enable false to close the table Meta time series function
723,EventParser Always parser when encountering SQL statement execution error 2018-06-28 11:43:52.463 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2018-06-28 11:43:52.470 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties] 2018-06-28 11:43:52.727 [main] WARN o.s.beans.GenericTypeAwarePropertyDescriptor - Invalid JavaBean property 'connectionCharset' being accessed! Ambiguous write methods found next to actually used [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.nio.charset.Charset)]: [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.lang.String)] 2018-06-28 11:43:52.817 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2018-06-28 11:43:52.817 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties] 2018-06-28 11:43:53.154 [main] ERROR com.alibaba.druid.pool.DruidDataSource - testWhileIdle is true validationQuery not set 2018-06-28 11:43:53.598 [main] INFO c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-example 2018-06-28 11:43:53.743 [main] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - subscribe filter change to .*\..* 2018-06-28 11:43:53.743 [main] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - start successful.... 2018-06-28 11:43:53.779 [destination = example address = /172.16.0.2:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position {"identity":{"slaveId":-1 "sourceAddress":{"address":"172.16.0.2" "port":3306}} "postion":{"gtid":"" "included":false "journalName":"mysql-bin.000004" "position":15872 "serverId":1 "timestamp":1530090101000}} 2018-06-28 11:43:54.357 [destination = example address = /172.16.0.2:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : GRANT ALL PRIVILEGES ON *.* TO 'canal'@'%' IDENTIFIED WITH 'mysql_native_password' AS '*E3619321C1A937C46A0D8BD1DAC39F93B27D4458' com.alibaba.fastsql.sql.parser.ParserException: syntax error error in :'IDENTIFIED WITH 'mysql_native_password' A' expect BY actual WITH pos 58 line 1 column 55 token WITH at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:361) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.parser.SQLParser.accept(SQLParser.java:369) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseGrant(SQLStatementParser.java:1022) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:266) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.applyHistoryOnMemory(DatabaseTableMeta.java:382) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:121) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:84) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171] 2018-06-28 11:43:54.357 [destination = example address = /172.16.0.2:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : GRANT SELECT REPLICATION SLAVE REPLICATION CLIENT ON *.* TO 'canal'@'%' IDENTIFIED WITH 'mysql_native_password' AS '*E3619321C1A937C46A0D8BD1DAC39F93B27D4458' com.alibaba.fastsql.sql.parser.ParserException: syntax error error in :'IDENTIFIED WITH 'mysql_native_password' A' expect BY actual WITH pos 89 line 1 column 86 token WITH at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:361) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.parser.SQLParser.accept(SQLParser.java:369) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseGrant(SQLStatementParser.java:1022) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:266) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.applyHistoryOnMemory(DatabaseTableMeta.java:382) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:121) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:84) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171] 2018-06-28 11:43:54.384 [destination = example address = /172.16.0.2:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000004 position=15872 serverId=1 gtid= timestamp=1530090101000] 2018-06-28 11:43:54.543 [destination = example address = /172.16.0.2:3306 EventParser] WARN c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : CREATE TABLE `t_comment` ( `id` bigint NOT NULL AUTO_INCREMENT `chapter_id` bigint not NULL `comment_member_id` bigint not NULL `comment_nickname` varchar(32) not NULL `comment_headicon` varchar(256) not NULL `is_reply` int(11) DEFAULT 0 COMMENT Number of responses `content` TEXT NOT NULL COMMENT content `agrees` int(11) NOT NULL DEFAULT 0 COMMENT Endorsed `product_time` bigint(20) DEFAULT '0' COMMENT Production time `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8 COMMENT = Comment form **2018-06-28 11:43:54.577 [destination = example address = /172.16.0.2:3306 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Cannot replicate anonymous transaction when @@GLOBAL.GTID_MODE = ON at file ./mysql-bin.000004 position 16561.; the first event 'mysql-bin.000004' at 15872 the last event read from './mysql-bin.000004' at 16626 the last byte read from './mysql-bin.000004' at 16626.** at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171] 2018-06-28 11:43:54.577 [destination = example address = /172.16.0.2:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /172.16.0.2:3306 has an error retrying. caused by java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Cannot replicate anonymous transaction when @@GLOBAL.GTID_MODE = ON at file ./mysql-bin.000004 position 16561.; the first event 'mysql-bin.000004' at 15872 the last event read from './mysql-bin.000004' at 16626 the last byte read from './mysql-bin.000004' at 16626. at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171] 2018-06-28 11:43:54.580 [destination = example address = /172.16.0.2:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Cannot replicate anonymous transaction when @@GLOBAL.GTID_MODE = ON at file ./mysql-bin.000004 position 16561.; the first event 'mysql-bin.000004' at 15872 the last event read from './mysql-bin.000004' at 16626 the last byte read from './mysql-bin.000004' at 16626. at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) at java.lang.Thread.run(Thread.java:748) ] = Cannot replicate anonymous transaction when @@GLOBAL.GTID_MODE = ON at file ./mysql-bin.000004 position 16561.; the first event 'mysql-bin.000004' at 15872 the last event read from './mysql-bin.000004' at 16626 the last byte read from './mysql-bin.000004' at 16626. MySQL exception
722,canal Server and mysql deployed on the same machine to resolve binlog failed Canal version 1 0 25 Mysql version 5 7 10 At the beginning I will be canal The server is deployed on the same machine as mysql to start the normal operation of the database. The binlog log is updated but the client of the canal is not read. The server is deployed on other machines to capture the binlog information. It is not clear why this phenomenon occurs or is it that I missed the configuration? canal 25 version is not recommended Replaced a version 1 0 24 indeed
721,canal Cannot read bin log OpenJDK 64-Bit Server VM warning: ignoring option PermSize=96m; support was removed in 8.0 OpenJDK 64-Bit Server VM warning: ignoring option MaxPermSize=256m; support was removed in 8.0 OpenJDK 64-Bit Server VM warning: UseCMSCompactAtFullCollection is deprecated and will likely be removed in a future release. 2018-06-27 14:20:28.945 [main] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## set default uncaught exception handler 2018-06-27 14:20:29.015 [main] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## load canal configurations 2018-06-27 14:20:29.020 [main] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## start the canal server. 2018-06-27 14:20:29.096 [main] INFO com.alibaba.otter.canal.deployer.CanalController - ## start the canal server[172.16.0.2:8765] 2018-06-27 14:20:29.794 [main] WARN o.s.beans.GenericTypeAwarePropertyDescriptor - Invalid JavaBean property 'connectionCharset' being accessed! Ambiguous write methods found next to actually used [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.nio.charset.Charset)]: [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.lang.String)] 2018-06-27 14:20:30.194 [main] ERROR com.alibaba.druid.pool.DruidDataSource - testWhileIdle is true validationQuery not set 2018-06-27 14:20:30.634 [destination = example address = /172.16.0.2:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /172.16.0.2:3306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: connect /172.16.0.2:3306 failure Caused by: java.io.IOException: connect /172.16.0.2:3306 failure at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:77) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:79) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:87) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:163) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171] Caused by: java.io.IOException: Error When doing Client Authentication:ErrorPacket [errorNumber=1045 fieldCount=-1 message=Access denied for user 'canal'@'172.16.0.2' (using password: YES) sqlState=28000 sqlStateMarker=#] at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:199) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:74) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] ... 4 common frames omitted 2018-06-27 14:20:30.645 [destination = example address = /172.16.0.2:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: connect /172.16.0.2:3306 failure Caused by: java.io.IOException: connect /172.16.0.2:3306 failure at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:77) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:79) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:87) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:163) at java.lang.Thread.run(Thread.java:748) Caused by: java.io.IOException: Error When doing Client Authentication:ErrorPacket [errorNumber=1045 fieldCount=-1 message=Access denied for user 'canal'@'172.16.0.2' (using password: YES) sqlState=28000 sqlStateMarker=#] at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:199) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:74) ... 4 more ] 2018-06-27 14:20:30.659 [main] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## the canal server is running now ...... You want mysql to create a canal with a name GRANT SELECT REPLICATION SLAVE REPLICATION CLIENT ON *.* TO 'canal'@'%' IDENTIFIED BY 'canal';
720,canal.instance.filter.regex Is there a limit to the length of this property value? For example, because the same database table that the same mysql needs to take in the demand is different, I am worried that this value is getting bigger and bigger. Is there a length limit? Regular expressions have a length limit. I think it is 4k.
719,Why can&#39;t clients connect to different instances? 2018-06-27 11:05:51 165 INFO (ZooKeeper.java:438)- Initiating client connection connectString=10.204.52.173:2181 10.204.52.174:2181 10.204.52.175:2181 sessionTimeout=90000 watcher=com.alibaba.otter.canal.common.zookeeper.ZkClientx@2ca65ce4 2018-06-27 11:05:51 184 INFO (ClientCnxn.java:1032)- Opening socket connection to server 10.204.52.174/10.204.52.174:2181. Will not attempt to authenticate using SASL (unknown error) 2018-06-27 11:05:51 186 INFO (ClientCnxn.java:876)- Socket connection established to 10.204.52.174/10.204.52.174:2181 initiating session 2018-06-27 11:05:51 190 INFO (ClientCnxn.java:1299)- Session establishment complete on server 10.204.52.174/10.204.52.174:2181 sessionid = 0x263fdc9445e0090 negotiated timeout = 40000 2018-06-27 11:05:51 191 INFO (ZkClient.java:449)- zookeeper state changed (SyncConnected) I have two clients connected to the same instance of the same server, why is it stuck? zookeeper state changed (SyncConnected)
718,v1.0.26 alpha 3 how to use v1.0.26 alpha 3 how to use Install example Configured in the middle Data is written to kafka Incomplete Missing data or
717,Ddl new field exception com mysql jdbc exceptions jdbc4 MySQLSyntaxErrorException Duplicate column name 'name2' Prerequisites configured multiple channels The source tables of each channel are the same table. The target table is a different table of the same library of another cluster. mysql5.7.20 Canal 1 0 26 ddl new field error is as follows 2018-06-26 15:02:25.895 [pipelineId = 15 taskName = LoadWorker] ERROR com.alibaba.otter.node.etl.load.LoadTask - [15] loadWork executor is error! data:EtlEventData[currNid=8 nextNid=8 desc=[MemoryPipeKey[identity=Identity[channelId=15 pipelineId=15 processId=89] time=1529996545875 dataType=DB_BATCH]] processId=89 startTime=1529996545271 endTime=<null> firstTime=1529996545000 batchId=6 number=1 size=<null> exts=<null> pipelineId=15] com.alibaba.otter.node.etl.load.exception.LoadException: java.util.concurrent.ExecutionException: com.alibaba.otter.node.etl.load.exception.LoadException: com.alibaba.otter.node.etl.load.exception.LoadException: org.springframework.jdbc.BadSqlGrammarException: StatementCallback; bad SQL grammar []; nested exception is com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Duplicate column name 'name2' Caused by: java.util.concurrent.ExecutionException: com.alibaba.otter.node.etl.load.exception.LoadException: com.alibaba.otter.node.etl.load.exception.LoadException: org.springframework.jdbc.BadSqlGrammarException: StatementCallback; bad SQL grammar []; nested exception is com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Duplicate column name 'name2' at java.util.concurrent.FutureTask.report(FutureTask.java:122) [na:1.8.0_102] at java.util.concurrent.FutureTask.get(FutureTask.java:192) [na:1.8.0_102] at com.alibaba.otter.node.etl.load.loader.db.DataBatchLoader.load(DataBatchLoader.java:107) ~[node.etl-4.2.16-SNAPSHOT.jar:na] at com.alibaba.otter.node.etl.load.loader.OtterLoaderFactory.load(OtterLoaderFactory.java:50) ~[node.etl-4.2.16-SNAPSHOT.jar:na] at com.alibaba.otter.node.etl.load.LoadTask$1.run(LoadTask.java:85) ~[node.etl-4.2.16-SNAPSHOT.jar:na] at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_102] at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_102] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_102] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_102] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_102] Caused by: com.alibaba.otter.node.etl.load.exception.LoadException: com.alibaba.otter.node.etl.load.exception.LoadException: org.springframework.jdbc.BadSqlGrammarException: StatementCallback; bad SQL grammar []; nested exception is com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Duplicate column name 'name2' Caused by: com.alibaba.otter.node.etl.load.exception.LoadException: org.springframework.jdbc.BadSqlGrammarException: StatementCallback; bad SQL grammar []; nested exception is com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Duplicate column name 'name2' It is estimated that DDL is repeatedly executed. Ignoring the current DDL exception can be skipped. How to ignore this? I am the otter configuration implementation of the otter only skip the select exception and skip the Load exception. The above is the load exception. But if you skip the load exception, you will be afraid to skip all load exceptions instead of just How to solve this problem for ddl exceptions? There is an option to ignore DDL exceptions Skipping exceptions will appear without syncing For example, the same source table is synchronized to different table targets of the same target library. Table 1 Synchronization success target table 2 is skipped and the exception is not synchronized.
716,tableMetaTSDB finds a place to enter an infinite loop In this class MysqlEventParser The following method **### /** * Find a transaction start position that is closest to the timestamp in the specified binlog according to the given timestamp. * EndPosition is given for the last binlog to avoid endless queries */ private EntryPosition findAsPerTimestampInSpecificLogFile(MysqlConnection mysqlConnection final Long startTimestamp final EntryPosition endPosition final String searchBinlogFile final Boolean justForPositionTimestamp)** I specified in my configuration file. canal.instance.master.journal.name=mysql-bin.000344 canal.instance.master.position=106199547 After turning on the tsdb function When the canal service starts The program will enter here to find the starting point Since the program is not found, it will be consistently found even larger than 106199547. Still looking for the lead has been out of the initialization phase and then it will appear Client cannot create cursor 。https://github.com/alibaba/canal/issues/715 Is the site not present or is it I am looking for this order. show binlog events in'mysql-bin.000344' mysql-bin.000344 106199500 Rotate 609344520 106199547 mysql-bin.000345;pos=4 This is the last record Then went to take 106199547 As canal instance master position Then the above source code enters the infinite loop Did you have no new data after getting a spot? Dataable But already entered the next file 106199547 Is the last site of mysql bin 000344 But here is Mysql bin 000345 file The latest 26 version optimizes the judgment exit condition as
715,canal1.0.25 After the otter client starts In zookeeper Not created in cursor Do not update binlog location information We have encountered this problem at the moment. https://github.com/alibaba/canal/issues/541 Disable tsdb function 1 0 25 version has some problems for 1 0 26
714,Filter does not work The server specifies the canal instance filter regex Client connector subscribe But when the server parses the binlog When encountering a log of a table that has been dropped by a table that does not need to be synchronized Still will report an error Say desc Db_name table_name failed In fact, these tables will not appear in the canal instance filter regex I don&#39;t know how to solve it.
713,CanalParseException: parse row data failed. CanalParseException: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first The synchronization process suddenly reports an error Mysql5 7 20 canal 1 0 25 related installation package manager deployer 4 2 15 tar gz node deployer 4 2 15 tar gz ps This canal nesting inside if the canal has a fixed version in the high version, the manage and node will also provide The corresponding repair version or the installation node canal independent deployment canal non-nested installation instructions The error message is as follows com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4832) ~[guava-18.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:160) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:759) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:428) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:114) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:66) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:337) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:184) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:145) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) [canal.parse-1.0.25.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_102] Caused by: java.io.IOException: should execute connector.connect() first at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.<init>(MysqlQueryExecutor.java:30) ~[canal.parse.driver-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:87) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMetaByDB(TableMetaCache.java:80) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:30) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:55) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:50) ~[canal.parse-1.0.25.jar:na] at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache.get(LocalCache.java:3937) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830) ~[guava-18.0.jar:na] ... 10 common frames omitted Look at Issue604 https github com alibaba canal issues 640 Upgrade to version 26 Ok, I will try it first. Thank you.
712,org.h2.message.DbException: General error: "java.lang.IllegalStateException: The file is locked: nio:/usr/local/canal/conf/business/h2.mv.db Version 1 0 25 The canal service starts with 2 instances and the following 2 problems occur. 1.canal Unauthenticated after server startup user Non-authenticated user connection 2 lock file 2018-06-22 14:08:32 database: flush org.h2.message.DbException: General error: "java.lang.IllegalStateException: The file is locked: nio:/usr/local/canal/conf/business/h2.mv.db [1.4.196/7]" [50000-196] at org.h2.message.DbException.get(DbException.java:168) at org.h2.message.DbException.convert(DbException.java:295) at org.h2.mvstore.db.MVTableEngine$1.uncaughtException(MVTableEngine.java:95) at org.h2.mvstore.MVStore.panic(MVStore.java:378) at org.h2.mvstore.MVStore.<init>(MVStore.java:361) at org.h2.mvstore.MVStore$Builder.open(MVStore.java:2930) at org.h2.mvstore.db.MVTableEngine$Store.open(MVTableEngine.java:155) at org.h2.mvstore.db.MVTableEngine.init(MVTableEngine.java:100) at org.h2.engine.Database.getPageStore(Database.java:2476) at org.h2.engine.Database.open(Database.java:697) at org.h2.engine.Database.openDatabase(Database.java:276) at org.h2.engine.Database.<init>(Database.java:270) at org.h2.engine.Engine.openSession(Engine.java:64) at org.h2.engine.Engine.openSession(Engine.java:176) at org.h2.engine.Engine.createSessionAndValidate(Engine.java:154) at org.h2.engine.Engine.createSession(Engine.java:137) at org.h2.engine.Engine.createSession(Engine.java:27) at org.h2.engine.SessionRemote.connectEmbeddedOrServer(SessionRemote.java:354) at org.h2.jdbc.JdbcConnection.<init>(JdbcConnection.java:116) at org.h2.jdbc.JdbcConnection.<init>(JdbcConnection.java:100) at org.h2.Driver.connect(Driver.java:69) at com.alibaba.druid.pool.DruidAbstractDataSource.createPhysicalConnection(DruidAbstractDataSource.java:1510) at com.alibaba.druid.pool.DruidAbstractDataSource.createPhysicalConnection(DruidAbstractDataSource.java:1575) at com.alibaba.druid.pool.DruidDataSource$CreateConnectionThread.run(DruidDataSource.java:2450) Caused by: org.h2.jdbc.JdbcSQLException: General error: "java.lang.IllegalStateException: The file is locked: nio:/usr/local/canal/conf/business/h2.mv.db [1.4.196/7]" [50000-196] at org.h2.message.DbException.getJdbcSQLException(DbException.java:345) ... 24 more Caused by: java.lang.IllegalStateException: The file is locked: nio:/usr/local/canal/conf/business/h2.mv.db [1.4.196/7] at org.h2.mvstore.DataUtils.newIllegalStateException(DataUtils.java:765) at org.h2.mvstore.FileStore.open(FileStore.java:173) at org.h2.mvstore.MVStore.<init>(MVStore.java:347) ... 19 more @agapple This error is still the first time to see if the last canal has an abnormal crash. This is the first time that these two situations have occurred in the official environment. @agapple The file is Locked is estimated to repeatedly start the same instance to check this file /usr/local/canal/conf/business/h2.mv.db The corresponding file handle is opened by which process
711,Canal connects Alibaba Cloud RDS The relevant rds configuration file does not seem to be ah, another canal instance rds open url https rds aliyuncs com This configuration is for the external network Is there an intranet address? This is RDS The configuration of binlog&#39;s openapi download is not yet fully formed. If you need to use it, you can understand the code. 参考 https github com alibaba canal wiki FAQ
710,java.io.IOException: connect /xxx:3306 failure:java.io.IOException: Unexpected End Start the canal service and report this error canal version 1 0 22 2018-06-19 11:26:44.358 [destination = example address = /xxx:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /xxx:3306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: connect /xxx:3306 failure:java.io.IOException: Unexpected End Stream at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.readBytesAsBuffer(PacketManager.java:22) at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.readHeader(PacketManager.java:13) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:199) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:71) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:52) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:85) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:158) at java.lang.Thread.run(Thread.java:745) Caused by: java.io.IOException: connect /xxx:3306 failure:java.io.IOException: Unexpected End Stream at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.readBytesAsBuffer(PacketManager.java:22) at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.readHeader(PacketManager.java:13) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:199) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:71) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:52) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:85) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:158) at java.lang.Thread.run(Thread.java:745) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:74) ~[canal.parse.driver-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:52) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:85) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:158) ~[canal.parse-1.0.22.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_11] Port 3306 is open java.io.IOException: connect /xxx:3306 failure:java.io.IOException: Unexpected End Stream Can try 1 0 26
709,Increase the running status monitoring of the canal kafka client Increase the running status monitoring of the canal kafka client kafka Client runs cluster and also canal The client waits for the first to start and waits for the blocking to start. based on Topic groupId preempts the directory in zookeeper as /otter/canal/topics/example/groupId tks @rewerma For the use of kafka, you can write a simple explanatory document for me. @agapple Instructions for use After packaging, use the canal folder under the kafka target as the server conf canal properties Configuration modification canal.withoutNetty = True other configuration items are the same as before Added conf kafka yml configuration servers: slave1:6667 slave2:6667 slave3:6667 # kafka Server address retries: 0 # kafka number of retries batchSize: 16384 # kafka Batch size lingerMs: 1 bufferMemory: 33554432 canalBatchSize: 50 # Lot size unit of canal k If the database write operation is large, it is recommended to change to 1M. filterTransactionEntry: true canalDestinations: - canalDestination: example # Canal instance topic: example # kafka topic partition: # Partition topics: # A destination can correspond to multiple topics - topic: example partition: canal kafka Client reference test Moderate com.alibaba.otter.canal.kafka.client.running.CanalKafkaClientExample
708,Data processing failure will be repeated after the rollback Because it is pressed Batch id If ack and rollback fail in one of the batches, the process starts to roll back and then reprocesses the batch data that has just failed to process. If the processing of the batch fails, the processing is successful. Repeated processing @agapple Repeat cannot be avoided That can only be weighted in your own code. I have encountered this situation I am persisting the data of the current batch to the database record exception information after catching the exception. Then submit the data of the batch normally In order to avoid repeated processing In addition to this measure, you need to avoid the situation of abnormal crash when you deal with half of it.
707,Canal starts normally Canal client subscription stuck Error log 2018 06 20 13:43:46.730 [ZkClient-EventThread-26-vm153:2181 vm154:2181 vm155:2181] ERROR org.I0Itec.zkclient.ZkEventThread - Error handling event ZkEvent[Data of /otter/canal/destinations/yuantong_order_mysql_10.1.240.87_3307_0/1001/running changed sent to com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor$1@4e7d0621] com.alibaba.otter.canal.protocol.exception.CanalClientException: something goes wrong in initRunning method. at com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor.initRunning(ClientRunningMonitor.java:142) at com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor$1.handleDataDeleted(ClientRunningMonitor.java:71) at org.I0Itec.zkclient.ZkClient$6.run(ZkClient.java:549) at org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71) Caused by: com.alibaba.otter.canal.protocol.exception.CanalClientException: java.net.ConnectException: Refuse to connect at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.doConnect(SimpleCanalConnector.java:171) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.access$000(SimpleCanalConnector.java:48) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector$1.processActiveEnter(SimpleCanalConnector.java:396) at com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor.processActiveEnter(ClientRunningMonitor.java:207) at com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor.initRunning(ClientRunningMonitor.java:118) ... 3 common frames omitted Caused by: java.net.ConnectException: Refuse to connect at sun.nio.ch.Net.connect0(Native Method) at sun.nio.ch.Net.connect(Net.java:454) at sun.nio.ch.Net.connect(Net.java:446) at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.doConnect(SimpleCanalConnector.java:132) ... 7 common frames omitted Sometimes it will always be stuck on this error and need to be restarted to be normal. But sometimes this error will occur but the last version that can be successfully subscribed to does not need to restart the canal is 24 I sometimes have this problem. Can you solve it? Try the latest v1 1 1
706,Why do I start two java client connections canal Server can&#39;t connect Start two client connections canalserver The second startup has been stuck Subcribe did not start there successfully
705,mysql5.7.22 canal prompt java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not open log file 2018-06-20 12:02:29.240 [destination = dev address = /192.168.255.2:3306 EventParser] INFO c.a.otter.canal.parse.inbound.mysql.MysqlConnection - COM_BINLOG_DUMP with position:BinlogDumpCommandPacket[binlogPosition=309144234 slaveServerId=1201 binlogFileName=mysql-bin.000333 command=18] 2018-06-20 12:02:29.244 [destination = dev address = /192.168.255.2:3306 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not open log file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) ~[canal.parse-1.0.25-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122) [canal.parse-1.0.25-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) [canal.parse-1.0.25-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171] 2018-06-20 12:02:29.244 [destination = dev address = /192.168.255.2:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /192.168.255.2:3306 has an error retrying. caused by java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not open log file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) ~[canal.parse-1.0.25-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122) ~[canal.parse-1.0.25-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) ~[canal.parse-1.0.25-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171] 2018-06-20 12:02:29.244 [destination = dev address = /192.168.255.2:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:dev[java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not open log file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) at java.lang.Thread.run(Thread.java:748) ] 2018-06-20 12:02:29.244 [destination = dev address = /192.168.255.2:3306 EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - disConnect MysqlConnection to /192.168.255.2:3306... Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not open log file Check the validity of binlog files and sites
704,Canal does not recognize the column when the primary key set by the table limits the length of the field CREATE TABLE `job_execution_script` ( `app_id` INT(11) NOT NULL `job_id` INT(11) NOT NULL `script_name` VARCHAR(512) NOT NULL DEFAULT '' `committer_name` VARCHAR(128) NOT NULL DEFAULT '' PRIMARY KEY (`app_id` `job_id` `script_name`(100) `committer_name`) ) ENGINE=INNODB DEFAULT CHARSET=utf8; When the table structure is as above, canal will report “unknow column `script_name`(100)” ERROR Thank you The exception stack is 2018-06-20 10:51:32.960 [destination = ido001 address = ido001/180.137.128.151:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:ido001[java.lang.RuntimeException: unknow column : `script_name`(100) at com.alibaba.otter.canal.parse.inbound.TableMeta.getFieldMetaByName(TableMeta.java:73) at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.processTableElement(MemoryTableMeta.java:215) at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.parse(MemoryTableMeta.java:151) at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.find(MemoryTableMeta.java:106) at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.compareTableMetaDbAndMemory(DatabaseTableMeta.java:288) at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.applySnapshotToDB(DatabaseTableMeta.java:251) at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:129) at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) at java.lang.Thread.run(Thread.java:745) ] You are using the latest version 1 0 26, which I remember has been fixed. I am using the 1 0 25 version of this problem still exists Try the 1 0 26 version
703,RDS binlog retention period RDS binlog defaults to 18 hours if I want to backtrack Re-spent from the specified cursor Should be unable to find the binlog file or the corresponding location RDSbinlog saved in oss needs to download oss ​​for local parsing 参考 https github com alibaba canal wiki FAQ
702,Specify whether the Filter does not work or can get the monitoring data of the entire library. `connector.subscribe("databaseName.tableName");` This code I specified the filter can still receive the binlog data of all the tables. Is there any problem with the configuration? Search for history issues The filter uses a regular expression. You need to escape this regex to use. For example, your example databaseName tableName needs to be converted into > databaseName\\\\.tableName Can be used in the regular expression to represent the meaning of any character is the point because a single backslash represents the escaping and therefore needs to be escaped, so it becomes the same as above. take a look FAQ : https://github.com/alibaba/canal/wiki/FAQ
701,canal It will appear after running for two days or so. Abnormally unstable Can&#39;t see what effect the canal Is there a mechanism for error recovery? com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:145) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:87) [canal.parse-1.0.25.jar:na] Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4832) ~[guava-18.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:759) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:160) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:428) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:114) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:66) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:337) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:184) ~[canal.parse-1.0.25.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171] Caused by: java.io.IOException: should execute connector.connect() first at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.<init>(MysqlQueryExecutor.java:30) ~[canal.parse.driver-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMetaByDB(TableMetaCache.java:80) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:30) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:55) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:50) ~[canal.parse-1.0.25.jar:na] at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache.get(LocalCache.java:3937) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830) ~[guava-18.0.jar:na] ... 10 common frames omitted At the same time, 3 database instances are connected. Only one of the instances is down and there seems to be no way to recover. Try the 1 0 26 version
700,Reduce the kafka dependent version to declare that the server&#39;s running is volatile tks
699,Could not find first log file name in binary log index file mysql:10.1.25-MariaDB Canal tried 1 0 26alpha3 1 0 25 Position is not configured in the configuration file Already rummaged through similar issues in the issues solution to try to remove canal Meta file h2 mv db file will not work After the mysql side changes canal Example also reported this first com.alibaba.fastsql.sql.parser.ParserException: syntax error error in :'utf8 PAGE_CHECKSUM=0 pos 626 line 13 column 50 token IDENTIFIER PAGE_CHECKSUM Next report this Could not find first log file name in binary log index file More specific anomalies 2018-06-15 11:30:11.805 [destination = example address = /10.10.38.39:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just show master status 2018-06-15 11:32:09.338 [destination = example address = /10.10.38.39:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TEMPORARY TABLE `ALL_PLUGINS` ( `PLUGIN_NAME` varchar(64) NOT NULL DEFAULT '' `PLUGIN_VERSION` varchar(20) NOT NULL DEFAULT '' `PLUGIN_STATUS` varchar(16) NOT NULL DEFAULT '' `PLUGIN_TYPE` varchar(80) NOT NULL DEFAULT '' `PLUGIN_TYPE_VERSION` varchar(20) NOT NULL DEFAULT '' `PLUGIN_LIBRARY` varchar(64) DEFAULT NULL `PLUGIN_LIBRARY_VERSION` varchar(20) DEFAULT NULL `PLUGIN_AUTHOR` varchar(64) DEFAULT NULL `PLUGIN_DESCRIPTION` longtext `PLUGIN_LICENSE` varchar(80) NOT NULL DEFAULT '' `LOAD_OPTION` varchar(64) NOT NULL DEFAULT '' `PLUGIN_MATURITY` varchar(12) NOT NULL DEFAULT '' `PLUGIN_AUTH_VERSION` varchar(80) DEFAULT NULL ) ENGINE=Aria DEFAULT CHARSET=utf8 PAGE_CHECKSUM=0 com.alibaba.fastsql.sql.parser.ParserException: syntax error error in :'utf8 PAGE_CHECKSUM=0 pos 723 line 15 column 50 token IDENTIFIER PAGE_CHECKSUM at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:361) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:498) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:84) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:32:09.340 [destination = example address = /10.10.38.39:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TEMPORARY TABLE `COLUMNS` ( `TABLE_CATALOG` varchar(512) NOT NULL DEFAULT '' `TABLE_SCHEMA` varchar(64) NOT NULL DEFAULT '' `TABLE_NAME` varchar(64) NOT NULL DEFAULT '' `COLUMN_NAME` varchar(64) NOT NULL DEFAULT '' `ORDINAL_POSITION` bigint(21) unsigned NOT NULL DEFAULT '0' `COLUMN_DEFAULT` longtext `IS_NULLABLE` varchar(3) NOT NULL DEFAULT '' `DATA_TYPE` varchar(64) NOT NULL DEFAULT '' `CHARACTER_MAXIMUM_LENGTH` bigint(21) unsigned DEFAULT NULL `CHARACTER_OCTET_LENGTH` bigint(21) unsigned DEFAULT NULL `NUMERIC_PRECISION` bigint(21) unsigned DEFAULT NULL `NUMERIC_SCALE` bigint(21) unsigned DEFAULT NULL `DATETIME_PRECISION` bigint(21) unsigned DEFAULT NULL `CHARACTER_SET_NAME` varchar(32) DEFAULT NULL `COLLATION_NAME` varchar(32) DEFAULT NULL `COLUMN_TYPE` longtext NOT NULL `COLUMN_KEY` varchar(3) NOT NULL DEFAULT '' `EXTRA` varchar(27) NOT NULL DEFAULT '' `PRIVILEGES` varchar(80) NOT NULL DEFAULT '' `COLUMN_COMMENT` varchar(1024) NOT NULL DEFAULT '' ) ENGINE=Aria DEFAULT CHARSET=utf8 PAGE_CHECKSUM=0 com.alibaba.fastsql.sql.parser.ParserException: syntax error error in :'utf8 PAGE_CHECKSUM=0 pos 1078 line 22 column 50 token IDENTIFIER PAGE_CHECKSUM at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:361) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:498) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:84) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:32:09.341 [destination = example address = /10.10.38.39:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TEMPORARY TABLE `EVENTS` ( `EVENT_CATALOG` varchar(64) NOT NULL DEFAULT '' `EVENT_SCHEMA` varchar(64) NOT NULL DEFAULT '' `EVENT_NAME` varchar(64) NOT NULL DEFAULT '' `DEFINER` varchar(189) NOT NULL DEFAULT '' `TIME_ZONE` varchar(64) NOT NULL DEFAULT '' `EVENT_BODY` varchar(8) NOT NULL DEFAULT '' `EVENT_DEFINITION` longtext NOT NULL `EVENT_TYPE` varchar(9) NOT NULL DEFAULT '' `EXECUTE_AT` datetime DEFAULT NULL `INTERVAL_VALUE` varchar(256) DEFAULT NULL `INTERVAL_FIELD` varchar(18) DEFAULT NULL `SQL_MODE` varchar(8192) NOT NULL DEFAULT '' `STARTS` datetime DEFAULT NULL `ENDS` datetime DEFAULT NULL `STATUS` varchar(18) NOT NULL DEFAULT '' `ON_COMPLETION` varchar(12) NOT NULL DEFAULT '' `CREATED` datetime NOT NULL DEFAULT '0000-00-00 00:00:00' `LAST_ALTERED` datetime NOT NULL DEFAULT '0000-00-00 00:00:00' `LAST_EXECUTED` datetime DEFAULT NULL `EVENT_COMMENT` varchar(64) NOT NULL DEFAULT '' `ORIGINATOR` bigint(10) NOT NULL DEFAULT '0' `CHARACTER_SET_CLIENT` varchar(32) NOT NULL DEFAULT '' `COLLATION_CONNECTION` varchar(32) NOT NULL DEFAULT '' `DATABASE_COLLATION` varchar(32) NOT NULL DEFAULT '' ) ENGINE=Aria DEFAULT CHARSET=utf8 PAGE_CHECKSUM=0 com.alibaba.fastsql.sql.parser.ParserException: syntax error error in :'utf8 PAGE_CHECKSUM=0 pos 1234 line 26 column 50 token IDENTIFIER PAGE_CHECKSUM at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:361) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:498) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:84) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:32:09.342 [destination = example address = /10.10.38.39:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TEMPORARY TABLE `PARAMETERS` ( `SPECIFIC_CATALOG` varchar(512) NOT NULL DEFAULT '' `SPECIFIC_SCHEMA` varchar(64) NOT NULL DEFAULT '' `SPECIFIC_NAME` varchar(64) NOT NULL DEFAULT '' `ORDINAL_POSITION` int(21) NOT NULL DEFAULT '0' `PARAMETER_MODE` varchar(5) DEFAULT NULL `PARAMETER_NAME` varchar(64) DEFAULT NULL `DATA_TYPE` varchar(64) NOT NULL DEFAULT '' `CHARACTER_MAXIMUM_LENGTH` int(21) DEFAULT NULL `CHARACTER_OCTET_LENGTH` int(21) DEFAULT NULL `NUMERIC_PRECISION` int(21) DEFAULT NULL `NUMERIC_SCALE` int(21) DEFAULT NULL `DATETIME_PRECISION` bigint(21) unsigned DEFAULT NULL `CHARACTER_SET_NAME` varchar(64) DEFAULT NULL `COLLATION_NAME` varchar(64) DEFAULT NULL `DTD_IDENTIFIER` longtext NOT NULL `ROUTINE_TYPE` varchar(9) NOT NULL DEFAULT '' ) ENGINE=Aria DEFAULT CHARSET=utf8 PAGE_CHECKSUM=0 com.alibaba.fastsql.sql.parser.ParserException: syntax error error in :'utf8 PAGE_CHECKSUM=0 pos 853 line 18 column 50 token IDENTIFIER PAGE_CHECKSUM at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:361) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:498) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:84) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:32:09.342 [destination = example address = /10.10.38.39:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TEMPORARY TABLE `PARTITIONS` ( `TABLE_CATALOG` varchar(512) NOT NULL DEFAULT '' `TABLE_SCHEMA` varchar(64) NOT NULL DEFAULT '' `TABLE_NAME` varchar(64) NOT NULL DEFAULT '' `PARTITION_NAME` varchar(64) DEFAULT NULL `SUBPARTITION_NAME` varchar(64) DEFAULT NULL `PARTITION_ORDINAL_POSITION` bigint(21) unsigned DEFAULT NULL `SUBPARTITION_ORDINAL_POSITION` bigint(21) unsigned DEFAULT NULL `PARTITION_METHOD` varchar(18) DEFAULT NULL `SUBPARTITION_METHOD` varchar(12) DEFAULT NULL `PARTITION_EXPRESSION` longtext `SUBPARTITION_EXPRESSION` longtext `PARTITION_DESCRIPTION` longtext `TABLE_ROWS` bigint(21) unsigned NOT NULL DEFAULT '0' `AVG_ROW_LENGTH` bigint(21) unsigned NOT NULL DEFAULT '0' `DATA_LENGTH` bigint(21) unsigned NOT NULL DEFAULT '0' `MAX_DATA_LENGTH` bigint(21) unsigned DEFAULT NULL `INDEX_LENGTH` bigint(21) unsigned NOT NULL DEFAULT '0' `DATA_FREE` bigint(21) unsigned NOT NULL DEFAULT '0' `CREATE_TIME` datetime DEFAULT NULL `UPDATE_TIME` datetime DEFAULT NULL `CHECK_TIME` datetime DEFAULT NULL `CHECKSUM` bigint(21) unsigned DEFAULT NULL `PARTITION_COMMENT` varchar(80) NOT NULL DEFAULT '' `NODEGROUP` varchar(12) NOT NULL DEFAULT '' `TABLESPACE_NAME` varchar(64) DEFAULT NULL ) ENGINE=Aria DEFAULT CHARSET=utf8 PAGE_CHECKSUM=0 com.alibaba.fastsql.sql.parser.ParserException: syntax error error in :'utf8 PAGE_CHECKSUM=0 pos 1323 line 27 column 50 token IDENTIFIER PAGE_CHECKSUM at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:361) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:498) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:84) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:32:09.343 [destination = example address = /10.10.38.39:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TEMPORARY TABLE `PLUGINS` ( `PLUGIN_NAME` varchar(64) NOT NULL DEFAULT '' `PLUGIN_VERSION` varchar(20) NOT NULL DEFAULT '' `PLUGIN_STATUS` varchar(16) NOT NULL DEFAULT '' `PLUGIN_TYPE` varchar(80) NOT NULL DEFAULT '' `PLUGIN_TYPE_VERSION` varchar(20) NOT NULL DEFAULT '' `PLUGIN_LIBRARY` varchar(64) DEFAULT NULL `PLUGIN_LIBRARY_VERSION` varchar(20) DEFAULT NULL `PLUGIN_AUTHOR` varchar(64) DEFAULT NULL `PLUGIN_DESCRIPTION` longtext `PLUGIN_LICENSE` varchar(80) NOT NULL DEFAULT '' `LOAD_OPTION` varchar(64) NOT NULL DEFAULT '' `PLUGIN_MATURITY` varchar(12) NOT NULL DEFAULT '' `PLUGIN_AUTH_VERSION` varchar(80) DEFAULT NULL ) ENGINE=Aria DEFAULT CHARSET=utf8 PAGE_CHECKSUM=0 com.alibaba.fastsql.sql.parser.ParserException: syntax error error in :'utf8 PAGE_CHECKSUM=0 pos 719 line 15 column 50 token IDENTIFIER PAGE_CHECKSUM at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:361) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:498) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:84) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:32:09.344 [destination = example address = /10.10.38.39:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TEMPORARY TABLE `PROCESSLIST` ( `ID` bigint(4) NOT NULL DEFAULT '0' `USER` varchar(128) NOT NULL DEFAULT '' `HOST` varchar(64) NOT NULL DEFAULT '' `DB` varchar(64) DEFAULT NULL `COMMAND` varchar(16) NOT NULL DEFAULT '' `TIME` int(7) NOT NULL DEFAULT '0' `STATE` varchar(64) DEFAULT NULL `INFO` longtext `TIME_MS` decimal(22 3) NOT NULL DEFAULT '0.000' `STAGE` tinyint(2) NOT NULL DEFAULT '0' `MAX_STAGE` tinyint(2) NOT NULL DEFAULT '0' `PROGRESS` decimal(7 3) NOT NULL DEFAULT '0.000' `MEMORY_USED` int(7) NOT NULL DEFAULT '0' `EXAMINED_ROWS` int(7) NOT NULL DEFAULT '0' `QUERY_ID` bigint(4) NOT NULL DEFAULT '0' `INFO_BINARY` blob `TID` bigint(4) NOT NULL DEFAULT '0' ) ENGINE=Aria DEFAULT CHARSET=utf8 PAGE_CHECKSUM=0 com.alibaba.fastsql.sql.parser.ParserException: syntax error error in :'utf8 PAGE_CHECKSUM=0 pos 774 line 19 column 50 token IDENTIFIER PAGE_CHECKSUM at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:361) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:498) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:84) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:32:09.344 [destination = example address = /10.10.38.39:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TEMPORARY TABLE `ROUTINES` ( `SPECIFIC_NAME` varchar(64) NOT NULL DEFAULT '' `ROUTINE_CATALOG` varchar(512) NOT NULL DEFAULT '' `ROUTINE_SCHEMA` varchar(64) NOT NULL DEFAULT '' `ROUTINE_NAME` varchar(64) NOT NULL DEFAULT '' `ROUTINE_TYPE` varchar(9) NOT NULL DEFAULT '' `DATA_TYPE` varchar(64) NOT NULL DEFAULT '' `CHARACTER_MAXIMUM_LENGTH` int(21) DEFAULT NULL `CHARACTER_OCTET_LENGTH` int(21) DEFAULT NULL `NUMERIC_PRECISION` int(21) DEFAULT NULL `NUMERIC_SCALE` int(21) DEFAULT NULL `DATETIME_PRECISION` bigint(21) unsigned DEFAULT NULL `CHARACTER_SET_NAME` varchar(64) DEFAULT NULL `COLLATION_NAME` varchar(64) DEFAULT NULL `DTD_IDENTIFIER` longtext `ROUTINE_BODY` varchar(8) NOT NULL DEFAULT '' `ROUTINE_DEFINITION` longtext `EXTERNAL_NAME` varchar(64) DEFAULT NULL `EXTERNAL_LANGUAGE` varchar(64) DEFAULT NULL `PARAMETER_STYLE` varchar(8) NOT NULL DEFAULT '' `IS_DETERMINISTIC` varchar(3) NOT NULL DEFAULT '' `SQL_DATA_ACCESS` varchar(64) NOT NULL DEFAULT '' `SQL_PATH` varchar(64) DEFAULT NULL `SECURITY_TYPE` varchar(7) NOT NULL DEFAULT '' `CREATED` datetime NOT NULL DEFAULT '0000-00-00 00:00:00' `LAST_ALTERED` datetime NOT NULL DEFAULT '0000-00-00 00:00:00' `SQL_MODE` varchar(8192) NOT NULL DEFAULT '' `ROUTINE_COMMENT` longtext NOT NULL `DEFINER` varchar(189) NOT NULL DEFAULT '' `CHARACTER_SET_CLIENT` varchar(32) NOT NULL DEFAULT '' `COLLATION_CONNECTION` varchar(32) NOT NULL DEFAULT '' `DATABASE_COLLATION` varchar(32) NOT NULL DEFAULT '' ) ENGINE=Aria DEFAULT CHARSET=utf8 PAGE_CHECKSUM=0 com.alibaba.fastsql.sql.parser.ParserException: syntax error error in :'utf8 PAGE_CHECKSUM=0 pos 1603 line 33 column 50 token IDENTIFIER PAGE_CHECKSUM at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:361) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:498) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:84) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:32:09.345 [destination = example address = /10.10.38.39:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TEMPORARY TABLE `SYSTEM_VARIABLES` ( `VARIABLE_NAME` varchar(64) NOT NULL DEFAULT '' `SESSION_VALUE` varchar(2048) DEFAULT NULL `GLOBAL_VALUE` varchar(2048) DEFAULT NULL `GLOBAL_VALUE_ORIGIN` varchar(64) NOT NULL DEFAULT '' `DEFAULT_VALUE` varchar(2048) DEFAULT NULL `VARIABLE_SCOPE` varchar(64) NOT NULL DEFAULT '' `VARIABLE_TYPE` varchar(64) NOT NULL DEFAULT '' `VARIABLE_COMMENT` varchar(2048) NOT NULL DEFAULT '' `NUMERIC_MIN_VALUE` varchar(21) DEFAULT NULL `NUMERIC_MAX_VALUE` varchar(21) DEFAULT NULL `NUMERIC_BLOCK_SIZE` varchar(21) DEFAULT NULL `ENUM_VALUE_LIST` longtext `READ_ONLY` varchar(3) NOT NULL DEFAULT '' `COMMAND_LINE_ARGUMENT` varchar(64) DEFAULT NULL ) ENGINE=Aria DEFAULT CHARSET=utf8 PAGE_CHECKSUM=0 com.alibaba.fastsql.sql.parser.ParserException: syntax error error in :'utf8 PAGE_CHECKSUM=0 pos 768 line 16 column 50 token IDENTIFIER PAGE_CHECKSUM at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:361) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:498) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:84) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:32:09.346 [destination = example address = /10.10.38.39:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TEMPORARY TABLE `TRIGGERS` ( `TRIGGER_CATALOG` varchar(512) NOT NULL DEFAULT '' `TRIGGER_SCHEMA` varchar(64) NOT NULL DEFAULT '' `TRIGGER_NAME` varchar(64) NOT NULL DEFAULT '' `EVENT_MANIPULATION` varchar(6) NOT NULL DEFAULT '' `EVENT_OBJECT_CATALOG` varchar(512) NOT NULL DEFAULT '' `EVENT_OBJECT_SCHEMA` varchar(64) NOT NULL DEFAULT '' `EVENT_OBJECT_TABLE` varchar(64) NOT NULL DEFAULT '' `ACTION_ORDER` bigint(4) NOT NULL DEFAULT '0' `ACTION_CONDITION` longtext `ACTION_STATEMENT` longtext NOT NULL `ACTION_ORIENTATION` varchar(9) NOT NULL DEFAULT '' `ACTION_TIMING` varchar(6) NOT NULL DEFAULT '' `ACTION_REFERENCE_OLD_TABLE` varchar(64) DEFAULT NULL `ACTION_REFERENCE_NEW_TABLE` varchar(64) DEFAULT NULL `ACTION_REFERENCE_OLD_ROW` varchar(3) NOT NULL DEFAULT '' `ACTION_REFERENCE_NEW_ROW` varchar(3) NOT NULL DEFAULT '' `CREATED` datetime DEFAULT NULL `SQL_MODE` varchar(8192) NOT NULL DEFAULT '' `DEFINER` varchar(189) NOT NULL DEFAULT '' `CHARACTER_SET_CLIENT` varchar(32) NOT NULL DEFAULT '' `COLLATION_CONNECTION` varchar(32) NOT NULL DEFAULT '' `DATABASE_COLLATION` varchar(32) NOT NULL DEFAULT '' ) ENGINE=Aria DEFAULT CHARSET=utf8 PAGE_CHECKSUM=0 com.alibaba.fastsql.sql.parser.ParserException: syntax error error in :'utf8 PAGE_CHECKSUM=0 pos 1228 line 24 column 50 token IDENTIFIER PAGE_CHECKSUM at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:361) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:498) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:84) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:32:09.347 [destination = example address = /10.10.38.39:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TEMPORARY TABLE `VIEWS` ( `TABLE_CATALOG` varchar(512) NOT NULL DEFAULT '' `TABLE_SCHEMA` varchar(64) NOT NULL DEFAULT '' `TABLE_NAME` varchar(64) NOT NULL DEFAULT '' `VIEW_DEFINITION` longtext NOT NULL `CHECK_OPTION` varchar(8) NOT NULL DEFAULT '' `IS_UPDATABLE` varchar(3) NOT NULL DEFAULT '' `DEFINER` varchar(189) NOT NULL DEFAULT '' `SECURITY_TYPE` varchar(7) NOT NULL DEFAULT '' `CHARACTER_SET_CLIENT` varchar(32) NOT NULL DEFAULT '' `COLLATION_CONNECTION` varchar(32) NOT NULL DEFAULT '' `ALGORITHM` varchar(10) NOT NULL DEFAULT '' ) ENGINE=Aria DEFAULT CHARSET=utf8 PAGE_CHECKSUM=0 com.alibaba.fastsql.sql.parser.ParserException: syntax error error in :'utf8 PAGE_CHECKSUM=0 pos 626 line 13 column 50 token IDENTIFIER PAGE_CHECKSUM at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:361) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:498) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:84) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:32:17.341 [destination = example address = /10.10.38.39:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000001×{G? position=4239821 serverId=3839 gtid=<null> timestamp=1529033212000] 2018-06-15 11:32:18.653 [destination = example address = /10.10.38.39:3306 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:32:18.655 [destination = example address = /10.10.38.39:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /10.10.38.39:3306 has an error retrying. caused by java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:32:18.658 [destination = example address = /10.10.38.39:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) at java.lang.Thread.run(Thread.java:748) ] 2018-06-15 11:32:30.620 [destination = example address = /10.10.38.39:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just show master status 2018-06-15 11:32:45.110 [destination = example address = /10.10.38.39:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000001×{G? position=4240327 serverId=3839 gtid=<null> timestamp=1529033528000] 2018-06-15 11:32:45.287 [destination = example address = /10.10.38.39:3306 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:32:45.288 [destination = example address = /10.10.38.39:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /10.10.38.39:3306 has an error retrying. caused by java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:32:45.289 [destination = example address = /10.10.38.39:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) at java.lang.Thread.run(Thread.java:748) ] 2018-06-15 11:32:56.095 [destination = example address = /10.10.38.39:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just show master status 2018-06-15 11:35:34.307 [destination = example address = /10.10.38.39:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000001×{G? position=4240471 serverId=3839 gtid=<null> timestamp=1529033565000] 2018-06-15 11:35:34.724 [destination = example address = /10.10.38.39:3306 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:35:34.725 [destination = example address = /10.10.38.39:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /10.10.38.39:3306 has an error retrying. caused by java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:35:34.725 [destination = example address = /10.10.38.39:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) at java.lang.Thread.run(Thread.java:748) ] 2018-06-15 11:35:52.169 [destination = example address = /10.10.38.39:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just show master status 2018-06-15 11:50:06.738 [destination = example address = /10.10.38.39:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000001×{G? position=4240821 serverId=3839 gtid=<null> timestamp=1529033734000] 2018-06-15 11:50:07.002 [destination = example address = /10.10.38.39:3306 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:50:07.003 [destination = example address = /10.10.38.39:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /10.10.38.39:3306 has an error retrying. caused by java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:50:07.006 [destination = example address = /10.10.38.39:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) at java.lang.Thread.run(Thread.java:748) ] 2018-06-15 11:50:24.984 [destination = example address = /10.10.38.39:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just show master status 2018-06-15 11:51:40.902 [destination = example address = /10.10.38.39:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000001×{G? position=4241171 serverId=3839 gtid=<null> timestamp=1529034606000] 2018-06-15 11:51:41.216 [destination = example address = /10.10.38.39:3306 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-06-15 11:51:41.217 [destination = example address = /10.10.38.39:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /10.10.38.39:3306 has an error retrying. caused by java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:105) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:146) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] PAGE_CHECKSUM This is the keyword of mariadb. Is the keyword of mariadb Look at this error is not recognized when creating a local memory table What if you need to replace the in-memory database with mysql? I closed tsdb and it is normal now. In the instance properties configured canal instance defaultDatabaseName aaa or will receive changes to all databases on the current instance. The meaning of this default is This default has no meaning I have seen the same problem.There is my log: environment:mysql5.7.16-log Source distribution canal:canal.deployer-1.0.24 2018-07-11 16:44:26.054 [destination = example address = /***.***.**.**:3307 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) [canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210) [canal.parse-1.0.22.jar:na] at java.lang.Thread.run(Unknown Source) [na:1.8.0_161] 2018-07-11 16:44:26.054 [destination = example address = /***.***.**.**:3307 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /***.***.**.**:3307 has an error retrying. caused by java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210) ~[canal.parse-1.0.22.jar:na] at java.lang.Thread.run(Unknown Source) [na:1.8.0_161] 2018-07-11 16:44:26.055 [destination = example address = /***.***.**.**:3307 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210) at java.lang.Thread.run(Unknown Source) ] sqlstate = HY000 errmsg = Could not find first log file name in binary log index file Binlog is deleted but I can execute the sql to find out that file show binary logs
698,ClientRunningMonitor shutdown delayExector threadpool #697 #697 tks
697,ClientRunningMonitor shutdown delayExector threadpool Version：1.0.26.alpha2 When the client has a ha switch in cluster mode, com alibaba otter canal client impl running ClientRunningMonitor Will submit Job to delayExector must execute delayExector shutdown in the stop method. Otherwise, the application cannot be gracefully shut down. Submit pr please assign the issue to me. Thanx.
696,Problems with canal1 0 24 and 1 0 25 Spring scene default-instance.xml The 1 0 24 version of the connector subscribe method sometimes gets stuck and does not get the binlog subscription message. The information on the zookeeper is normal but the binlog message is not consumed. 1 0 25 version will not have this problem Now the production environment is using 1 0 24. I want to ask if this is a bug already in 1 0 24? Kill a thread Dump to see where the card is generally not getting the running lock on zk is a few client nodes @nbqyqx As far as a client does not need zk direct connection, it can&#39;t receive binlog message for 1 025 and can receive 1 0 24. Now nothing can be done. 2018-06-15 10:32:57.010 [destination = szctest address = /47.97.185.74:3306 EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - connect MysqlConnection to /47.97.185.74:3306... 2018-06-15 10:32:57.072 [destination = szctest address = /47.97.185.74:3306 EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - handshake initialization packet received prepare the client authentication packet to send 2018-06-15 10:32:57.073 [destination = szctest address = /47.97.185.74:3306 EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - client authentication packet is sent out. 2018-06-15 10:32:57.105 [destination = szctest address = /47.97.185.74:3306 EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - disConnect MysqlConnection to /47.97.185.74:3306... 2018-06-15 10:32:57.105 [destination = szctest address = /47.97.185.74:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /47.97.185.74:3306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: connect /47.97.185.74:3306 failure:java.io.IOException: Error When doing Client Authentication:ErrorPacket [errorNumber=1045 fieldCount=-1 message=Access denied for user 'retl'@'113.116.141.191' (using password: YES) sqlState=28000 sqlStateMarker=#] at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:208) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:71) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:56) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:86) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:157) at java.lang.Thread.run(Unknown Source) Caused by: java.io.IOException: connect /47.97.185.74:3306 failure:java.io.IOException: Error When doing Client Authentication:ErrorPacket [errorNumber=1045 fieldCount=-1 message=Access denied for user 'retl'@'113.116.141.191' (using password: YES) sqlState=28000 sqlStateMarker=#] at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:208) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:71) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:56) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:86) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:157) at java.lang.Thread.run(Unknown Source) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:74) ~[canal.parse.driver-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:56) ~[canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:86) ~[canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:157) ~[canal.parse-1.0.24.jar:na] at java.lang.Thread.run(Unknown Source) [na:1.8.0_172] Silent 47 97 185 74 3306 This is my mysql But this is 啥 'retl'@'113.116.141.191' The reason is that the configuration in my instance is different from the configuration in the default instance xml. My instance is canal instance master dbUsername And xml uses canal instance dbUsername prior to When changing group Didn&#39;t get it back
695,Increase kafka on canal Binlog information generation consumption support 1 in canal The server directly subscribes to the destination to get the message and send it to kafka. 2 Add the canal withoutNetty attribute to the canal properties to close the netty service on the server side. 3 increase kafka&#39;s client to directly consume message data The original canal server -- netty -- message --> nio -- canal client Architecture Architecture supported by kafka canal server -- kafka prducer -- message --> KAFKA --> kafka consumer -- canal kafka client Great support! Looking forward to accept the patch. https://github.com/alibaba/canal/issues/669 Very nice PR, I will take a closer look Has been merged High efficiency ：）
694,Canal resolution speed problem In a library within 40 minutes today, we did 2kw data insertion operation resulting in a large number of binglog generation, but the canal parsing speed is very slow. It took nearly 6 hours to catch up with the actual site. I don&#39;t know how to tune this. Refer to the performance in the FAQ https github com alibaba canal wiki FAQ
693,Cannot monitor binlog Use 1 0 23 to listen when mysql binlog configuration log bin mysql186 bin this structure can not be monitored whether it is related to the name of the binlog file No one has reported any similar questions, please describe the steps to reproduce. It should be that I switched the cluster mode back and forth to clean up the files generated under zk.
692,The problem specified by the timestamp when the specified site is synchronized If you only use the timestamp to specify that the binlog file name is not used, the auxiliary code will get the binlog file at the head and tail position. If the dba is coarsely managed or otherwise, the binlog file will be physically deleted and the binlog start file will not be obtained. Key code ``` class MysqlEventParser 。。。 // Find the binlog location based on time private EntryPosition findByStartTimeStamp(MysqlConnection mysqlConnection Long startTimestamp) { EntryPosition endPosition = findEndPosition(mysqlConnection); EntryPosition startPosition = findStartPosition(mysqlConnection); 。。。 /* Query the current binlog location */ private EntryPosition findStartPosition(MysqlConnection mysqlConnection) { try { ResultSetPacket packet = mysqlConnection.query("show binlog events limit 1"); List<String> fields = packet.getFieldValues(); if (CollectionUtils.isEmpty(fields)) { throw new CanalParseException("command : 'show binlog events limit 1' has an error! pls check. you need (at least one of) the SUPER REPLICATION CLIENT privilege(s) for this operation"); } EntryPosition endPosition = new EntryPosition(fields.get(0) Long.valueOf(fields.get(1))); return endPosition; } catch (IOException e) { throw new CanalParseException("command : 'show binlog events limit 1' has an error!" e); } } ``` show binlog events limit 1 Will report an error > MySQL [(none)]> show binlog events limit 1; > ERROR 29 (HY000): File './mysqlmaster-bin.000001' not found (Errcode: 2 - No such file or directory) Is it used? show binary logs; Command acquisition size more than the 0 Files then use show binlog events It will be better to get the starting point information. Consider submitting a PR to me
691,Java cluster link error com.alibaba.otter.canal.protocol.exception.CanalClientException: java.io.IOException: end of stream when reading header at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:277) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:248) at com.alibaba.otter.canal.client.impl.ClusterCanalConnector.getWithoutAck(ClusterCanalConnector.java:174) at com.dangdang.canal.DDPimJob.run(DDPimJob.java:55) at java.lang.Thread.run(Unknown Source) Caused by: java.io.IOException: end of stream when reading header at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.read(SimpleCanalConnector.java:374) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.readNextPacket(SimpleCanalConnector.java:365) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.receiveMessages(SimpleCanalConnector.java:282) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:275) You still report the version number. I think it should be a template or a variety of questions. Look at what you are using is the 25 version
690,Can the canal instance filter regex support matching multiple schemas? For example, I have 3 database sub-libraries, respectively, pay_n_1 pay_n_2 Pay_n_3 Other libraries are not my concern, I only care about these three canal.instance.filter.regex How to write, I write as canal instance filter regex pay_n_ Does not work pay_n_\\\w+\\..\* that&#39;s fine
689,Can the canal instance be created manually? If the instance of the question is to be created manually each time, if it is automatically configured How should I configure it every time I seem to only automatically generate a folder but the client will report an error when connecting?
688,After the canal instance defaultDatabaseName is set to the specified library, you can also receive messages for the addition and deletion of other libraries. As the title I only specified an instance instance, only one library is specified, but all library changes can be monitored when consumed. Check out the history of the issue @agapple Thank you
687,Canal on windows The server will report the error from time to time. The remote host forcibly closes an existing connection. The error log is the instance log. The canal log does not report the error log as follows 2018-06-11 19:21:30.110 [New I/O server worker #1-1] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x0c80035e /10.204.241.135:58426 => /10.204.241.135:11111] exception=java.io.IOException: The remote host forcibly closed an existing connection at sun.nio.ch.SocketDispatcher.write0(Native Method) at sun.nio.ch.SocketDispatcher.write(Unknown Source) at sun.nio.ch.IOUtil.writeFromNativeBuffer(Unknown Source) at sun.nio.ch.IOUtil.write(Unknown Source) at sun.nio.ch.SocketChannelImpl.write(Unknown Source) at org.jboss.netty.channel.socket.nio.SocketSendBufferPool$PooledSendBuffer.transferTo(SocketSendBufferPool.java:243) at org.jboss.netty.channel.socket.nio.NioWorker.write0(NioWorker.java:470) at org.jboss.netty.channel.socket.nio.NioWorker.writeFromUserCode(NioWorker.java:388) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:137) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) at org.jboss.netty.channel.Channels.write(Channels.java:611) at org.jboss.netty.channel.Channels.write(Channels.java:578) at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:28) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:144) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:444) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261) at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:350) at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) at java.lang.Thread.run(Unknown Source) Mainly the timeout mechanism causes the 26 version to do some optimization Https github com alibaba canal issues 640 Take a look at this
686,canal server Swap is full and causes disk space 100 to be used Such as the question v1 0 26 alpha 2 version canal server Swap is full and causes disk space 100 to be used Swap memory %Cpu(s): 0.2 us 0.2 sy 0.0 ni 99.6 id 0.0 wa 0.0 hi 0.0 si 0.0 st KiB Mem: 8193776 total 8050240 used 143536 free 87860 buffers KiB Swap: 0 total 0 used 0 free 6344652 cached canal This swap of server cached Only increase or decrease, resulting in a final disk space usage of 100 Current rootfs 60G 12G 46G 20% / Restart canal Server later swap The cached is 0. The disk space is back to normal and then the next round is only increasing.
685,Database connection problem @agapple Hello, I am based on V25 now. Canceled the client Changed to eventStore2Kafka Now that the database connection has a problem, can you help me to see it? ![gf vm9d xhrh z s5 f ps](https://user-images.githubusercontent.com/24663485/41216549-63ed2b40-6d87-11e8-99d9-d1c750d0d7d9.png)
684,Turn on detection and often report Connection reset by peer See other issues saying that it is recommended to turn on the heartbeat, so turn it on canal.instance.detecting.enable =true canal.instance.detecting.heartbeatHaEnable = true The result is very easy to make mistakes when it does not affect the data capture. The next day, you can&#39;t capture the data. 1 0 25 version of my two parameters are false or unstable. This is the client interval for a while, even the server will be out of Connection. reset by Peer, but it is not necessarily in our alpha environment. I tested it in my local area for one afternoon. Recommended to update to version 26 Https github com alibaba canal issues 640 Take a look at this
683,Ask v1 0 26 alpha The 2 version has consumption accumulation and the server side disk is full. As the title asks at v1 0 26 alpha The 2 version yesterday had about four hours to find that the consumption of canal was delayed. The current consumption data is mainly due to the update data. The delay is the database that was updated two hours ago and then canal. Client only consumes insert data no problem other canal The server&#39;s disk does not know that it was occupied by the file. After the restart of the server, the disk space returned to normal. However, the service was not restored after a period of time. It was automatically recovered and no obvious error was found. So I would like to ask if you have encountered such a problem. And what is the cause of the problem? Thank you.
682,Fix #631 Boolean java type support tks
681,fix issue #680 [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=681) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=681) before we can accept your contribution.<br/><hr/>**winger** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=681) it.</sub> [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=681) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=681) before we can accept your contribution.<br/><hr/>**winger** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=681) it.</sub> This trunk has fixed a non-such change mode.
680,When table Unique key Tsdb parsing error when it is a non-standard field name Table structure as shown below ![image](https://user-images.githubusercontent.com/33280738/41092395-58152322-6a7b-11e8-9d54-3208c4863e27.png) canal Via fastsql The results of the analysis are as follows ![image](https://user-images.githubusercontent.com/33280738/41092506-9d2a60bc-6a7b-11e8-918b-407415af0e6d.png) And at this time canal by Name 200 does not correspond to the field name when comparing the memory with the actual table field Download the latest 1 0 26 release package
679,Merge pull request #1 from alibaba/master merge from master [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=679) <br/>All committers have signed the CLA. [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=679) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=679) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=679) it.</sub>
678,com.alibaba.otter.canal.parse.exception.CanalParseException: can't find start position for example ![image](https://user-images.githubusercontent.com/9798724/41083138-0893606a-6a63-11e8-89e7-9fad84702144.png) ![image](https://user-images.githubusercontent.com/9798724/41083182-1f12c8f8-6a63-11e8-81f8-66bb0c8e52d3.png) Can&#39;t find a suitable place
677,canal.instance.gtidon This default value does not force configuration to ensure forward compatibility version v1.0.26 alpha 3 2018-06-07 12:25:29.861 [main] ERROR c.a.o.c.common.zookeeper.running.ServerRunningMonitor - start failed com.google.common.collect.ComputationException: com.alibaba.otter.canal.common.CanalException: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'instance' defined in class path resource [spring/default-instance.xml]: Cannot resolve reference to bean 'eventParser' while setting bean property 'eventParser'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'eventParser' defined in class path resource [spring/default-instance.xml]: Initialization of bean failed; nested exception is org.springframework.beans.TypeMismatchException: Failed to convert property value of type 'java.lang.String' to required type 'boolean' for property 'isGTIDMode'; nested exception is java.lang.IllegalArgumentException: Invalid boolean value [] at com.google.common.collect.MapMaker$ComputingMapAdapter.get(MapMaker.java:889) ~[guava-18.0.jar:na] at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.start(CanalServerWithEmbedded.java:98) ~[canal.server-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.deployer.CanalController$2$1.processActiveEnter(CanalController.java:128) ~[canal.deployer-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.common.zookeeper.running.ServerRunningMonitor.processActiveEnter(ServerRunningMonitor.java:245) ~[canal.common-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.common.zookeeper.running.ServerRunningMonitor.initRunning(ServerRunningMonitor.java:150) ~[canal.common-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.common.zookeeper.running.ServerRunningMonitor.start(ServerRunningMonitor.java:104) ~[canal.common-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.deployer.CanalController.start(CanalController.java:415) [canal.deployer-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.deployer.CanalLauncher.main(CanalLauncher.java:38) [canal.deployer-1.0.26-SNAPSHOT.jar:na] Caused by: com.alibaba.otter.canal.common.CanalException: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'instance' defined in class path resource [spring/default-instance.xml]: Cannot resolve reference to bean 'eventParser' while setting bean property 'eventParser'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'eventParser' defined in class path resource [spring/default-instance.xml]: Initialization of bean failed; nested exception is org.springframework.beans.TypeMismatchException: Failed to convert property value of type 'java.lang.String' to required type 'boolean' for property 'isGTIDMode'; nested exception is java.lang.IllegalArgumentException: Invalid boolean value [] Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'instance' defined in class path resource [spring/default-instance.xml]: Cannot resolve reference to bean 'eventParser' while setting bean property 'eventParser'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'eventParser' defined in class path resource [spring/default-instance.xml]: Initialization of bean failed; nested exception is org.springframework.beans.TypeMismatchException: Failed to convert property value of type 'java.lang.String' to required type 'boolean' for property 'isGTIDMode'; nested exception is java.lang.IllegalArgumentException: Invalid boolean value [] at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:334) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE] at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:108) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE] at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyPropertyValues(AbstractAutowireCapableBeanFactory.java:1417) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE] at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1158) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE] at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:519) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE] at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:458) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE] at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:296) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE] at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:223) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE] at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:293) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE] at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:194) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE] at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:633) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE] at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:932) ~[spring-context-3.2.9.RELEASE.jar:3.2.9.RELEASE]
676,This is also 404 ah manager deployer xyz tar gz https://github.com/alibaba/otter/releases/download/otter-x.y.z/manager.deployer-x.y.z.tar.gz Replace xyz with a specific version
675,Server multiple instances can start up to 5 jobs at a time a canal server 15 instances are deployed. Only 5 instances can get the location information and restart the five are not fixed. Have one of them reported SHOW VIEW Command does not have permission to open this permission to the account Only select replication in the documentation slave，replication Client permission 2018-06-06 19:18:22.458 [destination = mysql.05.upanb.cn address = mysql.05.upanb.cn/192.168.250.214:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address mysql.05.upanb.cn/192.168.250.214:3306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: ErrorPacket [errorNumber=1142 fieldCount=-1 message=SHOW VIEW command denied to user 'canal'@'192.168.249.54' for table 'taglist' sqlState=42000 sqlStateMarker=#] with command: show create table `pointstore`.`admin`;show create table `pointstore`.`blacklist`;show create table `pointstore`.`clearuk`;show create table `pointstore`.`duibaorder`;show create table `pointstore`.`point_detail_description`;show create table `pointstore`.`returnpoint`;show create table `pointstore`.`s_admin`;show create table `pointstore`.`t_adminlog`;show create table `pointstore`.`t_broker`;show create table `pointstore`.`t_broker_copy`;show create table `pointstore`.`t_brokercompany`;show create table `pointstore`.`t_exchangerule`;show create table `pointstore`.`t_luckproduct`;show create table `pointstore`.`t_luckrecord`;show create table `pointstore`.`t_luckrecord2`;show create table `pointstore`.`t_luckrecord3`;show create table `pointstore`.`t_luckrecord4`;show create table `pointstore`.`t_oauthinfo`;show create table `pointstore`.`t_oauthinfo_bak1`;show create table `pointstore`.`t_oauthinfo_copy`;show create table `pointstore`.`t_pointactivity`;show create table `pointstore`.`t_pointdetail_new`;show create table `pointstore`.`t_pointmain_new`;show create table `pointstore`.`t_pointmain_new_20180212`;show create table `pointstore`.`t_pointmain_new_bak`;show create table `pointstore`.`t_pointmain_new_copy`;show create table `pointstore`.`t_pointproduct`;show create table `pointstore`.`t_pointrule`;show create table `pointstore`.`t_pointsetting`;show create table `pointstore`.`t_pointstoreadminauthority`;show create table `pointstore`.`t_pointstoreinvoice`;show create table `pointstore`.`t_pointstorevisitlog`;show create table `pointstore`.`t_productinventory`;show create table `pointstore`.`t_productpic`;show create table `pointstore`.`t_productscategory`;show create table `pointstore`.`t_productsexchange_new`;show create table `pointstore`.`t_productsexpand`;show create table `pointstore`.`t_productshow`;show create table `pointstore`.`t_provincecityarea`;show create table `pointstore`.`t_ruletocity`;show create table `pointstore`.`taglist`;show create table `pointstore`.`test`;show create table `pointstore`.`timelist`;show create table `pointstore`.`v_brokerpoint`;show create table `pointstore`.`v_luckrecord`;show create table `pointstore`.`v_pointstoreinvoicetotal`;show create table `pointstore`.`v_pointstoreinvoicetotal_a`;show create table `pointstore`.`v_productshow`;show create table `pointstore`.`v_productshowlist`;show create table `pointstore`.`v_productwarehouse`;show create table `pointstore`.`yearlist`; Caused by: java.io.IOException: ErrorPacket [errorNumber=1142 fieldCount=-1 message=SHOW VIEW command denied to user 'canal'@'192.168.249.54' for table 'taglist' sqlState=42000 sqlStateMarker=#] 2018-06-06 19:18:22.459 [destination = mysql.05.upanb.cn address = mysql.05.upanb.cn/192.168.250.214:3306 EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - disConnect MysqlConnection to mysql.05.upanb.cn/192.168.250.214:3306... 2018-06-06 19:18:22.459 [destination = mysql.05.upanb.cn address = mysql.05.upanb.cn/192.168.250.214:3306 EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - disConnect MysqlConnection to mysql.05.upanb.cn/192.168.250.214:3306.. Tsdb often report errors can not be used? 2018-06-07 00:44:16.671 [destination = mysql.13.upanb.cn address = mysql.13.upanb.cn/192.168.250.226:3306 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - pls submit github issue show create table ddl:CREATE TABLE `group_order_history_detail_new` ( `result_id` int(11) NOT NULL DEFAULT '0' COMMENT order number `year` int(11) NOT NULL DEFAULT '2018' COMMENT year `month` int(11) NOT NULL DEFAULT '2' COMMENT month `order_id` int(11) NOT NULL DEFAULT '0' COMMENT Order center order number `product_id` int(11) NOT NULL DEFAULT '0' COMMENT Project ID `product_name` varchar(50) NOT NULL DEFAULT '' COMMENT project name `broker_id` int(11) NOT NULL DEFAULT '0' COMMENT Broker ID `broker_name` varchar(50) NOT NULL DEFAULT '' COMMENT Broker name `broker_phone` varchar(50) NOT NULL DEFAULT '' COMMENT Broker phone number `building_id` int(11) NOT NULL DEFAULT '0' COMMENT Property ID `sale_id` int(11) NOT NULL DEFAULT '0' COMMENT Sales ID `sale_name` varchar(50) NOT NULL DEFAULT '' COMMENT Salesperson name `sale_group_id` int(11) NOT NULL DEFAULT '0' COMMENT Sales team ID `sale_group_name` varchar(45) NOT NULL DEFAULT '' COMMENT Sales team name `expand_id` int(11) NOT NULL DEFAULT '0' COMMENT Extended ID `expand_name` varchar(50) NOT NULL DEFAULT '' COMMENT Developer name `expand_group_id` int(11) NOT NULL DEFAULT '0' COMMENT Expansion team ID `expand_group_name` varchar(45) NOT NULL DEFAULT '' COMMENT Expansion team name `accounts_amount` bigint(20) NOT NULL DEFAULT '0' COMMENT Receivables `out_accounts_amount` bigint(20) NOT NULL DEFAULT '0' COMMENT Payable `actual_ascription_amount` bigint(20) NOT NULL DEFAULT '0' COMMENT Attributable income receivable `sale_actual_ascription_amount` bigint(20) NOT NULL DEFAULT '0' COMMENT Sales vesting income `expand_actual_ascription_amount` bigint(20) NOT NULL DEFAULT '0' COMMENT Expand vesting income `public_amount` bigint(20) NOT NULL DEFAULT '0' COMMENT Public pool income attributable income `cav_accounts_amount` bigint(20) NOT NULL DEFAULT '0' COMMENT Actually received less than 180 days `out_cav_accounts_amount` bigint(20) NOT NULL DEFAULT '0' COMMENT More than 180 days of actual receipts `sale_divide_rate` double(5 4) NOT NULL DEFAULT '0.0000' COMMENT Sales expansion share `subscription_time` timestamp NOT NULL DEFAULT '0000-00-00 00:00:00' ON UPDATE CURRENT_TIMESTAMP COMMENT Subscription time `handle_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT Customer transaction date `confirm_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT Upload receipt time `confirmation_cycle` int(11) NOT NULL DEFAULT '0' COMMENT Confirmation cycle `sale_actual_accounts_amount` bigint(20) NOT NULL DEFAULT '0' COMMENT Sales team receivable amount `sale_actual_cav_accounts_amount` bigint(20) NOT NULL DEFAULT '0' COMMENT Sales team received amount `expand_actual_accounts_amount` bigint(20) NOT NULL DEFAULT '0' COMMENT Expansion group receivable amount `expand_actual_cav_accounts_amount` bigint(20) NOT NULL DEFAULT '0' COMMENT Expansion team received amount `is_baoxiao` tinyint(4) NOT NULL DEFAULT '0' COMMENT Whether the order is underwritten Non-underwriting 0 Underwriting 1 Class underwriting `actual_cost` bigint(20) NOT NULL DEFAULT '0' COMMENT The actual sales expense is actually reported in the actual express report. `fixed_amount` bigint(20) NOT NULL DEFAULT '0' COMMENT fixed income `cav_fixed_amount` bigint(20) NOT NULL DEFAULT '0' COMMENT Returned fixed income `receivable_commission` bigint(20) NOT NULL DEFAULT '0' COMMENT Commission receivable `cav_commission` bigint(20) NOT NULL DEFAULT '0' COMMENT Commissioned `pay_commission` bigint(20) NOT NULL DEFAULT '0' COMMENT Commission payable `cav_commission` bigint(20) NOT NULL DEFAULT '0' COMMENT Commissioned `pay_commission` bigint(20) NOT NULL DEFAULT '0' COMMENT Commission payable `receivable_deal` bigint(20) NOT NULL DEFAULT '0' COMMENT Receivable award `cav_deal` bigint(20) NOT NULL DEFAULT '0' COMMENT Returned prize `pay_deal` bigint(20) NOT NULL DEFAULT '0' COMMENT Coping Deal Award `receivable_jump` bigint(20) NOT NULL DEFAULT '0' COMMENT Point of hop `cav_jump` bigint(20) NOT NULL DEFAULT '0' COMMENT Jumpback point `pay_jump` bigint(20) NOT NULL DEFAULT '0' COMMENT Coping point `receivable_premium` bigint(20) NOT NULL DEFAULT '0' COMMENT Receivable premium `cav_premium` bigint(20) NOT NULL DEFAULT '0' COMMENT Returned premium `pay_premium` bigint(20) NOT NULL DEFAULT '0' COMMENT Pay premium `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT Creation time `close_time` timestamp NULL DEFAULT NULL ON UPDATE CURRENT_TIMESTAMP COMMENT Order closing time `is_close` tinyint(4) NOT NULL DEFAULT '0' COMMENT 0 Order not closed 1 Order closed `sys_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT system time `is_delete` tinyint(4) NOT NULL DEFAULT '0' COMMENT delete or not 0 not deleted 1 delete UNIQUE KEY `index` (`result_id` `year` `month`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 compare failed . db : TableMeta [schema=tops_data_gdm table=group_order_history_detail_new fileds= FieldMeta [columnName=result_id columnType=int(11) defaultValue=0 nullable=false key=true] FieldMeta [columnName=year columnType=int(11) defaultValue=2018 nullable=false key=true] FieldMeta [columnName=month columnType=int(11) defaultValue=2 nullable=false key=true] FieldMeta [columnName=order_id columnType=int(11) defaultValue=0 nullable=false key=false] FieldMeta [columnName=product_id columnType=int(11) defaultValue=0 nullable=false key=false] FieldMeta [columnName=product_name columnType=varchar(50) defaultValue= nullable=false key=false] FieldMeta [columnName=broker_id columnType=int(11) defaultValue=0 nullable=false key=false] FieldMeta [columnName=broker_name columnType=varchar(50) defaultValue= nullable=false key=false] FieldMeta [columnName=broker_phone columnType=varchar(50) defaultValue= nullable=false key=false] FieldMeta [columnName=building_id columnType=int(11) defaultValue=0 nullable=false key=false] FieldMeta [columnName=sale_id columnType=int(11) defaultValue=0 nullable=false key=false] FieldMeta [columnName=sale_name columnType=varchar(50) defaultValue= nullable=false key=false] FieldMeta [columnName=sale_group_id columnType=int(11) defaultValue=0 nullable=false key=false] FieldMeta [columnName=sale_group_name columnType=varchar(45) defaultValue= nullable=false key=false] FieldMeta [columnName=expand_id columnType=int(11) defaultValue=0 nullable=false key=false] FieldMeta [columnName=expand_name columnType=varchar(50) defaultValue= nullable=false key=false] FieldMeta [columnName=expand_group_id columnType=int(11) defaultValue=0 nullable=false key=false] FieldMeta [columnName=expand_group_name columnType=varchar(45) defaultValue= nullable=false key=false] FieldMeta [columnName=accounts_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=out_accounts_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=actual_ascription_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=sale_actual_ascription_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=expand_actual_ascription_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=public_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=cav_accounts_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=out_cav_accounts_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=cav_accounts_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=out_cav_accounts_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=sale_divide_rate columnType=double(5 4) defaultValue=0.0000 nullable=false key=false] FieldMeta [columnName=subscription_time columnType=timestamp defaultValue=0000-00-00 00:00:00 nullable=false key=false] FieldMeta [columnName=handle_time columnType=timestamp defaultValue=CURRENT_TIMESTAMP nullable=false key=false] FieldMeta [columnName=confirm_time columnType=timestamp defaultValue=CURRENT_TIMESTAMP nullable=false key=false] FieldMeta [columnName=confirmation_cycle columnType=int(11) defaultValue=0 nullable=false key=false] FieldMeta [columnName=sale_actual_accounts_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=sale_actual_cav_accounts_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=expand_actual_accounts_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=expand_actual_cav_accounts_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=is_baoxiao columnType=tinyint(4) defaultValue=0 nullable=false key=false] FieldMeta [columnName=actual_cost columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=fixed_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=cav_fixed_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=receivable_commission columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=cav_commission columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=pay_commission columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=receivable_deal columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=cav_deal columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=pay_deal columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=receivable_jump columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=cav_jump columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=pay_jump columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=receivable_premium columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=cav_premium columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=pay_premium columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=create_time columnType=timestamp defaultValue=CURRENT_TIMESTAMP nullable=false key=false] FieldMeta [columnName=close_time columnType=timestamp defaultValue=null nullable=true key=false] FieldMeta [columnName=is_close columnType=tinyint(4) defaultValue=0 nullable=false key=false] FieldMeta [columnName=sys_time columnType=timestamp defaultValue=CURRENT_TIMESTAMP nullable=false key=false] FieldMeta [columnName=is_delete columnType=tinyint(4) defaultValue=0 nullable=false key=false] ] mem : TableMeta [schema=tops_data_gdm table=group_order_history_detail_new fileds= FieldMeta [columnName=result_id columnType=int(11) defaultValue=0 nullable=false key=false] FieldMeta [columnName=year columnType=int(11) defaultValue=2018 nullable=false key=false] FieldMeta [columnName=month columnType=int(11) defaultValue=2 nullable=false key=false] FieldMeta [columnName=order_id columnType=int(11) defaultValue=0 nullable=false key=false] FieldMeta [columnName=product_id columnType=int(11) defaultValue=0 nullable=false key=false] FieldMeta [columnName=product_name columnType=varchar(50) defaultValue= nullable=false key=false] FieldMeta [columnName=broker_id columnType=int(11) defaultValue=0 nullable=false key=false] FieldMeta [columnName=broker_name columnType=varchar(50) defaultValue= nullable=false key=false] FieldMeta [columnName=broker_phone columnType=varchar(50) defaultValue= nullable=false key=false] FieldMeta [columnName=building_id columnType=int(11) defaultValue=0 nullable=false key=false] FieldMeta [columnName=sale_id columnType=int(11) defaultValue=0 nullable=false key=false] FieldMeta [columnName=building_id columnType=int(11) defaultValue=0 nullable=false key=false] FieldMeta [columnName=sale_id columnType=int(11) defaultValue=0 nullable=false key=false] FieldMeta [columnName=sale_name columnType=varchar(50) defaultValue= nullable=false key=false] FieldMeta [columnName=sale_group_id columnType=int(11) defaultValue=0 nullable=false key=false] FieldMeta [columnName=sale_group_name columnType=varchar(45) defaultValue= nullable=false key=false] FieldMeta [columnName=expand_id columnType=int(11) defaultValue=0 nullable=false key=false] FieldMeta [columnName=expand_name columnType=varchar(50) defaultValue= nullable=false key=false] FieldMeta [columnName=expand_group_id columnType=int(11) defaultValue=0 nullable=false key=false] FieldMeta [columnName=expand_group_name columnType=varchar(45) defaultValue= nullable=false key=false] FieldMeta [columnName=accounts_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=out_accounts_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=actual_ascription_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=sale_actual_ascription_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=expand_actual_ascription_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=public_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=cav_accounts_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=out_cav_accounts_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=sale_divide_rate columnType=double(5 4) defaultValue=0.0000 nullable=false key=false] FieldMeta [columnName=subscription_time columnType=timestamp defaultValue=0000-00-00 00:00:00 nullable=false key=false] FieldMeta [columnName=handle_time columnType=timestamp defaultValue=CURRENT_TIMESTAMP nullable=false key=false] FieldMeta [columnName=confirm_time columnType=timestamp defaultValue=CURRENT_TIMESTAMP nullable=false key=false] FieldMeta [columnName=confirmation_cycle columnType=int(11) defaultValue=0 nullable=false key=false] FieldMeta [columnName=sale_actual_accounts_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=sale_actual_cav_accounts_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=expand_actual_accounts_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=expand_actual_cav_accounts_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=is_baoxiao columnType=tinyint(4) defaultValue=0 nullable=false key=false] FieldMeta [columnName=actual_cost columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=fixed_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=cav_fixed_amount columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=receivable_commission columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=cav_commission columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=pay_commission columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=receivable_deal columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=cav_deal columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=pay_deal columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=receivable_jump columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=cav_jump columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=pay_jump columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=receivable_premium columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=cav_premium columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=pay_premium columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=create_time columnType=timestamp defaultValue=CURRENT_TIMESTAMP nullable=false key=false] FieldMeta [columnName=close_time columnType=timestamp defaultValue=null nullable=true key=false] FieldMeta [columnName=is_close columnType=tinyint(4) defaultValue=0 nullable=false key=false] FieldMeta [columnName=close_time columnType=timestamp defaultValue=null nullable=true key=false] FieldMeta [columnName=is_close columnType=tinyint(4) defaultValue=0 nullable=false key=false] FieldMeta [columnName=sys_time columnType=timestamp defaultValue=CURRENT_TIMESTAMP nullable=false key=false] FieldMeta [columnName=is_delete columnType=tinyint(4) defaultValue=0 nullable=false key=false] ] 2018-06-07 00:59:47.133 [destination = mysql.04.upanb.cn address = mysql.04.upanb.cn/192.168.250.213:3306 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - pls submit github issue show create table ddl:CREATE TABLE `tk_broker_tag` ( `broker_id` bigint(20) NOT NULL COMMENT Broker id `tag_id` bigint(20) NOT NULL COMMENT Tag id The primary key of the table tk_tag `tag_count` bigint(20) NOT NULL DEFAULT '0' COMMENT The number of times the broker was evaluated for the tag `sys_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP UNIQUE KEY `IX_broker_tag` (`broker_id` `tag_id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 compare failed . db : TableMeta [schema=tops_kber table=tk_broker_tag fileds= FieldMeta [columnName=broker_id columnType=bigint(20) defaultValue=null nullable=false key=true] FieldMeta [columnName=tag_id columnType=bigint(20) defaultValue=null nullable=false key=true] FieldMeta [columnName=tag_count columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=sys_time columnType=timestamp defaultValue=CURRENT_TIMESTAMP nullable=false key=false] ] mem : TableMeta [schema=tops_kber table=tk_broker_tag fileds= FieldMeta [columnName=broker_id columnType=bigint(20) defaultValue=null nullable=false key=false] FieldMeta [columnName=tag_id columnType=bigint(20) defaultValue=null nullable=false key=false] FieldMeta [columnName=tag_count columnType=bigint(20) defaultValue=0 nullable=false key=false] FieldMeta [columnName=sys_time columnType=timestamp defaultValue=CURRENT_TIMESTAMP nullable=false key=false] ] You are this version of the new version has fixed this problem 1.0.25 Neal Hu nealhu@apache.org > in June 7, 2018 09 51 agapple <notifications@github.com> Write > > You are this version of the new version has fixed this problem > > — > You are receiving this because you authored the thread. > Reply to this email directly view it on GitHub or mute the thread. Use the latest 1 0 26
674,v1.0.26 alpha The 2 version of the client has a jar package corresponding to the maven library in which maven library? ![image](https://user-images.githubusercontent.com/9798724/41032611-83024bf2-69b6-11e8-89fd-542e572f7ed1.png) Download code for local clean Install
673,Local test connection remote canal does not seem to get the message I found that I can receive messages when I use the canal service locally. Locally seems to have never received a message when using canal on a remote server `CanalConnectors.newSingleConnector(new InetSocketAddress("xxx.xxx.xxx.xxx" 11111) "example" "" "");` You can see such a log in the example example log every time you connect to the local remote. `2018-06-06 17:57:15.309 [New I/O server worker #1-1] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - subscribe filter change to .*\..*` This seems to have been connected to the remote cannal service but the local has not received the bin log message. canal Did you see the log? Is there an error? I changed it again this morning. 1 0 26 alpha3 There is no problem on a MySQL5 6 40. The local test seems to be no problem. I downloaded the same version of the canal on another server and found that MySQL5 7 21 seems to be not stable sometimes. Local can receive the message but most of the time is not enough. I don&#39;t know if it is compatible with 5 7 21 for canal. Have a look at canal Whether the server&#39;s log has some links or parsing exceptions **example.log** ``` 2018-06-07 11:03:51.958 [Thread-4] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - stop successful.... 2018-06-07 11:04:30.064 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2018-06-07 11:04:30.068 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties] 2018-06-07 11:04:30.218 [main] WARN o.s.beans.GenericTypeAwarePropertyDescriptor - Invalid JavaBean property 'connectionCharset' being accessed! Ambiguous write methods found next to actually used [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.nio.charset.Charset)]: [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.lang.String)] 2018-06-07 11:04:30.268 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2018-06-07 11:04:30.268 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties] 2018-06-07 11:04:30.493 [main] ERROR com.alibaba.druid.pool.DruidDataSource - testWhileIdle is true validationQuery not set 2018-06-07 11:04:30.769 [main] INFO c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-example 2018-06-07 11:04:30.882 [main] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - subscribe filter change to .*\..* 2018-06-07 11:04:30.882 [main] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - start successful.... 2018-06-07 11:04:30.908 [destination = example address = /127.0.0.1:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position {"identity":{"slaveId":-1 "sourceAddress":{"address":"localhost" "port":3306}} "postion":{"gtid":"" "included":false "journalName":"mysql-bin.000002" "position":5818 "serverId":1 "timestamp":1528340336000}} 2018-06-07 11:04:31.332 [destination = example address = /127.0.0.1:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000002 position=5818 serverId=1 gtid= timestamp=1528340336000] 2018-06-07 11:05:17.176 [New I/O server worker #1-2] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - subscribe filter change to .*\..* 2018-06-07 11:07:50.895 [New I/O server worker #1-3] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - subscribe filter change to .*\..* ``` **canal.log** ``` 2018-06-07 11:03:51.829 [Thread-4] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## stop the canal server 2018-06-07 11:03:51.962 [Thread-4] INFO com.alibaba.otter.canal.deployer.CanalController - ## stop the canal Server IP security processing 11111 2018-06-07 11:03:51.962 [Thread-4] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## canal server is down. Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=96m; support was removed in 8.0 Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=256m; support was removed in 8.0 Java HotSpot(TM) 64-Bit Server VM warning: UseCMSCompactAtFullCollection is deprecated and will likely be removed in a future release. 2018-06-07 11:04:29.664 [main] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## set default uncaught exception handler 2018-06-07 11:04:29.707 [main] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## load canal configurations 2018-06-07 11:04:29.707 [main] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## start the canal server. 2018-06-07 11:04:29.749 [main] INFO com.alibaba.otter.canal.deployer.CanalController - ## start the canal Server IP security processing 11111 2018-06-07 11:04:30.218 [main] WARN o.s.beans.GenericTypeAwarePropertyDescriptor - Invalid JavaBean property 'connectionCharset' being accessed! Ambiguous write methods found next to actually used [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.nio.charset.Charset)]: [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.lang.String)] 2018-06-07 11:04:30.493 [main] ERROR com.alibaba.druid.pool.DruidDataSource - testWhileIdle is true validationQuery not set 2018-06-07 11:04:30.908 [destination = example address = /127.0.0.1:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position {"identity":{"slaveId":-1 "sourceAddress":{"address":"localhost" "port":3306}} "postion":{"gtid":"" "included":false "journalName":"mysql-bin.000002" "position":5818 "serverId":1 "timestamp":1528340336000}} 2018-06-07 11:04:30.933 [main] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## the canal server is running now ...... 2018-06-07 11:04:31.332 [destination = example address = /127.0.0.1:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000002 position=5818 serverId=1 gtid= timestamp=1528340336000] ``` Meta log to see the consumption situation The problem is solved because a Java program on this server of MySQL5 7 21 has been connected to the 11111 port. I saw the log and found that the data was consumed by the program, which caused me to consume less locally. Summarize suggestions for others 1） Because the local can not consume the remote canal information, please upgrade to the 1 0 26 version of the upgrade and solve the problem that I can not consume a remote 5 6 40 bin log data. 2 If you still can&#39;t consume data after the upgrade, please check if other applications are consuming bin log data. Thanks everyone.
672,Mysql generated binlog large amount of data when syncing to Canal slow how to solve this Did some large amount of data testing, such as updating a 100W table to synchronize to 8 8 seconds to reach Canal When I get the data, the batchid is 1 and then I guess the RingBuffer on the server side. doPut should not be put to the data I think it should be Mysql to Canal Server will have a blocking process. Is there any way to reduce the blocking time? For example, mysql5 7 multi-threaded parallel operation Help analyze several processes written to binlog -> Canal received -> Canal parsing storage to memory -> Client receives a look at the specific bottleneck point 1. Writing binlog involves small IO should not be a problem 2. canal Network IO IO Thread when going to fetch 3. When client When setting the value is relatively large The IO operation designed by the above three points We recently found through testing while (fetcher.fetch()) Pull the data qps at 1000 Because the canal read data is single-threaded, if the insertion of millions of data in a moment can be extended to the canal client, the extension is still very large. I don’t know if the landlord solved this problem. You are delaying now how long My 1 million data is about 25 points. 50W data 5 minutes or so 50W data 5 minutes or so Is the single threaded method used? Synchronize data to es Still redis Add me wx aleenjava Explore I did this by opening 2 threads. A thread keeps fetching data and puts it in the blocking queue. Then the second thread is consumed from the blocking queue and then sent to MQ in order. @DevWithLin Is your side based on canal analysis completed docking kafka? I am using it here. Based on the configuration interface, it is currently useless for rabbitmq and activemq kafka. Follow-up will complete kafka @agapple I also intend Let canalClient support Sky Walking I did this by opening 2 threads. A thread keeps fetching data and puts it in the blocking queue. Then the second thread is consumed from the blocking queue and then sent to MQ in order. This is sent to MQ in a single-threaded consumption order What is the difference? When the MQ transmission takes longer than from Canal Server This makes sense when you get data. Where is the code? Do you share it? It&#39;s so simple that you don&#39;t need code to open two threads and a blocking queue to simulate production consumption. I just use this way Achieved Didn’t reach your effect 1 million data probably 20 minutes I am here very quickly. Where did you send it? Write to es in Written in kafka Almost like maxwell @chenglinjava68 Guess so ES bottleneck, I wrote more than 8000 messages on rabbitmq 8000 per second It’s a bit exaggerated.
671,canal V1 0 25 version error java nio channels ClosedByInterruptException null 2018-06-05 00:50:56.419 [destination = example address = testrds1.amazonaws.com/10.8.3.18:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address testrds1.corlbeqxlnts.us-west-2.rds.amazonaws.com/10.8.3.180:3306 has an error retrying. caused by java.nio.channels.ClosedByInterruptException: null at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannel.read(SocketChannel.java:49) ~[canal.parse.driver-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:151) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:77) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:137) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) ~[canal.parse-1.0.25.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144] 2018-06-05 00:50:56.425 [destination = example address = testrds1.amazonaws.com/10.8.3.18:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.nio.channels.ClosedByInterruptException at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannel.read(SocketChannel.java:49) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:151) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:77) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:137) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) at java.lang.Thread.run(Thread.java:748) ] Try 26 version 看下 Issue604 https github com alibaba canal issues 640 
670,Support mongo es synchronization [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=670) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=670) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=670) it.</sub> [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=670) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=670) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=670) it.</sub> 1. This change is a bit large and contains mongo&#39;s parsing and synchronization features should be split into client code. 2. The code has some conflicts
669,Canal Design based on Server What is the purpose of Client mode? Canal Design based on Server What is the purpose of Client mode? Is it more reliable and efficient to parse binlog directly on the server side and consume it? The question of the landlord can ask the meaning of the distributed system. In addition to making the programmable client more lightweight, what else to consider, the server can also be made distributed, and currently supports the server. Ha In order to facilitate the operation and maintenance management can also be directly embedded into the server directly connected to mysql For such efficiency and reliability will be higher, we intend to package canal The client&#39;s code support is configured as an embedded server. It also abstracts the extensible outconnector. The spi mechanism is currently doing the hbase es log4j connector to consider contributing to the community. Very welcome to submit PR The corresponding kafka related code has been merged Welcome to download https github com alibaba canal releases tag canal 1 0 26 preview 3
668,Canal Server Will the binlog offset information be synchronized when HA is switched? Will be able to record the location in the zookeeper Switch to default instance xml Record position information with zk Switch to StandBy DB Binlog It will not be right for this situation. It needs manual processing. I always thought that the HA here is automatic, but it would not be so simple. Forgive me for being in this closed The issue continues to ask because there is no place where it can communicate directly and efficiently.
667,canal Broken pipe Look at the code seems to be to update the meta_snapshot in MySQL What is the use of this table? ## canal Log ```bash 2018-06-03 21:24:30.826 [[scheduler-table-meta-snapshot]] ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - scheudle applySnapshotToDB faield com.alibaba.otter.canal.parse.exception.CanalParseException: java.net.SocketException: Broken pipe (Write failed) Caused by: java.net.SocketException: Broken pipe (Write failed) at java.net.SocketOutputStream.socketWrite0(Native Method) ~[na:1.8.0_171] at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111) ~[na:1.8.0_171] at java.net.SocketOutputStream.write(SocketOutputStream.java:143) ~[na:1.8.0_171] at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.write(BioSocketChannel.java:35) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.writeBody0(PacketManager.java:42) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.writeBody(PacketManager.java:35) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.query(MysqlQueryExecutor.java:55) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:94) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.compareTableMetaDbAndMemory(DatabaseTableMeta.java:296) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.applySnapshotToDB(DatabaseTableMeta.java:251) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.access$100(DatabaseTableMeta.java:45) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta$2.run(DatabaseTableMeta.java:84) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_171] at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) [na:1.8.0_171] at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) [na:1.8.0_171] at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) [na:1.8.0_171] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_171] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_171] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171] ``` ##canal-client ```bash mory=0.024801254 InstallDirSize=11.233233 LastUpdatetime=2018-05-31 17:01:28.0}] old=[\{LastUpdatetime=2018-05-31 17:01:23.0}]} 10:11:45.401 [pool-6-thread-1] INFO com.tops001.foundation.canal.connector.CanalLog4JConnector - \{"data":[\{"Id":1 NodeName new node "NodeCreateTime":1484977299000 "NodeIP":"WIN-8RG5KGC1JNM" "NodeLastUpdatetime":1527757293000 "IfCheckState":false}] "database":"tops_bops_task" "old":[\{"NodeLastUpdatetime":1527757288000}] "sql":"" "table":"task_node" "ts":1528078305401 "type":"UPDATE"} 10:11:45.401 [pool-6-thread-1] DEBUG com.tops001.foundation.canal.connector.es.syn.ElasticsearchSynService - Dml\{database='tops_bops_task' table='task_node' type='UPDATE' ts=1528078305401 sql='' data=[\{Id=1 NodeName new node NodeCreateTime=2017-01-21 13:41:39.0 NodeIP=WIN-8RG5KGC1JNM NodeLastUpdatetime=2018-05-31 17:01:33.0 IfCheckState=false}] old=[\{NodeLastUpdatetime=2018-05-31 17:01:28.0}]} 10:11:45.402 [Thread-12] WARN com.alibaba.otter.canal.client.impl.ClusterCanalConnector - something goes wrong when getWithoutAck data from server:null com.alibaba.otter.canal.protocol.exception.CanalClientException: java.io.IOException: end of stream when reading header at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:291) ~[canal.client-1.0.26.4.jar!/:?] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:264) ~[canal.client-1.0.26.4.jar!/:?] at com.alibaba.otter.canal.client.impl.ClusterCanalConnector.getWithoutAck(ClusterCanalConnector.java:182) ~[canal.client-1.0.26.4.jar!/:?] at com.tops001.foundation.canal.service.CanalWorker.process(CanalWorker.java:120) ~[classes!/:?] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_171] Caused by: java.io.IOException: end of stream when reading header at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.read(SimpleCanalConnector.java:396) ~[canal.client-1.0.26.4.jar!/:?] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.readNextPacket(SimpleCanalConnector.java:384) ~[canal.client-1.0.26.4.jar!/:?] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.readNextPacket(SimpleCanalConnector.java:368) ~[canal.client-1.0.26.4.jar!/:?] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.receiveMessages(SimpleCanalConnector.java:296) ~[canal.client-1.0.26.4.jar!/:?] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:289) ~[canal.client-1.0.26.4.jar!/:?] ... 4 more 10:11:50.417 [Thread-12] INFO com.alibaba.otter.canal.client.impl.ClusterCanalConnector - restart the connector for next round retry. ``` You opened the table of the table The ability of tsdb is a timed task non-mainline affects the reason why the client reads failed.
666,Compiling problems yourself in the Mac environment Dependent on this file can not get http://code.alibabatech.com/mvn/releases/com/alibaba/fastsql/fastsql/2.0.0_preview_228/fastsql-2.0.0_preview_228.pom Trouble putting this dependency on a reliable source [ERROR] Failed to execute goal on project canal.parse: Could not resolve dependencies for project com.alibaba.otter:canal.parse:jar:1.0.26-SNAPSHOT: Failed to collect dependencies at com.alibaba.fastsql:fastsql:jar:2.0.0_preview_228: Failed to read artifact descriptor for com.alibaba.fastsql:fastsql:jar:2.0.0_preview_228: Could not transfer artifact com.alibaba.fastsql:fastsql:pom:2.0.0_preview_228 from/to alibaba (http://code.alibabatech.com/mvn/releases/): Connect to code.alibabatech.com:80 [code.alibabatech.com/119.38.217.15] failed: Operation timed out -> [Help 1] same here same Https github com alibaba canal releases tag canal 1 0 26 preview 3 The corresponding tar package has a corresponding jar Fastsql will also open source the strongest sql Parser tool mysql resolution compatibility is the best
665,Cancan Whether to support cascading replication rt Canal Server from mysql Slave node copy binlog Should be supported I will test it first. As long as mysql opens the log_slave_update feature
664,About two canal Server with the same instance of the question Two canal The server is configured with two instance example1 and example2 to start one of the canal Server At this time, the server can synchronize the data of the two libraries of example1 and example2. At this time, the second canal is started. Server At this time, the server cannot consume example1 and example2. At this time, the cluster does not make much sense. Can the canal do operations like rebalance, such as rebalance after canal Server1 and canal Server2 synchronizes the data of one library separately
663,1.0.26 com.alibaba.fastsql.sql.parser.ParserException: TODO pos 545 line 9 column 83 token NULL durid Has been manually upgraded to 1 1 9 Start or report an error ` 2018-05-29 17:29:36.484 [destination = devmysql4308 address = /192.168.100.4:3308 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : /* ApplicationName=IntelliJ IDEA 2017.2.5 */ CREATE TABLE best_sign_cont_task ( id INTEGER PRIMARY KEY NOT NULL COMMENT Primary key AUTO_INCREMENT docid VARCHAR(50) NOT NULL DEFAULT '' COMMENT The original id of the original contract pdfid VARCHAR(50) NOT NULL DEFAULT '' COMMENT The id of the contract pdf contract_id VARCHAR(50) NOT NULL DEFAULT '' COMMENT Sign the contract id finish TINYINT(1) NOT NULL DEFAULT 0 COMMENT 0 Party A has not signed 1 Party A has signed Indicates completion of the task remark VARCHAR(100) NOT NULL DEFAULT '' COMMENT Remarks sys_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP NOT NULL COMMENT system time )ENGINE=INNODB DEFAULT CHARSET=utf8 COMMENT Create a task record on the contract com.alibaba.fastsql.sql.parser.ParserException: TODO pos 545 line 9 column 83 token NULL at com.alibaba.fastsql.sql.parser.SQLExprParser.notRationalRest(SQLExprParser.java:2557) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] at com.alibaba.fastsql.sql.parser.SQLExprParser.relationalRest(SQLExprParser.java:2323) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] at com.alibaba.fastsql.sql.parser.SQLExprParser.exprRest(SQLExprParser.java:109) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] at com.alibaba.fastsql.sql.parser.SQLExprParser.expr(SQLExprParser.java:97) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] at com.alibaba.fastsql.sql.dialect.mysql.parser.MySqlExprParser.parseColumnRest(MySqlExprParser.java:462) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] at com.alibaba.fastsql.sql.parser.SQLExprParser.parseColumnRest(SQLExprParser.java:2801) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] at com.alibaba.fastsql.sql.dialect.mysql.parser.MySqlExprParser.parseColumnRest(MySqlExprParser.java:545) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] at com.alibaba.fastsql.sql.dialect.mysql.parser.MySqlExprParser.parseColumn(MySqlExprParser.java:455) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] at com.alibaba.fastsql.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:170) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] at com.alibaba.fastsql.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:239) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:165) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:76) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:469) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:331) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:71) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.applyHistoryOnMemory(DatabaseTableMeta.java:382) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:121) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171] ` I downloaded it manually. Fastsql_2 0 0_preview_186 is targeting the problem Problem sql ```sql CREATE TABLE corp_best_sign_info ( sys_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP not null COMMENT system time )ENGINE=INNODB DEFAULT CHARSET=utf8 COMMENT Enterprise opening and signing information form ``` Adjusted after passing sql ```sql CREATE TABLE corp_best_sign_info ( sys_time TIMESTAMP not null DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT system time )ENGINE=INNODB DEFAULT CHARSET=utf8 COMMENT Enterprise opening and signing information form ``` The difference is not null Wrote in question on update current_timestamp After analogy with the other sqlparse This grammar is OK, please see if you can fix it. Is indeed a problem
662,Current master branch SimpleCanalConnector connect 方法 logical error # The main method running SimpleCanalClientTest will be abnormal. `process error!java.lang.NullPointerException: null at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.writeWithHeader(SimpleCanalConnector.java:392) ~[classes/:na] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.writeWithHeader(SimpleCanalConnector.java:380) ~[classes/:na] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:292) ~[classes/:na] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:280) ~[classes/:na] at com.alibaba.otter.canal.example.AbstractCanalClientTest.process(AbstractCanalClientTest.java:125) ~[classes/:na] at com.alibaba.otter.canal.example.AbstractCanalClientTest$2.run(AbstractCanalClientTest.java:85) [classes/:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171]` Troubleshoot is a connection method of SimpleCanalConnector ``` public void connect() throws CanalClientException { if (connected) { return; } if (runningMonitor != null) { if (!runningMonitor.isStart()) { runningMonitor.start(); } } else { waitClientRunning(); if (!running) { return; } doConnect(); if (filter != null) { // If there is a conditional description, it is automatically switched based on the last conditional subscription once. subscribe(filter); } if (rollbackOnConnect) { rollback(); } } connected = true; } private void waitClientRunning() { try { if (zkClientx != null) { if (!connected) {// Connected not throw new CanalClientException("should connect first"); } running = true; mutex.get();// Blocking waiting } } catch (InterruptedException e) { Thread.currentThread().interrupt(); throw new CanalClientException(e); } } ``` Running is initialized to false and only two methods change his value to waitClientRunning with stopRunning() 。 The connect method calls waitClientRunning when zkClienttx == null So running is still false so it doesn&#39;t call doConnect so it doesn&#39;t assign a value to writableChannel and causes a null pointer exception in the exception stack. The problem is the https github com alibaba canal commit 4141049bb90ce6dc1ce05a58c95f2d974fdd5865 https github com alibaba canal commit 4141049bb90ce6dc1ce05a58c95f2d974fdd5865 @rewerma 1 Block two lines of code on the line First place public void connect() throws CanalClientException { if (connected) { return; } if (runningMonitor != null) { if (!runningMonitor.isStart()) { runningMonitor.start(); } } else { waitClientRunning(); // if (!running) { // return; // } doConnect(); if (filter != null) { // If there is a conditional description, it is automatically switched based on the last conditional subscription once. subscribe(filter); } if (rollbackOnConnect) { rollback(); } } connected = true; } Second place public void subscribe(String filter) throws CanalClientException { waitClientRunning(); // if (!running) { // return; // } try { writeWithHeader(Packet.newBuilder() .setType(PacketType.SUBSCRIPTION) .setBody(Sub.newBuilder() .setDestination(clientIdentity.getDestination()) .setClientId(String.valueOf(clientIdentity.getClientId())) .setFilter(filter != null ? filter : "") .build() .toByteString()) .build() .toByteArray()); // Packet p = Packet.parseFrom(readNextPacket()); Ack ack = Ack.parseFrom(p.getBody()); if (ack.getErrorCode() > 0) { throw new CanalClientException("failed to subscribe with reason: " + ack.getErrorMessage()); } clientIdentity.setFilter(filter); } catch (IOException e) { throw new CanalClientException(e); } } The reason above has answered zookeeper to determine that there is a problem with a single machine can have no zookeeper already fixed
661,After generating the data backlog, after synchronizing for a period of time, an error will be reported, and then the synchronization will be resumed from the starting point, resulting in the inability to continue the subsequent synchronization. Version canal canal 1 0 26 something goes wrong when acking data from server:null com.alibaba.otter.canal.protocol.exception.CanalClientException: java.io.IOException: Broken pipe at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.ack(SimpleCanalConnector.java:339) at com.alibaba.otter.canal.client.impl.ClusterCanalConnector.ack(ClusterCanalConnector.java:246) at ins.framework.mysqltoes.client.AbstractCanalClientTest.process(AbstractCanalClientTest.java:140) at ins.framework.mysqltoes.client.AbstractCanalClientTest$2.run(AbstractCanalClientTest.java:93) at java.lang.Thread.run(Thread.java:745) Caused by: java.io.IOException: Broken pipe at sun.nio.ch.FileDispatcherImpl.write0(Native Method) at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47) at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93) at sun.nio.ch.IOUtil.write(IOUtil.java:65) at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:471) at java.nio.channels.Channels.writeFullyImpl(Channels.java:78) at java.nio.channels.Channels.writeFully(Channels.java:98) at java.nio.channels.Channels.access$000(Channels.java:61) at java.nio.channels.Channels$1.write(Channels.java:174) at java.nio.channels.Channels$WritableByteChannelImpl.write(Channels.java:458) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.writeWithHeader(SimpleCanalConnector.java:382) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.writeWithHeader(SimpleCanalConnector.java:369) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.ack(SimpleCanalConnector.java:333) ... 4 common frames omitted restart the connector for next round retry. something goes wrong when getWithoutAck data from server:null com.alibaba.otter.canal.protocol.exception.CanalClientException: something goes wrong with reason: something goes wrong with channel:[id: 0x58c3e872 /10.10.56.23:50475 => /10.10.56.23:11111] exception=com.alibaba.otter.canal.server.exception.CanalServerException: ack error clientId:1001 batchId:926 is not exist please check at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.receiveMessages(SimpleCanalConnector.java:317) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:294) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:269) at com.alibaba.otter.canal.client.impl.ClusterCanalConnector.getWithoutAck(ClusterCanalConnector.java:177) at ins.framework.mysqltoes.client.AbstractCanalClientTest.process(AbstractCanalClientTest.java:126) at ins.framework.mysqltoes.client.AbstractCanalClientTest$2.run(AbstractCanalClientTest.java:93) at java.lang.Thread.run(Thread.java:745) restart the connector for next round retry. ------------------------------------------------------------------------------------------------------------- The server side of the canal generates the thread error log hs_err_pid12248 log to record part of the stack information but there are the following exceptions. Is there some code missing? ------------------------------------------------------------------------------------------------------------- Internal exceptions (10 events): Event: 13725.231 Thread 0x00007f65bc006800 Exception <a 'java/lang/ClassNotFoundException': com/alibaba/otter/canal/protocol/position/PositionBeanInfo> (0x0000000715146718) thrown at [/HUDSON/workspace/8-2-build-linux-amd64/jdk8u51/3951/hotspot/src/share/vm/classfile/systemDictionary.cpp line 210 Event: 13725.231 Thread 0x00007f65bc006800 Exception <a 'java/lang/ClassNotFoundException': com/alibaba/otter/canal/protocol/position/PositionCustomizer> (0x0000000715156278) thrown at [/HUDSON/workspace/8-2-build-linux-amd64/jdk8u51/3951/hotspot/src/share/vm/classfile/systemDictionary.cpp line 2 Event: 13725.231 Thread 0x00007f65bc006800 Exception <a 'java/lang/ClassNotFoundException': com/alibaba/otter/canal/protocol/position/TimePositionCustomizer> (0x0000000715168aa0) thrown at [/HUDSON/workspace/8-2-build-linux-amd64/jdk8u51/3951/hotspot/src/share/vm/classfile/systemDictionary.cpp li Event: 13725.232 Thread 0x00007f65bc006800 Exception <a 'java/lang/ClassNotFoundException': com/alibaba/otter/canal/protocol/position/EntryPositionCustomizer> (0x000000071517e530) thrown at [/HUDSON/workspace/8-2-build-linux-amd64/jdk8u51/3951/hotspot/src/share/vm/classfile/systemDictionary.cpp l Event: 13725.413 Thread 0x00007f659c1da000 Exception <a 'java/lang/ClassCastException': sun.misc.Cleaner cannot be cast to java.lang.Runnable> (0x00000007168aa620) thrown at [/HUDSON/workspace/8-2-build-linux-amd64/jdk8u51/3951/hotspot/src/share/vm/interpreter/interpreterRuntime.cpp line 381] Event: 13725.416 Thread 0x00007f659c1da000 Exception <a 'java/lang/NoClassDefFoundError': javassist/ClassPath> (0x00000007168c0840) thrown at [/HUDSON/workspace/8-2-build-linux-amd64/jdk8u51/3951/hotspot/src/share/vm/classfile/systemDictionary.cpp line 199] Event: 13730.219 Thread 0x00007f65a000a000 Implicit null exception at 0x00007f66653cf735 to 0x00007f66653cfa61 Event: 13743.542 Thread 0x00007f659c1da000 Exception <a 'java/lang/InterruptedException'> (0x0000000709c92318) thrown at [/HUDSON/workspace/8-2-build-linux-amd64/jdk8u51/3951/hotspot/src/share/vm/runtime/objectMonitor.cpp line 1683] Event: 13743.548 Thread 0x00007f6594003800 Implicit null exception at 0x00007f6665ecfa12 to 0x00007f6665ed0209 Event: 13743.558 Thread 0x00007f6594003800 Implicit null exception at 0x00007f6665b17426 to 0x00007f6665b19e6d You are openjdk ？ ack error clientId:1001 batchId:926 is not exist please check， Should be canal The server side has abnormally restarted the instance Checked the canalserver log found some unsupported format format warning but does not affect the synchronization but once the above exception occurs, the server will also report this exception
660,1 0 26 latest version something goes wrong with channel:******clientId:1001 batchId:50560 is not exist please check Found this morning The canal log file in the logs canal directory is nearly 200M. The exception is the following. However, the meta log below each instance directory is the normal record. The description of the consumption site is normal consumption, but why does this exception persist? 2018-05-29 09:48:25.896 [New I/O server worker #1-1] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400 Caused by : something goes wrong with channel:[id: 0x01f1bec3 /127.0.0.1:35010 => /127.0.0.1:11111] exception=com.alibaba.otter.canal.server.exception.CanalServerException: ack error clientId:1001 batchId:50560 is not exist please check 2018-05-29 09:48:25.907 [New I/O server worker #1-1] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x01f1bec3 /127.0.0.1:35010 :> /127.0.0.1:11111] exception=java.nio.channels.ClosedChannelException at org.jboss.netty.channel.socket.nio.NioWorker.cleanUpWriteBuffer(NioWorker.java:649) at org.jboss.netty.channel.socket.nio.NioWorker.writeFromUserCode(NioWorker.java:370) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:137) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) at org.jboss.netty.channel.Channels.write(Channels.java:611) at org.jboss.netty.channel.Channels.write(Channels.java:578) at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:28) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:144) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.cleanup(ReplayingDecoder.java:542) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.channelDisconnected(ReplayingDecoder.java:450) at org.jboss.netty.channel.Channels.fireChannelDisconnected(Channels.java:360) at org.jboss.netty.channel.socket.nio.NioWorker.close(NioWorker.java:599) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:119) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) at org.jboss.netty.channel.Channels.close(Channels.java:720) at org.jboss.netty.channel.AbstractChannel.close(AbstractChannel.java:208) at org.jboss.netty.channel.ChannelFutureListener$1.operationComplete(ChannelFutureListener.java:46) at org.jboss.netty.channel.DefaultChannelFuture.notifyListener(DefaultChannelFuture.java:381) at org.jboss.netty.channel.DefaultChannelFuture.addListener(DefaultChannelFuture.java:148) at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:30) at com.alibaba.otter.canal.server.netty.NettyUtils.error(NettyUtils.java:51) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:200) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:444) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261) at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:350) at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745) This is a bug, when can I update it? The server side must have generated an exception or restarted the instance client reference demo to do a retry @agapple doConnect() Method is to retry Take a look at the client demo @agapple I looked at the connector disconnect method in AbstractCanalClientTest and tried it, but the server is still the same exception. protected void process() { int batchSize = 5 * 1024; while (running) { try { MDC.put("destination" destination); connector.connect(); connector.subscribe(); waiting = false; while (running) { Message message = connector.getWithoutAck(batchSize); // Get the specified amount of data long batchId = message.getId(); int size = message.getEntries().size(); if (batchId == -1 || size == 0) { // try { // Thread.sleep(1000); // } catch (InterruptedException e) { // } } else { printSummary(message batchId size); printEntry(message.getEntries()); } connector.ack(batchId); // Submit confirmation // connector.rollback(batchId); // Processing failure Rollback data } } catch (Exception e) { logger.error("process error!" e); } finally { **connector.disconnect();** MDC.remove("destination"); } } } @agapple Instance will restart the data of the restarted production line synchronization and repeat the analysis sent to kafka Face The extractive system of the distributed system is very difficult, generally requires business considerations of idempotent
659,Initialize sql error when synchronizing 1.0.25 Default configuration in the version documentation H2 database ` 2018-05-28 14:36:38.957 [destination = myexample address = /172.16.9.11:3307 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `columns_priv` ( `Host` char(60) COLLATE utf8_bin NOT NULL DEFAULT '' `Db` char(64) COLLATE utf8_bin NOT NULL DEFAULT '' `User` char(16) COLLATE utf8_bin NOT NULL DEFAULT '' `Table_name` char(64) COLLATE utf8_bin NOT NULL DEFAULT '' `Column_name` char(64) COLLATE utf8_bin NOT NULL DEFAULT '' `Timestamp` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP `Column_priv` set('Select' 'Insert' 'Update' 'References') CHARACTER SET utf8 NOT NULL DEFAULT '' PRIMARY KEY (`Host` `Db` `User` `Table_name` `Column_name`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='Column privileges' com.alibaba.druid.sql.parser.ParserException: error pos 428 line 8 column 17 token SET at com.alibaba.druid.sql.parser.SQLExprParser.name(SQLExprParser.java:1334) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLExprParser.parseDataType(SQLExprParser.java:2310) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLExprParser.parseDataType(SQLExprParser.java:2301) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlExprParser.parseColumn(MySqlExprParser.java:493) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:115) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.25.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101] 2018-05-28 14:36:38.960 [destination = myexample address = /172.16.9.11:3307 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `db` ( `Host` char(60) COLLATE utf8_bin NOT NULL DEFAULT '' `Db` char(64) COLLATE utf8_bin NOT NULL DEFAULT '' `User` char(16) COLLATE utf8_bin NOT NULL DEFAULT '' `Select_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Insert_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Update_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Delete_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Create_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Drop_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Grant_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `References_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Index_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Alter_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Create_tmp_table_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Lock_tables_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Create_view_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Show_view_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Create_routine_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Alter_routine_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Execute_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Event_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Trigger_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' PRIMARY KEY (`Host` `Db` `User`) KEY `User` (`User`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='Database privileges' com.alibaba.druid.sql.parser.ParserException: syntax error error in :''Y') CHARACTER SET utf8 NOT NULL DE' expect RPAREN actual IDENTIFIER pos 225 line 5 column 31 token IDENTIFIER CHARACTER at com.alibaba.druid.sql.parser.SQLParser.printError(SQLParser.java:284) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLParser.accept(SQLParser.java:292) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:191) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.25.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101] 2018-05-28 14:36:38.961 [destination = myexample address = /172.16.9.11:3307 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `event` ( `db` char(64) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL DEFAULT '' `name` char(64) NOT NULL DEFAULT '' `body` longblob NOT NULL `definer` char(77) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL DEFAULT '' `execute_at` datetime DEFAULT NULL `interval_value` int(11) DEFAULT NULL `interval_field` enum('YEAR' 'QUARTER' 'MONTH' 'DAY' 'HOUR' 'MINUTE' 'WEEK' 'SECOND' 'MICROSECOND' 'YEAR_MONTH' 'DAY_HOUR' 'DAY_MINUTE' 'DAY_SECOND' 'HOUR_MINUTE' 'HOUR_SECOND' 'MINUTE_SECOND' 'DAY_MICROSECOND' 'HOUR_MICROSECOND' 'MINUTE_MICROSECOND' 'SECOND_MICROSECOND') DEFAULT NULL `created` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP `modified` timestamp NOT NULL DEFAULT '0000-00-00 00:00:00' `last_executed` datetime DEFAULT NULL `starts` datetime DEFAULT NULL `ends` datetime DEFAULT NULL `status` enum('ENABLED' 'DISABLED' 'SLAVESIDE_DISABLED') NOT NULL DEFAULT 'ENABLED' `on_completion` enum('DROP' 'PRESERVE') NOT NULL DEFAULT 'DROP' `sql_mode` set('REAL_AS_FLOAT' 'PIPES_AS_CONCAT' 'ANSI_QUOTES' 'IGNORE_SPACE' 'NOT_USED' 'ONLY_FULL_GROUP_BY' 'NO_UNSIGNED_SUBTRACTION' 'NO_DIR_IN_CREATE' 'POSTGRESQL' 'ORACLE' 'MSSQL' 'DB2' 'MAXDB' 'NO_KEY_OPTIONS' 'NO_TABLE_OPTIONS' 'NO_FIELD_OPTIONS' 'MYSQL323' 'MYSQL40' 'ANSI' 'NO_AUTO_VALUE_ON_ZERO' 'NO_BACKSLASH_ESCAPES' 'STRICT_TRANS_TABLES' 'STRICT_ALL_TABLES' 'NO_ZERO_IN_DATE' 'NO_ZERO_DATE' 'INVALID_DATES' 'ERROR_FOR_DIVISION_BY_ZERO' 'TRADITIONAL' 'NO_AUTO_CREATE_USER' 'HIGH_NOT_PRECEDENCE' 'NO_ENGINE_SUBSTITUTION' 'PAD_CHAR_TO_FULL_LENGTH') NOT NULL DEFAULT '' `comment` char(64) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL DEFAULT '' `originator` int(10) unsigned NOT NULL `time_zone` char(64) CHARACTER SET latin1 NOT NULL DEFAULT 'SYSTEM' `character_set_client` char(32) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL `collation_connection` char(32) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL `db_collation` char(32) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL `body_utf8` longblob PRIMARY KEY (`db` `name`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COMMENT='Events' com.alibaba.druid.sql.parser.ParserException: error pos 1035 line 16 column 14 token SET at com.alibaba.druid.sql.parser.SQLExprParser.name(SQLExprParser.java:1334) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLExprParser.parseDataType(SQLExprParser.java:2310) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLExprParser.parseDataType(SQLExprParser.java:2301) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlExprParser.parseColumn(MySqlExprParser.java:493) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:115) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.25.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101] 2018-05-28 14:36:38.964 [destination = myexample address = /172.16.9.11:3307 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `func` ( `name` char(64) COLLATE utf8_bin NOT NULL DEFAULT '' `ret` tinyint(1) NOT NULL DEFAULT '0' `dl` char(128) COLLATE utf8_bin NOT NULL DEFAULT '' `type` enum('function' 'aggregate') CHARACTER SET utf8 NOT NULL PRIMARY KEY (`name`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='User defined functions' com.alibaba.druid.sql.parser.ParserException: syntax error error in :'te') CHARACTER SET utf8 NOT NULL ' expect RPAREN actual IDENTIFIER pos 221 line 5 column 39 token IDENTIFIER CHARACTER at com.alibaba.druid.sql.parser.SQLParser.printError(SQLParser.java:284) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLParser.accept(SQLParser.java:292) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:191) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.25.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101] 2018-05-28 14:36:38.967 [destination = myexample address = /172.16.9.11:3307 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `proc` ( `db` char(64) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL DEFAULT '' `name` char(64) NOT NULL DEFAULT '' `type` enum('FUNCTION' 'PROCEDURE') NOT NULL `specific_name` char(64) NOT NULL DEFAULT '' `language` enum('SQL') NOT NULL DEFAULT 'SQL' `sql_data_access` enum('CONTAINS_SQL' 'NO_SQL' 'READS_SQL_DATA' 'MODIFIES_SQL_DATA') NOT NULL DEFAULT 'CONTAINS_SQL' `is_deterministic` enum('YES' 'NO') NOT NULL DEFAULT 'NO' `security_type` enum('INVOKER' 'DEFINER') NOT NULL DEFAULT 'DEFINER' `param_list` blob NOT NULL `returns` longblob NOT NULL `body` longblob NOT NULL `definer` char(77) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL DEFAULT '' `created` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP `modified` timestamp NOT NULL DEFAULT '0000-00-00 00:00:00' `sql_mode` set('REAL_AS_FLOAT' 'PIPES_AS_CONCAT' 'ANSI_QUOTES' 'IGNORE_SPACE' 'NOT_USED' 'ONLY_FULL_GROUP_BY' 'NO_UNSIGNED_SUBTRACTION' 'NO_DIR_IN_CREATE' 'POSTGRESQL' 'ORACLE' 'MSSQL' 'DB2' 'MAXDB' 'NO_KEY_OPTIONS' 'NO_TABLE_OPTIONS' 'NO_FIELD_OPTIONS' 'MYSQL323' 'MYSQL40' 'ANSI' 'NO_AUTO_VALUE_ON_ZERO' 'NO_BACKSLASH_ESCAPES' 'STRICT_TRANS_TABLES' 'STRICT_ALL_TABLES' 'NO_ZERO_IN_DATE' 'NO_ZERO_DATE' 'INVALID_DATES' 'ERROR_FOR_DIVISION_BY_ZERO' 'TRADITIONAL' 'NO_AUTO_CREATE_USER' 'HIGH_NOT_PRECEDENCE' 'NO_ENGINE_SUBSTITUTION' 'PAD_CHAR_TO_FULL_LENGTH') NOT NULL DEFAULT '' `comment` text CHARACTER SET utf8 COLLATE utf8_bin NOT NULL `character_set_client` char(32) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL `collation_connection` char(32) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL `db_collation` char(32) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL `body_utf8` longblob PRIMARY KEY (`db` `name` `type`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COMMENT='Stored Procedures' com.alibaba.druid.sql.parser.ParserException: error pos 864 line 16 column 14 token SET at com.alibaba.druid.sql.parser.SQLExprParser.name(SQLExprParser.java:1334) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLExprParser.parseDataType(SQLExprParser.java:2310) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLExprParser.parseDataType(SQLExprParser.java:2301) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlExprParser.parseColumn(MySqlExprParser.java:493) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:115) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.25.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101] 2018-05-28 14:36:38.968 [destination = myexample address = /172.16.9.11:3307 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `procs_priv` ( `Host` char(60) COLLATE utf8_bin NOT NULL DEFAULT '' `Db` char(64) COLLATE utf8_bin NOT NULL DEFAULT '' `User` char(16) COLLATE utf8_bin NOT NULL DEFAULT '' `Routine_name` char(64) CHARACTER SET utf8 NOT NULL DEFAULT '' `Routine_type` enum('FUNCTION' 'PROCEDURE') COLLATE utf8_bin NOT NULL `Grantor` char(77) COLLATE utf8_bin NOT NULL DEFAULT '' `Proc_priv` set('Execute' 'Alter Routine' 'Grant') CHARACTER SET utf8 NOT NULL DEFAULT '' `Timestamp` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP PRIMARY KEY (`Host` `Db` `User` `Routine_name` `Routine_type`) KEY `Grantor` (`Grantor`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='Procedure privileges' com.alibaba.druid.sql.parser.ParserException: syntax error error in :'RE') COLLATE utf8_bin NOT NULL `' expect RPAREN actual IDENTIFIER pos 313 line 6 column 47 token IDENTIFIER COLLATE at com.alibaba.druid.sql.parser.SQLParser.printError(SQLParser.java:284) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLParser.accept(SQLParser.java:292) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:191) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.25.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101] 2018-05-28 14:36:38.972 [destination = myexample address = /172.16.9.11:3307 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `tables_priv` ( `Host` char(60) COLLATE utf8_bin NOT NULL DEFAULT '' `Db` char(64) COLLATE utf8_bin NOT NULL DEFAULT '' `User` char(16) COLLATE utf8_bin NOT NULL DEFAULT '' `Table_name` char(64) COLLATE utf8_bin NOT NULL DEFAULT '' `Grantor` char(77) COLLATE utf8_bin NOT NULL DEFAULT '' `Timestamp` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP `Table_priv` set('Select' 'Insert' 'Update' 'Delete' 'Create' 'Drop' 'Grant' 'References' 'Index' 'Alter' 'Create View' 'Show view' 'Trigger') CHARACTER SET utf8 NOT NULL DEFAULT '' `Column_priv` set('Select' 'Insert' 'Update' 'References') CHARACTER SET utf8 NOT NULL DEFAULT '' PRIMARY KEY (`Host` `Db` `User` `Table_name`) KEY `Grantor` (`Grantor`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='Table privileges' com.alibaba.druid.sql.parser.ParserException: error pos 422 line 8 column 16 token SET at com.alibaba.druid.sql.parser.SQLExprParser.name(SQLExprParser.java:1334) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLExprParser.parseDataType(SQLExprParser.java:2310) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLExprParser.parseDataType(SQLExprParser.java:2301) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlExprParser.parseColumn(MySqlExprParser.java:493) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:115) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.25.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101] 2018-05-28 14:36:38.974 [destination = myexample address = /172.16.9.11:3307 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `user` ( `Host` char(60) COLLATE utf8_bin NOT NULL DEFAULT '' `User` char(16) COLLATE utf8_bin NOT NULL DEFAULT '' `Password` char(41) CHARACTER SET latin1 COLLATE latin1_bin NOT NULL DEFAULT '' `Select_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Insert_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Update_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Delete_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Create_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Drop_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Reload_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Shutdown_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Process_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `File_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Grant_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `References_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Index_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Alter_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Show_db_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Super_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Create_tmp_table_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Lock_tables_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Execute_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Repl_slave_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Repl_client_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Create_view_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Show_view_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Create_routine_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Alter_routine_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Create_user_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Event_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Trigger_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Create_tablespace_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `ssl_type` enum('' 'ANY' 'X509' 'SPECIFIED') CHARACTER SET utf8 NOT NULL DEFAULT '' `ssl_cipher` blob NOT NULL `x509_issuer` blob NOT NULL `x509_subject` blob NOT NULL `max_questions` int(11) unsigned NOT NULL DEFAULT '0' `max_updates` int(11) unsigned NOT NULL DEFAULT '0' `max_connections` int(11) unsigned NOT NULL DEFAULT '0' `max_user_connections` int(11) unsigned NOT NULL DEFAULT '0' `plugin` char(64) COLLATE utf8_bin DEFAULT '' `authentication_string` text COLLATE utf8_bin `password_expired` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' PRIMARY KEY (`Host` `User`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='Users and global privileges' com.alibaba.druid.sql.parser.ParserException: syntax error error in :''Y') CHARACTER SET utf8 NOT NULL DE' expect RPAREN actual IDENTIFIER pos 256 line 5 column 31 token IDENTIFIER CHARACTER at com.alibaba.druid.sql.parser.SQLParser.printError(SQLParser.java:284) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLParser.accept(SQLParser.java:292) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:191) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.25.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101] 2018-05-28 14:38:19.380 [destination = myexample address = /172.16.9.11:3307 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - pls submit github issue show create table ddl:CREATE TABLE `hierquerysellist` ( `fieldtype_id` bigint(20) NOT NULL `whereclause` longtext NOT NULL `filtervariants` char(1) NOT NULL UNIQUE KEY `fieldtype_id` (`fieldtype_id`) USING BTREE ) ENGINE=MyISAM DEFAULT CHARSET=utf8 compare failed . db : TableMeta [schema=bboss table=hierquerysellist fileds= FieldMeta [columnName=fieldtype_id columnType=bigint(20) defaultValue=null nullable=false key=true] FieldMeta [columnName=whereclause columnType=longtext defaultValue=null nullable=false key=false] FieldMeta [columnName=filtervariants columnType=char(1) defaultValue=null nullable=false key=false] ] mem : TableMeta [schema=bboss table=hierquerysellist fileds= FieldMeta [columnName=fieldtype_id columnType=bigint(20) defaultValue=null nullable=false key=false] FieldMeta [columnName=whereclause columnType=longtext defaultValue=null nullable=false key=false] FieldMeta [columnName=filtervariants columnType=char(1) defaultValue=null nullable=false key=false] ] 2018-05-28 14:38:19.590 [destination = myexample address = /172.16.9.11:3307 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - pls submit github issue show create table ddl:CREATE TABLE `hierquerysellist_fields` ( `fieldtype_id` bigint(20) NOT NULL `sequencenr` smallint(6) NOT NULL `fieldname` varchar(50) NOT NULL UNIQUE KEY `fieldtype_id` (`fieldtype_id` `sequencenr`) USING BTREE ) ENGINE=MyISAM DEFAULT CHARSET=utf8 compare failed . db : TableMeta [schema=bboss table=hierquerysellist_fields fileds= FieldMeta [columnName=fieldtype_id columnType=bigint(20) defaultValue=null nullable=false key=true] FieldMeta [columnName=sequencenr columnType=smallint(6) defaultValue=null nullable=false key=true] FieldMeta [columnName=fieldname columnType=varchar(50) defaultValue=null nullable=false key=false] ] mem : TableMeta [schema=bboss table=hierquerysellist_fields fileds= FieldMeta [columnName=fieldtype_id columnType=bigint(20) defaultValue=null nullable=false key=false] FieldMeta [columnName=sequencenr columnType=smallint(6) defaultValue=null nullable=false key=false] FieldMeta [columnName=fieldname columnType=varchar(50) defaultValue=null nullable=false key=false] ] 2018-05-28 14:38:20.005 [destination = myexample address = /172.16.9.11:3307 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - pls submit github issue show create table ddl:CREATE TABLE `linkquerysellist` ( `fieldtype_id` bigint(20) NOT NULL `whereclause` longtext NOT NULL `filtervariants` char(1) NOT NULL UNIQUE KEY `fieldtype_id` (`fieldtype_id`) USING BTREE ) ENGINE=MyISAM DEFAULT CHARSET=utf8 compare failed . db : TableMeta [schema=bboss table=linkquerysellist fileds= FieldMeta [columnName=fieldtype_id columnType=bigint(20) defaultValue=null nullable=false key=true] FieldMeta [columnName=whereclause columnType=longtext defaultValue=null nullable=false key=false] FieldMeta [columnName=filtervariants columnType=char(1) defaultValue=null nullable=false key=false] ] mem : TableMeta [schema=bboss table=linkquerysellist fileds= FieldMeta [columnName=fieldtype_id columnType=bigint(20) defaultValue=null nullable=false key=false] FieldMeta [columnName=whereclause columnType=longtext defaultValue=null nullable=false key=false] FieldMeta [columnName=filtervariants columnType=char(1) defaultValue=null nullable=false key=false] ] 2018-05-28 14:38:20.837 [destination = myexample address = /172.16.9.11:3307 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - pls submit github issue show create table ddl:CREATE TABLE `parentlinkedsellist` ( `fieldtype_id` bigint(20) NOT NULL `whereclause` longtext NOT NULL `filtervariants` char(1) NOT NULL `linkfield` varchar(50) NOT NULL UNIQUE KEY `fieldtype_id` (`fieldtype_id`) USING BTREE ) ENGINE=MyISAM DEFAULT CHARSET=utf8 compare failed . db : TableMeta [schema=bboss table=parentlinkedsellist fileds= FieldMeta [columnName=fieldtype_id columnType=bigint(20) defaultValue=null nullable=false key=true] FieldMeta [columnName=whereclause columnType=longtext defaultValue=null nullable=false key=false] FieldMeta [columnName=filtervariants columnType=char(1) defaultValue=null nullable=false key=false] FieldMeta [columnName=linkfield columnType=varchar(50) defaultValue=null nullable=false key=false] ] mem : TableMeta [schema=bboss table=parentlinkedsellist fileds= FieldMeta [columnName=fieldtype_id columnType=bigint(20) defaultValue=null nullable=false key=false] FieldMeta [columnName=whereclause columnType=longtext defaultValue=null nullable=false key=false] FieldMeta [columnName=filtervariants columnType=char(1) defaultValue=null nullable=false key=false] FieldMeta [columnName=linkfield columnType=varchar(50) defaultValue=null nullable=false key=false] ] 2018-05-28 14:38:21.359 [destination = myexample address = /172.16.9.11:3307 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - pls submit github issue show create table ddl:CREATE TABLE `querysellist` ( `fieldtype_id` bigint(20) NOT NULL `query` longtext NOT NULL `filtervariants` char(1) NOT NULL `sort_order` varchar(1) NOT NULL UNIQUE KEY `fieldtype_id` (`fieldtype_id`) USING BTREE ) ENGINE=MyISAM DEFAULT CHARSET=utf8 compare failed . db : TableMeta [schema=bboss table=querysellist fileds= FieldMeta [columnName=fieldtype_id columnType=bigint(20) defaultValue=null nullable=false key=true] FieldMeta [columnName=query columnType=longtext defaultValue=null nullable=false key=false] FieldMeta [columnName=filtervariants columnType=char(1) defaultValue=null nullable=false key=false] FieldMeta [columnName=sort_order columnType=varchar(1) defaultValue=null nullable=false key=false] ] mem : TableMeta [schema=bboss table=querysellist fileds= FieldMeta [columnName=fieldtype_id columnType=bigint(20) defaultValue=null nullable=false key=false] FieldMeta [columnName=query columnType=longtext defaultValue=null nullable=false key=false] FieldMeta [columnName=filtervariants columnType=char(1) defaultValue=null nullable=false key=false] FieldMeta [columnName=sort_order columnType=varchar(1) defaultValue=null nullable=false key=false] ] 2018-05-28 14:38:28.334 [destination = myexample address = /172.16.9.11:3307 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - compare failed check log 2018-05-28 14:38:28.356 [destination = myexample address = /172.16.9.11:3307 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /172.16.9.11:3307 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: can't find init table meta for myexample with position : EntryPosition[included=false journalName=mysql-bin.000001 position=1314 serverId=1 timestamp=1527489302000] 2018-05-28 14:38:28.359 [destination = myexample address = /172.16.9.11:3307 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:myexample[com.alibaba.otter.canal.parse.exception.CanalParseException: can't find init table meta for myexample with position : EntryPosition[included=false journalName=mysql-bin.000001 position=1314 serverId=1 timestamp=1527489302000] ] 2018-05-28 14:38:40.151 [destination = myexample address = /172.16.9.11:3307 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just show master status 2018-05-28 14:40:01.773 [destination = myexample address = /172.16.9.11:3307 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `columns_priv` ( `Host` char(60) COLLATE utf8_bin NOT NULL DEFAULT '' `Db` char(64) COLLATE utf8_bin NOT NULL DEFAULT '' `User` char(16) COLLATE utf8_bin NOT NULL DEFAULT '' `Table_name` char(64) COLLATE utf8_bin NOT NULL DEFAULT '' `Column_name` char(64) COLLATE utf8_bin NOT NULL DEFAULT '' `Timestamp` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP `Column_priv` set('Select' 'Insert' 'Update' 'References') CHARACTER SET utf8 NOT NULL DEFAULT '' PRIMARY KEY (`Host` `Db` `User` `Table_name` `Column_name`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='Column privileges' com.alibaba.druid.sql.parser.ParserException: error pos 428 line 8 column 17 token SET at com.alibaba.druid.sql.parser.SQLExprParser.name(SQLExprParser.java:1334) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLExprParser.parseDataType(SQLExprParser.java:2310) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLExprParser.parseDataType(SQLExprParser.java:2301) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlExprParser.parseColumn(MySqlExprParser.java:493) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:115) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.25.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101] 2018-05-28 14:40:01.775 [destination = myexample address = /172.16.9.11:3307 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `db` ( `Host` char(60) COLLATE utf8_bin NOT NULL DEFAULT '' `Db` char(64) COLLATE utf8_bin NOT NULL DEFAULT '' `User` char(16) COLLATE utf8_bin NOT NULL DEFAULT '' `Select_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Insert_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Update_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Delete_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Create_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Drop_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Grant_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `References_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Index_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Alter_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Create_tmp_table_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Lock_tables_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Create_view_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Show_view_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Create_routine_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Alter_routine_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Execute_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Event_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Trigger_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' PRIMARY KEY (`Host` `Db` `User`) KEY `User` (`User`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='Database privileges' com.alibaba.druid.sql.parser.ParserException: syntax error error in :''Y') CHARACTER SET utf8 NOT NULL DE' expect RPAREN actual IDENTIFIER pos 225 line 5 column 31 token IDENTIFIER CHARACTER at com.alibaba.druid.sql.parser.SQLParser.printError(SQLParser.java:284) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLParser.accept(SQLParser.java:292) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:191) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.25.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101] 2018-05-28 14:40:01.776 [destination = myexample address = /172.16.9.11:3307 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `event` ( `db` char(64) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL DEFAULT '' `name` char(64) NOT NULL DEFAULT '' `body` longblob NOT NULL `definer` char(77) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL DEFAULT '' `execute_at` datetime DEFAULT NULL `interval_value` int(11) DEFAULT NULL `interval_field` enum('YEAR' 'QUARTER' 'MONTH' 'DAY' 'HOUR' 'MINUTE' 'WEEK' 'SECOND' 'MICROSECOND' 'YEAR_MONTH' 'DAY_HOUR' 'DAY_MINUTE' 'DAY_SECOND' 'HOUR_MINUTE' 'HOUR_SECOND' 'MINUTE_SECOND' 'DAY_MICROSECOND' 'HOUR_MICROSECOND' 'MINUTE_MICROSECOND' 'SECOND_MICROSECOND') DEFAULT NULL `created` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP `modified` timestamp NOT NULL DEFAULT '0000-00-00 00:00:00' `last_executed` datetime DEFAULT NULL `starts` datetime DEFAULT NULL `ends` datetime DEFAULT NULL `status` enum('ENABLED' 'DISABLED' 'SLAVESIDE_DISABLED') NOT NULL DEFAULT 'ENABLED' `on_completion` enum('DROP' 'PRESERVE') NOT NULL DEFAULT 'DROP' `sql_mode` set('REAL_AS_FLOAT' 'PIPES_AS_CONCAT' 'ANSI_QUOTES' 'IGNORE_SPACE' 'NOT_USED' 'ONLY_FULL_GROUP_BY' 'NO_UNSIGNED_SUBTRACTION' 'NO_DIR_IN_CREATE' 'POSTGRESQL' 'ORACLE' 'MSSQL' 'DB2' 'MAXDB' 'NO_KEY_OPTIONS' 'NO_TABLE_OPTIONS' 'NO_FIELD_OPTIONS' 'MYSQL323' 'MYSQL40' 'ANSI' 'NO_AUTO_VALUE_ON_ZERO' 'NO_BACKSLASH_ESCAPES' 'STRICT_TRANS_TABLES' 'STRICT_ALL_TABLES' 'NO_ZERO_IN_DATE' 'NO_ZERO_DATE' 'INVALID_DATES' 'ERROR_FOR_DIVISION_BY_ZERO' 'TRADITIONAL' 'NO_AUTO_CREATE_USER' 'HIGH_NOT_PRECEDENCE' 'NO_ENGINE_SUBSTITUTION' 'PAD_CHAR_TO_FULL_LENGTH') NOT NULL DEFAULT '' `comment` char(64) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL DEFAULT '' `originator` int(10) unsigned NOT NULL `time_zone` char(64) CHARACTER SET latin1 NOT NULL DEFAULT 'SYSTEM' `character_set_client` char(32) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL `collation_connection` char(32) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL `db_collation` char(32) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL `body_utf8` longblob PRIMARY KEY (`db` `name`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COMMENT='Events' com.alibaba.druid.sql.parser.ParserException: error pos 1035 line 16 column 14 token SET at com.alibaba.druid.sql.parser.SQLExprParser.name(SQLExprParser.java:1334) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLExprParser.parseDataType(SQLExprParser.java:2310) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLExprParser.parseDataType(SQLExprParser.java:2301) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlExprParser.parseColumn(MySqlExprParser.java:493) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:115) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.25.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101] 2018-05-28 14:40:01.778 [destination = myexample address = /172.16.9.11:3307 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `func` ( `name` char(64) COLLATE utf8_bin NOT NULL DEFAULT '' `ret` tinyint(1) NOT NULL DEFAULT '0' `dl` char(128) COLLATE utf8_bin NOT NULL DEFAULT '' `type` enum('function' 'aggregate') CHARACTER SET utf8 NOT NULL PRIMARY KEY (`name`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='User defined functions' com.alibaba.druid.sql.parser.ParserException: syntax error error in :'te') CHARACTER SET utf8 NOT NULL ' expect RPAREN actual IDENTIFIER pos 221 line 5 column 39 token IDENTIFIER CHARACTER at com.alibaba.druid.sql.parser.SQLParser.printError(SQLParser.java:284) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLParser.accept(SQLParser.java:292) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:191) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.25.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101] 2018-05-28 14:40:01.780 [destination = myexample address = /172.16.9.11:3307 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `proc` ( `db` char(64) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL DEFAULT '' `name` char(64) NOT NULL DEFAULT '' `type` enum('FUNCTION' 'PROCEDURE') NOT NULL `specific_name` char(64) NOT NULL DEFAULT '' `language` enum('SQL') NOT NULL DEFAULT 'SQL' `sql_data_access` enum('CONTAINS_SQL' 'NO_SQL' 'READS_SQL_DATA' 'MODIFIES_SQL_DATA') NOT NULL DEFAULT 'CONTAINS_SQL' `is_deterministic` enum('YES' 'NO') NOT NULL DEFAULT 'NO' `security_type` enum('INVOKER' 'DEFINER') NOT NULL DEFAULT 'DEFINER' `param_list` blob NOT NULL `returns` longblob NOT NULL `body` longblob NOT NULL `definer` char(77) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL DEFAULT '' `created` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP `modified` timestamp NOT NULL DEFAULT '0000-00-00 00:00:00' `sql_mode` set('REAL_AS_FLOAT' 'PIPES_AS_CONCAT' 'ANSI_QUOTES' 'IGNORE_SPACE' 'NOT_USED' 'ONLY_FULL_GROUP_BY' 'NO_UNSIGNED_SUBTRACTION' 'NO_DIR_IN_CREATE' 'POSTGRESQL' 'ORACLE' 'MSSQL' 'DB2' 'MAXDB' 'NO_KEY_OPTIONS' 'NO_TABLE_OPTIONS' 'NO_FIELD_OPTIONS' 'MYSQL323' 'MYSQL40' 'ANSI' 'NO_AUTO_VALUE_ON_ZERO' 'NO_BACKSLASH_ESCAPES' 'STRICT_TRANS_TABLES' 'STRICT_ALL_TABLES' 'NO_ZERO_IN_DATE' 'NO_ZERO_DATE' 'INVALID_DATES' 'ERROR_FOR_DIVISION_BY_ZERO' 'TRADITIONAL' 'NO_AUTO_CREATE_USER' 'HIGH_NOT_PRECEDENCE' 'NO_ENGINE_SUBSTITUTION' 'PAD_CHAR_TO_FULL_LENGTH') NOT NULL DEFAULT '' `comment` text CHARACTER SET utf8 COLLATE utf8_bin NOT NULL `character_set_client` char(32) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL `collation_connection` char(32) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL `db_collation` char(32) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL `body_utf8` longblob PRIMARY KEY (`db` `name` `type`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COMMENT='Stored Procedures' com.alibaba.druid.sql.parser.ParserException: error pos 864 line 16 column 14 token SET at com.alibaba.druid.sql.parser.SQLExprParser.name(SQLExprParser.java:1334) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLExprParser.parseDataType(SQLExprParser.java:2310) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLExprParser.parseDataType(SQLExprParser.java:2301) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlExprParser.parseColumn(MySqlExprParser.java:493) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:115) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.25.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101] 2018-05-28 14:40:01.783 [destination = myexample address = /172.16.9.11:3307 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `procs_priv` ( `Host` char(60) COLLATE utf8_bin NOT NULL DEFAULT '' `Db` char(64) COLLATE utf8_bin NOT NULL DEFAULT '' `User` char(16) COLLATE utf8_bin NOT NULL DEFAULT '' `Routine_name` char(64) CHARACTER SET utf8 NOT NULL DEFAULT '' `Routine_type` enum('FUNCTION' 'PROCEDURE') COLLATE utf8_bin NOT NULL `Grantor` char(77) COLLATE utf8_bin NOT NULL DEFAULT '' `Proc_priv` set('Execute' 'Alter Routine' 'Grant') CHARACTER SET utf8 NOT NULL DEFAULT '' `Timestamp` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP PRIMARY KEY (`Host` `Db` `User` `Routine_name` `Routine_type`) KEY `Grantor` (`Grantor`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='Procedure privileges' com.alibaba.druid.sql.parser.ParserException: syntax error error in :'RE') COLLATE utf8_bin NOT NULL `' expect RPAREN actual IDENTIFIER pos 313 line 6 column 47 token IDENTIFIER COLLATE at com.alibaba.druid.sql.parser.SQLParser.printError(SQLParser.java:284) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLParser.accept(SQLParser.java:292) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:191) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.25.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101] 2018-05-28 14:40:01.789 [destination = myexample address = /172.16.9.11:3307 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTab` use mysql After configuration There are the following errors `2018-05-28 15:06:27.531 [destination = myexample address = /172.16.9.11:3307 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `user` ( `Host` char(60) COLLATE utf8_bin NOT NULL DEFAULT '' `User` char(16) COLLATE utf8_bin NOT NULL DEFAULT '' `Password` char(41) CHARACTER SET latin1 COLLATE latin1_bin NOT NULL DEFAULT '' `Select_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Insert_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Update_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Delete_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Create_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Drop_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Reload_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Shutdown_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Process_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `File_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Grant_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `References_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Index_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Alter_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Show_db_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Super_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Create_tmp_table_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Lock_tables_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Execute_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Repl_slave_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Repl_client_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Create_view_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Show_view_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Create_routine_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Alter_routine_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Create_user_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Event_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Trigger_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Create_tablespace_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `ssl_type` enum('' 'ANY' 'X509' 'SPECIFIED') CHARACTER SET utf8 NOT NULL DEFAULT '' `ssl_cipher` blob NOT NULL `x509_issuer` blob NOT NULL `x509_subject` blob NOT NULL `max_questions` int(11) unsigned NOT NULL DEFAULT '0' `max_updates` int(11) unsigned NOT NULL DEFAULT '0' `max_connections` int(11) unsigned NOT NULL DEFAULT '0' `max_user_connections` int(11) unsigned NOT NULL DEFAULT '0' `plugin` char(64) COLLATE utf8_bin DEFAULT '' `authentication_string` text COLLATE utf8_bin `password_expired` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' PRIMARY KEY (`Host` `User`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='Users and global privileges'` Try to change to version 26. This is a bug in version 25. I upgraded to 26 is also the problem, but I feel like this is a permission problem. I gave all the permissions to the canal user in mysql. No problem. GRANT ALL PRIVILEGES ON *.* TO 'canal'@'%' ; FLUSH PRIVILEGES; LS Correct Answer Version v1 0 25 Open After tsdb It’s good to have this error deleted.
658,Canal start error v1 0 25 2018-05-28 01:42:57.009 [main] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## the canal server is running now ...... 2018-05-28 01:42:57.876 [destination = example address = /127.0.0.1:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just show master status 2018-05-28 01:44:10.494 [destination = example address = /127.0.0.1:3306 EventParser] WARN com.taobao.tddl.dbsync.binlog.CharsetConversion - Unexpect mysql charset: 246 2018-05-28 01:44:10.495 [destination = example address = /127.0.0.1:3306 EventParser] WARN com.taobao.tddl.dbsync.binlog.CharsetConversion - Unexpect mysql charset: 246 2018-05-28 01:44:10.495 [destination = example address = /127.0.0.1:3306 EventParser] WARN com.taobao.tddl.dbsync.binlog.CharsetConversion - Unexpect mysql charset: 246 2018-05-28 01:44:10.495 [destination = example address = /127.0.0.1:3306 EventParser] WARN com.taobao.tddl.dbsync.binlog.LogEvent - unsupported character set in query log: This is a warning Not wrong @agapple @DevWithLin Two canal implementations in active/standby mode, whose slaveId is the same, only one job The active and standby modes should be different. 1 0 26 will randomly generate one by default.
657,Propose a suggestion about MemoryMetaManager optimization An exception was encountered when the project started the subscription. The CanalServerWithEmbedded embedded key configuration MetaMode MIXED & IndexMode MEMORY_META_FAILBACK stack output is as follows ``` dump address /xxx.xx.xx.xxx:3306 has an error retrying. caused by java.util.ConcurrentModificationException at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:901) at java.util.ArrayList$Itr.next(ArrayList.java:851) at com.alibaba.otter.canal.parse.index.MetaLogPositionManager.getLatestIndexBy(MetaLogPositionManager.java:56) at com.alibaba.otter.canal.parse.index.FailbackLogPositionManager.getLatestIndexBy(FailbackLogPositionManager.java:68) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPositionInternal(MysqlEventParser.java:567) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPosition(MysqlEventParser.java:509) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:167) at java.lang.Thread.run(Thread.java:745) ``` Look at the code to find that the Subscribe method in the CanalServerWithEmbedded call the MetaManager&#39;s subscribe method will eventually call the MemoryMetaManager&#39;s subscribe method. After getting the list corresponding to the destination in the destination, modify the list to add the element code to the list. ``` public synchronized void subscribe(ClientIdentity clientIdentity) throws CanalMetaManagerException { List<ClientIdentity> clientIdentitys = destinations.get(clientIdentity.getDestination()); if (clientIdentitys.contains(clientIdentity)) { clientIdentitys.remove(clientIdentity); } clientIdentitys.add(clientIdentity); } ``` There is also a method listAllSubscribeInfo in the MemoryMetaManager class that directly returns the list code corresponding to the key in the destinations. ``` public synchronized List<ClientIdentity> listAllSubscribeInfo(String destination) throws CanalMetaManagerException { return destinations.get(destination); } ``` The problem I encountered was that when the getLatestIndexBy method of the MetaLogPositionManager traversed the result of the metaManager listAllSubscribeInfo call, the business code thread called the subscribe method to modify the traversed list collection resulting in a ConcurrentModificationException. ``` public LogPosition getLatestIndexBy(String destination) { List<ClientIdentity> clientIdentities = metaManager.listAllSubscribeInfo(destination); LogPosition result = null; if (!CollectionUtils.isEmpty(clientIdentities)) { // Try to find a minimal logPosition for (ClientIdentity clientIdentity : clientIdentities) { When traversing here, the linked list is modified by other threads. LogPosition position = (LogPosition) metaManager.getCursor(clientIdentity); if (position == null) { continue; } if (result == null) { result = position; } else { result = CanalEventUtils.min(result position); } } } return result; } ``` Here I feel that the listAllSubscribeInfo method in the MemoryMetaManager has a relatively large hidden danger. Externally, the reference may be traversed or modified in different threads. If you copy a new list every time you return it, you don&#39;t know if it is too big for the loss of efficiency. If you don&#39;t want too many copies, you can return unmodifiableList to prevent other classes from modifying this collection. But this is still possible when other classes traverse this collection. I have this ConcurrentModificationException I want to ask you how you think about this issue. @agapple I also encountered this problem. I am subscribing to a destination scenario in multiple clients. ` 2018-05-29 15:02:37.929 ERROR [test-firehose-20] c.a.o.c.c.a.LogAlarmHandler:19 - destination:test-firehose-20[java.util.ConcurrentModificationException at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:901) at java.util.ArrayList$Itr.next(ArrayList.java:851) at com.alibaba.otter.canal.parse.index.MetaLogPositionManager.getLatestIndexBy(MetaLogPositionManager.java:56) at com.alibaba.otter.canal.parse.index.FailbackLogPositionManager.getLatestIndexBy(FailbackLogPositionManager.java:68) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPositionInternal(MysqlEventParser.java:405) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPosition(MysqlEventParser.java:347) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:164) at java.lang.Thread.run(Thread.java:748) ` listAllSubscribeInfo is modified to return a copy object
656,In the latest release version regex Filtering does not work canal-server Configured regex as follows canal.instance.dbUsername=canal canal.instance.dbPassword=canal canal.instance.defaultDatabaseName=EDU_DISCOUNT_PRICE_TEST canal.instance.connectionCharset=UTF-8 # table regex #canal.instance.filter.regex=.*\\..* canal.instance.filter.regex=EDU_DISCOUNT_PRICE_TEST\\..* # table black regex #canal.instance.filter.black.regex=ONLINE_EDU_TEST\\..* ################################################# Client Code connector.subscribe("EDU_DISCOUNT_PRICE_TEST.t_online_discount"); The table update information database is also configured with filtering but still has irrelevant libraries. The binlog format is ROW canal The positioning problem is really inconvenient. QQ group can&#39;t add in and ask for answers. `connector.subscribe("EDU_DISCOUNT_PRICE_TEST.t_online_discount"); ` Change to `connector.subscribe("EDU_DISCOUNT_PRICE_TEST\\.t_online_discount");` LS Correct Answer
655,SimpleCanalClientTest Even Canal Always reported NullPointerException error Follow QuickStart with ClientSample Built Canal Environment but run SimpleCanalClientTest Always report a NullPointerException Error repeatedly check the configuration port, etc. did not find any problems, I hope to give suggestions to check the error. The specific error message is as follows: 2018-05-26 19:53:29.679 [Thread-1] ERROR com.alibaba.otter.canal.example.AbstractCanalClientTest - process error! java.lang.NullPointerException: null at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.writeWithHeader(SimpleCanalConnector.java:392) ~[classes/:na] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.writeWithHeader(SimpleCanalConnector.java:380) ~[classes/:na] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:292) ~[classes/:na] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:280) ~[classes/:na] at com.alibaba.otter.canal.example.AbstractCanalClientTest.process(AbstractCanalClientTest.java:125) ~[classes/:na] The problem was found to be local 1 26 snapshot The version of the source code is not successfully converted to 1 25 Successful build After the problem disappears
654,The server always closes unexpectedly Version 1 0 22 ![image](https://user-images.githubusercontent.com/15358736/40471583-a9d97202-5f69-11e8-8c22-ae02a30da03a.png) Process terminated unexpectedly Is it because sql reported missed more? It is recommended to use version 1 0 26
653,rowChange eventType in ROW mode = 'query' show binlog events Results are as follows ![aaaaaaaaaa](https://user-images.githubusercontent.com/21019407/40354822-61a69268-5de7-11e8-9809-49405492b60f.jpg) The number of entries consumed by the client is 3, which are TRANSACTIONBEGIN ROWDATA and TRANSACTIONEND, where ROWDATA is parsed and rowChange is printed as follows ![bbbbbbbbbb](https://user-images.githubusercontent.com/21019407/40354993-e068b05e-5de7-11e8-9cd9-985081534f54.jpg) Corresponding binlog configuration ![ccccccccc](https://user-images.githubusercontent.com/21019407/40355107-26765a9c-5de8-11e8-9907-80a065b961f9.jpg) The client configuration error caused the canal server to ignore the binlog of the corresponding table.
652,parser Table in information_schema error 2018-05-22 15:12:39.374 [destination = example address = zhenghongchen.cn/60.205.220.19:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TEMPORARY TABLE `SYSTEM_VARIABLES` ( `VARIABLE_NAME` varchar(64) NOT NULL DEFAULT '' `SESSION_VALUE` varchar(2048) DEFAULT NULL `GLOBAL_VALUE` varchar(2048) DEFAULT NULL `GLOBAL_VALUE_ORIGIN` varchar(64) NOT NULL DEFAULT '' `DEFAULT_VALUE` varchar(2048) DEFAULT NULL `VARIABLE_SCOPE` varchar(64) NOT NULL DEFAULT '' `VARIABLE_TYPE` varchar(64) NOT NULL DEFAULT '' `VARIABLE_COMMENT` varchar(2048) NOT NULL DEFAULT '' `NUMERIC_MIN_VALUE` varchar(21) DEFAULT NULL `NUMERIC_MAX_VALUE` varchar(21) DEFAULT NULL `NUMERIC_BLOCK_SIZE` varchar(21) DEFAULT NULL `ENUM_VALUE_LIST` longtext DEFAULT NULL `READ_ONLY` varchar(3) NOT NULL DEFAULT '' `COMMAND_LINE_ARGUMENT` varchar(64) DEFAULT NULL ) ENGINE=Aria DEFAULT CHARSET=utf8 PAGE_CHECKSUM=0 com.alibaba.fastsql.sql.parser.ParserException: syntax error error in :'utf8 PAGE_CHECKSUM=0' expect IDENTIFIER actual IDENTIFIER pos 781 line 16 column 50 token IDENTIFIER PAGE_CHECKSUM at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:305) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:427) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:76) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:469) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:331) ~[fastsql-2.0.0_preview_186.jar:2.0.0_preview_186] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:71) ~[classes/:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [classes/:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [classes/:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:84) [classes/:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:173) [classes/:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_51] Found the reason is fastsql 2 0 0_preview_186 jar is not good for MariaDB&#39;s aria engine table support Version 1 0 26 SNAPSHOT
651,How to synchronize blob fields Blob field with column getValue is a string can not be converted to a byte. There is a column getValueBytes method seems to be unable to get his original byte value
650,Mysql5 7 new table will cause the canal server to hang ![qq 20180522085329](https://user-images.githubusercontent.com/30070755/40336666-a225b214-5d9d-11e8-9257-389d5d78cd0c.png) It is recommended to use version 1 0 26
649,Fastsql preview package can not be downloaded After compiling the latest master code, the local compilation fails. The fastsql preview package cannot be downloaded. The maven address is ``` <dependency> <groupId>com.alibaba.fastsql</groupId> <artifactId>fastsql</artifactId> <version>2.0.0_preview_228</version> </dependency> ```
648,TSDB show create Table does not filter View Show after opening TSDB create The situation that table can&#39;t find the table is a closer look because there are some views in our library. In fact, I don’t understand it because I want to use it. show create Table is used as before describe tablename Isn&#39;t the way very good? show create After the table fails, it will fall back to desc table name. Mainly to solve the unique Key acquisition show create Table can be correctly extracted for multiple unique keys
647,Several parameters in the rds_instance properties configuration file canal.instance.rds.open.accesskey= canal.instance.rds.open.secretkey= canal.instance.rds.instanceId= canal.instance.rds.startTime= canal.instance.rds.endTime= These parameters seem to have no meaning in the introduction. Trouble giving To set whether to read data from the location of a fixed binlog file or use canal.instance.master.journal.name= canal.instance.master.position= canal.instance.master.timestamp= Configuration, but delete after configuration meta.dat After it didn&#39;t take effect, I saw the spot in the meta dat or the site I didn&#39;t set myself.
646,a little improve about the safety of canal high available Zookeeper session timeout due to stalls such as gc Again initRunning fails even if other canal Server delay initRunning Need to stop subscribing to binglog @agapple We have had a brief discussion about this before. see #600 I did some optimization Please review @agapple Look at the trouble 3Q ^-^ @spccold Closed for 啥
645,a little improve about the safety of canal high available Zookeeper session timeout due to stalls such as gc Again initRunning fails even if other canal Server delay initRunning Need to stop subscribing to binglog @agapple We have had a brief discussion about this before. see #600 I did some optimization Please review [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=645) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=645) before we can accept your contribution.<br/><hr/>**wuwo** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=645) it.</sub> [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=645) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=645) before we can accept your contribution.<br/><hr/>**wuwo** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=645) it.</sub> see #646
644,A slave with the same server_uuid/server_id as this slave has connected to the master MANAGER NODE4 2 14 constantly appears the same server_uuid server_id during the run. I don&#39;t know how to eliminate this error. pid:31 nid:11 exception:canal:MiddleEast:java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = A slave with the same server_uuid/server_id as this slave has connected to the master; the first event 'mysql-bin.000011' at 77235 the last event read from './mysql-bin.000012' at 150 the last byte read from './mysql-bin.000012' at 150. at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:121) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) at java.lang.Thread.run(Thread.java:748) This problem is not reported by the auto cnf file that originally stored server_uuid and re-generated the new server_uuid by restarting the Mysql database. The same fault is reported after a few days. After trial, it is judged that if pipline turns on timeout or delayed alarm monitoring and these monitors have the automatic restart option enabled, then when the monitor thinks that the timeout will start a new synchronization process, the remote node is actually only because the network speed is too slow. The corresponding time did not really hang, so opening a new process led to a duplicate slave in the request master appeared A slave with the same server_uuid/server_id as this slave has connected to the The master&#39;s prompt currently only retains the abnormal monitoring on the monitoring. The rest of the monitoring operation is still normal. Solve the problem just like the new canal 1 0 26 will use the practice of random uuid to circumvent
643,canal server log Show parse error Mysql for the company&#39;s internal customized version corresponding to mysql official may be 5 5 Is it feasible to modify the parsing rules here? If you can, can provide a learning path to customize the parser here? Thanks .mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `procs_priv` ( `Host` char(60) COLLATE utf8_bin NOT NULL DEFAULT '' `Db` char(64) COLLATE utf8_bin NOT NULL DEFAULT '' `User` char(16) COLLATE utf8_bin NOT NULL DEFAULT '' `Routine_name` char(64) CHARACTER SET utf8 NOT NULL DEFAULT '' `Routine_type` enum('FUNCTION' 'PROCEDURE') COLLATE utf8_bin NOT NULL `Grantor` char(77) COLLATE utf8_bin NOT NULL DEFAULT '' `Proc_priv` set('Execute' 'Alter Routine' 'Grant') CHARACTER SET utf8 NOT NULL DEFAULT '' `Timestamp` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP PRIMARY KEY (`Host` `Db` `User` `Routine_name` `Routine_type`) KEY `Grantor` (`Grantor`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='Procedure privileges' com.alibaba.druid.sql.parser.ParserException: syntax error error in :'RE') COLLATE utf8_bin NOT NULL `' expect RPAREN actual IDENTIFIER pos 313 line 6 column 47 token IDENTIFIER COLLATE at com.alibaba.druid.sql.parser.SQLParser.printError(SQLParser.java:284) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLParser.accept(SQLParser.java:292) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:191) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.25.jar:na] Has been fixed with the latest version of canal
642,WritableByteChannel channel Is null Use version 1 0 26 CanalConnector connector = CanalConnectors.newSingleConnector(new InetSocketAddress(ip 11111) destination "" ""); 1 Create a CanalConnector Discover the channel after the object Don&#39;t know what the problem is for null 2 mysql account is also generated directly according to the instructions of the tutorial 3 server also started start cmd : java -Xms128m -Xmx512m -XX:PermSize=128m -Djava.awt.headless=true -Djava.net.preferIPv4Stack=true -Dapplication.codeset=UTF-8 -Dfile.encoding=UTF-8 -server -Xdebug -Xnoagent -Djava.compiler=NONE -Xrunjdwp:transport=dt_socket address=9099 server=y suspend=n -DappName=otter-canal -Dlogback.configurationFile="E:\canal-dep\bin\\..\conf\logback.xml" -Dcanal.conf="E:\canal-dep\bin\\..\conf\canal.properties" -classpath "E:\canal-dep\bin\\..\conf\..\lib\*;E:\canal-dep\bin\\..\conf" java -Xms128m -Xmx512m -XX:PermSize=128m -Djava.awt.headless=true -Djava.net.preferIPv4Stack=true -Dapplication.codeset=UTF-8 -Dfile.encoding=UTF-8 -server -Xdebug -Xnoagent -Djava.compiler=NONE -Xrunjdwp:transport=dt_socket address=9099 server=y suspend=n -DappName=otter-canal -Dlogback.configurationFile="E:\canal-dep\bin\\..\conf\logback.xml" -Dcanal.conf="E:\canal-dep\bin\\..\conf\canal.properties" -classpath "E:\canal-dep\bin\\..\conf\..\lib\*;E:\canal-dep\bin\\..\conf" com.alibaba.otter.canal.deployer.CanalLauncher Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0 Listening for transport dt_socket at address: 9099 2018-05-17 15:17:09.947 [main] WARN o.s.beans.GenericTypeAwarePropertyDescriptor - Invalid JavaBean property 'connectionCharset' being accessed! Ambiguous write methods found next to actually used [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.nio.charset.Charset)]: [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.lang.String)] 2018-05-17 15:17:10.143 [main] ERROR com.alibaba.druid.pool.DruidDataSource - testWhileIdle is true validationQuery not set 2018-05-17 15:17:10.427 [destination = example address = /192.168.2.165:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just show master status 4. Run canal example 1 0 26 SNAPSHOT Flashback Self-compiled package Already fixed, you can re-download 1 0 26 alpha 3
641,Table structure parsing failed unknow column tag: canal-1.0.26-preview-2 There is a column name conf_key in the table. 2018-05-17 13:28:36.412 [destination = xxxxx address = xxxxx EventParser] ERROR c.a.otter.canal.p arse.inbound.mysql.MysqlEventParser - dump address /10.4.217.125:5002 has an error retrying. caused by java.lang.RuntimeException: unknow column : `conf_key`(8) at com.alibaba.otter.canal.parse.inbound.TableMeta.getFieldMetaByName(TableMeta.java:74) ~[canal.parse-1.0.26-SN APSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.processTableElement(MemoryTableMeta.java:227 ) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.parse(MemoryTableMeta.java:153) ~[canal.pars e-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.find(MemoryTableMeta.java:108) ~[canal.parse -1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.compareTableMetaDbAndMemory(DatabaseTableM eta.java:289) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.applySnapshotToDB(DatabaseTableMeta.java:2 51) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:129) ~[can al.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParse r.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) ~[canal.parse-1 .0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_45] ![image](https://user-images.githubusercontent.com/1378499/40158776-dc4bd130-59d8-11e8-9f1c-7c82d3d4774a.png) And 1 0 25 Compare MemoryTableMeta If you delete these lines, you won&#39;t go wrong.
640,Channal often closes Hello, I have run this for a few hours, and I have configured 9 under confi. instance Then some instances will report the following log. The binlog file change is also not available. This problem is often encountered. The code is always getting the status of the data. 2018-05-16 16:07:37.638 [New I/O server worker #1-7] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x58f255d8 /192.168.47.232:59420 :> /192.16 8.47.243:11111] exception=java.nio.channels.ClosedChannelException at org.jboss.netty.channel.socket.nio.NioWorker.cleanUpWriteBuffer(NioWorker.java:643) at org.jboss.netty.channel.socket.nio.NioWorker.writeFromUserCode(NioWorker.java:370) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:137) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) at org.jboss.netty.channel.Channels.write(Channels.java:611) at org.jboss.netty.channel.Channels.write(Channels.java:578) at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:28) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:144) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:275) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndFireMessageReceived(ReplayingDecoder.java:525) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:506) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.cleanup(ReplayingDecoder.java:541) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.channelDisconnected(ReplayingDecoder.java:449) at org.jboss.netty.channel.Channels.fireChannelDisconnected(Channels.java:360) at org.jboss.netty.channel.socket.nio.NioWorker.close(NioWorker.java:593) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:119) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) at org.jboss.netty.channel.Channels.close(Channels.java:720) at org.jboss.netty.channel.AbstractChannel.close(AbstractChannel.java:200) at org.jboss.netty.channel.ChannelFutureListener$1.operationComplete(ChannelFutureListener.java:46) at org.jboss.netty.channel.DefaultChannelFuture.notifyListener(DefaultChannelFuture.java:381) at org.jboss.netty.channel.DefaultChannelFuture.addListener(DefaultChannelFuture.java:148) at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:30) at com.alibaba.otter.canal.server.netty.NettyUtils.error(NettyUtils.java:51) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:200) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:275) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndFireMessageReceived(ReplayingDecoder.java:525) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:506) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:443) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261) at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:349) at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:280) at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:200) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603) at java.lang.Thread.run(Thread.java:722) I have also appeared in this type of you something goes wrong with Channel error, as long as the client has been connected to the server, it&#39;s okay. I tested the client and ran for a day and night. It didn&#39;t break, but once the client didn&#39;t connect to the canal server for more than a certain period of time, it didn&#39;t know how long it would connect to the server again. The following error will occur 2018-05-15 15:13:52.219 [New I/O server worker #1-2] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x24e1e7e8 /192.168.2.128:55307 => /192.168.1.38:11111 ] exception=java.io .IOException: Connection reset by peer at sun.nio.ch .FileDispatcherImpl.read0(Native Method) at sun.nio.ch .SocketDispatcher.read(SocketDispatcher.java:39) at sun.nio.ch .IOUtil.readIntoNativeBuffer(IOUtil.java:223) at sun.nio.ch .IOUtil.read(IOUtil.java:192) at sun.nio.ch .SocketChannelImpl.read(SocketChannelImpl.java:379) at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:322) at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745) Please explain the version of the canal Thank you for answering. I used 1 0 25 @DevWithLin You change to 1 0 26 try Can explain what caused the reason? @DevWithLin The 25 version uses nio to do binlog reading is not stable, it is not recommended. Netty has idle timeout close mechanism. The client can set the connector when creating the connector. If you do not set the server to default to 5 minutes, the channel will be disconnected. If you get the data again, you will need to reconnect the connector. com.alibaba.otter.canal.server.netty.handler.ClientAuthenticationHandler IdleStateHandler idleStateHandler = new IdleStateHandler(NettyUtils.hashedWheelTimer readTimeout writeTimeout 0 TimeUnit.MILLISECONDS); ctx.getPipeline().addBefore(SessionHandler.class.getName() IdleStateHandler.class.getName() idleStateHandler); IdleStateAwareChannelHandler idleStateAwareChannelHandler = new IdleStateAwareChannelHandler() { public void channelIdle(ChannelHandlerContext ctx IdleStateEvent e) throws Exception { logger.warn("channel:{} idle timeout exceeds close channel to save server resources..." ctx.getChannel()); ctx.getChannel().close(); } }; ctx.getPipeline().addBefore(SessionHandler.class.getName() IdleStateAwareChannelHandler.class.getName() idleStateAwareChannelHandler); @agapple The 26 version will restart the instance from time to time. I have a canal service about 20 instances. Is there a limit? There is no limit on the number of instances. Is scan true turned on? Find the changed file. The 25 version uses nio to do binlog reading is not stable, it is not recommended. @agapple Then which version do you recommend to read the binlog log stably? 1.0.26 1 0 26 will also appear from time to time Connection reset by The client at which the binlog change can be captured is the official example. Fa34eb0e59f257574996cb03da58cd2e8353e94d Adjusted the default idle link timeout before only 1 minute idle timeout adjusted to 1 hour ![image](https://user-images.githubusercontent.com/834743/41271502-9e424160-6e43-11e8-984b-29f03d187889.png) Everyone has a fine management of overtime, and can only adjust to large-scale management. The channel I used in the 1 0 25 version is also turned off once it is turned off. The locus will no longer move but the meta dat is cleared. Restart is ok again. If the channel is closed due to timeout, the eventParser will re-establish connection with mysql. It should be said that the location should continue to move. @fangchunsheng Your site may be another problem 1 0 25 and mysql interact with netty Nio mode changed to bio in version 26 can try 1 0 26
639,Exception fetch failed by table Meta version 1 0 26 tsdb enable true The original sql is on the table card_record Performed two alter operations alter table card_record modify column customization_id bigint unsigned NOT NULL COMMENT Custom id | see details alter table card_record modify column upgraded_customization_id bigint unsigned NOT NULL COMMENT Customized id after upgrade | see details Online one alter operation step is 1. DROP TABLE IF EXISTS `_card_record_gho` 2. DROP TABLE IF EXISTS `_card_record_del` 3. create table `yushitai_test`.`_card_record_gho` like `yushitai_test`.`card_record` 4. alter table `yushitai_test`.`_card_record_gho` modify column customization_id bigint unsigned NOT NULL COMMENT 'ŚģöŚą∂id' 5. insert Data to _card_record_gho 6. rename table `yushitai_test`.`card_record` to `yushitai_test`.`_card_record_del` `yushitai_test`.`_card_record_gho` to `yushitai_test`.`card_record` Manually set the site to the second alter operation time before re-consuming when performing the second alter When the operation reaches step 5, an exception occurs. fetch failed by table meta:`yushitai_test`.`_card_record_gho` 2018-05-14 15:32:44.078 [destination = yushitai_test address = /10.32.200.228:5002 EventParser] WARN c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : DROP TABLE IF EXISTS `_card_record_gho` /* generated by server */ 2018-05-14 15:32:44.078 [destination = yushitai_test address = /10.32.200.228:5002 EventParser] WARN c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : DROP TABLE IF EXISTS `_card_record_del` /* generated by server */ 2018-05-14 15:32:44.078 [destination = yushitai_test address = /10.32.200.228:5002 EventParser] WARN c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : DROP TABLE IF EXISTS `_card_record_ghc` /* generated by server */ 2018-05-14 15:32:44.079 [destination = yushitai_test address = /10.32.200.228:5002 EventParser] WARN c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : create /* gh-ost */ table `yushitai_test`.`_card_record_ghc` ( id bigint auto_increment last_update timestamp not null DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP hint varchar(64) charset ascii not null value varchar(255) charset ascii not null primary key(id) unique key hint_uidx(hint) ) auto_increment=256 2018-05-14 15:32:44.079 [destination = yushitai_test address = /10.32.200.228:5002 EventParser] WARN c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : create /* gh-ost */ table `yushitai_test`.`_card_record_gho` like `yushitai_test`.`card_record` 2018-05-14 15:32:44.080 [destination = yushitai_test address = /10.32.200.228:5002 EventParser] WARN c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : alter /* gh-ost */ table `yushitai_test`.`_card_record_gho` modify column customization_id bigint unsigned NOT NULL COMMENT 'ŚģöŚą∂id' 2018-05-14 15:32:44.085 [destination = yushitai_test address = /10.32.200.228:5002 EventParser] WARN c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : create /* gh-ost */ table `yushitai_test`.`_card_record_del` ( id int auto_increment primary key ) engine=InnoDB comment='ghost-cut-over-sentry' 2018-05-14 15:32:44.090 [destination = yushitai_test address = /10.32.200.228:5002 EventParser] WARN c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : DROP TABLE IF EXISTS `_card_record_del` /* generated by server */ 2018-05-14 15:32:44.090 [destination = yushitai_test address = /10.32.200.228:5002 EventParser] WARN c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : rename /* gh-ost */ table `yushitai_test`.`card_record` to `yushitai_test`.`_card_record_del` `yushitai_test`.`_card_record_gho` to `yushitai_test`.`card_record` 2018-05-14 15:32:44.091 [destination = yushitai_test address = /10.32.200.228:5002 EventParser] WARN c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : DROP TABLE IF EXISTS `_card_record_ghc` /* generated by server */ 2018-05-14 15:32:44.104 [destination = yushitai_test address = /10.32.200.228:5002 EventParser] WARN c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : rename table `_card_record_del` to _card_record_del_bak20180508125310 2018-05-14 15:32:45.093 [destination = yushitai_test address = /10.32.200.228:5002 EventParser] WARN c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : DROP TABLE IF EXISTS `_card_record_gho` /* generated by server */ 2018-05-14 15:32:45.094 [destination = yushitai_test address = /10.32.200.228:5002 EventParser] WARN c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : DROP TABLE IF EXISTS `_card_record_del` /* generated by server */ 2018-05-14 15:32:45.094 [destination = yushitai_test address = /10.32.200.228:5002 EventParser] WARN c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : DROP TABLE IF EXISTS `_card_record_ghc` /* generated by server */ 2018-05-14 15:32:45.096 [destination = yushitai_test address = /10.32.200.228:5002 EventParser] WARN c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : create /* gh-ost */ table `yushitai_test`.`_card_record_ghc` ( id bigint auto_increment last_update timestamp not null DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP hint varchar(64) charset ascii not null value varchar(255) charset ascii not null primary key(id) unique key hint_uidx(hint) ) auto_increment=256 2018-05-14 15:32:45.096 [destination = yushitai_test address = /10.32.200.228:5002 EventParser] WARN c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : create /* gh-ost */ table `yushitai_test`.`_card_record_gho` like `yushitai_test`.`card_record` 2018-05-14 15:32:45.097 [destination = yushitai_test address = /10.32.200.228:5002 EventParser] WARN c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : alter /* gh-ost */ table `yushitai_test`.`_card_record_gho` modify column upgraded_customization_id bigint unsigned NOT NULL COMMENT 'ŚćáÁļßŚźéŚģöŚą∂id' 2018-05-14 15:32:45.103 [destination = yushitai_test address = /10.32.200.228:5002 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - ERROR ## parse this event has an error last position : [EntryPosition[included=false journalName=mysql-bin.000056 position=286998741 serverId=32114196 timestamp=1525755246000]] com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: fetch failed by table meta:`yushitai_test`.`_card_record_gho` Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: fetch failed by table meta:`yushitai_test`.`_card_record_gho` Caused by: java.io.IOException: ErrorPacket [errorNumber=1146 fieldCount=-1 message=Table 'yushitai_test._card_record_gho' doesn't exist sqlState=42S02 sqlStateMarker=#] with command: show create table `yushitai_test`.`_card_record_gho` at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.query(MysqlQueryExecutor.java:61) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:94) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:167) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:759) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:428) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:114) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:66) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:337) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:184) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:152) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_45] Positioning as a ddl parsing bug
638,Why does address store hostname instead of ip { "@type": "com.alibaba.otter.canal.protocol.position.LogPosition" "identity": { "slaveId": -1 "sourceAddress": { "address": "DESKTOP-1RFA2LT" "port": 3306 } } "postion": { "included": false "journalName": "mysql-bin.000011" "position": 3382 "serverId": 1 "timestamp": 1526277481000 } } The result after the default serialization has no special treatment
637,canal1.0.22 Startup problem 2018-05-11 00:00:24.243 [destination = yuntu-management address = rm-wz91b2j3ypnw1fbr6o.mysql.rds.aliyuncs.com/112.74.151.78:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - disconnect address rm-wz91b2j3ypnw1fbr6o.mysql.rds.aliyuncs.com/112.74.151.78:3306 has an error retrying. caused by java.io.IOException: KILL DUMP 470730777 failure:java.io.IOException: ErrorPacket [errorNumber=1094 fieldCount=-1 message=Unknown thread id: 470730777 sqlState=HY000 sqlStateMarker=#] with command: KILL CONNECTION 470730777 at com.alibaba.otter.canal.parse.driver.mysql.MysqlUpdateExecutor.update(MysqlUpdateExecutor.java:49) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.disconnect(MysqlConnector.java:104) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.disconnect(MysqlConnection.java:60) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:242) at java.lang.Thread.run(Thread.java:745) Try the latest version 1 0 26
636,canal File store Or mix store When is there a plan to develop? Such as the title No data can be written to kafka for the time being. @agapple Are you still hesitating to write kafka or rocketmq these days? I think we can use our company&#39;s rocketmq. @agapple Hello, I would like to ask us to study the data pipeline service. It is not very clear that after binlog analysis, it is better to write to mq. This can provide subscription service through mq. This is a relatively simple way for downstream clients and pipeline services. Decoupling But canal itself implements a complex subscription api similar to mq. Although there is a java client sdk provided, but go rust these are not inconvenient to use. So there are a few questions I would like to ask 1. Canal&#39;s own implementation of the subscription api is based on what considerations are considering some potential pits of mq such as message size limit atleast Once and other factors? 2. Canal&#39;s subscription api though provided for client The mechanism of ack should be to try to let the client not receive duplicate messages. Suppose the client acks every time a client processes a message. But the ack site has persistence and disaster tolerance. Leader switch client Will not receive a message repeatedly OR It is still possible to receive duplicate messages 3. If the client is likely to receive duplicate messages, it is recommended that the client do idempotent processing. 4. How to deal with large transactions such as a few G size canal api? hello，anybody here？ The server client structure of the canal is mainly based on the operation and maintenance management. Before thinking, there is an idea to build a set of kafka capabilities. However, considering the maturity proposal, it is directly stored in the client and then stored in the MQ.
634,How Canal guarantees consistent transactions under high concurrency Hi， I would like to ask how the Canal is implemented in the following situations. Transaction A Start first Update A field is A Transaction B After launching Update The above field is B But the A transaction is committed after the B transaction How does Canal guarantee that the value is updated correctly? Canal is only responsible for parsing binlog and you first understand what binlog is.
633,Binlog site advance ddl operation lost canal Version 1 0 25 Manually set the timestamp of the cursor to T1 current time is t2 t2 > t1 If t1 t2 produces ddl within this time Operation binlog The binlog ddl operation is lost when the site is moved forward to t1. canal server log as follows canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : CREATE TABLE canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : alter table screen_fee add This is not a ddl loss but a return to T1 time after re-parsing found at t1 -> In d2, ddl has been added to tsdb and it has been added.
631,MySQL driver package supports Boolean type canal does not support boolean type conversion The length of tinyint in the MySQL driver package is 1 for sqlType 7 Convert to Java Boolean Type Reference MySQL Driver Source ![image](https://user-images.githubusercontent.com/9798724/39748475-bff67406-52e2-11e8-9233-21df121524b9.png) And Canal has a length of 1 for tinyint, which corresponds to a boolean type that sqlType 6 cannot handle as Java. There is no special treatment for tinyint 1 that is currently unified according to the tinyint type 6 Is there any plan support? Consider submitting a PR to me https://github.com/alibaba/canal/pull/682 @agapple please assign the issue to me :)
630,Same server Unable to open two client subscriptions Such as the title Start a server Then on the same client Subscribe with two threads Report batchid does not exist Because the subscriber must be unique Or because of a consistent subscription ID An instance can only be subscribed successfully at the same time. Other clients will block standby and wait for the working client to release the subscription before the subscription is successful. LS Correct Answer
629,How to configure to subscribe to only specified libraries Not all DBs Such as the title Can be configured with canal instance filter regex 考 Admin AdminGuide https github com alibaba canal wiki AdminGuide solved Covered by the client
628,DML of rowChage.getSql() Can&#39;t get sql information DML of rowChage.getSql() Can&#39;t get sql information @rewerma The getSql method is to get the DDL statement. LS Correct Answer Then DML can&#39;t get SQL. Maxwell is able to get sql and want to join this function. You have to understand what event event it gets. sql 5 6 has a rowsQueryLogEvent with sql text corresponding to DML
627,canal Server will often hang and the process does not exit canal The server will often hang and the process does not exit. The server is not monitored. Is it hanged? It is written that the monitoring process script is hung up and restarted. But there is nothing to delete the meta dat every time you restart. Otherwise report Thread ID does not exist Hope to be able to do humanity Add a bit on the Amazon rds to use this domain name to access the IP address of the regular domain name will automatically change when canal Server is not parsed in the ip log has been reported error only restart canal Server can solve maintenance is very troublesome, can add automatic reconnection function There has always been an automatic retry mechanism suspected to be jvm dns The cache causes the ip to resolve to the same ip. You can try to close the jvm. dns cache
626,XA supporting MySQL5 7 Binlog event Support MySQL XA capabilities after 5 7 https mysqlserverteam com improvements to xa support in mysql 5 7 ` TRANSACTION_CONTEXT_EVENT= 36 VIEW_CHANGE_EVENT= 37 /* Prepared XA transaction terminal event similar to Xid */ XA_PREPARE_LOG_EVENT= 38 ` > | mysql-bin.000011 | 13688 | Gtid | 1 | 13753 | SET @@SESSION.GTID_NEXT= 'f1ceb61a-a5d5-11e7-bdee-107c3dbcf8a7:26' | | mysql-bin.000011 | 13753 | Query | 1 | 13846 | XA START X'74657374' X'' 1 | | mysql-bin.000011 | 13846 | Rows_query | 1 | 13897 | # update test set id = id + 5 | | mysql-bin.000011 | 13897 | Table_map | 1 | 13985 | table_id: 256 (test.test) | | mysql-bin.000011 | 13985 | Update_rows_v1 | 1 | 14167 | table_id: 256 flags: STMT_END_F | | mysql-bin.000011 | 14167 | Query | 1 | 14258 | XA END X'74657374' X'' 1 | | mysql-bin.000011 | 14258 | XA_prepare | 1 | 14298 | XA PREPARE X'74657374' X'' 1 | | mysql-bin.000011 | 14298 | Gtid | 1 | 14363 | SET @@SESSION.GTID_NEXT= 'f1ceb61a-a5d5-11e7-bdee-107c3dbcf8a7:27' | | mysql-bin.000011 | 14363 | Query | 1 | 14459 | XA ROLLBACK X'74657374' X'' 1 | | mysql-bin.000011 | 14459 | Gtid | 1 | 14524 | SET @@SESSION.GTID_NEXT= 'f1ceb61a-a5d5-11e7-bdee-107c3dbcf8a7:28' | | mysql-bin.000011 | 14524 | Query | 1 | 14617 | XA START X'74657374' X'' 1 | | mysql-bin.000011 | 14617 | Rows_query | 1 | 14668 | # update test set id = id + 5 | | mysql-bin.000011 | 14668 | Table_map | 1 | 14756 | table_id: 256 (test.test) | | mysql-bin.000011 | 14756 | Update_rows_v1 | 1 | 14938 | table_id: 256 flags: STMT_END_F | | mysql-bin.000011 | 14938 | Query | 1 | 15029 | XA END X'74657374' X'' 1 | | mysql-bin.000011 | 15029 | XA_prepare | 1 | 15069 | XA PREPARE X'74657374' X'' 1 | | mysql-bin.000011 | 15069 | Gtid | 1 | 15134 | SET @@SESSION.GTID_NEXT= 'f1ceb61a-a5d5-11e7-bdee-107c3dbcf8a7:29' | | mysql-bin.000011 | 15134 | Query | 1 | 15228 | XA COMMIT X'74657374' X'' 1 mysql Xa&#39;s binlog submission format will be recorded to the binlog in the prepare and visible to the outside and will continue to commit or rollback to xa. If you perform rollback, you need to perform the rollback of the corresponding SQL xa. Rollback does not record the corresponding reverse binlog needs to be cached in memory xa Prepare change http://mysql.taobao.org/monthly/2017/11/06/， A good introduction to MySQL XA Above address 404 The above address can be removed by removing the comma from the back. http://mysql.taobao.org/monthly/2017/11/06/ Output format is similar ``` ================> binlog[mysql-bin.000012:35614] BEGIN ----> Thread id: 3 ------> XA START X'63' X'' 1 END ----> transaction id: 0 ------> XA END X'63' X'' 1 ================> binlog[mysql-bin.000012:36022] ----------------> binlog[mysql-bin.000012:35459] ------> XA ROLLBACK X'62' X'' 1 ----------------> binlog[mysql-bin.000012:36209] ------> XA COMMIT X'63' X'' 1 ``` Print xa information ``` protected void printXAInfo(List<Pair> pairs) { if (pairs == null) { return; } String xaType = null; String xaXid = null; for (Pair pair : pairs) { String key = pair.getKey(); if (StringUtils.endsWithIgnoreCase(key "XA_TYPE")) { xaType = pair.getValue(); } else if (StringUtils.endsWithIgnoreCase(key "XA_XID")) { xaXid = pair.getValue(); } } if (xaType != null && xaXid != null) { logger.info(" ------> " + xaType + " " + xaXid); } } ```
625,batchId:73 is not the firstly:72 2018-05-06 10:26:53.805 [destination = example address = /127.0.0.1:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position {"identity":{"slaveId":-1 "sourceAddress":{"address":"localhost" "port":3306}} "postion":{"included":false "journalName":"mysql-bin.000002" "position":26359074 "serverId":1 "timestamp":1525570849000}} 2018-05-06 10:26:53.845 [destination = example address = /127.0.0.1:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000002 position=26359074 serverId=1 timestamp= 1525570849000] 2018-05-06 10:29:45.154 [New I/O server worker #1-1] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400 Caused by : something goes wrong with channel:[id: 0x61b78711 /192.168.0.111:60228 => /192.168.0.111:11111] exception=com.alibaba.otter.canal.meta.exception.CanalMetaManagerException: batchId:72 is not the firstly:71 2018-05-06 10:29:45.171 [New I/O server worker #1-1] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x61b78711 /192.168.0.111:60228 :> /192.168.0.111:11111] exception=java.nio.channels.ClosedChannelException at org.jboss.netty.channel.socket.nio.NioWorker.cleanUpWriteBuffer(NioWorker.java:649) at org.jboss.netty.channel.socket.nio.NioWorker.writeFromUserCode(NioWorker.java:370) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:137) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) at org.jboss.netty.channel.Channels.write(Channels.java:611) at org.jboss.netty.channel.Channels.write(Channels.java:578) at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:28) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:144) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.cleanup(ReplayingDecoder.java:542) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.channelDisconnected(ReplayingDecoder.java:450) at org.jboss.netty.channel.Channels.fireChannelDisconnected(Channels.java:360) at org.jboss.netty.channel.socket.nio.NioWorker.close(NioWorker.java:599) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:119) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) at org.jboss.netty.channel.Channels.close(Channels.java:720) at org.jboss.netty.channel.AbstractChannel.close(AbstractChannel.java:208) at org.jboss.netty.channel.ChannelFutureListener$1.operationComplete(ChannelFutureListener.java:46) at org.jboss.netty.channel.DefaultChannelFuture.notifyListener(DefaultChannelFuture.java:381) at org.jboss.netty.channel.DefaultChannelFuture.addListener(DefaultChannelFuture.java:148) at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:30) at com.alibaba.otter.canal.server.netty.NettyUtils.error(NettyUtils.java:51) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:200) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:444) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261) at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:350) at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) No order ack No order ack, what can you say in more detail? Thank you. @mennyzfy When calling getWithoutAck to get the Message, it will generate an incremented batchId and assign it to the id field of the Message. When the ack is needed, the client needs to ensure that the message is acknowledged in the order of getting the message. @zwangbo I am following this demo to run the following code while (running) { try { MDC.put("destination" destination); connector.connect(); connector.subscribe(""); while (running) { Message message = connector.getWithoutAck(batchSize); // Get the specified amount of data long batchId = message.getId(); int size = message.getEntries().size(); if (batchId == -1 || size == 0) { try { Thread.sleep(1000); } catch (InterruptedException e) { } } else { resolveEntry(message.getEntries()); } connector.ack(batchId); // Submit confirmation } } catch (Exception e) { logger.error("process error!" e); } finally { connector.disconnect(); MDC.remove("destination"); } } It is also confirmed after the message is consumed, and why the order of the single thread will be different. @agapple Although it is really caused by the lack of sequential confirmation of ack, but this invisibly reduces my throughput and tps a lot. I am here by multi-threading and can only be single-threaded. @cjj137783 This piece can consider taking the asynchronous processing and then doing a buffer to control the ack order and ack not to lose. Although I think this function is also good on the canal side. Otter&#39;s approach is to make an asynchronous buffer to order ack batchId @mennyzfy How to solve the problem? Why would you miss a branchID? Reference FAQ : https://github.com/alibaba/canal/wiki/FAQ
624,Caused by: java.net.ConnectException: Connection refused: connect The canal server has successfully run the new maven project and the test refuses to connect. ![](https://ww1.sinaimg.cn/large/005zWjpngy1fr2wcvwun6j310q0eemyo.jpg) Own environment or network problem
623,Add the stopRunning method on the CanalConnector interface to gracefully stop the client [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=623) <br/>All committers have signed the CLA. [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=623) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=623) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=623) it.</sub> tks
622,block waiting interrupted by stop request resolve issue https://github.com/alibaba/canal/issues/619 [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=622) <br/>All committers have signed the CLA. [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=622) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=622) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=622) it.</sub> tks
621,close zkclientx if necessary when stop canal and fix CanalInstanceWithManager#doInitEventParser bug Tks is indeed a code copy problem
620,close zkclientx when stop canal [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=620) <br/>All committers have signed the CLA. How closed? Newly mentioned one #621 Well already merged
619,In the cluster Can&#39;t gracefully stop when you can&#39;t get exclusive lock in client mode com.alibaba.otter.canal.client.impl.SimpleCanalConnector:448 mutex.get();// Blocking waiting Unable to exit It is recommended to add a stopSubscribe method to the CanalConnector interface to stop blocking waiting. You can only solve the problem of blocking when subscribe. If it is connect or other operations are locked by mutex get You can use thread interrupt to exit the capture and overwhelm the exception at stop. Only the subscribe is processed because this situation is very common in the case of cluster. Neal Hu nealhu@apache.org > in May 18, 2018 10 18 agapple <notifications@github.com> Write > > You can only solve the problem of blocking when subscribe. If it is connect or other operations are locked by mutex get > > — > You are receiving this because you authored the thread. > Reply to this email directly view it on GitHub or mute the thread. > There is a PR submission that provides a stopRunning method to quit
618,Support for synchronizing binlog through GTID 1. Increase the instance level configuration gdidon to indicate whether the instance has GTID mode enabled. 2. Driver package adds GTID related class GTIDSet UUIDSet is GTID indicates BinlogDumpGTIDCommandPacket is COM_BINLOG_DUMP_GTID command 3. Completing the resolution of the GtidLogEvent event under the dbsync package 4. The EntryType enumeration item is added under the protocol package. GTIDLOG is the GTIDLOG event in the binlog stream. The header is added. The field gtid is described here. 5. Parse package under ErosaConnection to increase the dump method according to GTID synchronization 6. Parse the package under the AbstractEventParser start. If the instance uses the GTID mode, send the COM_BINLOG_DUMP_GTID command to mysql. [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=618) <br/>All committers have signed the CLA. Very good Pull After the merger, I will adjust the details slightly. :smile: Very happy to contribute good job
617,something goes wrong when acking data from server:null The client reports the error as follows 2018-05-03 17:26:50.735 [pool-2-thread-1] WARN com.alibaba.otter.canal.client.impl.ClusterCanalConnector.?:? - something goes wrong when acking data from server:null com.alibaba.otter.canal.protocol.exception.CanalClientException: java.io.IOException: Broken pipe at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.ack(SimpleCanalConnector.java:324) at com.alibaba.otter.canal.client.impl.ClusterCanalConnector.ack(ClusterCanalConnector.java:253) at com.lyj.order.es.sync.service.impl.AbstractCanalListenerServiceImpl.lambda$startListener$2(AbstractCanalListenerServiceImpl.java:137) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) Caused by: java.io.IOException: Broken pipe at sun.nio.ch.FileDispatcherImpl.write0(Native Method) at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47) at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93) at sun.nio.ch.IOUtil.write(IOUtil.java:65) at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:471) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.writeWithHeader(SimpleCanalConnector.java:358) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.ack(SimpleCanalConnector.java:318) ... 5 more The server reports an error as follows ERROR com.alibaba.otter.canal.server.netty.NettyUtils 50 - ErrotCode:400 Caused by: something goes wrong with channel:[id:0x0f4b9efb /10.0.39.56:50004 => /10.0.11.161:11111] exception=com.alibaba.otter.canal.server.exception.CanalServerException: ack error clientId:1001 batchld:718 is not exist pleasecheck Please help me to see what the problem is. @agapple Welcome to God The server instance restarts or a rollback client takes a retry.
616,canal.instance.filter.regex Perl regular expression problem use canal\\.canal.* Cannot get binlog with canal test1 You can read more adminGuide or FAQ
615,Manage management has no example to see it? Can you see how your internal manager management is done? Can refer to the manager of the otter
614,Cursor stops updating Hello, I encountered a problem during the use of canal described below We currently use canal to deploy a canal in a single machine. Server and a canal client，canal The server side runs for a while after the meta dat and the cursor stop updating the client side can not accept the data change push meta log as shown below 2018-04-23 15:06:39.109 - clientId:1001 cursor:[mysql-bin.000673 73407748 1524467197000] address[rm-uf6u3c8i3cejqaia6.mysql.rds.aliyuncs.com/172.19.153.0:3306] 2018-04-23 15:06:45.109 - clientId:1001 cursor:[mysql-bin.000673 73410335 1524467203000] address[rm-uf6u3c8i3cejqaia6.mysql.rds.aliyuncs.com/172.19.153.0:3306] 2018-04-23 15:06:51.109 - clientId:1001 cursor:[mysql-bin.000673 73412633 1524467209000] address[rm-uf6u3c8i3cejqaia6.mysql.rds.aliyuncs.com/172.19.153.0:3306] 2018-04-23 15:06:57.109 - clientId:1001 cursor:[mysql-bin.000673 73438765 1524467215000] address[rm-uf6u3c8i3cejqaia6.mysql.rds.aliyuncs.com/172.19.153.0:3306] 2018-04-23 15:07:02.109 - clientId:1001 cursor:[mysql-bin.000673 73456385 1524467221000] address[rm-uf6u3c8i3cejqaia6.mysql.rds.aliyuncs.com/172.19.153.0:3306] On 04 23, it will not be updated. It is useless after deleting the meta dat and restarting the server. And there is no cursor information in the meta dat. ``` {"clientDatas":[{"clientIdentity":{"clientId":1001 "destination":"rm-uf6u3c8i3cejqaia6" "filter":""}}] "destination":"rm-uf6u3c8i3cejqaia6"} ``` If you see an annoyance, please reply. If you feel that the description is unclear, please indicate thank you. The version is 1 0 25 This is aliyun&#39;s rds Have to pay attention to whether the parsing process is abnormal. When the version is 1 0 25, this error is reported. ``` 2018-04-27 18:29:02.113 [destination = financial address = /10.44.88.50:13306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address EulerOS-BaseTemplate/10.44.88.50:13306 has an error retrying. caused by java.nio.channels.ClosedByInterruptException: null at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannel.read(SocketChannel.java:49) ~[canal.parse.driver-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:151) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:77) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:137) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) ~[canal.parse-1.0.25.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131] 2018-04-27 18:29:02.118 [destination = financial address = /10.44.88.50:13306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:financial[java.nio.channels.ClosedByInterruptException at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannel.read(SocketChannel.java:49) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:151) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:77) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:137) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) at java.lang.Thread.run(Thread.java:748) ] ``` When I upgraded to 1 0 26 SNAPSHOT, I just reported this error and then parsed it. ``` 2018-05-09 10:10:56.214 [destination = mysql-qry-1 address = mysql-qry-1.db.prod.alsh.xingbianli.com/10.0.96.8:3306 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: EOF encountered. at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.read(BioSocketChannel.java:80) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:174) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:80) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:144) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131] 2018-05-09 10:10:56.215 [destination = mysql-qry-1 address = mysql-qry-1.db.prod.alsh.xingbianli.com/10.0.96.8:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address mysql-qry-1.db.prod.alsh.xingbianli.com/10.0.96.8:3306 has an error retrying. caused by java.io.IOException: EOF encountered. at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.read(BioSocketChannel.java:80) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:174) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:80) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:144) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131] 2018-05-09 10:10:56.217 [destination = mysql-qry-1 address = mysql-qry-1.db.prod.alsh.xingbianli.com/10.0.96.8:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:mysql-qry-1[java.io.IOException: EOF encountered. at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannel.read(BioSocketChannel.java:80) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:174) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:80) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:144) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) at java.lang.Thread.run(Thread.java:748) ] 2018-05-09 10:11:11.241 [destination = mysql-qry-1 address = mysql-qry-1.db.prod.alsh.xingbianli.com/10.0.96.8:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address mysql-qry-1.db.prod.alsh.xingbianli.com/10.0.96.8:3306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: connect mysql-qry-1.db.prod.alsh.xingbianli.com/10.0.96.8:3306 failure Caused by: java.io.IOException: connect mysql-qry-1.db.prod.alsh.xingbianli.com/10.0.96.8:3306 failure at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:77) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:77) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:87) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:160) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131] Caused by: java.net.ConnectException: Connection refused (Connection refused) at java.net.PlainSocketImpl.socketConnect(Native Method) ~[na:1.8.0_131] at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[na:1.8.0_131] at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[na:1.8.0_131] at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[na:1.8.0_131] at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[na:1.8.0_131] at java.net.Socket.connect(Socket.java:589) ~[na:1.8.0_131] at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannelPool.open(BioSocketChannelPool.java:20) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannelPool.open(SocketChannelPool.java:18) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:72) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] ... 4 common frames omitted 2018-05-09 10:11:11.242 [destination = mysql-qry-1 address = mysql-qry-1.db.prod.alsh.xingbianli.com/10.0.96.8:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:mysql-qry-1[com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: connect mysql-qry-1.db.prod.alsh.xingbianli.com/10.0.96.8:3306 failure Caused by: java.io.IOException: connect mysql-qry-1.db.prod.alsh.xingbianli.com/10.0.96.8:3306 failure at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:77) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:77) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:87) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:160) at java.lang.Thread.run(Thread.java:748) Caused by: java.net.ConnectException: Connection refused (Connection refused) at java.net.PlainSocketImpl.socketConnect(Native Method) at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) at java.net.Socket.connect(Socket.java:589) at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannelPool.open(BioSocketChannelPool.java:20) at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannelPool.open(SocketChannelPool.java:18) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:72) ... 4 more ] 2018-05-09 10:11:24.537 [destination = mysql-qry-1 address = mysql-qry-1.db.prod.alsh.xingbianli.com/10.0.96.8:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address mysql-qry-1.db.prod.alsh.xingbianli.com/10.0.96.8:3306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: connect mysql-qry-1.db.prod.alsh.xingbianli.com/10.0.96.8:3306 failure Caused by: java.io.IOException: connect mysql-qry-1.db.prod.alsh.xingbianli.com/10.0.96.8:3306 failure at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:77) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:77) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:87) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:160) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131] Caused by: java.net.ConnectException: Connection refused (Connection refused) at java.net.PlainSocketImpl.socketConnect(Native Method) ~[na:1.8.0_131] at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[na:1.8.0_131] at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[na:1.8.0_131] at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[na:1.8.0_131] at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[na:1.8.0_131] at java.net.Socket.connect(Socket.java:589) ~[na:1.8.0_131] at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannelPool.open(BioSocketChannelPool.java:20) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannelPool.open(SocketChannelPool.java:18) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:72) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] ... 4 common frames omitted 2018-05-09 10:11:24.538 [destination = mysql-qry-1 address = mysql-qry-1.db.prod.alsh.xingbianli.com/10.0.96.8:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:mysql-qry-1[com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: connect mysql-qry-1.db.prod.alsh.xingbianli.com/10.0.96.8:3306 failure Caused by: java.io.IOException: connect mysql-qry-1.db.prod.alsh.xingbianli.com/10.0.96.8:3306 failure at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:77) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:77) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:87) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:160) at java.lang.Thread.run(Thread.java:748) Caused by: java.net.ConnectException: Connection refused (Connection refused) at java.net.PlainSocketImpl.socketConnect(Native Method) at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) at java.net.Socket.connect(Socket.java:589) at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannelPool.open(BioSocketChannelPool.java:20) at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannelPool.open(SocketChannelPool.java:18) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:72) ... 4 more ] 2018-05-09 10:11:35.676 [destination = mysql-qry-1 address = mysql-qry-1.db.prod.alsh.xingbianli.com/10.0.96.8:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address mysql-qry-1.db.prod.alsh.xingbianli.com/10.0.96.8:3306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: connect mysql-qry-1.db.prod.alsh.xingbianli.com/10.0.96.8:3306 failure Caused by: java.io.IOException: connect mysql-qry-1.db.prod.alsh.xingbianli.com/10.0.96.8:3306 failure at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:77) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:77) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:87) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:160) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131] Caused by: java.net.ConnectException: Connection refused (Connection refused) at java.net.PlainSocketImpl.socketConnect(Native Method) ~[na:1.8.0_131] at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[na:1.8.0_131] at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[na:1.8.0_131] at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[na:1.8.0_131] at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[na:1.8.0_131] at java.net.Socket.connect(Socket.java:589) ~[na:1.8.0_131] at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannelPool.open(BioSocketChannelPool.java:20) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannelPool.open(SocketChannelPool.java:18) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:72) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] ... 4 common frames omitted 2018-05-09 10:11:35.676 [destination = mysql-qry-1 address = mysql-qry-1.db.prod.alsh.xingbianli.com/10.0.96.8:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:mysql-qry-1[com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: connect mysql-qry-1.db.prod.alsh.xingbianli.com/10.0.96.8:3306 failure Caused by: java.io.IOException: connect mysql-qry-1.db.prod.alsh.xingbianli.com/10.0.96.8:3306 failure at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:77) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:77) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:87) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:160) at java.lang.Thread.run(Thread.java:748) Caused by: java.net.ConnectException: Connection refused (Connection refused) at java.net.PlainSocketImpl.socketConnect(Native Method) at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) at java.net.Socket.connect(Socket.java:589) at com.alibaba.otter.canal.parse.driver.mysql.socket.BioSocketChannelPool.open(BioSocketChannelPool.java:20) at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannelPool.open(SocketChannelPool.java:18) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:72) ... 4 more ] ``` Find the code is here ![image](https://user-images.githubusercontent.com/20578246/39792999-ffc6327c-5375-11e8-9fca-b1db368d953f.png) The link between the account and the database is also broken. Excuse me, what is the reason? Thank you. After a period of about half an hour, I reconnected. Caused by: java.net.ConnectException: Connection refused (Connection refused)
613,java.nio.channels.ClosedByInterruptException 2018-04-27 18:29:02.113 [destination = financial address = /10.44.88.50:13306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address EulerOS-BaseTemplate/10.44.88.50:13306 has an error retrying. caused by java.nio.channels.ClosedByInterruptException: null at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannel.read(SocketChannel.java:49) ~[canal.parse.driver-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:151) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:77) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:137) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) ~[canal.parse-1.0.25.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131] 2018-04-27 18:29:02.118 [destination = financial address = /10.44.88.50:13306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:financial[java.nio.channels.ClosedByInterruptException at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannel.read(SocketChannel.java:49) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:151) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:77) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:137) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) at java.lang.Thread.run(Thread.java:748) ] Canal 1 0 25 version destination：financial，deduct The scene deduct has been able to monitor mysql binlog financial monitoring after a while throwing this exception and then canal The client has been spending canal The financal of the server, but the financial cannot monitor the binlog. It is normal after the restart, but it has been repeated for a while. About 2 5hours financal Destination is hung up Try the 1 0 26 version We are using 1 0 26 also have this problem in the DUMP Times Java nio channels ClosedByInterruptException According to the online feedback 1 0 24 without this problem is doing downgrade verification
612,c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta 1 0 25 version canal.instance.tsdb.enable=false 2018-04-27 17:11:17.990 [destination = billingondemandfee01db address = /10.179.171.159:40002 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - pls submit github issue show create table ddl:CREATE TABLE `t_billing_accumulator_conf` ( `serviceTypeCode` varchar(64) NOT NULL `resourceTypeCode` varchar(64) NOT NULL `accumulateFactorName` varchar(64) NOT NULL `amount` int(11) NOT NULL `measureId` int(11) NOT NULL `logTime` datetime DEFAULT NULL `memo` varchar(200) DEFAULT NULL `extendParams` varchar(1024) DEFAULT NULL UNIQUE KEY `uniqueRecord` (`serviceTypeCode` `resourceTypeCode` `accumulateFactorName`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 compare failed . db : TableMeta [schema=billingondemandfee01db table=t_billing_accumulator_conf fileds= FieldMeta [columnName=serviceTypeCode columnType=varchar(64) defaultValue=null nullable=false key=true] FieldMeta [columnName=resourceTypeCode columnType=varchar(64) defaultValue=null nullable=false key=true] FieldMeta [columnName=accumulateFactorName columnType=varchar(64) defaultValue=null nullable=false key=true] FieldMeta [columnName=amount columnType=int(11) defaultValue=null nullable=false key=false] FieldMeta [columnName=measureId columnType=int(11) defaultValue=null nullable=false key=false] FieldMeta [columnName=logTime columnType=datetime defaultValue=null nullable=true key=false] FieldMeta [columnName=memo columnType=varchar(200) defaultValue=null nullable=true key=false] FieldMeta [columnName=extendParams columnType=varchar(1024) defaultValue=null nullable=true key=false] ] mem : TableMeta [schema=billingondemandfee01db table=t_billing_accumulator_conf fileds= FieldMeta [columnName=serviceTypeCode columnType=varchar(64) defaultValue=null nullable=false key=false] FieldMeta [columnName=resourceTypeCode columnType=varchar(64) defaultValue=null nullable=false key=false] FieldMeta [columnName=accumulateFactorName columnType=varchar(64) defaultValue=null nullable=false key=false] FieldMeta [columnName=amount columnType=int(11) defaultValue=null nullable=false key=false] FieldMeta [columnName=measureId columnType=int(11) defaultValue=null nullable=false key=false] FieldMeta [columnName=logTime columnType=datetime defaultValue=null nullable=true key=false] FieldMeta [columnName=memo columnType=varchar(200) defaultValue=null nullable=true key=false] FieldMeta [columnName=extendParams columnType=varchar(1024) defaultValue=null nullable=true key=false] ] 2018-04-27 17:11:20.911 [destination = billingondemandfee01db address = /10.179.171.159:40002 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - compare failed check log Known issues can be tested using the 1 0 26 version
611,Canal can not capture binlog data normally Canal start normally can not capture binlog data, please teach the gods Look at the FAQ
610,DDL statement comment Chinese appears garbled MySQL charset is Utf8mb4 collation is utf8mb4_general_ci The table statement during the test is as follows ```sql CREATE TABLE `xxx` ( `id` int(11) NOT NULL AUTO_INCREMENT COMMENT Primary key id increment `run_time` int(13) NOT NULL DEFAULT '0' COMMENT Total running time `init_time` int(13) NOT NULL DEFAULT '0' COMMENT Initialization time `simulation_time` int(13) NOT NULL DEFAULT '0' COMMENT Simulation calculation takes time seconds `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT Creation time `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT Update time PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT hadoop processing capability standard monitor running time ``` Canal The SQL statement obtained after receiving the server is as follows ```sql create table `xxx` ( `id` int(11) NOT NULL auto_increment COMMENT 'šłĽťĒģidÔľĆŤá™ŚĘě' `run_time` int(13) NOT NULL DEFAULT 0 COMMENT 'ŤŅźŤ°ĆśÄĽśó∂ťēŅ-Áßí' `init_time` int(13) NOT NULL DEFAULT 0 COMMENT 'ŚąĚŚßčŚĆĖśó∂ťēŅ-Áßí' `simulation_time` int(13) NOT NULL DEFAULT 0 COMMENT 'ś®°śčüŤģ°ÁģóŤÄóśó∂-Áßí' `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT 'ŚąõŚĽļśó∂ťóī' `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT 'śõīśĖįśó∂ťóī' PRIMARY KEY(`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT 'hadoopŚ§ĄÁźÜŤÉĹŚäõś†áŚáÜÁõĎśéßÁ®čŚļŹŤŅźŤ°Ćśó∂ťēŅ'; ``` Possible cause of the problem in canal When the server receives the binlog log, it will get it from the binlog log. clientCharset = 45 thus getting charsetName = MacCentralEurope re-decodes the above SQL with Chinese garbled characters. https://github.com/alibaba/canal/blob/f46133d1168071741e8e3e4235aa635c5870a976/dbsync/src/main/java/com/taobao/tddl/dbsync/binlog/event/QueryLogEvent.java#L482 https://github.com/alibaba/canal/blob/f46133d1168071741e8e3e4235aa635c5870a976/dbsync/src/main/java/com/taobao/tddl/dbsync/binlog/CharsetConversion.java#L106 ```java // Are these two definitions correct? putEntry(45 "utf8mb4" "utf8mb4_general_ci" "MacCentralEurope"); putEntry(46 "utf8mb4" "utf8mb4_bin" "MacCentralEurope"); ``` You better debug the code that the event receives is 啥 1 MySQL charset is Utf8mb4 collation is Utf8mb4_general_ci corresponds to the mysql official website ID= 45 execute this sql in the production library ```SQL SHOW COLLATION WHERE Charset = 'utf8mb4'; ``` It’s the same ![image](https://user-images.githubusercontent.com/4264237/39353745-2b1011a8-4a3b-11e8-973d-ceda62a823c7.png) 2 and in the debug code in the following code execution process from the binlog log clientCharset = 45 thus getting charsetName = MacCentralEurope ```java /* A 2nd variable part; this is common to all versions */ final int queryLen = dataLen - dbLen - 1; dbname = buffer.getFixString(dbLen + 1); if (clientCharset >= 0) { charsetName = CharsetConversion.getJavaCharset(clientCharset); if ((charsetName != null) && (Charset.isSupported(charsetName))) { query = buffer.getFixString(queryLen charsetName); } else { logger.warn("unsupported character set in query log: " + "\n ID = " + clientCharset + " Charset = " + CharsetConversion.getCharset(clientCharset) + " Collation = " + CharsetConversion.getCollation(clientCharset)); query = buffer.getFixString(queryLen); } } else { query = buffer.getFixString(queryLen); } ``` Debug screenshot ![image](https://user-images.githubusercontent.com/4264237/39354991-8745483c-4a3e-11e8-89b4-7d3d54a943de.png) 3 I try to put ```java putEntry(45 "utf8mb4" "utf8mb4_general_ci" "MacCentralEurope"); ``` Changed to ```java putEntry(45 "utf8mb4" "utf8mb4_general_ci" "UTF-8"); ``` Then garbled is solved Debug screenshot ![image](https://user-images.githubusercontent.com/4264237/39354985-80b28a34-4a3e-11e8-8765-3238ef3d0b02.png) 4 in the code with utf8mb4 corresponding javaCharset only 45，46 Two MacCentralEurope is all other than this. ```UTF-8``` ![image](https://user-images.githubusercontent.com/4264237/39353950-bc57543c-4a3b-11e8-9ba1-0a066b8f9573.png) https://github.com/alibaba/canal/blob/f46133d1168071741e8e3e4235aa635c5870a976/dbsync/src/main/java/com/taobao/tddl/dbsync/binlog/CharsetConversion.java#L106 It should be that the typo at the time has been fixed.
609,Data parsing type error insert->query header { version: 1 logfileName: "mysql-bin.001545\302\243\302\235E\004" logfileOffset: 86861287 serverId: 5243 serverenCode: "UTF-8" executeTime: 1524571682000 sourceType: MYSQL schemaName: "" tableName: "" eventLength: 112 eventType: QUERY } entryType: ROWDATA storeValue: "\020\aZYinsert into data_compare.lzw_receive_json_log(min_id max_id) values (116521634 116521636)" Is it because of the garbled problem of parsing? This is the mysql5 6 rows_query_log_event is a query
608,no match ack positionLogPosition message[batchId=1 size=259] 15:00:18.080 [kafka-producer-network-thread | producer-1] DEBUG o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Using older server API v2 to send PRODUCE {acks=-1 timeout=30000 partitionSizes=[case_flow_topic-1=3032]} with correlation id 9 to node 1003 . . 15:00:18.623 [kafka-producer-network-thread | producer-1] DEBUG o.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Using older server API v2 to send PRODUCE {acks=-1 timeout=30000 partitionSizes=[case_flow_topic-1=4021]} with correlation id 56 to node 1003 com.alibaba.otter.canal.protocol.exception.CanalClientException: something goes wrong with reason: something goes wrong with channel:[id: 0x6c969f24 /192.168.113.5:38172 => /192.168.113.5:11111] exception=com.alibaba.otter.canal.store.CanalStoreException: no match ack positionLogPosition[identity=LogIdentity[sourceAddress=192.168.113.243/192.168.113.243:3306 slaveId=-1] postion=EntryPosition[included=false journalName=mysql-bin.001543 position=916315835 serverId=1803306 timestamp=1524553202000]] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.receiveMessages(SimpleCanalConnector.java:245) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:222) at com.alibaba.otter.canal.client.impl.ClusterCanalConnector.getWithoutAck(ClusterCanalConnector.java:183) at com.zichan360.bigdata.canal.client.caseflow.SyncCaseFlowTables.main(SyncCaseFlowTables.java:32) 15:00:24.652 [main] INFO c.a.o.c.c.impl.ClusterCanalConnector - restart the connector for next round retry. message[batchId=2 size=259] . . . Code String destination = args[0]; String topic = args[1]; CanalConnector connector = CanalConnectors.newClusterConnector("zk" destination "" ""); int batchSize = 1000; int emptyCount = 0; try { connector.connect(); connector.rollback(); int totalEmptyCount = 120; while (emptyCount < totalEmptyCount) { Message message = connector.getWithoutAck(batchSize 2000L TimeUnit.MILLISECONDS); long batchId = message.getId(); int size = message.getEntries().size(); if (batchId == -1 || size == 0) { emptyCount++; try { Thread.sleep(1000); } catch (InterruptedException e) { } } else { emptyCount = 0; System.out.printf("message[batchId=%s size=%s] \n" batchId size); produceKafkaData(message.getEntries() topic); try { Thread.sleep(1000); } catch (InterruptedException e) { } } try { connector.ack(batchId); } catch (Exception e) { e.printStackTrace(); connector.rollback(batchId); } } } finally { connector.disconnect(); } no match ack It is estimated that the server restarts the instance client and does a retry recovery.
607,Server starts multiple instances. There is always an instance not started. I have two servers to start 3 instances respectively. Server1 started successfully 1 Server2 started successfully 2 There is no error in the instance of failed startup Just started when I didn&#39;t go to find the meta dat under the location instance folder and there is no postion information. The relationship with the library should be small because the instance of failed startup is not fixed. In general, a server recommended configuration is to start up to several instances. There is no constraint on the number of instances. You have to open the client to have the meta dat position.
606,Why do I need to restart 2 canal after the canal server is down? The server can reacquire the binlog log data. Ask everyone questions about the problem as follows Why do I need to restart 2 canal after the canal 1 0 25 server is down? The server can re-acquire the database binlog log data and restart the database. The binlog log data cannot be obtained. I tried it a few times today and I have no problem. Looking back and looking for the reason.
605,Binlog data loss in case of large transaction When testing canal sync data to es from mysql Dump out a table when exporting every 250 lines into a transaction and then truncate the table and then import the exported sql found that after the synchronization is completed, es will be less than three hundred data less than mysql total data volume 4W after debugging found to be canal In the process of synchronous binlog, data loss occurs. Most transactions are completely synchronized. There are several transactions that only synchronize less than 100 data, and then divide the transaction into 10 transactions per 10 rows of data, there will be no data loss. Is this related to the network status or just because the transaction is too big? How did this problem be dealt with?
604,Can&#39;t find com.alibaba.fastsql:fastsql:jar:2.0.0_preview_228 Changed to com alibaba fastsql fastsql jar 2 0 0_preview_186 [ERROR] Failed to execute goal on project canal.parse: Could not resolve dependencies for project com.alibaba.otter:canal.parse:jar:1.0.26-SNAPSHOT: Failure to find com.alibaba.fastsql:fastsql:jar:2.0.0_preview_228 in http://maven.aliyun.com/nexus/content/groups/public was cached in the local repository resolution will not be reattempted until the update interval of nexus-aliyun has elapsed or updates are forced -> [Help 1] com.alibaba.fastsql:fastsql:jar:2.0.0_preview_228 When will this package be released? Take a look at the binary distribution package.
603,com.alibaba.otter.canal.protocol.exception.CanalClientException: java.io.IOException: The software in your host has aborted an established connection 2018-04-23 15:05:05.780-[ERROR]-[Thread-0]-com.clickplus.canal.AbstractCanalClientTest:151-process error! com.alibaba.otter.canal.protocol.exception.CanalClientException: java.io.IOException: The software in your host has aborted an established connection at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.ack(SimpleCanalConnector.java:257) ~[canal.client-1.0.4.jar:na] at com.clickplus.canal.AbstractCanalClientTest.process(AbstractCanalClientTest.java:143) ~[classes/:na] at com.clickplus.canal.AbstractCanalClientTest$2.run(AbstractCanalClientTest.java:86) [classes/:na] at java.lang.Thread.run(Thread.java:722) [na:1.7.0_17] Caused by: java.io.IOException: The software in your host has aborted an established connection at sun.nio.ch.SocketDispatcher.write0(Native Method) ~[na:1.7.0_17] at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:51) ~[na:1.7.0_17] at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:94) ~[na:1.7.0_17] at sun.nio.ch.IOUtil.write(IOUtil.java:65) ~[na:1.7.0_17] at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:450) ~[na:1.7.0_17] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.writeWithHeader(SimpleCanalConnector.java:289) ~[canal.client-1.0.4.jar:na] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.ack(SimpleCanalConnector.java:253) ~[canal.client-1.0.4.jar:na] ... 3 common frames omitted The problem description is best detailed or not good analysis @jnliao Thank you for answering the small amount of data when the canal run is no problem. When there is a large transaction, the thousands of levels of the insert will report this error. The buffersize and the client&#39;s batchsize are useless. I have encountered this problem when I used to process data. 1 usage scenario I am using canal 1 0 25 The client uses the scheduled task to process the log data obtained by the canal server in real time and each timed task uses the same client link object database to instantly update 1w data when the canal client processes the data for a while. Report the mistake you said Caused by: java.io.IOException: The software in your host has aborted an established connection 2 solutions Each timed task uses the newly created client link object and disconnects at the end of the task Https github com alibaba canal issues 640 Take a look at this
602,Mvn after moving out of code Build canal common jar 1 0 26 SNAPSHOT Package not found Could not find artifact com.alibaba.otter:canal.common:jar:1.0.26-SNAPSHOT in nexus-aliyun (http://maven.aliyun.com/nexus/content/groups/public) Checked only 1 0 25 as well as 9.0.0 I am on common Package re install After success No aliyun of deploy Permission
601,Canal runs for about 38 hours, the client reports Broken Pipe server did not report any reason com.alibaba.otter.canal.protocol.exception.CanalClientException: java.io.IOException: Broken pipe at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.ack(SimpleCanalConnector.java:343) Caused by: java.io.IOException: Broken pipe at sun.nio.ch.FileDispatcherImpl.write0(Native Method) at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47) at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93) at sun.nio.ch.IOUtil.write(IOUtil.java:65) at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:471) at java.nio.channels.Channels.writeFullyImpl(Channels.java:78) at java.nio.channels.Channels.writeFully(Channels.java:98) at java.nio.channels.Channels.access$000(Channels.java:61) at java.nio.channels.Channels$1.write(Channels.java:174) at java.nio.channels.Channels$WritableByteChannelImpl.write(Channels.java:458) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.writeWithHeader(SimpleCanalConnector.java:400) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.writeWithHeader(SimpleCanalConnector.java:387) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.ack(SimpleCanalConnector.java:337) ... 6 more This is actively disconnected by the server. Why is the server actively disconnected? Is this a problem with canal or how to deal with it? Check the server&#39;s log The server did not report an error and the client reported an error. I also have this question. Is this what I am doing? I am using the 25 version. Https github com alibaba canal issues 640 Take a look at this
599,Large amount of data Canal server reported caoc server netty handler ClientAuthenticationHandler - idle timeout exceeds close channel to save server resources Problem scene When the tens of thousands of data are in the insert operation, the canal server will report c.a.o.c.server.netty.handler.ClientAuthenticationHandler - channel:[id: 0x3c24ea0e /192.168.1.111:64402 => /192.168.1.111:11111] idle timeout exceeds close channel to save server resources error The client&#39;s log shows that the subscribed binlog will repeat 0 10000 recirculation 0 10000 Synchronization of the table that causes the delay time to become very long can&#39;t get in. This situation is to expand the canal&#39;s ringbuffer and the client&#39;s batchsize? The local test did not solve the problem. You link this idle when this exception Timeout timeout is best you need to observe the running stack I also encountered this problem and how to solve it. Server side batchMode is set to itemSize Then the batchSize on the client side is set to a smaller default timeout. 30s Can ack off Https github com alibaba canal issues 640 Take a look at this
598,An rds Mysql library configuration three instances only two can read the binlog A table of a library on rds is divided into a thousand sheets, so three instances are configured and then each filter is different. However, only two instances can be normally consumed to binlog logs. Another instance even if The table has data and can&#39;t be consumed. batchId Returned is 1 I also encountered similar problems. There is always an instance that does not have to get the postion information in the meta dat under the location instance file. Upgrade to version 1 0 26 is normal
597,com.alibaba.otter.canal.protocol.exception.CanalClientException: java.io.IOException: Broken pipe； There is no problem when the amount of data is small. When there is 10,000 data synchronization insertion, canal will report com alibaba otter canal protocol exception CanalClientException java.io.IOException: Broken pipe error The phenomenon is that canal has been reading and inserting tens of thousands of log loop calls, causing it to be inserted and deleted in mysql, then inserting and deleting all the time. What is the reason? Https github com alibaba canal issues 640 Take a look at this
596,In the canal cluster mode, the client calls the connector subscribe schema1. hi ，dear I encountered the following problems when using canal for development. 1. Problem Description In the canal cluster environment, the client calls the connector subscribe schema1. The root does not listen to the log of the table data changes in the schema1 database and changes the connector subscribe. The function of monitoring all the tables before restarting the client again is also invalid. The canal server is not restarted. Use but modify the zookeeper of the canal server to point to restart the canal server and resume listening to all the tables. If you call the subscribe method with the filter parameter again at the client, the above problem will be repeated. 2. surroundings Machine A canal server 1 Version 1 0 24 Machine B canal server 2 Version 1 0 24 Machine C zookeeper Is there any special requirement for the filter writing in the connector subscribe? Is there just a regular expression or is there any other rule? QQ group can&#39;t go in, so only email inquiries hope to receive your reply. Thank you. If the database to be subscribed to is the test table is user and log Call connector.subscribe("test.user test.log"); LS Correct Answer
595,'show master status' has an error! 2018-04-02 04:12:46.303 [destination = example address = /192.168.1.111:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just show master status 2018-04-02 04:12:46.307 [destination = example address = /192.168.1.111:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /192.168.1.111:3306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: command : 'show master status' has an error! pls check. you need (at least one of) the SUPER REPLICATION CLIENT privilege(s) for this operation You can set the database user permissions to GRANT SELECT SUPER REPLICATION SLAVE REPLICATION CLIENT SHOW VIEW ON *.* TO 'xxx'@'%' Or GRANT ALL PRIVILEGES ON *.* TO 'xxx'@'%' Try This setting is not easy to use. Is it because of the version? Will the solution be solved without sunsetyan?
594,canal Always in prepare to find start position just show master status 2018-04-19 14:43:12.055 [Thread-4] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - stop successful.... 2018-04-19 14:43:17.670 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2018-04-19 14:43:17.677 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [walleter/instance.properties] 2018-04-19 14:43:17.755 [main] WARN org.springframework.beans.TypeConverterDelegate - PropertyEditor [com.sun.beans.editors.EnumEditor] found through deprecated global PropertyEditorManager fallback - consider using a more isolated form of registration e.g. on the BeanWrapper/BeanFactory! 2018-04-19 14:43:17.886 [main] INFO c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-walleter 2018-04-19 14:43:17.921 [main] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - start successful.... 2018-04-19 14:43:17.952 [destination = walleter address = /10.45.14.188:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just show master status 2018-04-19 14:46:55.129 [Thread-4] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - stop CannalInstance for null-walleter 2018-04-19 14:46:55.134 [Thread-4] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - stop successful.... prepare to find start position just show master status This is not an abnormal state But the database postion has changed Canal has not changed Always been this movement From NetEase Mailbox Master On April 26, 2018 19:36，agapple<notifications@github.com> Write prepare to find start position just show master status This is not an abnormal state — You are receiving this because you authored the thread. Reply to this email directly view it on GitHub or mute the thread. @mersap Maybe your position configuration is too old. Mysql has deleted the corresponding binlog Maybe the default batchSize given in the example is too big. Just change the small point.
593,1.0.26 Is this version of the bug caused? Is the version of canal compatible with all versions of mysql5 x? My is 5 1 73 2018-04-01 14:11:50.712 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2018-04-01 14:11:51.147 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties] 2018-04-01 14:11:52.319 [main] WARN o.s.beans.GenericTypeAwarePropertyDescriptor - Invalid JavaBean property 'connectionCharset' being accessed! Ambiguous write methods found next to actually used [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.lang.String)]: [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.nio.charset.Charset)] 2018-04-01 14:11:52.700 [main] INFO c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-example 2018-04-01 14:11:52.774 [main] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - start successful.... 2018-04-01 14:11:52.815 [destination = example address = /192.168.1.110:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /192.168.1.110:3306 has an error retrying. caused by java.lang.NoSuchMethodError: method java.net.InetSocketAddress.getHostString with signature ()Ljava.lang.String; was not found. at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.<init>(MysqlConnector.java:53) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.<init>(MysqlConnector.java:63) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.<init>(MysqlConnection.java:70) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.buildMysqlConnection(MysqlEventParser.java:308) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.buildErosaConnection(MysqlEventParser.java:76) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:154) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(libgcj.so.10) [na:na] 2018-04-01 14:11:52.960 [destination = example address = /192.168.1.110:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.lang.NoSuchMethodError: method java.net.InetSocketAddress.getHostString with signature ()Ljava.lang.String; was not found. at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.<init>(MysqlConnector.java:53) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.<init>(MysqlConnector.java:63) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.<init>(MysqlConnection.java:70) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.buildMysqlConnection(MysqlEventParser.java:308) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.buildErosaConnection(MysqlEventParser.java:76) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:154) at java.lang.Thread.run(libgcj.so.10) ] 2018-04-01 14:11:52.965 [destination = example address = /192.168.1.110:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - parse events has an error java.lang.NullPointerException: null at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.afterDump(MysqlEventParser.java:137) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:249) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(libgcj.so.10) ~[na:na] java.lang.NoSuchMethodError: method java.net.InetSocketAddress.getHostString with signature ()Ljava.lang.String; was not found. You are this jdk version. The 25 version also has this problem jdk1 8 corresponding Placement i canal.instance.dbUsername=canal canal.instance.dbPassword=canal canal.instance.defaultDatabaseName=test canal.instance.connectionCharset=UTF-8 This I can only suspect that jdk&#39;s problem code level oracle jdk7 and jdk8 I have run through Have a specific recurrence method and then reopen it.
592,Turn on druid ddl true and use tsdb will change the table structure has been reported to column size is not Match problem Recently, I encountered a problem because druid always reported that I could not pass the parsing of our table structure, so I turned off druid&#39;s ddl and canal. query The identification of ddl If the column of the business table is updated at this time, for example, when the new event is added, the number of fields in the event will be large and the number of columns in the tableMeta will be small and an exception will be thrown. ` public TableMeta getTableMeta(String schema String table boolean useCache EntryPosition position) { TableMeta tableMeta = null; if (tableMetaTSDB != null) { //TODO I think this should be added to the useCache to determine if you use the cache to go from the cache. If you do not use the cache, go to the database directly. tableMeta = tableMetaTSDB.find(schema table); if (tableMeta == null) { // Because the condition changes, the first time the tableMeta is not taken, it needs to be retrieved from the db and recorded in the snapshot. String fullName = getFullName(schema table); try { ResultSetPacket packet = connection.query("show create table " + fullName); String createDDL = null; if (packet.getFieldValues().size() > 0) { createDDL = packet.getFieldValues().get(1); } // Forced to overwrite memory values tableMetaTSDB.apply(position schema createDDL "first"); tableMeta = tableMetaTSDB.find(schema table); } catch (IOException e) { throw new CanalParseException("fetch failed by table meta:" + fullName e); } } return tableMeta; } else { if (!useCache) { tableMetaDB.invalidate(getFullName(schema table)); } return tableMetaDB.getUnchecked(getFullName(schema table)); } }` If you turn off tsdb, you can&#39;t solve the problem of column mismatch. If DDL can&#39;t parse the DDL corresponding to the feedback, push the improvement of TSDB. The DDL error is because the underlying Druid package problem is not available for waiting for Druid update because of the use of the line. The TSDB problem is personally considered to be the logical problem of the tsdb branch in the Canal package. We temporarily stopped using tsdb. No new problems found Using a different package of fastsql and druid
591,About sqltype Some doubts hi， Look at CanalEntry java in SqlType is the type of field in java getSqlType is the value. Sorry, I didn&#39;t find the corresponding java type. Can I find it there? thx found it > found it where
590,Fix issue #586 event Size is not accurate CalculateSize changed from binlog event size to serializedSize memory size is more accurate and controllable [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=590) <br/>All committers have signed the CLA. The implementation principle of this getSerializedSize method is to traverse all the data for size calculation. There is some cost. There are already new programs. Get the length after serialization
589,Some questions about EventTransactionBuffer and MemoryEventStoreWithBuffer # My understanding now ## EventTransactionBuffer Sink will distribute the event to the EventTransactionBuffer. If a complete transaction is collected or the size limit of the buffer is reached, the flush will be triggered. The cached data will be sent to the MemoryEventStoreWithBuffer via the callback after the filter operation. ## MemoryEventStoreWithBuffer Receive a List Event should be just sent list transaction check if the space is enough to traverse the data in the list into the entries one by one Each time the data is retrieved, it is based on the incoming batchSize. The PositionRange will calculate the message after the completion. The header and end of the getPosition will be set to the ack point of the last TRANSACTIONBEGIN or TRANSACTIONEND in this message. Will be passed in the metadata management updateCursor in order to let the start or end of the transaction always recorded in the cursor # My question Now we are developing something that wants to use Transaction as a unit to send messages to the downstream. If the oversized transaction is split into multiple transaction packages, a Boolean value is recorded to indicate whether it is a transaction end package. The main contradiction is that MemoryEventStoreWithBuffer stores it. The granularity is the entry. So if you take it according to the batchSIze, you may get the data in the same transaction. If you take the data from MemoryEventStoreWithBuffer and then nest a cache like EventTransactionBuffer When ack, you also need to keep the original batchId to ack At present, I feel that I can implement the cache of the memory in my own MemoryTransactionStoreWithBuffer. The transaction package is used in this form. After the transaction is in the form of the transaction package, the initEventStore method in the CanalInstanceWithManager override can be used to replace the MemoryEventStoreWithBuffer with the MemoryTransactionStoreWithBuffer. I would like to ask about the design of the canal design at the time. Have you considered providing a put get ack in the transaction package? Then my current idea is not reasonable. I don’t know if my above statement is clear enough to reply. To consider the situation of large transactions, we generally do not recommend such a lot of work inside. You can support at the client level. For example, the client layer itself supports asynchronous ack. You can make a buffer yourself and call back a transaction. Thanks for suggesting that the client level can do this but I still have some considerations. At present, we have a built-in canal to provide a message middleware to provide downstream data subscription. Now it is necessary to send transaction granularity to the message middleware. The business is almost always a small transaction. The client layer getWithoutAck method takes out the batchSize data and puts it into a buffer. It feels that there is another layer in the code structure. In the case that the batchSize data contains multiple transactions, it is necessary to coordinate the sending of the message to the mq according to the transaction and after consuming a batchSize data. Go ack according to MessageId Canal Because the Trasaction sent to mq in this case has nothing to do with the MessageId pulled out from the canal. If the server guarantees that a Message is a transaction package, it can be sent directly to the ack logic. If you consider the server level, can you avoid the impact of large transactions? Consider flushing a transaction packet with a flag bit. If a small transaction is a separate transaction packet, the flag indicates that there is no subsequent large transaction. Since the buffer is full, it is cut into multiples, except that the last one is marked as having subsequent packets. It should be able to handle the problem of big business too. My demand is not implemented on the server side. After writing, the feeling is that the data that getWithoutAck takes out is a large transaction with a transaction package as a unit. If it is marked, it is recommended to operate on the server. Good thanks for the hard work If a big thing for a large table contains too much data it is easy to raise OOM problems before the project sets the error in the project. BatchSize 5 1024 The result of the client is often aborted. This is not a rare situation. A bad history system often modifies thousands of things. The data is even empty, and the amount of data in the table itself is not small. @shang7053 Well, there will be a cap on the big transaction that will cut off multiple flushes. Read your description and look at the source code 1 server-side canal instance transactionn size configuration of this parameter and the client side&#39;s batchSize setting should be set to be more reasonable 2 If the length of a complete transaction data exceeds the batchSize set by the client, for example, the actual length of a transaction is 1 5 times batchSize. The first time get gets a batchSize data again, it will only give me the rest. 0 5 times the data will not return the data of the next transaction to me. @zwangbo @jingshenbusi6530 At present, the message acquisition implemented in Canal is granular in the MemoryEventStoreWithBuffer, so the batchSize is only for how many Entryes are taken, and no truncation is returned for the transaction. So feeling 1. These two configurations do not have the necessary link batchSize mainly to see their own consumption efficiency adjustments just fine. 2. As long as there is enough data stored in the buff, every time you get a batchSize event, it will not be truncated by the transaction, but the transaction header will be set to the address that can be ack when it enters MemoryEventStoreWithBuffer, that is, when ACK is a MessageId. Always select the position of the head and tail of the transaction in a BatchSize group LS understands correctly
588,Could not find artifact com.alibaba.fastsql:fastsql:jar:2.0.0_preview_159 Appears when initializing the project Caused by: org.eclipse.aether.resolution.DependencyResolutionException: Could not find artifact com.alibaba.fastsql:fastsql:jar:2.0.0_preview_159 in central Currently included in the binary distribution package @agapple Binary file where the latest 228 386 did not see Https github com alibaba canal releases tag canal 1 0 26 preview 3 Here are the binary packages
587,Can this project be packaged into a jar? Can this project be packaged into a jar instead of a tar file project? Which of the main files is used to start which file is started? Take a look at the quickStart in the wiki
586,MemoryEventStoreWithBuffer event Size is not accurate MemoryEventStoreWithBuffer ` private long calculateSize(Event event) { // Return the size of the event directly in the binlog return event.getEntry().getHeader().getEventLength(); }` The binlog event is resolved by Canal. The actual memory usage is more than ten times that of event_length. * The memory usage of the instance calculated by bufferMemUnit is not accurate. Is it considered to change to event getEntry getSerializedSize? Test Results calculateSize 179，SerializedSize 1790 calculateSize 8291， ，SerializedSize 155000 Is there a problem with the size of the binlog will expand the field type and other information will be more inflated than before The new version 1 0 26 has been calculated according to the serialization size.
585,RowsQueryEvent does not support the black and white list of table names filter regex and filter black regex filter How to consider the source code LogEventConvert inside the ROWS_QUERY_LOG_EVENT ANNOTATE_ROWS_EVENT did not filter the table name In addition Also for the description of the canal instance filter query dml attribute in the document > Whether to ignore the DML query statement such as insert update delete Table mysql5 6 ROW mode can contain the query record of the statement mode A little bit does not understand set to true after DML excludes select or exclude insert update delete Table If I use canal is to want to synchronize data to another storage medium can be set to true RowsQueryLogEvent can only get the exact match when tableName cannot get the library name expression filter
584,1 0 19 version canal instance filter query dcl dml Filter failure 1 0 19 version canal instance filter query dcl dml Filter failure will still print logs such as update delete insert Are you set to true or false?
583,Consultation on database version 5 7 18 Currently our database is 5 7 18 but in your documentation it is supported 5 7 brackets and it is specifically noted that it is 5 7 13 So does it mean that only 5 7 13 is tested? Does it affect the 18 version? The 18 version has not been tested. You can do some evaluation. The general small version will not have binlog changes. Ok, we haven’t encountered a minor version in the official environment. I saw the change. Log does not seem to specify the change of binlog
582,Canal and mycat Can can be used with mycat? Can only connect directly to a physical MySQL Mycat is just a sql route, he can not produce binlog log canal is also difficult for women without rice
581,HA mode normally shuts down a server node zookeeper switch node after the client does not consume data I first started two servers to register to zookeeper. Client connects zookeeper and selects a node to subscribe. When I close the server node that is being subscribed to Zookeeper switched the node switch to another node But the client reports an error 16:13:46.186 [main] WARN c.a.o.c.c.impl.ClusterCanalConnector - something goes wrong when getWithoutAck data from server:/192.168.1.158:11111 com.alibaba.otter.canal.protocol.exception.CanalClientException: java.io.IOException: end of stream when reading header at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:224) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:207) at com.alibaba.otter.canal.client.impl.ClusterCanalConnector.getWithoutAck(ClusterCanalConnector.java:165) at ClientSample.start(ClientSample.java:43) at ClientSample.main(ClientSample.java:116) Caused by: java.io.IOException: end of stream when reading header at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.read(SimpleCanalConnector.java:307) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.readNextPacket(SimpleCanalConnector.java:295) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.receiveMessages(SimpleCanalConnector.java:229) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:222) ... 4 more Then subscribing to the data is just non-stop printing 16:19:29.846 [main-SendThread(192.168.0.81:2181)] DEBUG org.apache.zookeeper.ClientCnxn - Got ping response for sessionid: 0x161835c99bb1978 after 3ms Switch to the new server log 2018-04-10 15:40:09.114 [pool-2-thread-1] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2018-04-10 15:40:09.118 [pool-2-thread-1] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [gjj/instance.properties] 2018-04-10 15:40:09.284 [pool-2-thread-1] WARN o.s.beans.GenericTypeAwarePropertyDescriptor - Invalid JavaBean property 'connectionCharset' being accessed! Ambiguous write methods found next to actually used [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.nio.charset.Charset)]: [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.lang.String)] 2018-04-10 15:40:09.381 [pool-2-thread-1] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2018-04-10 15:40:09.387 [pool-2-thread-1] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [gjj/instance.properties] 2018-04-10 15:40:09.496 [New I/O server worker #1-1] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400 Caused by : something goes wrong with channel:[id: 0x6cf56c87 /192.168.43.133:54479 => /192.168.1.158:11111] exception=com.alibaba.otter.canal.server.exception.CanalServerException: destination:gjj should start first 2018-04-10 15:40:09.501 [New I/O server worker #1-1] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400 Caused by : something goes wrong with channel:[id: 0x6cf56c87 /192.168.43.133:54479 :> /192.168.1.158:11111] exception=com.alibaba.otter.canal.server.exception.CanalServerException: destination:gjj should start first 2018-04-10 15:40:09.509 [New I/O server worker #1-1] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x6cf56c87 /192.168.43.133:54479 :> /192.168.1.158:11111] exception=java.nio.channels.ClosedChannelException at org.jboss.netty.channel.socket.nio.NioWorker.cleanUpWriteBuffer(NioWorker.java:649) at org.jboss.netty.channel.socket.nio.NioWorker.writeFromUserCode(NioWorker.java:370) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:137) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) at org.jboss.netty.channel.Channels.write(Channels.java:611) at org.jboss.netty.channel.Channels.write(Channels.java:578) at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:30) at com.alibaba.otter.canal.server.netty.NettyUtils.error(NettyUtils.java:51) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:200) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.cleanup(ReplayingDecoder.java:542) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.channelDisconnected(ReplayingDecoder.java:450) at org.jboss.netty.channel.Channels.fireChannelDisconnected(Channels.java:360) at org.jboss.netty.channel.socket.nio.NioWorker.close(NioWorker.java:599) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:119) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) at org.jboss.netty.channel.Channels.close(Channels.java:720) at org.jboss.netty.channel.AbstractChannel.close(AbstractChannel.java:208) at org.jboss.netty.channel.ChannelFutureListener$1.operationComplete(ChannelFutureListener.java:46) at org.jboss.netty.channel.DefaultChannelFuture.notifyListener(DefaultChannelFuture.java:381) at org.jboss.netty.channel.DefaultChannelFuture.addListener(DefaultChannelFuture.java:148) at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:30) at com.alibaba.otter.canal.server.netty.NettyUtils.error(NettyUtils.java:51) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:200) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:444) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261) at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:350) at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) 2018-04-10 15:40:09.649 [pool-2-thread-1] ERROR com.alibaba.druid.pool.DruidDataSource - testWhileIdle is true validationQuery not set 2018-04-10 15:40:10.337 [pool-2-thread-1] INFO c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-gjj 2018-04-10 15:40:10.392 [pool-2-thread-1] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - subscribe filter change to bshousingfund.bsh_data_log 2018-04-10 15:40:10.392 [pool-2-thread-1] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - start successful.... 2018-04-10 15:40:11.537 [destination = gjj address = /192.168.0.72:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position {"identity":{"slaveId":-1 "sourceAddress":{"address":"192.168.0.72" "port":3306}} "postion":{"included":false "journalName":"mysql-bin.000017" "position":864668422 "serverId":2821031644 "timestamp":1523346001000}} 2018-04-10 15:41:13.152 [New I/O server worker #1-3] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - subscribe filter change to bshousingfund.bsh_data_log 2018-04-10 16:07:49.172 [New I/O server worker #1-4] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - subscribe filter change to bshousingfund.bsh_data_log I saw that it was up at the end and switched to the new server. The server is switched over but the client consumption cannot be consumed normally. @CXninesuns Encountered the same problem zk on the running instance has been switched address but the node on zk is still the old address so the client can not continue to consume data I use canal_1 0 20 This problem has arisen I do not know if you have solved the problem I suggest using the 26 alpha version to try to impress the high-availability switch to fix a few problems.
580,Unable to connect to the canal server Use the following method to create a canal connection `CanalConnector canalConnector = CanalConnectors.newSingleConnector(new InetSocketAddress(canalHost Integer.parseInt(canalPort)) canalDestination "" ""); canalConnector.connect();` But when the connect method is called, the error is reported as follows: ` Caused by: com.alibaba.otter.canal.protocol.exception.CanalClientException: java.net.ConnectException: Connection timed out: connect at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.doConnect(SimpleCanalConnector.java:178) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.connect(SimpleCanalConnector.java:102)` What is the reason? It has been good to start from today and suddenly reported this wrong version of the canal used for canal deployer 1 0 25 Find the reason is that the firewall of the server where the canal server is located intercepts the port 11111 and opens this port.
579,canal server Can&#39;t get RDS binlog? The situation is like this we mysql Is the rds canal of Alibaba Cloud Canal after server startup The client also starts and then writes data to the mysql database. At the beginning, it can be seen that the client has received the data and parsed it into hbase and the data is not lost. Then, after a while, the client will not receive the data. When I restart the canal After the server, I can see that the client can receive the data and parse it. The deployed otter also appears to be running for a few days or a month and cannot receive data from the can result in the synchronization channel stop on the otter. @songpengpeng Are you configuring multiple destinations? destination Is the serveid in each destination the same? @mjjian0 You are using the 啥 version recommended to use more than 1 0 24 @agapple I am using 1 0 25 version @agapple In this case meta.dat There is no information about cursor and postion inside. Several destination directories meta.dat Is the clientId the same? Is this relevant? @agapple Multiple instances When configured Canal instance rds instanceId parameter needs to be configured? Guaranteed unique What you see is rds The localbinlog parsing configuration file is not directly connected binlog @agapple Yes rds_instance.properties Not this one Parsing the configuration of the rds library? Rds_instance just pulls oss on rds Binlog is not used for parsing @agapple Um, our mysql is Aliyun. Rds then the configuration file I am using this Reference FAQ https://github.com/alibaba/canal/wiki/FAQ
578,Ask when the stable version of 1 0 26 is released. Thank you. Thank you for asking There are currently 12 known bugs that need to be fixed. Thank you for your reply. I would like to ask if there will be an approximate time. For example, will it be slightly more accurate in June or before eleven or the end? Thank you. Expected to be within nearly 12 months A year has passed Thank you very much for your reply The latest version has been released
577,Change the log level of the canal server to DEBUG Then infinitely fast print the following log and then get no change data. What is the reason? 2018-04-09 15:12:46.395 [destination = xxx_2 address = /xxx.xxx.xxx.xxx:3306 EventParser] DEBUG c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - set EntryPosition[included=false journalName=mysql-bin.000789 position=8764275 serverId=10294 timestamp=1523224556000] to be pending start position before finding another proper one... 2018-04-09 15:39:33.727 [destination = xxx address = /192.168.xxx.94:3306 EventParser] DEBUG c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - set EntryPosition[included=false journalName=mysql-bin.000789 position=12934350 serverId=xxx94 timestamp=1523224841000] to be pending start position before finding another proper one... 2018-04-09 15:39:33.804 [destination = xxx address = /192.168.xxx.94:3306 EventParser] DEBUG c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - compare exit condition:mysql-bin.000789 12934977 1523224841000 startTimestamp=1523259171081... 2018-04-09 15:39:33.805 [destination = xxx address = /192.168.xxx.94:3306 EventParser] DEBUG c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - set EntryPosition[included=false journalName=mysql-bin.000789 position=12934977 serverId=xxx94 timestamp=1523224841000] to be pending start position before finding another proper one... 2018-04-09 15:39:33.886 [destination = xxx address = /192.168.xxx.94:3306 EventParser] DEBUG c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - compare exit condition:mysql-bin.000789 12935073 1523224841000 startTimestamp=1523259171081... 2018-04-09 15:39:33.886 [destination = xxx address = /192.168.xxx.94:3306 EventParser] DEBUG c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - set EntryPosition[included=false journalName=mysql-bin.000789 position=12935073 serverId=xxx94 timestamp=1523224841000] to be pending start position before finding another proper one... 2018-04-09 15:39:33.972 [destination = xxx address = /192.168.xxx.94:3306 EventParser] DEBUG c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - compare exit condition:mysql-bin.000789 12935687 1523224841000 startTimestamp=1523259171081... 2018-04-09 15:39:33.972 [destination = xxx address = /192.168.xxx.94:3306 EventParser] DEBUG c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - set EntryPosition[included=false journalName=mysql-bin.000789 position=12935687 serverId=xxx94 timestamp=1523224841000] to be pending start position before finding another proper one... 2018-04-09 15:39:34.052 [destination = xxx address = /192.168.xxx.94:3306 EventParser] DEBUG c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - compare exit condition:mysql-bin.000789 12935783 1523224841000 startTimestamp=1523259171081... 2018-04-09 15:39:34.052 [destination = xxx address = /192.168.xxx.94:3306 EventParser] DEBUG c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - set EntryPosition[included=false journalName=mysql-bin.000789 position=12935783 serverId=xxx94 timestamp=1523224841000] to be pending start position before finding another proper one... 2018-04-09 15:39:34.128 [destination = xxx address = /192.168.xxx.94:3306 EventParser] DEBUG c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - compare exit condition:mysql-bin.000789 12938728 1523224841000 startTimestamp=1523259171081... 2018-04-09 15:39:34.128 [destination = xxx address = /192.168.xxx.94:3306 EventParser] DEBUG c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - set EntryPosition[included=false journalName=mysql-bin.000789 position=12938728 serverId=xxx94 timestamp=1523224841000] to be pending start position before finding another proper one... 2018-04-09 15:39:34.207 [destination = xxx address = /192.168.xxx.94:3306 EventParser] DEBUG c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - compare exit condition:mysql-bin.000789 12938824 1523224841000 startTimestamp=1523259171081... 2018-04-09 15:39:34.207 [destination = xxx address = /192.168.xxx.94:3306 EventParser] DEBUG c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - set EntryPosition[included=false journalName=mysql-bin.000789 position=12938824 serverId=xxx94 timestamp=1523224841000] to be pending start position before finding another proper one... 2018-04-09 15:39:34.280 [destination = xxx address = /192.168.xxx.94:3306 EventParser] DEBUG c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - compare exit condition:mysql-bin.000789 12939336 1523224841000 startTimestamp=1523259171081... This is the debug information printed when locating the site.
576,Canal deployer 1 0 26 SNAPSHOT error There are two instance configurations for example and example1. After a normal startup, the client will report an error after a while. java.lang.NullPointerException at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.writeWithHeader(SimpleCanalConnector.java:358) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.ack(SimpleCanalConnector.java:318) ack Null pointer for batchId Error code behavior connector ack batchId The code line and I have not changed it myself.
575,MySQLSyntaxErrorException: Duplicate column name 'REQUIRED' Inexplicable, the following problem has arisen, but the list is not duplicated. I don’t know what it is because of trouble. pid:1 nid:1 exception:setl:com.alibaba.otter.node.etl.load.exception.LoadException: java.util.concurrent.ExecutionException: com.alibaba.otter.node.etl.load.exception.LoadException: com.alibaba.otter.node.etl.load.exception.LoadException: org.springframework.jdbc.BadSqlGrammarException: StatementCallback; bad SQL grammar []; nested exception is com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Duplicate column name 'REQUIRED' Caused by: java.util.concurrent.ExecutionException: com.alibaba.otter.node.etl.load.exception.LoadException: com.alibaba.otter.node.etl.load.exception.LoadException: org.springframework.jdbc.BadSqlGrammarException: StatementCallback; bad SQL grammar []; nested exception is com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Duplicate column name 'REQUIRED' DDL repeat synchronization
574,Canal monitoring filtering problem binlog mode is Mixed Binlog mode is Mixed How to implement binlog monitoring only for a fixed library name Using version 1 0 26 currently supports filtering based on full SQL parsing
573,Connect to Ali RDS When the active/standby switchover occurs, the canal reports an error and restarts the canal.
572,A comment appears in the ddl of the binlog causing this message to be discarded. When dba operates ddl, for example, if a field is added, if there is a comment before the command, canal will not resolve the event. But if there is no previous line comment, it can be parsed into this binlog. ` /** * <code>optional bool isDdl = 10 [default = false];</code> * * <pre> ** Whether the identifier is a ddl statement * * </pre> */ public boolean getIsDdl() { return isDdl_; }` That is, if a comment is specified when ddl is executed `# at 708574121 #180403 0:33:33 server id 248037 end_log_pos 708574490 CRC32 0x0e8bdfed Query thread_id=15086636 exec_time=1647 error_code=0 SET TIMESTAMP=1522686813/*!*/; -- Comment comment ALTER TABLE on_ord_act_paid03 ADD receivables_mch_id VARCHAR(50) DEFAULT NULL COMMENT 'Foo' MODIFY COLUMN payment_method SMALLINT NOT NULL COMMENT 'Bar' /*!*/; # at 708574490 #180403 1:01:00 server id 248037 end_log_pos 708574578 CRC32 0x51d11ba5 Query thread_id=15086688 exec_time=0 error_code=0 SET TIMESTAMP=1522688460/*!*/;` Then this binlog&#39;s rowChange getIsDdl should currently return false If removed Comment comment Return true Current database related information Server version: 5.6.21-log Source distribution innodb_version 5.6.21 Binlog configuration information of the current instance `| binlog_cache_size | 2097152 | | binlog_checksum | CRC32 | | binlog_direct_non_transactional_updates | OFF | | binlog_format | ROW | | binlog_max_flush_queue_time | 0 | | binlog_order_commits | ON | | binlog_row_image | FULL | | binlog_rows_query_log_events | OFF | | binlog_stmt_cache_size | 2097152 | | binlogging_impossible_mode | IGNORE_ERROR | | innodb_api_enable_binlog | OFF | | innodb_locks_unsafe_for_binlog | OFF | | log_bin | ON | | log_bin_basename | /data/mysql_3306/mysql-bin | | log_bin_index | /data/mysql_3306/mysql-bin.index | | log_bin_trust_function_creators | OFF | | log_bin_use_v1_row_events | OFF | | max_binlog_cache_size | 18446744073709547520 | | max_binlog_size | 1073741824 | | max_binlog_stmt_cache_size | 18446744073709547520 | | simplified_binlog_gtid_recovery | OFF | | sql_log_bin | ON | | sync_binlog | 100 |` mysql.conf [mysqld] basedir=/usr/local/mysql datadir=/data/mysql_3306 socket=/data/mysql_3306/mysql.sock port=3306 server_id=248037 log-bin=mysql-bin binlog_format = ROW relay-log=mysqld-relay-bin sync_binlog = 100 relay_log_purge=0 log_slave_updates query_cache_type = 0 query_cache_size = 0 table_definition_cache = 2048 table_open_cache = 4096 table_open_cache_instances = 8 sql_mode=NO_ENGINE_SUBSTITUTION STRICT_TRANS_TABLES skip-name-resolve back_log = 100 max_connections = 1000 max_connect_errors = 100000 max_allowed_packet = 16M binlog_cache_size = 2M binlog_stmt_cache_size = 2M max_heap_table_size = 256M tmp_table_size = 8M sort_buffer_size = 8M thread_cache_size = 256 ft_min_word_len = 4 thread_stack = 192K long_query_time = 3 tmpdir = /tmp key_buffer_size = 8M read_buffer_size = 8M join_buffer_size = 8M read_rnd_buffer_size = 8M bulk_insert_buffer_size = 8M character_set_server = utf8 collation_server = utf8_general_ci transaction_isolation = REPEATABLE-READ default_storage_engine = InnoDB default_tmp_storage_engine = InnoDB innodb_buffer_pool_size = 30G innodb_file_per_table = 1 innodb_file_io_threads = 4 innodb_thread_concurrency = 32 innodb_flush_log_at_trx_commit = 2 innodb_log_buffer_size = 256M innodb_log_file_size = 512M innodb_log_files_in_group = 3 innodb_flush_method = O_DIRECT innodb_lock_wait_timeout = 120 innodb_open_files = 32768 innodb_online_alter_log_max_size = 32G slow_query_log = on long_query_time = 0.5 slow_query_log_file = mysql-slow [mysql] default-character-set=utf8 sock = /data/mysql_3306/mysql.sock [mysqld_safe] log-error=/data/mysql_3306/mysqld.err pid-file=/data/mysql_3306/mysqld.pid Try to use the latest 26 version of DDL-based SQL full parsing to support annotations
571,java.net.SocketTimeoutException 1、canal Server and client After running for a while, this abnormal restart still reports this exception. After deleting the meta dat and restarting, there is data again. Is this for you? 2、canal No matter whether the network or the physical machine has a problem, how to ensure that the data is not lost. ![default](https://user-images.githubusercontent.com/23053967/38133429-c0a550e4-3441-11e8-8080-79e82950539e.png) As shown in the figure, I want to restart the synchronization data restart from this location. Look at the wiki
570,‘FULLTEXT KEY’ and ‘ADD INDEX USING BTREE’ cause parse error v1.0.25 `parse faield : ALTER TABLE `xxxxx` ADD INDEX `idx_order_id` (`order_id`) USING BTREE com.alibaba.druid.sql.parser.ParserException: syntax error error in :'SING BTREE' expect IDENTIFIER actual IDENTIFIER pos 132 line 4 column 45 token IDENTIFIER BTREE at com.alibaba.druid.sql.parser.SQLParser.printError(SQLParser.java:284) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:421) ~[druid-1.1.6.jar:1.1.6] ` `2018-03-26 09:11:52.244 [destination = kd_caesar_yf address = test.kuaihuoyun.com/118.178.142.131:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `xxxx` ( `id` int(10) unsigned NOT NULL AUTO_INCREMENT COMMENT '' ... FULLTEXT KEY `ft_query_oid` (`query_oid`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='' com.alibaba.druid.sql.parser.ParserException: syntax error error in :' KEY `ft_query_oid` (`query_oid`) )' expect RPAREN actual IDENTIFIER pos 2255 line 37 column 16 token IDENTIFIER `ft_query_oid` at com.alibaba.druid.sql.parser.SQLParser.printError(SQLParser.java:284) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLParser.accept(SQLParser.java:292) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:191) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.apply(DatabaseTableMeta.java:104) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.apply(TableMetaCache.java:203) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseQueryEvent(LogEventConvert.java:194) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:107) [canal.parse-1.0.25.jar:na] ` Can provide a complete SQL or try the 1 0 26 version @agapple sql as follows ALTER TABLE xxxxx ADD INDEX idx_order_id(order_id) USING BTREE; CREATE TABLE xxxx ( id int(10) unsigned NOT NULL AUTO_INCREMENT COMMENT '' ft_query_oid varchar(32) NOT NULL default '' FULLTEXT KEY ft_query_oid(query_oid) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='' ; I have this problem also appear here. The specific code is as follows 2018-04-03 11:39:30.726 [destination = eam address = /172.16.XX.XX:XXXX EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : ALTER TABLE `TB_AT_CREDITCARD_WITHHOLD_INFO` ADD UNIQUE INDEX `IDX_UNIQ_IDCARD_ACCOUNT` (`CUSTOMER_ID_CARD` `CUSTOMER_BANK_ACCOUNT`) USING BTREE com.alibaba.druid.sql.parser.ParserException: syntax error error in :'SING BTREE' expect IDENTIFIER actual IDENTIFIER pos 139 line 2 column 96 token IDENTIFIER BTREE at com.alibaba.druid.sql.parser.SQLParser.printError(SQLParser.java:284) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:421) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.applyHistoryOnMemory(DatabaseTableMeta.java:387) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:121) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.25.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] Confirmed as a DDL parsing bug 26 will be fixed in the near future Fixed after upgrading fastsql Hello fastsql can only find 2 0 0_preview_186 and 2 0 0_preview_151 version Please ask the version address of 371 http://mvnrepository.com/artifact/com.alibaba.fastsql/fastsql I can download one from my binary package.
569,msyql 5.6.16-log canal server1.0.26 Parsing varchar fields appear > symbol msyql Ali cloud rds High availability version 5.6.16-log ，canal server Parsing varchar fields Chinese characters appear > symbol It’s garbled. It is best to give a reproducible test method
568,canal1.0.26/1.0.25 Delete meta dat Set canal instance master timestamp to 1522209915000 After startup, I found that after reading the canal message, I read the real of the message. Size is much less than the set batchSize sometimes even result is Null Excluding mysql&#39;s binlog records is not enough for any reason? real Size is the number of non-records obtained by memory size
567,Canal deployer 1 0 26 SNAPSHOT version rds can not get log information Configure local mysql and then operate to get operation information The operation on the rds cannot be obtained. Only configure instance properties. Do you need to configure rds_instance properties? Rds reported the error in the canal log 2018-03-28 15:28:06.345 [New I/O server worker #1-2] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x729a1c21 /192.168.10.156:64481 => /192.168.10.156:11111] exception=java.io.IOException: The remote host forcibly closed an existing connection at sun.nio.ch.SocketDispatcher.read0(Native Method) at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43) at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223) at sun.nio.ch.IOUtil.read(IOUtil.java:192) at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380) at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:322) at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) exception=java.io.IOException: The remote host forcibly closed an existing connection Estimate and canal Server broken chain The database is still running. I feel that the canal cannot connect to the database. How can I handle it?
566,canal After the server starts, it will generate a lot of unauthenticated user Non-authenticated user connection canal Server is unauthenticated after stopping user Non-authenticated user connection disappeared canal version 1.0.24，canal Server is the active/standby mode Recommended for 1 0 26 version
565,canal After the server starts, it will generate a lot of unauthenticated user Non-authenticated user connection canal Server is unauthenticated after stopping user The non-authenticated user connection disappears, but the database connection of the instance configuration is not disconnected and the link is in the sleep state. Canal version 1 0 24
564,CanalConnector connector=CanalConnectors.newClusterConnector(canalProperties.getZkServers() canalProperties.getDestination() "" ""); CanalConnector connector=CanalConnectors.newClusterConnector(canalProperties.getZkServers() canalProperties.getDestination() "" How to set password on the server side Https github com alibaba canal issues 192
563,How does the receiving end distinguish whether the record is a Row or a STATEMENT? How does the receiving end distinguish whether the record is a Row or a STATEMENT? String sql=rowChage.getSql(); Can you judge whether sql is empty or not? Read more about the documentation.
562,canal.instance.filter.regex The canal instance filter regex on the server side and the connector subscribe subscribe_pattern on the client side have a relationship. I tried to find out that the server side settings do not seem to work. connector.subscribe(subscribe_pattern) Is the best posture LS Correct Answer
561,How to reset the location of the Canal on the client. The current location is monitored by the remote Canal. The binlog has been deleted for a long time. message message getId 为 1 ## How to reset the location of Canal on the client side ### Problem Description The current location is monitored by mysql in remote Canal. The binlog has been deleted for a long time. message Message getId is always 1 message.getEntries().size() Forever 0 can never read the data ### doubt How can I change it? The configuration of the remote Canal service or other oh oh, the client resets the location to the latest Just ignore the binlog before listening to the latest binlog message. ## Canal Service destination configuration ``` canal.instance.mysql.slaveId = 617 # position info canal.instance.master.address = 10.1.6.226:3307 canal.instance.master.journal.name = canal.instance.master.position = canal.instance.master.timestamp = #canal.instance.standby.address = #canal.instance.standby.journal.name = #canal.instance.standby.position = #canal.instance.standby.timestamp = # username/password canal.instance.dbUsername = mescanal canal.instance.dbPassword = mescanal canal.instance.defaultDatabaseName = canal.instance.connectionCharset = UTF-8 # table regex canal.instance.filter.regex = .*\\..* # table black regex canal.instance.filter.black.regex = ``` ## meta.data ``` cat meta.dat {"clientDatas":[{"clientIdentity":{"clientId":1001 "destination":"mysql_b7_gaotaiunyue" "filter":".*\\..*"} "cursor":{"identity":{"slaveId":-1 "sourceAddress":{"address":"10.1.6.226" "port":3307}} "postion":{"included":false "journalName":"my3307-bin.000303" "position":77593731 "serverId":3307 "timestamp":1521010166000}}}] "destination":"mysql_b7_gaotaiunyue"} ``` ## This is the log code ``` 2018-03-22 10:41:54.051 [destination = mysql_b7_gaotianyue address = /10.1.6.226:3307 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /10.1.6.226:3307 has an error retrying. caused by java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210) ~[canal.parse-1.0.22.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_65] 2018-03-22 10:41:54.051 [destination = mysql_b7_gaotianyue address = /10.1.6.226:3307 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:mysql_b7_liupengchun[java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210) at java.lang.Thread.run(Thread.java:745) ``` Could not find first log file name in binary log index File location does not exist Can the same feeling be automatically reset to the latest site if the site is abnormally wrong? +1 More occurrences Ibid. If it is a case of rds such as Alibaba Cloud, a vip hanging backend master and backup will automatically relocate the rest of the mysql situation because the binlog is deleted even if repositioning is useless. +1 Reference FAQ : https://github.com/alibaba/canal/wiki/FAQ +1 Was killed
560,Start error PropertyAccessException 1: org.springframework.beans.MethodInvocationException: Property 'tsdbSpringXml' threw exception; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'tableMetaTSDB' defined in class path resource [spring/tsdb/h2-tsdb.xml]: Cannot resolve reference to bean 'metaHistoryDAO' while setting bean property 'metaHistoryDAO'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'metaHistoryDAO' defined in class path resource [spring/tsdb/h2-tsdb.xml]: Cannot resolve reference to bean 'sqlMapClient' while setting bean property 'sqlMapClient'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlMapClient' defined in class path resource [spring/tsdb/h2-tsdb.xml]: Cannot resolve reference to bean 'dataSource' while setting bean property 'dataSource'; nested exception is org.springframework.beans.factory.CannotLoadBeanClassException: Error loading class [com.alibaba.druid.pool.DruidDataSource] for bean with name 'dataSource' defined in class path resource [spring/tsdb/h2-tsdb.xml]: problem with class file or dependent class; nested exception is java.lang.LinkageError: loading constraint violated at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyPropertyValues(AbstractAutowireCapableBeanFactory.java:1453) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE] at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1158) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE] at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:519) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE] at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:458) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE] at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:296) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE] at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:223) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE] at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:293) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE] at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:194) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE] at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:328) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE] ... 30 common frames omitted Caused by: org.springframework.beans.PropertyBatchUpdateException: Failed properties: Property 'tsdbSpringXml' threw exception; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'tableMetaTSDB' defined in class path resource [spring/tsdb/h2-tsdb.xml]: Cannot resolve reference to bean 'metaHistoryDAO' while setting bean property 'metaHistoryDAO'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'metaHistoryDAO' defined in class path resource [spring/tsdb/h2-tsdb.xml]: Cannot resolve reference to bean 'sqlMapClient' while setting bean property 'sqlMapClient'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlMapClient' defined in class path resource [spring/tsdb/h2-tsdb.xml]: Cannot resolve reference to bean 'dataSource' while setting bean property 'dataSource'; nested exception is org.springframework.beans.factory.CannotLoadBeanClassException: Error loading class [com.alibaba.druid.pool.DruidDataSource] for bean with name 'dataSource' defined in class path resource [spring/tsdb/h2-tsdb.xml]: problem with class file or dependent class; nested exception is java.lang.LinkageError: loading constraint violated at org.springframework.beans.AbstractPropertyAccessor.setPropertyValues(AbstractPropertyAccessor.java:101) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE] at org.springframework.beans.AbstractPropertyAccessor.setPropertyValues(AbstractPropertyAccessor.java:57) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE] at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyPropertyValues(AbstractAutowireCapableBeanFactory.java:1450) ~[spring-beans-3.2.9.RELEASE.jar:3.2.9.RELEASE] ... 38 common frames omitted java.lang.LinkageError: loading constraint Violed has done secondary development
559,canal.instance.rds.startTime Cancan support Alibaba Cloud&#39;s RDS but canal.instance.rds.startTime= canal.instance.rds.endTime= I don&#39;t know how to set the time format. How to see this time in the Alibaba Cloud RDS console Thank you This is getting rds oss Binlog parsing this time is standard GMT 0 time zone time
558,com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. More content descriptions ![image](https://user-images.githubusercontent.com/18360996/37639699-6a6d3832-2c4d-11e8-9004-32a054adc0de.png) Otter also often has a suspended animation. There is no log channel. The pipeline is not suspended. It is also working. But after the source library data changes, there is no operation to restart the channel. Basically, you can recover. I don’t know what the reason is. otter Manager I don&#39;t know if it&#39;s the way we use it. If I have hundreds of tables that need to synchronize data, there is no quicker configuration method. Otter&#39;s log section suggests strengthening a bunch of logs but not much valuable information output
557,Start the error, I really admire Ali’s programmers, blowing NB’s very powerful things, I’m not flattering. 2018-03-15 13:35:46.673 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2018-03-15 13:35:46.677 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties] 2018-03-15 13:35:46.782 [main] WARN o.s.beans.GenericTypeAwarePropertyDescriptor - Invalid JavaBean property 'connectionCharset' being accessed! Ambiguous write methods found next to actually used [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.nio.charset.Charset)]: [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.lang.String)] 2018-03-15 13:35:46.786 [main] INFO c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-example 2018-03-15 13:35:47.162 [main] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - subscribe filter change to olot\..* 2018-03-15 13:35:47.163 [main] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - start successful.... 2018-03-15 13:35:47.163 [destination = example address = /127.0.0.1:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position by switch ::1520690568000 2018-03-15 13:35:47.445 [destination = example address = /127.0.0.1:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - Didn't find the corresponding binlog files from mysql-bin.000001 to mysql-bin.000002 2018-03-15 13:35:47.447 [destination = example address = /127.0.0.1:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /127.0.0.1:3306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: can't find start position for example 2018-03-15 13:35:47.449 [destination = example address = /127.0.0.1:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: can't find start position for example ] 2018-03-15 13:35:47.452 [nioEventLoopGroup-2-5] ERROR c.a.o.canal.parse.driver.mysql.socket.SocketChannelPool - business error. java.lang.NullPointerException: null at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannel.writeCache(SocketChannel.java:34) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannelPool$BusinessHandler.channelRead0(SocketChannelPool.java:96) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannelPool$BusinessHandler.channelRead0(SocketChannelPool.java:73) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) ~[netty-all-4.1.6.Final.jar:4.1.6.Final] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:373) [netty-all-4.1.6.Final.jar:4.1.6.Final] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:359) [netty-all-4.1.6.Final.jar:4.1.6.Final] at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:351) [netty-all-4.1.6.Final.jar:4.1.6.Final] at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1334) [netty-all-4.1.6.Final.jar:4.1.6.Final] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:373) [netty-all-4.1.6.Final.jar:4.1.6.Final] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:359) [netty-all-4.1.6.Final.jar:4.1.6.Final] at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:926) [netty-all-4.1.6.Final.jar:4.1.6.Final] at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:129) [netty-all-4.1.6.Final.jar:4.1.6.Final] at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:651) [netty-all-4.1.6.Final.jar:4.1.6.Final] at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:574) [netty-all-4.1.6.Final.jar:4.1.6.Final] at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:488) [netty-all-4.1.6.Final.jar:4.1.6.Final] at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:450) [netty-all-4.1.6.Final.jar:4.1.6.Final] at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:873) [netty-all-4.1.6.Final.jar:4.1.6.Final] at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) [netty-all-4.1.6.Final.jar:4.1.6.Final] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131] 2018-03-15 13:35:57.859 [destination = example address = /127.0.0.1:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position by switch ::1520690568000 2018-03-15 13:35:58.136 [destination = example address = /127.0.0.1:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - Didn't find the corresponding binlog files from mysql-bin.000001 to mysql-bin.000002 2018-03-15 13:35:58.137 [destination = example address = /127.0.0.1:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /127.0.0.1:3306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: can't find start position for example 2018-03-15 13:35:58.137 [destination = example address = /127.0.0.1:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: can't find start position for example ] 2018-03-15 13:35:58.145 [nioEventLoopGroup-2-3] ERROR c.a.o.canal.parse.driver.mysql.socket.SocketChannelPool - business error. java.lang.NullPointerException: null at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannel.writeCache(SocketChannel.java:34) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannelPool$BusinessHandler.channelRead0(SocketChannelPool.java:96) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannelPool$BusinessHandler.channelRead0(SocketChannelPool.java:73) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) ~[netty-all-4.1.6.Final.jar:4.1.6.Final] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:373) [netty-all-4.1.6.Final.jar:4.1.6.Final] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:359) [netty-all-4.1.6.Final.jar:4.1.6.Final] at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:351) [netty-all-4.1.6.Final.jar:4.1.6.Final] at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1334) [netty-all-4.1.6.Final.jar:4.1.6.Final] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:373) [netty-all-4.1.6.Final.jar:4.1.6.Final] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:359) [netty-all-4.1.6.Final.jar:4.1.6.Final] at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:926) [netty-all-4.1.6.Final.jar:4.1.6.Final] at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:129) [netty-all-4.1.6.Final.jar:4.1.6.Final] at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:651) [netty-all-4.1.6.Final.jar:4.1.6.Final] at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:574) [netty-all-4.1.6.Final.jar:4.1.6.Final] at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:488) [netty-all-4.1.6.Final.jar:4.1.6.Final] at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:450) [netty-all-4.1.6.Final.jar:4.1.6.Final] at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:873) [netty-all-4.1.6.Final.jar:4.1.6.Final] at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) [netty-all-4.1.6.Final.jar:4.1.6.Final] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131] If there is a problem, there is no need to go online to a snapshot version.
556,Centos can be edited [INFO] Total time: 2.336s [INFO] Finished at: Thu Mar 15 13:48:55 CST 2018 [INFO] Final Memory: 18M/248M [INFO] ------------------------------------------------------------------------ [ERROR] Failed to execute goal on project canal.parse: Could not resolve dependencies for project com.alibaba.otter:canal.parse:jar:1.0.26-SNAPSHOT: Failed to collect dependencies for [com.alibaba.otter:canal.common:jar:1.0.26-SNAPSHOT (compile) com.alibaba.otter:canal.protocol:jar:1.0.26-SNAPSHOT (compile) com.alibaba.otter:canal.meta:jar:1.0.26-SNAPSHOT (compile) com.alibaba.otter:canal.sink:jar:1.0.26-SNAPSHOT (compile) com.alibaba.otter:canal.parse.dbsync:jar:1.0.26-SNAPSHOT (compile) com.alibaba.otter:canal.filter:jar:1.0.26-SNAPSHOT (compile) com.alibaba.otter:canal.parse.driver:jar:1.0.26-SNAPSHOT (compile) com.alibaba:druid:jar:1.1.9 (compile) com.alibaba.fastsql:fastsql:jar:2.0.0_preview_135 (compile) mysql:mysql-connector-java:jar:5.1.40 (compile) org.apache.ibatis:ibatis-sqlmap:jar:2.3.4.726 (compile) com.h2database:h2:jar:1.4.196 (compile) org.apache.httpcomponents:httpclient:jar:4.5.1 (compile) org.apache.commons:commons-compress:jar:1.9 (compile) junit:junit:jar:4.12 (test) org.springframework:spring-test:jar:3.2.9.RELEASE (test)]: Failed to read artifact descriptor for com.alibaba.fastsql:fastsql:jar:2.0.0_preview_135: Could not transfer artifact com.alibaba.fastsql:fastsql:pom:2.0.0_preview_135 from/to alibaba (http://code.alibabatech.com/mvn/releases/): Connection to http://code.alibabatech.com refused: Connection refused (Connection refused) -> [Help 1] [ERROR] [ERROR] To see the full stack trace of the errors re-run Maven with the -e switch. [ERROR] Re-run Maven using the -X switch to enable full debug logging. [ERROR] [ERROR] For more information about the errors and possible solutions please read the following articles: [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException [ERROR] Re-update the code fastsql code has been uploaded to the maven repository
555,renew an InetSocketAddress to resolve address again resolve [issue427](https://github.com/alibaba/otter/issues/427) [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=555) <br/>All committers have signed the CLA. [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=555) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=555) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=555) it.</sub> tks
554,javax.mail.AuthenticationFailedException: 526 Authentication failure Environment Ali cloud enterprise mailbox SSL port SMTP Request user verification 2018-03-12 21:51:02.457 ERROR c.a.otter.manager.biz.common.alarm.AbstractAlarmService - send alarm [AlarmMessage[message=pid:1 nid:2 exception:canal:ZjcsToDsb:java.io.IOException: socket read timeout occured ! readSize = 4 readableBytes = 0 timeout = 16000 at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannel.read(SocketChannel.java:172) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:174) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:80) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:144) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) at java.lang.Thread.run(Thread.java:745) receiveKey=otterteam]] to drgoon agent error! org.springframework.mail.MailAuthenticationException: Authentication failed; nested exception is javax.mail.AuthenticationFailedException: 526 Authentication failure[0] at org.springframework.mail.javamail.JavaMailSenderImpl.doSend(JavaMailSenderImpl.java:392) ~[spring-context-support-3.1.2.RELEASE.jar:3.1.2.RELEASE] at org.springframework.mail.javamail.JavaMailSenderImpl.send(JavaMailSenderImpl.java:306) ~[spring-context-support-3.1.2.RELEASE.jar:3.1.2.RELEASE] at org.springframework.mail.javamail.JavaMailSenderImpl.send(JavaMailSenderImpl.java:296) ~[spring-context-support-3.1.2.RELEASE.jar:3.1.2.RELEASE] at com.alibaba.otter.manager.biz.common.alarm.DefaultAlarmService.doSendMail(DefaultAlarmService.java:62) ~[manager.biz-4.2.16-SNAPSHOT.jar:na] at com.alibaba.otter.manager.biz.common.alarm.DefaultAlarmService.doSend(DefaultAlarmService.java:52) ~[manager.biz-4.2.16-SNAPSHOT.jar:na] at com.alibaba.otter.manager.biz.common.alarm.AbstractAlarmService.sendAlarmInternal(AbstractAlarmService.java:60) [manager.biz-4.2.16-SNAPSHOT.jar:na] at com.alibaba.otter.manager.biz.common.alarm.AbstractAlarmService.access$000(AbstractAlarmService.java:38) [manager.biz-4.2.16-SNAPSHOT.jar:na] at com.alibaba.otter.manager.biz.common.alarm.AbstractAlarmService$1.run(AbstractAlarmService.java:77) [manager.biz-4.2.16-SNAPSHOT.jar:na] at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) [na:1.7.0_79] at java.util.concurrent.FutureTask.run(FutureTask.java:262) [na:1.7.0_79] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_79] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_79] at java.lang.Thread.run(Thread.java:745) [na:1.7.0_79] Caused by: javax.mail.AuthenticationFailedException: 526 Authentication failure[0] at com.sun.mail.smtp.SMTPTransport$Authenticator.authenticate(SMTPTransport.java:826) ~[mail-1.4.7.jar:1.4.7] at com.sun.mail.smtp.SMTPTransport.authenticate(SMTPTransport.java:761) ~[mail-1.4.7.jar:1.4.7] at com.sun.mail.smtp.SMTPTransport.protocolConnect(SMTPTransport.java:685) ~[mail-1.4.7.jar:1.4.7] at javax.mail.Service.connect(Service.java:295) ~[mail-1.4.7.jar:1.4.7] at org.springframework.mail.javamail.JavaMailSenderImpl.doSend(JavaMailSenderImpl.java:389) ~[spring-context-support-3.1.2.RELEASE.jar:3.1.2.RELEASE] ... 12 common frames omitted JavaMailSenderImpl only tuned 163 and gmail other needs to do some debugging
553,Modify optimization to automatically clean or expand memory when the default cache size is not enough. Try to avoid frequent allocation or move memory control memory maximum allocation space or abnormal self-healing. All these checks and processing are done in the write cache thread. Fixing Deactivating Channels in Synchronization Management May Cause Related Node Processes to Crash A fatal error has been detected by the Java Runtime Environment: EXCEPTION_ACCESS_VIOLATION (0xc0000005) This is the succession #536 Continue to improve, it has been running stably 4 Day cooperation otter of PR currently supports large attachments over the actual measurement 8 MB ）。 note The log added by this modification does exist. #548 The dislocation phenomenon mentioned in the first contact canal Still don&#39;t know much about to fix The code has been submitted to Dcanal socketChannel netty Can switch Received thanks
552,The bug is too many canal instance master position does not work or report a little useful information canal.instance.master.journal.name=mysql-bin.000001 canal.instance.master.position=1 canal.instance.master.timestamp=1509862400000 Set the offset but it doesn&#39;t work or scan from the last line WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position The minimum position is 4
551,Execute a commented ddl statement canal parsing ddl eventType is QUERY binlog for row mode is a bug Canal analysis Execution statement with comments Test start #2018-02-22 Test end alter table test_2018 add loan_amount_backstage_adjust1 varchar(64) DEFAULT NULL COMMENT Test content 1 no 2 Analysis result eventType: QUERY isDdl：false sql: "#\305\233\304\266\304\215\305\244\304\256\304\223start\r\n#2018-02-22\r\n#\305\233\304\266\304\215\305\244\304\256\304\223end\r\nalter table test_2018 add loan_amount_backstage_adjust varchar(64) DEFAULT NULL COMMENT \'\305\233\304\266\304\215\305\244\304\256\304\223\305\232\303\234\303\226\305\232\304\243\304\273\303\224\304\276\303\2661\303\224\304\276\303\266\305\233\303\263\342\200\240\303\224\304\276\304\2062\303\224\304\276\303\266\305\233\303\272\304\214\'" ddlSchemaName: "test" —————————————————————————————————————————— Execute statement without comment alter table test_2018 add loan_amount_backstage_adjust1 varchar(64) DEFAULT NULL COMMENT Test content 1 no 2 Analysis result eventType: ALTER isDdl: true sql: "alter table test_2018 add loan_amount_backstage_adjust1 varchar(64) DEFAULT NULL COMMENT \'\305\233\304\266\304\215\305\244\304\256\304\223\305\232\303\234\303\226\305\232\304\243\304\273\303\224\304\276\303\2661\303\224\304\276\303\266\305\233\303\263\342\200\240\303\224\304\276\304\2062\303\224\304\276\303\266\305\233\303\272\304\214\'" ddlSchemaName: "test" Canal parsing ddl eventType is QUERY binlog and is row mode insert update delete Is it normal? Is this canal not considered? What version of mysql Is it a note with a number? @agapple Yes @fefine 5.6.29 @agapple As long as the note with the number Parsing eventType QUERY Upgrade canal to the latest version can be solved @liuwenfeng554 I feel that this problem is a pit. Is the upgrade solved? @renjin1984 It is said that the new version Not stable enough Temporarily not rising Follow-up if you have time to look
550,com.alibaba.otter.canal.protocol.exception.CanalClientException: java.io.IOException: Broken pipe This is the problem that runs the example sample code after installing the canal. com.alibaba.otter.canal.protocol.exception.CanalClientException: java.io.IOException: Broken pipe at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:296) ~[canal.client-1.0.25.jar:na] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:269) ~[canal.client-1.0.25.jar:na] at com.iwaimai.CanalTest.AbstractCanalClientTest.process(AbstractCanalClientTest.java:116) ~[classes/:na] at com.iwaimai.CanalTest.AbstractCanalClientTest$2.run(AbstractCanalClientTest.java:83) [classes/:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_162] Caused by: java.io.IOException: Broken pipe at sun.nio.ch.FileDispatcherImpl.write0(Native Method) ~[na:1.8.0_162] at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47) ~[na:1.8.0_162] at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93) ~[na:1.8.0_162] at sun.nio.ch.IOUtil.write(IOUtil.java:65) ~[na:1.8.0_162] at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:471) ~[na:1.8.0_162] at java.nio.channels.Channels.writeFullyImpl(Channels.java:78) ~[na:1.8.0_162] at java.nio.channels.Channels.writeFully(Channels.java:98) ~[na:1.8.0_162] at java.nio.channels.Channels.access$000(Channels.java:61) ~[na:1.8.0_162] at java.nio.channels.Channels$1.write(Channels.java:174) ~[na:1.8.0_162] at java.nio.channels.Channels$WritableByteChannelImpl.write(Channels.java:458) ~[na:1.8.0_162] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.writeWithHeader(SimpleCanalConnector.java:382) ~[canal.client-1.0.25.jar:na] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.writeWithHeader(SimpleCanalConnector.java:369) ~[canal.client-1.0.25.jar:na] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:281) ~[canal.client-1.0.25.jar:na] ... 4 common frames omitted 10:26:39.350 [Thread-0] ERROR c.i.C.AbstractCanalClientTest - process error! com.alibaba.otter.canal.protocol.exception.CanalClientException: something goes wrong with reason: something goes wrong with channel:[id: 0x57943e05 /10.11.61.139:53053 => /10.11.61.139:11111] exception=com.alibaba.otter.canal.server.exception.CanalServerException: destination:example should start first at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.receiveMessages(SimpleCanalConnector.java:317) ~[canal.client-1.0.25.jar:na] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:294) ~[canal.client-1.0.25.jar:na] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:269) ~[canal.client-1.0.25.jar:na] at com.iwaimai.CanalTest.AbstractCanalClientTest.process(AbstractCanalClientTest.java:116) ~[classes/:na] at com.iwaimai.CanalTest.AbstractCanalClientTest$2.run(AbstractCanalClientTest.java:83) [classes/:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_162] 10:26:39.351 [Thread-0] ERROR c.i.C.AbstractCanalClientTest - process error! com.alibaba.otter.canal.protocol.exception.CanalClientException: something goes wrong with reason: something goes wrong with channel:[id: 0x4f01a4c1 /10.11.61.139:53054 => /10.11.61.139:11111] exception=com.alibaba.otter.canal.server.exception.CanalServerException: destination:example should start first at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.receiveMessages(SimpleCanalConnector.java:317) ~[canal.client-1.0.25.jar:na] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:294) ~[canal.client-1.0.25.jar:na] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:269) ~[canal.client-1.0.25.jar:na] at com.iwaimai.CanalTest.AbstractCanalClientTest.process(AbstractCanalClientTest.java:116) ~[classes/:na] at com.iwaimai.CanalTest.AbstractCanalClientTest$2.run(AbstractCanalClientTest.java:83) [classes/:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_162] 10:26:39.352 [Thread-0] ERROR c.i.C.AbstractCanalClientTest - process error! com.alibaba.otter.canal.protocol.exception.CanalClientException: something goes wrong with reason: something goes wrong with channel:[id: 0x7e3c718f /10.11.61.139:53055 => /10.11.61.139:11111] exception=com.alibaba.otter.canal.server.exception.CanalServerException: destination:example should start first at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.receiveMessages(SimpleCanalConnector.java:317) ~[canal.client-1.0.25.jar:na] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:294) ~[canal.client-1.0.25.jar:na] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:269) ~[canal.client-1.0.25.jar:na] at com.iwaimai.CanalTest.AbstractCanalClientTest.process(AbstractCanalClientTest.java:116) ~[classes/:na] at com.iwaimai.CanalTest.AbstractCanalClientTest$2.run(AbstractCanalClientTest.java:83) [classes/:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_162] The client does a retry Upgrade the 26 version
549,Parsing errors and asking for help 2018-03-07 16:33:44.617 [destination = example address = /127.0.0.1:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `procs_priv` ( `Host` char(60) COLLATE utf8_bin NOT NULL DEFAULT '' `Db` char(64) COLLATE utf8_bin NOT NULL DEFAULT '' `User` char(16) COLLATE utf8_bin NOT NULL DEFAULT '' `Routine_name` char(64) CHARACTER SET utf8 NOT NULL DEFAULT '' `Routine_type` enum('FUNCTION' 'PROCEDURE') COLLATE utf8_bin NOT NULL `Grantor` char(77) COLLATE utf8_bin NOT NULL DEFAULT '' `Proc_priv` set('Execute' 'Alter Routine' 'Grant') CHARACTER SET utf8 NOT NULL DEFAULT '' `Timestamp` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP PRIMARY KEY (`Host` `Db` `User` `Routine_name` `Routine_type`) KEY `Grantor` (`Grantor`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='Procedure privileges' com.alibaba.druid.sql.parser.ParserException: syntax error error in :'RE') COLLATE utf8_bin NOT NULL `' expect RPAREN actual IDENTIFIER pos 313 line 6 column 47 token IDENTIFIER COLLATE at com.alibaba.druid.sql.parser.SQLParser.printError(SQLParser.java:284) ~[druid-1.1.8.jar:1.1.8] at com.alibaba.druid.sql.parser.SQLParser.accept(SQLParser.java:292) ~[druid-1.1.8.jar:1.1.8] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:217) ~[druid-1.1.8.jar:1.1.8] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:243) ~[druid-1.1.8.jar:1.1.8] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:161) ~[druid-1.1.8.jar:1.1.8] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:72) ~[druid-1.1.8.jar:1.1.8] at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.8.jar:1.1.8] at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:297) ~[druid-1.1.8.jar:1.1.8] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:71) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-03-07 16:33:44.801 [destination = example address = /127.0.0.1:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /127.0.0.1:3306 has an error retrying. caused by java.lang.NullPointerException: null at com.alibaba.druid.sql.dialect.mysql.visitor.MySqlOutputVisitor.visit(MySqlOutputVisitor.java:611) ~[druid-1.1.8.jar:1.1.8] at com.alibaba.druid.sql.ast.expr.SQLCharExpr.accept0(SQLCharExpr.java:50) ~[druid-1.1.8.jar:1.1.8] at com.alibaba.druid.sql.ast.SQLObjectImpl.accept(SQLObjectImpl.java:41) ~[druid-1.1.8.jar:1.1.8] at com.alibaba.druid.sql.dialect.mysql.visitor.MySqlOutputVisitor.visit(MySqlOutputVisitor.java:275) ~[druid-1.1.8.jar:1.1.8] at com.alibaba.druid.sql.ast.statement.SQLColumnDefinition.accept0(SQLColumnDefinition.java:167) ~[druid-1.1.8.jar:1.1.8] at com.alibaba.druid.sql.ast.SQLObjectImpl.accept(SQLObjectImpl.java:41) ~[druid-1.1.8.jar:1.1.8] at com.alibaba.druid.sql.visitor.SQLASTOutputVisitor.printTableElements(SQLASTOutputVisitor.java:2567) ~[druid-1.1.8.jar:1.1.8] at com.alibaba.druid.sql.dialect.mysql.visitor.MySqlOutputVisitor.visit(MySqlOutputVisitor.java:457) ~[druid-1.1.8.jar:1.1.8] at com.alibaba.druid.sql.dialect.mysql.ast.statement.MySqlCreateTableStatement.accept0(MySqlCreateTableStatement.java:89) ~[druid-1.1.8.jar:1.1.8] at com.alibaba.druid.sql.dialect.mysql.ast.statement.MySqlCreateTableStatement.accept0(MySqlCreateTableStatement.java:82) ~[druid-1.1.8.jar:1.1.8] at com.alibaba.druid.sql.ast.SQLObjectImpl.accept(SQLObjectImpl.java:41) ~[druid-1.1.8.jar:1.1.8] at com.alibaba.druid.sql.dialect.mysql.ast.statement.MySqlCreateTableStatement.output(MySqlCreateTableStatement.java:359) ~[druid-1.1.8.jar:1.1.8] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.snapshot(MemoryTableMeta.java:138) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.applySnapshotToDB(DatabaseTableMeta.java:241) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:129) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-03-07 16:33:44.802 [destination = example address = /127.0.0.1:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.lang.NullPointerException at com.alibaba.druid.sql.dialect.mysql.visitor.MySqlOutputVisitor.visit(MySqlOutputVisitor.java:611) at com.alibaba.druid.sql.ast.expr.SQLCharExpr.accept0(SQLCharExpr.java:50) at com.alibaba.druid.sql.ast.SQLObjectImpl.accept(SQLObjectImpl.java:41) at com.alibaba.druid.sql.dialect.mysql.visitor.MySqlOutputVisitor.visit(MySqlOutputVisitor.java:275) at com.alibaba.druid.sql.ast.statement.SQLColumnDefinition.accept0(SQLColumnDefinition.java:167) at com.alibaba.druid.sql.ast.SQLObjectImpl.accept(SQLObjectImpl.java:41) at com.alibaba.druid.sql.visitor.SQLASTOutputVisitor.printTableElements(SQLASTOutputVisitor.java:2567) at com.alibaba.druid.sql.dialect.mysql.visitor.MySqlOutputVisitor.visit(MySqlOutputVisitor.java:457) at com.alibaba.druid.sql.dialect.mysql.ast.statement.MySqlCreateTableStatement.accept0(MySqlCreateTableStatement.java:89) at com.alibaba.druid.sql.dialect.mysql.ast.statement.MySqlCreateTableStatement.accept0(MySqlCreateTableStatement.java:82) at com.alibaba.druid.sql.ast.SQLObjectImpl.accept(SQLObjectImpl.java:41) at com.alibaba.druid.sql.dialect.mysql.ast.statement.MySqlCreateTableStatement.output(MySqlCreateTableStatement.java:359) at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.snapshot(MemoryTableMeta.java:138) at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.applySnapshotToDB(DatabaseTableMeta.java:241) at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:129) at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) at java.lang.Thread.run(Thread.java:748) ] Druid does not currently support enum type submitted issue Https github com alibaba canal releases tag canal 1 0 26 preview 2 Try using this version of the package
548,For binlog Dump itself is blocking read and write using simpler BIO This pr will cover other open pr 536，539。 Recent tests have found some other issues with SocketChannel When constructing a new SocketChannel, the EventLoopGroup adjusts the next nearby EventLoop to register. After a long-running session of an instance, if the interrupt is interrupted multiple times, it may round up to the EventLoop of a channel of another instance. Since the MDC log is not set before and after logging the log, it will be recorded. Multiple event channels are shared by multiple channels in other instance log files. Using time slices will make reading performance worse if consumed quickly. BIO handles flash and interrupt with smaller SO_TIMEOUT 1s Other changes READ_TIMEOUT_MILLISECONDS modified to MASTER_HEARTBEAT_PERIOD_SECONDS + 10) * 1000. The 1s case occurred several times in the test of an instance without traffic placement for one day. Timeout frequency is about a few hours @lcybo I do not know #547 Is the problem related to this? @lcybo According to your previous suggestions, I am right. #536 The modification is slightly adjusted. I think the asynchronous mode is also good. The reading and writing can be performed asynchronously. @wingerx When you press the sync and asynchronous modes separately, try again recently because you find that there are other problems, etc. @wingerx Thank you for your test. Please ask the position and show in the meta of the canal when the pressure is stopped. master The gap between status is how much the network traffic of the interface used by dump is what it is. @Wu-Jianqiang BIO and NIO each have their own advantages for each scenario. MySQL Dump, after the handshake successfully subscribes to the specified location, canal unilaterally receive MySQL data read and write can be asynchronous netty The handler does just write the read data to the cache in essence or read the long connection for such a finite number of constants. I think that traditional BIO is more suitable. If you continue to use Netty, there are still some problems to be solved. If multiple channels are registered to the same EventLoop multiple interrupts reconnect or modify the io netty eventLoopThreads multiple instances of business The error log is logged to an instance and an EventLoop handles multiple channels at the same time. Instead, it affects performance. Implementing an EventExecutorChooser guarantees an EventLoop a Channel record. MDC is also needed when error dispatch. The code has been merged and the design is currently reserved for bio and netty for easy verification and comparison.
547,Canal Server Parsing performance issues I don&#39;t know if someone has ever measured it. Canal Analytical performance? Current pressure measurement v1.0.26 alpha 1 Modifying the code to mask possible reasons for slow consumption will event Do not put in the store, start about every second can decode A million-level event, but after a period of time, it drops to a hundred-level event every second. Subsequent positioning is when the pressure stop is stopped, I don’t know if anyone has encountered this problem. Fill in the test data Pure test parsing because there is no store, there is no consumer impact 1. Pure insert Scene test ![image](https://user-images.githubusercontent.com/33280738/36956528-cf5f270e-2069-11e8-9962-18fd16789b1a.png) 2. Canal Server Received data * use v1.0.26 Version SocketChannel versus SocketChannelPool Test situation ![image](https://user-images.githubusercontent.com/33280738/36957343-b5f908fc-206e-11e8-903c-cd2a9a9e4c6f.png) When the pressure stop is stopped, the performance drops sharply at this time. Did not end synchronously The red line marks the moment when the pressure measurement is turned off. ![image](https://user-images.githubusercontent.com/33280738/36957403-23727e86-206f-11e8-8756-808af88c7aee.png) After the pressure test is restarted, the performance rebounds but the jitter is large. After a period of time, the performance returns to the state before the stop. ![image](https://user-images.githubusercontent.com/33280738/36957479-a5c07a6e-206f-11e8-9e49-bf0f3287f50b.png) ![image](https://user-images.githubusercontent.com/33280738/36957515-cde4f088-206f-11e8-9132-492a18168cdf.png) * use #548 @lcybo Simple BIO provided Version SocketChannel versus SocketChannelPool Test situation ![image](https://user-images.githubusercontent.com/33280738/36957583-44780c9e-2070-11e8-990f-3fb942380ff2.png) Performance improves when the pressure stop is stopped and MySQL slave Observed network traffic is consistent ![image](https://user-images.githubusercontent.com/33280738/36957655-d170aa5c-2070-11e8-8313-3bd71242fc90.png) Is it a conclusion that the read performance based on the BIO mode is more stable? @wingerx PR has an asynchronous repair version #553 Try again if you have time
546,Canalserver upgrade to 1 0 25 startup appears create connection SQLException 2018-03-02 16:10:49.030 [Druid-ConnectionPool-Create-124634922] ERROR com.alibaba.druid.pool.DruidDataSource - create connection SQLException url: errorCode 0 state null java.sql.SQLException: connect error url driverClass org.h2.Driver at com.alibaba.druid.pool.DruidAbstractDataSource.createPhysicalConnection(DruidAbstractDataSource.java:1579) ~[druid-1.1.6.jar:1.1.6] at com.alibaba.druid.pool.DruidDataSource$CreateConnectionThread.run(DruidDataSource.java:2450) ~[druid-1.1.6.jar:1.1.6] Initialize an embedded H2 memory database at startup to see the specific reasons for the startup failure.
545,Question canal Can subscribe to the library Binlog? canal Is to subscribe to the main library from the role of the library Bin log now I have two questions 1.canal Whether to support subscription bin log from the library 2.canal Subscribe to the main library bin-log Whether the performance loss of the main library is necessary to migrate to the subscription from the library bin log Which one has tried to subscribe to the help from the library bin log? Thank you. canal Do you want to subscribe to the bin log from the library? I also want to know this question. At present, we have a requirement to synchronize libraries in many different regions to a total library. We also consider how canal can monitor multiple servers with one client. At present, the cluster client in the example is an active/standby mode that does not meet this requirement. But if you start multiple CanalConnectors, you should be able to solve it. @yookacn Do you want a canal server to subscribe to multiple master instances? If you can configure multiple destinations using the default mode, you can achieve smileMrLee, thank you for telling me that I haven’t considered the fact that I have subscribed to multiple instances from the server and found the code to meet my needs.
544,Canal can&#39;t start Mysql version 5 6 39 Use canal version 1 0 25 as follows Detailed description using mysql version 5 6 39 Use canal version 1 0 25 phenomenon ![image](https://user-images.githubusercontent.com/5006160/36886406-31d37966-1e27-11e8-8c9d-a4832afb76c5.png) When canal starts Found stuck in this line by jstack Flip the code ![image](https://user-images.githubusercontent.com/5006160/36886423-4a49d81e-1e27-11e8-8da3-5b3e2b0a21a0.png) I can&#39;t connect here. Blocked all the time Impossible to be a permission issue Because the location of the binlog has been read out. Must Https github com alibaba canal releases tag canal 1 0 26 preview 2 Try using this version of the package Must I try this package. Thank you I also encountered this problem always stuck in the reconnect or connect read position after reading the site.
543,The change type is update but there is no column in the field. getUpdated is true. All is false. Such as the title Description of the complex scene
542,Canal startup failed MysqlEventParser - ERROR ## findAsPerTimestampInSpecificLogFile has an error] anyway Every time you start the canal will fail ## Database configuration ``` [mysqld] log_bin=mysql-bin binlog_format='ROW' server_id=1 datadir=/var/lib/mysql socket=/var/lib/mysql/mysql.sock symbolic-links=0 sql_mode=NO_ENGINE_SUBSTITUTION STRICT_TRANS_TABLES [mysqld_safe] log-error=/var/log/mysqld.log pid-file=/var/run/mysqld/mysqld.pid ``` ## mysql The permission configuration of the canal user can see that there is no problem with the permission. ``` mysql> select host user password from mysql.user where user like '%canal%'; +-----------+-------+-------------------------------------------+ | host | user | password | +-----------+-------+-------------------------------------------+ | % | canal | *E3619321C1A937C46A0D8BD1DAC39F93B27D4458 | | 127.0.0.1 | canal | *E3619321C1A937C46A0D8BD1DAC39F93B27D4458 | | localhost | canal | *E3619321C1A937C46A0D8BD1DAC39F93B27D4458 | +-----------+-------+-------------------------------------------+ 3 rows in set (0.05 sec) ``` ## canal Configuration ``` ## mysql serverId canal.instance.mysql.slaveId=0 # position info canal.instance.master.address=127.0.0.1:3306 canal.instance.master.journal.name= canal.instance.master.position= canal.instance.master.timestamp= # table meta tsdb info canal.instance.tsdb.enable=true canal.instance.tsdb.dir=${canal.file.data.dir:../conf}/${canal.instance.destination:} canal.instance.tsdb.url=jdbc:h2:${canal.instance.tsdb.dir}/h2;CACHE_SIZE=1000;MODE=MYSQL; #canal.instance.tsdb.url=jdbc:mysql://127.0.0.1:3306/canal_tsdb canal.instance.tsdb.dbUsername=canal canal.instance.tsdb.dbPassword=canal #canal.instance.standby.address = #canal.instance.standby.journal.name = #canal.instance.standby.position = #canal.instance.standby.timestamp = # username/password canal.instance.dbUsername=canal canal.instance.dbPassword=canal canal.instance.defaultDatabaseName=canal_test canal.instance.connectionCharset=UTF-8 # table regex canal.instance.filter.regex=.*\\..* # table black regex canal.instance.filter.black.regex= ``` ## Start canal canal.log ``` Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=96m; support was removed in 8.0 Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=256m; support was removed in 8.0 Java HotSpot(TM) 64-Bit Server VM warning: UseCMSCompactAtFullCollection is deprecated and will likely be removed in a future release. 2018-03-01 12:04:36.678 [main] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## set default uncaught exception handler 2018-03-01 12:04:36.781 [main] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## load canal configurations 2018-03-01 12:04:36.783 [main] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## start the canal server. 2018-03-01 12:04:36.932 [main] INFO com.alibaba.otter.canal.deployer.CanalController - ## start the canal server[172.17.251.157:11111] 2018-03-01 12:04:37.993 [main] WARN o.s.beans.GenericTypeAwarePropertyDescriptor - Invalid JavaBean property 'connectionCharset' being accessed! Ambiguous write methods found next to actually used [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.nio.charset.Charset)]: [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.lang.String)] 2018-03-01 12:04:38.661 [main] ERROR com.alibaba.druid.pool.DruidDataSource - testWhileIdle is true validationQuery not set 2018-03-01 12:04:39.284 [main] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## the canal server is running now ...... 2018-03-01 12:04:40.265 [destination = example address = /127.0.0.1:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just show master status 2018-03-01 12:04:43.385 [destination = example address = /127.0.0.1:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - ERROR ## findAsPerTimestampInSpecificLogFile has an error java.io.IOException: connect /127.0.0.1:3306 failure at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:74) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.reconnect(MysqlConnector.java:83) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.reconnect(MysqlConnection.java:76) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findAsPerTimestampInSpecificLogFile(MysqlEventParser.java:713) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findEndPositionWithMasterIdAndTimestamp(MysqlEventParser.java:373) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPositionInternal(MysqlEventParser.java:428) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPosition(MysqlEventParser.java:347) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:164) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_91] Caused by: java.io.IOException: socket read timeout occured ! at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannel.read(SocketChannel.java:78) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.readHeader(PacketManager.java:18) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:150) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:71) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] ... 8 common frames omitted 2018-03-01 12:04:43.390 [destination = example address = /127.0.0.1:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /127.0.0.1:3306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: can't find start position for example 2018-03-01 12:04:43.398 [destination = example address = /127.0.0.1:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: can't find start position for example ] 2018-03-01 12:05:05.951 [destination = example address = /127.0.0.1:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /127.0.0.1:3306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: connect /127.0.0.1:3306 failure Caused by: java.io.IOException: connect /127.0.0.1:3306 failure at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:74) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:87) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:160) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_91] Caused by: java.io.IOException: socket read timeout occured ! at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannel.read(SocketChannel.java:78) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.readHeader(PacketManager.java:18) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:150) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:71) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] ... 4 common frames omitted 2018-03-01 12:05:05.952 [destination = example address = /127.0.0.1:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: connect /127.0.0.1:3306 failure Caused by: java.io.IOException: connect /127.0.0.1:3306 failure at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:74) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:72) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:87) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:160) at java.lang.Thread.run(Thread.java:745) Caused by: java.io.IOException: socket read timeout occured ! at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannel.read(SocketChannel.java:78) at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.readHeader(PacketManager.java:18) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:150) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:71) ... 4 more ] ``` me too mysql 5.7.19 Https github com alibaba canal releases tag canal 1 0 26 preview 2 Try using this version of the package @snailbing Is the problem solved?
541,canal1.0.25 In zookeeper Not created in cursor Do not update binlog location information Getting started is canal_1.0.24 Zk_3 4 10 has a cursor The node also has binlog location information mysql_5 7 20 But do not update the binlog location information in zk and later upgrade to canal_1 0 25 Still not updated Then manually deleted zk Under /otter table of Contents Restart canal Results are not created cursor node Configuration process reference https://github.com/alibaba/canal/wiki/AdminGuide Written in the address After the 1 0 25 version, there will be a process of locating the location by default. https github com alibaba canal releases tag canal 1 0 26 preview 2 Try using this version of the package
540,In group mode, sometimes database data changes cannot be read. Scene two machines Machine one Deploy canal server And mysql1 Machine two deployment mysql2 The instance properties are as follows canal.instance.master1.address=127.0.0.1:3306 canal.instance.master2.address=host2:3306 Deployed canal The server database change of the server could not be read. If the instance properties are modified to canal.instance.master1.address=host1:3306 canal.instance.master2.address=host2:3306 Can read correctly I don&#39;t know if it is my configuration problem. Still a code bug Basically no environmental problems
539,A simple change to the problem in PR 536 For more information, please see issue 537 https github com alibaba canal issues 537 Supports the following netty and bio modes to provide canal socketChannel bio netty to switch the default selection of bio mode The earliest use of bio mode for long links for high-throughput scenarios, bio mode will have advantages, mainly can not solve the problem of link suspended animation There is more conflict with the main code. The human flesh merges the code. You can close the master code before submitting the PR next time.
538,How long does rds keep on binlog? Such as the title Today, when I reset the spending point, I can&#39;t find the point of consumption. But I just reset it to 6 hours ago. The default should be to save 18 hours.
537,Some problems with cache in SocketChannel Hello author Recent projects have also encountered problems in pr 487 https github com alibaba canal pull 487 if the client consumes fast enough canal Server will expand directByteBuf to overflow Pr 487 https github com alibaba canal pull 487 does solve the problem but raises the problem of pr 536 https github com alibaba canal pull 536 pr[#536](https://github.com/alibaba/canal/pull/536) have some problem 1. MASTER_HEARTBEAT_PERIOD is using change master The unit of the to statement is the second canal can not use change master To can only use set MASTER_HEARTBEAT_PERIOD unit is nanosecond can test 2. Dynamically go to adjust the ByteBuf according to packetSize. The logic is more complicated. In fact, you can set the initial capacity according to maxThreeBytes slightly less than 16M. Combine pr 487 https github com alibaba canal pull 487 参见pr 539 https github com alibaba canal pull 539  The BTW dump thread uses netty nio to complicate the code here, using InputStream directly and then setting a shorter SO_TIMEOUT such as 1 second. 1. For interrupted scenes try { return in.read(bytes index length); } catch (SocketTimeoutException e) { Check if the thread is interrupted every 1s } 2. For scenes that require timeout try { return in.read(bytes index length); } catch (SocketTimeoutException e) { Similar to the current implementation, determine whether accumulatedWaitTime SO_TIMEOUT is more than real timeout } This can deal with the problem of interrupted scenes and flash blocking. It will also be easier to implement dump. IO has more problems This is just my proposalha PR code has been merged
536,Major repair SocketChannel The default cache size is 1MB. If it is not enough, it needs to be automatically expanded. Otherwise, the IO timeout will be caused by insufficient cache space. Usually copy contains CLOB/BLOB Field type data will encounter this problem. 1 0 24 version begins to show no response and no error log is upgraded to 1 0 25 version after the report socket read timeout occured The error gives the mistake to think that the network or database problem is copied between the two private networks forwarded by the public network SSH. After debugging, the trace is found to be a fixed 1MB cache size. The actual running test found that the one-time readSize reached 15MB and above. 1.0.26-SNAPSHOT Compilation and release have been running stably [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=536) <br/>All committers have signed the CLA. [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=536) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=536) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=536) it.</sub> Lcybo thank you for correcting I searched for the clues based on the clues you provided. MySQL jdbc in com.mysql.jdbc.MysqlIO The source code is excerpted as follows ` if (versionMeetsMinimum(4 0 8)) { this.maxThreeBytes = (256 * 256 * 256) - 1; this.useNewLargePackets = true; } else { this.maxThreeBytes = 255 * 255 * 255; this.useNewLargePackets = false; } ` It seems that it can be easily set to 16MB = 256 * 256 * 256 However, I still hope that the necessary checks and detailed logs in the code will be too brainy. Well, actually I think dump IO does not use netty here. NIO&#39;s need to separate the read and write threads introduces unnecessary complexity Using BIO to capture by setting a small SO_TIMEOUT can circumvent the flash problem and handle thread interrupt scenarios. tks
535,canal When the server starts, it gives a mistake to the uniquekey. failed ``` 2018-02-25 10:23:39.643 [destination = example address = rds9h7gi6v2fo2og5202.mysql.rds.aliyuncs.com/100.98.57.68:3306 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - pls submit github issue show create table ddl:CREATE TABLE `center_institution_admin` ( `id` bigint(20) NOT NULL AUTO_INCREMENT `account` varchar(20) NOT NULL COMMENT account number `cinst_id` bigint(20) NOT NULL COMMENT Institution id `status` int(11) DEFAULT '1' COMMENT 0 off 1 open `gmt_create` datetime DEFAULT CURRENT_TIMESTAMP `gmt_modify` datetime DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP `name` varchar(50) DEFAULT NULL COMMENT Account role name `parent_id` bigint(20) NOT NULL DEFAULT '0' COMMENT Parent account ID `permissions` varchar(500) DEFAULT '0' COMMENT Account privilege code `remark` varchar(256) DEFAULT NULL `default_account` int(11) DEFAULT '0' PRIMARY KEY (`id`) UNIQUE KEY `inst_account_unique_index` (`cinst_id` `account`) KEY `parent_id_index` (`parent_id`) KEY `center_institution_admin_account_index` (`account`) ) ENGINE=InnoDB AUTO_INCREMENT=967573674662035553 DEFAULT CHARSET=utf8 COMMENT Central Organization Management Account Form compare failed . db : TableMeta [schema=xiaomai table=center_institution_admin fileds= FieldMeta [columnName=id columnType=bigint(20) nullable=false key=true defaultValue=null extra=auto_increment unique=false] FieldMeta [columnName=account columnType=varchar(20) nullable=false key=false defaultValue=null extra= unique=false] FieldMeta [columnName=cinst_id columnType=bigint(20) nullable=false key=false defaultValue=null extra= unique=false] FieldMeta [columnName=status columnType=int(11) nullable=true key=false defaultValue=1 extra= unique=false] FieldMeta [columnName=gmt_create columnType=datetime nullable=true key=false defaultValue=CURRENT_TIMESTAMP extra= unique=false] FieldMeta [columnName=gmt_modify columnType=datetime nullable=true key=false defaultValue=CURRENT_TIMESTAMP extra=on update CURRENT_TIMESTAMP unique=false] FieldMeta [columnName=name columnType=varchar(50) nullable=true key=false defaultValue=null extra= unique=false] FieldMeta [columnName=parent_id columnType=bigint(20) nullable=false key=false defaultValue=0 extra= unique=false] FieldMeta [columnName=permissions columnType=varchar(500) nullable=true key=false defaultValue=0 extra= unique=false] FieldMeta [columnName=remark columnType=varchar(256) nullable=true key=false defaultValue=null extra= unique=false] FieldMeta [columnName=default_account columnType=int(11) nullable=true key=false defaultValue=0 extra= unique=false] ] mem : TableMeta [schema=xiaomai table=center_institution_admin fileds= FieldMeta [columnName=id columnType=bigint(20) nullable=false key=true defaultValue=null extra=null unique=false] FieldMeta [columnName=account columnType=varchar(20) nullable=false key=false defaultValue=null extra=null unique=true] FieldMeta [columnName=cinst_id columnType=bigint(20) nullable=false key=false defaultValue=null extra=null unique=true] FieldMeta [columnName=status columnType=int(11) nullable=true key=false defaultValue=1 extra=null unique=false] FieldMeta [columnName=gmt_create columnType=datetime nullable=true key=false defaultValue=CURRENT_TIMESTAMP extra=null unique=false] FieldMeta [columnName=gmt_modify columnType=datetime nullable=true key=false defaultValue=CURRENT_TIMESTAMP extra=null unique=false] FieldMeta [columnName=name columnType=varchar(50) nullable=true key=false defaultValue=null extra=null unique=false] FieldMeta [columnName=parent_id columnType=bigint(20) nullable=false key=false defaultValue=0 extra=null unique=false] FieldMeta [columnName=permissions columnType=varchar(500) nullable=true key=false defaultValue=0 extra=null unique=false] FieldMeta [columnName=remark columnType=varchar(256) nullable=true key=false defaultValue=null extra=null unique=false] FieldMeta [columnName=default_account columnType=int(11) nullable=true key=false defaultValue=0 extra=null unique=false] ] ``` Using the current master branch code https github com alibaba canal issues 507 The problem seems to be unresolved The UK in the test table is a joint index cinst_id Account therefore the unique flag on a separate field should be false but the data in mem is also true for a separate field By desc Table such a way to get uk when the multi-column situation will appear MUL column leads to the inability to accurately obtain the corresponding uk column repair way using show create Table for complete parsing
534,com.alibaba.otter.canal.parse.exception.CanalParseException: column size is not match for table After running for a while, the background always reports the error and does not know what the situation is. Did the next ddl change caused the column to not match Use the latest version 26 to open tsdb can solve The 26 version has opened the account for tsdb for a while or encountered this problem.
533,findAsPerTimestampInSpecificLogFile has an error ![qq 20180223182635](https://user-images.githubusercontent.com/2507568/36590126-8ceea370-18c8-11e8-9cfb-2175cf9535f9.png) Cluster mode starts a server successfully starts another server error as shown in the figure
532,fix(dbsync): json column of zero length has no value value parsing s… @see https://github.com/LavaCoref/canal/issues/1 [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=532) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=532) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=532) it.</sub> [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=532) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=532) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=532) it.</sub> tks
531,I would like to ask if bufferMemUnit 16 in MemoryEventStoreWithBuffer *1024 Will it cause the card to run after the program has been running for a while? MemoryEventStoreWithBuffer中 bufferSize = 16 * 1024，bufferMemUnit=16 *1024，maxMemSize = batchSize * bufferMemUnit = 16 * 1024 * 16 *1024 ， The program will be stuck in the mysql database after more than 2 hours of normal operation. Kill off socket There is no other obvious abnormality in the link. 。 Is this the cause? bufferMemUnit default size is =1024 How much memory does the server have? server jvm Memory with 8G synchronous database Binlog file size has about 80 G per day From NetEase Mailbox Master On February 22, 2018 09:45，fefine Write How much memory does the server have? — You are receiving this because you authored the thread. Reply to this email directly view it on GitHub or mute the thread. I have such a problem here. But not sure where it is Your bufferMemUnit How much is configured? On February 23, 2018 09:27，fefine Write I have such a problem here. But not sure where it is — You are receiving this because you authored the thread. Reply to this email directly view it on GitHub or mute the thread. Default configuration bufferSize = 16 * 1024，bufferMemUnit= 1024 These two products are 16MB. There should be no memory shortage. You are stuck there Expressed as What symptoms? Look at this issue Instance suspended https github com alibaba canal issues 527 Looking at a bit like the socket timeout card owner can try the latest 1 1 1 version
530,Will canal not support rds mysql5 6 version? canalV1 0 25 start will not have permission exception 2018-02-12 11:38:47.267 [destination = example address = rm-xxxxx.mysql.rds.aliyuncs.com/47.206.15.124:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: ErrorPacket [errorNumber=1142 fieldCount=-1 message=SHOW command denied to user 'canal_monitor'@'122.177.246.186' for table 'db' sqlState=42000 sqlStateMarker=#] with command: show create table `mysql`.`columns_priv`;show create table `mysql`.`db`;show create table `mysql`.`db_view`;show create table `mysql`.`dml_health_check`;show create table `mysql`.`event`;show create table `mysql`.`failover_info`;show create table `mysql`.`func`;show create table `mysql`.`general_log`;show create table `mysql`.`ha_health_check`;show create table `mysql`.`help_category`;show create table `mysql`.`help_keyword`;show create table `mysql`.`help_relation`;show create table `mysql`.`help_topic`;show create table `mysql`.`innodb_index_stats`;show create table `mysql`.`innodb_table_stats`;show create table `mysql`.`ndb_binlog_index`;show create table `mysql`.`plugin`;show create table `mysql`.`proc`;show create table `mysql`.`procs_priv`;show create table `mysql`.`proxies_priv`;show create table `mysql`.`servers`;show create table `mysql`.`slave_master_info`;show create table `mysql`.`slave_relay_log_info`;show create table `mysql`.`slave_worker_info`;show create table `mysql`.`slow_log`;show create table `mysql`.`slow_log_view`;show create table `mysql`.`tables_priv`;show create table `mysql`.`time_zone`;show create table `mysql`.`time_zone_leap_second`;show create table `mysql`.`time_zone_name`;show create table `mysql`.`time_zone_transition`;show create table `mysql`.`time_zone_transition_type`;show create table `mysql`.`user`;show create table `mysql`.`user_view`; Caused by: java.io.IOException: ErrorPacket [errorNumber=1142 fieldCount=-1 message=SHOW command denied to user 'canal_monitor'@'122.177.246.186' for table 'db' sqlState=42000 sqlStateMarker=#] with command: show create table `mysql`.`columns_priv`;show create table `mysql`.`db`;show create table `mysql`.`db_view`;show create table `mysql`.`dml_health_check`;show create table `mysql`.`event`;show create table `mysql`.`failover_info`;show create table `mysql`.`func`;show create table `mysql`.`general_log`;show create table `mysql`.`ha_health_check`;show create table `mysql`.`help_category`;show create table `mysql`.`help_keyword`;show create table `mysql`.`help_relation`;show create table `mysql`.`help_topic`;show create table `mysql`.`innodb_index_stats`;show create table `mysql`.`innodb_table_stats`;show create table `mysql`.`ndb_binlog_index`;show create table `mysql`.`plugin`;show create table `mysql`.`proc`;show create table `mysql`.`procs_priv`;show create table `mysql`.`proxies_priv`;show create table `mysql`.`servers`;show create table `mysql`.`slave_master_info`;show create table `mysql`.`slave_relay_log_info`;show create table `mysql`.`slave_worker_info`;show create table `mysql`.`slow_log`;show create table `mysql`.`slow_log_view`;show create table `mysql`.`tables_priv`;show create table `mysql`.`time_zone`;show create table `mysql`.`time_zone_leap_second`;show create table `mysql`.`time_zone_name`;show create table `mysql`.`time_zone_transition`;show create table `mysql`.`time_zone_transition_type`;show create table `mysql`.`user`;show create table `mysql`.`user_view`; at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.queryMulti(MysqlQueryExecutor.java:109) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.queryMulti(MysqlConnection.java:93) at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:173) at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) at java.lang.Thread.run(Thread.java:748) 1. You can specify a table of subscriptions instead of all 2. Authorize sufficient permissions @agapple I have tested the specified subscription to a table as you said, the result is the same error Ali cloud rds Mysql5 6 version of my work order customer service said that does not support SHOW CREATE TABLE Mysql db_view permissions such as the use of high-rights account implementation does not always report permission is not caused by this? Yes, if you don&#39;t specify a table, then the default will be to subscribe to all the tables before the subscription will take the table structure globally, so similar permissions will be reported.
529,TimelineTransactionBarrier import CanalSinkException; Maven Compile Some of Warning - `TimelineTransactionBarrier.java` in `import com.alibaba.otter.canal.sink.exception.CanalSinkException;` - Fix Maven Wranning: The expression ${pom.version} is deprecated. Please use ${project.version} instead. - Fix MavenWranning : 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-jar-plugin is missing. [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=529) <br/>All committers have signed the CLA. [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=529) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=529) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=529) it.</sub> tks
528,Use mysql master heartbeat to detect phycial tcp connection failure. In testing the network negative Case when canal Dump read will always block SO_KEEPALIVE in SocketChannelPool takes two hours by default Can open the master After heartbeat 5 5, let the master take the heartbeat to the canal when he is free. Event canal will drop this event Tested by set @master_heartbeat_period Time unit is nanosecond DirectLogFetcher calls the read timeout time with timeout set to MASTER_HEARTBEAT_PERIOD 1s Make sure to exceed the heartbeat interval and then timeout to prevent accidental killing tks
527,Instance suspended animation The server side is running normally but the Instance all threads are not responding. The log level is turned on as Info. The exception log is not found. This is the server log ![http://7xo1fz.com1.z0.glb.clouddn.com/server.png](http://7xo1fz.com1.z0.glb.clouddn.com/server.png) This is the Instance log. ![http://7xo1fz.com1.z0.glb.clouddn.com/server.png](http://7xo1fz.com1.z0.glb.clouddn.com/instance.png) I also opened another timed thread in the Instance but did not respond during the stuck time. The database has no other special operations this afternoon. Ask what this might be. No data changes There is data changing I am canal server The normal binlog will be consumed by the client at startup, but if there is no binlong generated for a few hours and then binlog is generated, the server will not read these binlogs. @agapple If there is no data change, the binlog will not be consumed any more. The result of my test needs to be enabled in order to avoid this situation. Detecting and using insert Retl xdual as heartbeat sql Is there any other solution? You are using the 1 0 25 version of your own package. Yes download source package @fefine Use the latest 26 alpha 2 code try again for tcp plus so_timeout mechanism can respond to long time no binlog site is mysql Server actively disconnected
526,is it possible that canal set with multiple mysql database source I will use canal to.. database sync.. daily every 10minuites snapshot. but we use many mysql database servers. I want set mltiple mysql servers as canal's input. example. canal.instance1.master.address = 192.168.0.1:3306 <-- MYSQL A canal.instance2.master.address = 192.168.0.2:3306 <-- MYSQL B(not a slave of A) How can I set many mysql servers as canal's instance You can use group-instance.xml In the canal.properties set ***canal.instance.global.spring.xml = classpath:spring/group-instance.xml*** then update the conf/example/instance.properties like: ``` canal.instance.master1.address=192.168.0.1:3306 canal.instance.master1.journal.name= canal.instance.master1.position=4 canal.instance.master1.timestamp= canal.instance.master2.address=192.168.0.2:3306 canal.instance.master2.journal.name=mysql-bin.000394 canal.instance.master2.position=4 canal.instance.master2.timestamp= ``` and then update the conf/spring/group-instance.xml like this: ``` <beans> <bean id="eventParser" class="com.alibaba.otter.canal.parse.inbound.group.GroupEventParser"> <property name="eventParsers"> <list> <ref bean="eventParser1" /> <ref bean="eventParser2" /> </list> </property> </bean> <bean id="eventParser1" class="com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser"> <!-- database1 info --> <property name="masterInfo"> <bean class="com.alibaba.otter.canal.parse.support.AuthenticationInfo"> <property name="address" value="${canal.instance.master1.address}" /> <property name="username" value="${canal.instance.dbUsername:retl}" /> <property name="password" value="${canal.instance.dbPassword:retl}" /> <property name="defaultDatabaseName" value="${canal.instance.defaultDatabaseName:retl}" /> </bean> </property> <property name="masterPosition"> <bean class="com.alibaba.otter.canal.protocol.position.EntryPosition"> <property name="journalName" value="${canal.instance.master1.journal.name}" /> <property name="position" value="${canal.instance.master1.position}" /> <property name="timestamp" value="${canal.instance.master1.timestamp}" /> </bean> </property> </bean> <bean id="eventParser2" class="com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser"> <!-- database2 info --> <property name="masterInfo"> <bean class="com.alibaba.otter.canal.parse.support.AuthenticationInfo"> <property name="address" value="${canal.instance.master2.address}" /> <property name="username" value="${canal.instance.dbUsername:retl}" /> <property name="password" value="${canal.instance.dbPassword:retl}" /> <property name="defaultDatabaseName" value="${canal.instance.defaultDatabaseName:retl}" /> </bean> </property> <property name="masterPosition"> <bean class="com.alibaba.otter.canal.protocol.position.EntryPosition"> <property name="journalName" value="${canal.instance.master2.journal.name}" /> <property name="position" value="${canal.instance.master2.position}" /> <property name="timestamp" value="${canal.instance.master2.timestamp}" /> </bean> </property> </bean> </beans> ``` That's exactly what i find. Now I will do that! Thank for your speedy answer.
525,Mysql new field After the field is added, the value of the field is modified. The consumer cannot get the update of this field. Ability to open TSDB with the latest version of version 26
524,Parse binlog after opening binlog_rows_query_log_events switch is all query type Phenomenon and #517 the same The dml statement caught by the client after opening the binlog_rows_query_log_events switch is EventType QUERY Type and rowChange getIsDdl is false Because the project needs to do different processing for the ddl statement, I want to confirm the judgment condition of the ddl statement in the wiki. ![](https://camo.githubusercontent.com/73b451bd00ff40ec80ed3bd4904d4052fd34fa03/687474703a2f2f646c322e69746579652e636f6d2f75706c6f61642f6174746163686d656e742f303039302f363438332f36316366313161622d313932342d333730352d386236662d6534363431316263363036612e6a7067) Judging conditions are obviously problematic under such conditions The DDL statement is judged to create drop alter Related operations such as table
523,Merge pull request #1 from alibaba/master merge from master [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=523) <br/>All committers have signed the CLA. [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=523) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=523) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=523) it.</sub>
522,Fix deadlock bug When starting through zk cursor, the first Event type is TransactionEnd. Then the trigger condition of txState becomes 2. Not only the judgment of isTransactionEnd in ddl and dcl clear method should be put into txState intValue. == The back of 2 will otherwise lead to deadlock [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=522) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=522) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=522) it.</sub> Thanks for the high quality bug fixes @lulu2panpan By the way, you group Sink in the source of the TPS probably because the previous design method has a large number of thread notification mechanism is not optimal performance design ideal situation should be a merge Sort mode of the two-tier store of the merge queue will be more ok @agapple I also encountered this problem for a period of time, the data has not changed, and the data change server will not read. Which version is this solved? @handmail Is used group sink ？ Performance is indeed a bit of a problem Some sub-libraries do not generate data for a long time. We are forced to generate binlog through heartbeat sql. The heartbeat is 3s. In this case, the daily delay time is about 5s. There is still a big bottleneck for high-concurrency scenarios. Yes, the follow-up can design a GroupXXParser to support multi-database aggregation performance will be much better This feature is still very necessary, so that an example is ok, which greatly saves operation and maintenance costs. Have time to design it together later. @lulu2panpan If you want to invite you to participate in the code of canal, you can contact me at jianghang115 gmail com.
521,ServerRunningMonitor caused when modifying InstanceConfig not generated or destroyed Running status is inconsistent with the actual problem If the instance is not started on the server side or has been stopped, the client will subscribe to the ServerRunningMonitor in the SessionHandler and try to start the instance. _// Try to start if it has been started to ignore if (!embeddedServer.isStart(clientIdentity.getDestination())) { ServerRunningMonitor runningMonitor = ServerRunningMonitors.getRunningMonitor(clientIdentity.getDestination()); if (!runningMonitor.isStart()) { runningMonitor.start(); } }_ Then CanalServerWithEmbedded start final String First line in destination _final CanalInstance canalInstance = canalInstances.get(destination);_ Will be unable to find InstanceConfig in generate Exception thrown not found in instance _if (config == null) { throw new CanalServerException("can't find destination:{}"); }_ ServerRunningMonitor start sets the state to true on the first line. It is ignored when the instance is started normally when it is started. If the startup fails, all the changed states should be rolled back because start stop will be in scan and netty Work thread so start stop plus synchronization Thanks for a fault-tolerant consideration
520,canal Ha time Cannot guarantee normal service panning canal server ha If you kill directly -9 Mode off ha can not be very good translation service will appear when it is dropped in case Use the command to stop normally server Then this problem does not occur kill 9 need zookeeper session Timeout timeout to do ha migration
519,Canal for the perl regular filter of the library table, why the full matching of the library name table name can not get the log ![image](https://user-images.githubusercontent.com/20380664/35789590-2cec3a0e-0a78-11e8-858c-54bf51ba011b.png) Mysql data parsing attention to the table can be accurate to the specific library by the library name method but using mysql test1 mysql.test2 This way can not resolve the log of the corresponding table is not written on the canal document Do you consider the following point? void subscribe(String filter) throws com.alibaba.otter.canal.protocol.exception.CanalClientException When the client subscribes to the repeated subscription, the corresponding filter information is updated. Description a. If the filter information in this subscription is empty, use canal directly. Filter information configured on the server server b. If the filter information in this subscription is not empty, it will directly replace canal. The filter information configured on the server server is subject to the submission. TODO: Follow-up can be considered if the filter submitted this time is not empty, it is on the canal when performing filtering. server filter + The intersection processing of this filter reaches only one copy of binlog data and multiple clients consume different tables. parameter clientIdentity - Throw com.alibaba.otter.canal.protocol.exception.CanalClientException Use mysql test1 mysql test2 Reference FAQ https://github.com/alibaba/canal/wiki/FAQ
518,Initial startup log asks for Issues ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - pls submit github issue show create table ddl:CREATE TABLE `REPO_VER_REFS` ( `NODE_ID` varbinary(16) NOT NULL `REFS_DATA` longblob NOT NULL UNIQUE KEY `REPO_VER_REFS_IDX` (`NODE_ID`) ) ENGINE=InnoDB DEFAULT CHARSET=latin1 compare failed . db : TableMeta [schema=urule table=REPO_VER_REFS fileds= FieldMeta [columnName=NODE_ID columnType=varbinary(16) defaultValue=null nullable=false key=true] FieldMeta [columnName=REFS_DATA columnType=longblob defaultValue=null nullable=false key=false] ] mem : TableMeta [schema=urule table=REPO_VER_REFS fileds= FieldMeta [columnName=NODE_ID columnType=varbinary(16) defaultValue=null nullable=false key=false] FieldMeta [columnName=REFS_DATA columnType=longblob defaultValue=null nullable=false key=false] ] Similar problem code has been fixed
517,The type of rowChage getEventType is all QUERY But rowChage getSql get sql statement is INSERT or UPDATE or DELETE As the title monitors mysql a database, all types obtained by rowChage getEventType are QUERY. But the statement that rowChage getSql gets is INSERT or UPDATE or DELETE. It is suspected that rowChage getEventType has not obtained the value and given a default QUERY value. Statement of the log printed in the code Logger info This operation type rowChage getEventType toString logger.info("sql------>:"+rowChage.getSql()); Whether logger info is ddl rowChage getIsDdl Log printed by the console [2018-02-01 11 29 02 614 INFO Thread 6 cp mycode canal ClusterCanalClient handleData This operation type QUERY [2018-02-01 11:29:02.615][INFO][Thread-6]c.p.mycode.canal.ClusterCanalClient-handleData:sql------>:DELETE FROM QRTZ_CREDITCARD_FIRED_TRIGGERS WHERE ENTRY_ID = 'cfc-sit-jd-web04.haomoney.local15174069672781517407028267' [2018-02-01 11 29 02 615 INFO Thread 6 c p mycode canal ClusterCanalClient handleData 是否是ddl false Binlog non-row mode
516,How to ensure continuous connection I would like to ask if the connection between the canal and the main library is dead or the network interrupt canal have a reconnection mechanism? Some look at the example Client example
515,canal Client spring version problem canal The client is still using the 3 2 9 RELEASE version of the spring. If you use the spring4 dependency directly, you will get strange exceptions. You need to exclude the spring-related dependencies in the project. You always feel that there are some risks in the process. Suggest 1. Can you keep up with the spring version of the upgrade example spring3 -> spring4 2. If canal uses spring&#39;s features very little, can you set the spring dependency to optional? reference：#371 In addition, I also have a version conflict in the logback during use. It is also recommended to become an optional dependency. Canal uses only spring dependency injection LS Correct Answer
514,When the data is changed, the client only parses the start and end of the transaction. When the data is changed, the client only parses the start and end of the transaction. The message returned by the getEntries method has only two elements. A corresponding transaction starts with a corresponding transaction. No row changes. The data is filtered
513,Does Canal&#39;s Filter support prefix matching? For example, the schema is actionlog. The four-digit table is the actionlog. The eight-digit filter can match this. How do you need to write it? Can write actionlog d 4 actionlog d 8
512,Server startup has failed for help Canal instance tsdb enable has the following error whether it is turned on or off ERROR com.alibaba.druid.pool.DruidDataSource - create connection SQLException url: jdbc:h2:../conf/example/h2;CACHE_SIZE=1000;MODE=MYSQL; errorCode 28000 state 28000 org.h2.jdbc.JdbcSQLException: Wrong user name or password [28000-196] My instance configuration `################################################# ## mysql serverId canal.instance.mysql.slaveId=1234 # position info canal.instance.master.address=192.168.3.200:3306 canal.instance.master.journal.name=mysql-bin.000001 canal.instance.master.position= canal.instance.master.timestamp= # table meta tsdb info canal.instance.tsdb.enable=false canal.instance.tsdb.dir=${canal.file.data.dir:../conf}/${canal.instance.destination:} canal.instance.tsdb.url=jdbc:h2:${canal.instance.tsdb.dir}/h2;CACHE_SIZE=1001;MODE=MYSQL; #canal.instance.tsdb.url=jdbc:mysql://127.0.0.1:3306/Test #canal.instance.tsdb.dbUsername=canal #canal.instance.tsdb.dbPassword=canal #canal.instance.standby.address = #canal.instance.standby.journal.name = #canal.instance.standby.position = #canal.instance.standby.timestamp = # username/password canal.instance.dbUsername=canal canal.instance.dbPassword=canal canal.instance.defaultDatabaseName= canal.instance.connectionCharset=UTF-8 # table regex canal.instance.filter.regex=Test\\..* # table black regex canal.instance.filter.black.regex= ################################################# ` >b. The principle of canal is to simulate itself as mysql Slave so here must be done as mysql Slave related permissions CREATE USER canal IDENTIFIED BY 'canal'; GRANT SELECT REPLICATION SLAVE REPLICATION CLIENT ON *.* TO 'canal'@'%'; -- GRANT ALL PRIVILEGES ON *.* TO 'canal'@'%' ; FLUSH PRIVILEGES; https://github.com/alibaba/canal/wiki/QuickStart Is mysql not setting the canal username and password? @HeChuanXUPT Hello, please confirm that it is not a permission issue. @hsh075623201 If you don&#39;t use tsdb, you can try the previous version. 1.0.19 ， 1 0 19 This is my successful start. Encountered the same problem @WilliamGai Note tsdb is fine. Encountered the same problem @hsh075623201 Excuse me, you are putting Is tsdb fully commented? I commented and then reported the exception. java.sql.SQLException: connect error url driverClass org.h2.Driver H2 problem There are a few feedbacks If you know more about H2, you can try to optimize it. H2 as a table The historical version of ddl will be stored based on the default jdbc configuration of H2. ``` canal.instance.tsdb.dir=${canal.file.data.dir:../conf}/${canal.instance.destination:} canal.instance.tsdb.url=jdbc:h2:${canal.instance.tsdb.dir}/h2;CACHE_SIZE=1000;MODE=MYSQL; canal.instance.tsdb.dbUsername=canal canal.instance.tsdb.dbPassword=canal ``` Interpretation is the first time you will create conf instance instance mv db and set the access password to canal canal. If the second reopen will verify xx mv db whether there are multiple processes at the same time will appear java lang IllegalStateException The file is Locked will also verify that the access password for this time is the same as when it was first created. If you really encounter some inexplicable problems, the omnipotent solution removes the conf corresponding xx mv db will reinitialize an h2 local file
511,Can you use kafka together? Hello there Can you use kafka directly to write data to kafka or you need to write your own consumer? Write it yourself now Thread with maxwell debezium It is recommended to modify the otter loader Support other data sources otter managed canal
510,Connect the mysql service after the portal service starts. Newspaper java.io.IOException: socket read timeout occured ! Mysql service has started user permissions and also assigned a few minutes after starting the canal can not connect to the mysql error, but after a few minutes, I have been connected to this problem has been bothering me for a few days. `java.io.IOException: connect /10.11.30.3:3306 failure at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:74) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:162) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131] Caused by: java.io.IOException: socket read timeout occured ! at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannel.read(SocketChannel.java:78) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.readHeader(PacketManager.java:18) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:150) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:71) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] ... 3 common frames omitted 2018-01-26 09:35:09.559 [destination = example address = /10.11.30.3:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: connect /10.11.30.3:3306 failure at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:74) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:72) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:162) at java.lang.Thread.run(Thread.java:748) Caused by: java.io.IOException: socket read timeout occured ! at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannel.read(SocketChannel.java:78) at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.readHeader(PacketManager.java:18) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:150) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:71) ... 3 more ] 2018-01-26 09:35:27.296 [destination = example address = /10.11.30.3:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /10.11.30.3:3306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: connect /10.11.30.3:3306 failure Caused by: java.io.IOException: connect /10.11.30.3:3306 failure at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:74) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:87) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:160) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131] Caused by: java.io.IOException: socket read timeout occured ! at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannel.read(SocketChannel.java:78) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.readHeader(PacketManager.java:18) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:150) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:71) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] ... 4 common frames omitted 2018-01-26 09:35:27.298 [destination = example address = /10.11.30.3:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: connect /10.11.30.3:3306 failure Caused by: java.io.IOException: connect /10.11.30.3:3306 failure at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:74) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:72) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:87) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:160) at java.lang.Thread.run(Thread.java:748) Caused by: java.io.IOException: socket read timeout occured ! at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannel.read(SocketChannel.java:78) at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.readHeader(PacketManager.java:18) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:150) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:71) ... 4 more ] 2018-01-26 09:35:47.022 [destination = example address = /10.11.30.3:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position {"identity":{"slaveId":-1 "sourceAddress":{"address":"10.11.30.3" "port":3306}} "postion":{"included":false "journalName":"mysql-log-bin.000005" "position":4353 "serverId":999 "timestamp":1516859980000}}` I am also the problem, I don&#39;t know how it happened. mysql5 7 13 It seems that the official description of this version of mysql should be no problem, do not know how to appear this problem 2018-01-31 17:04:22.134 [destination = yfmalldb address = /127.0.0.1:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : GRANT SELECT REPLICATION SLAVE REPLICATION CLIENT ON *.* TO 'canal'@'127.0.0.1' IDENTIFIED WITH 'mysql_native_password' AS '*E3619321C1A937C46A0D8BD1DAC39F93B27D4458' com.alibaba.druid.sql.parser.ParserException: syntax error error in :'FIED WITH 'mysql_native_password' A' expect BY actual WITH pos 97 line 1 column 93 token WITH at com.alibaba.druid.sql.parser.SQLParser.printError(SQLParser.java:283) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] at com.alibaba.druid.sql.parser.SQLParser.accept(SQLParser.java:292) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] at com.alibaba.druid.sql.parser.SQLStatementParser.parseGrant(SQLStatementParser.java:738) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:207) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.applyHistoryOnMemory(DatabaseTableMeta.java:388) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:121) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_111] 2018-01-31 17:04:22.134 [destination = yfmalldb address = /127.0.0.1:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : GRANT SELECT REPLICATION SLAVE REPLICATION CLIENT ON *.* TO 'canal'@'localhost' IDENTIFIED WITH 'mysql_native_password' AS '*E3619321C1A937C46A0D8BD1DAC39F93B27D4458' com.alibaba.druid.sql.parser.ParserException: syntax error error in :'FIED WITH 'mysql_native_password' A' expect BY actual WITH pos 97 line 1 column 93 token WITH at com.alibaba.druid.sql.parser.SQLParser.printError(SQLParser.java:283) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] at com.alibaba.druid.sql.parser.SQLParser.accept(SQLParser.java:292) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] at com.alibaba.druid.sql.parser.SQLStatementParser.parseGrant(SQLStatementParser.java:738) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:207) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.applyHistoryOnMemory(DatabaseTableMeta.java:388) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:121) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_111] Looked like it was an authorization issue but the permissions were handled in accordance with the instructions. Caused by: java.io.IOException: socket read timeout occured ! at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannel.read(SocketChannel.java:78) This means that if your source library exceeds 100 seconds, no new data will be automatically reconnected.
509,canal Client pulls binlog server error java nio channels ClosedByInterruptException null ------------------------------------------------------------------------------------------- 2018-01-26 14:04:56.245 INFO 121070 --- [ Thread-4] com.gome.canalmq.service.MonitorService : lmis speed:2461.1record/sec can al speed:6273.7record/sec 2018-01-26 14:05:06.245 INFO 121070 --- [ Thread-4] com.gome.canalmq.service.MonitorService : lmis speed:3666.6record/sec can al speed:8312.6record/sec 2018-01-26 14:05:16.246 INFO 121070 --- [ Thread-4] com.gome.canalmq.service.MonitorService : lmis speed:4077.8record/sec can al speed:12065.4record/sec 2018-01-26 14:05:26.246 INFO 121070 --- [ Thread-4] com.gome.canalmq.service.MonitorService : lmis speed:2859.6record/sec can al speed:6839.5record/sec 26 2018-01-26 14:05:36.246 INFO 121070 --- [ Thread-4] com.gome.canalmq.service.MonitorService : lmis speed:329.2record/sec cana l speed:920.5record/sec 27 2018-01-26 14:05:46.246 INFO 121070 --- [ Thread-4] com.gome.canalmq.service.MonitorService : lmis speed:149.3record/sec cana l speed:419.7record/sec 2018-01-26 14:05:56.247 INFO 121070 --- [ Thread-4] com.gome.canalmq.service.MonitorService : lmis speed:76.4record/sec canal speed:212.2record/sec 2018-01-26 14:06:06.247 INFO 121070 --- [ Thread-4] com.gome.canalmq.service.MonitorService : lmis speed:23.6record/sec canal speed:68.9record/sec 2018-01-26 14:06:16.248 INFO 121070 --- [ Thread-4] com.gome.canalmq.service.MonitorService : lmis speed:58.0record/sec canal speed:165.8record/sec 2018-01-26 14:06:26.248 INFO 121070 --- [ Thread-4] com.gome.canalmq.service.MonitorService : lmis speed:38.7record/sec canal speed:108.9record/sec 2018-01-26 14:06:36.248 INFO 121070 --- [ Thread-4] com.gome.canalmq.service.MonitorService : lmis speed:29.9record/sec canal speed:86.6record/sec ------------------------------------------------------------------------------------------- 2018-01-26 14:05:22.693 [destination = lmis65 address = /10.128.35.65:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address 10.128.35.65/10.128.35.65:3306 has an error retrying. caused by java.nio.channels.ClosedByInterruptException: null at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannel.read(SocketChannel.java:49) ~[canal.parse.driver-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:151) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:77) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:137) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) ~[canal.parse-1.0.25.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_91] 2018-01-26 14:05:22.697 [destination = lmis65 address = /10.128.35.65:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:lmis65[java.nio.channels.ClosedByInterruptException at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannel.read(SocketChannel.java:49) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:151) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:77) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:137) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) at java.lang.Thread.run(Thread.java:745) ] 2018-01-26 14:05:41.309 [destination = lmis65 address = /10.128.35.65:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position {"identity":{"slaveId":-1 "sourceAddress":{"address":"10.128.35.65" "port":3306}} "postion":{"included":false "journalName":"mysql-bin.003759" "position":891656455 "serverId":1 "timestamp":1516929456000}} surroundings Canal version 1 0 25 Zk management centos7 2 Mysql version 5 7 11 canal 1 0 25 and 1 0 26 are very slow, I changed to the old version 1 0 24, no problem. I also fell to 1 0 24, no problem, I will report an error at 1 0 25 ``` 2018-01-25 19:36:37.799 [destination = databus address = /127.0.0.1:43306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - parse events has an error com.alibaba.otter.canal.parse.exception.CanalParseException: dump address /127.0.0.1:43306 has an error retrying. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: connect /127.0.0.1:43306 failure Caused by: java.io.IOException: connect /127.0.0.1:43306 failure at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:72) ~[canal.parse.driver-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:71) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:88) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:160) ~[canal.parse-1.0.25.jar:na] at java.lang.Thread.run(Thread.java:745) ~[na:1.7.0_79] Caused by: java.nio.channels.ClosedByInterruptException: null at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannel.read(SocketChannel.java:55) ~[canal.parse.driver-1.0.25.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.readHeader(PacketManager.java:12) ~[canal.parse.driver-1.0.25.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:148) ~[canal.parse.driver-1.0.25.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:69) ~[canal.parse.driver-1.0.25.jar:na] ... 4 common frames omitted ``` It is recommended to upgrade to version 26 to solve the problem that the timeout period is too short and the link mysql is switched back to bio.
508,canal client
507,desc tableName versus show create table tableName Inconsistent comparison CREATE TABLE `IM_GROUP_USER` ( `GROUPID` int(11) NOT NULL `USERID` int(11) NOT NULL `MANA` int(11) DEFAULT NULL `NOTE` varchar(200) COLLATE gbk_bin DEFAULT NULL UNIQUE KEY `IM_GROUP_USER_INDEX` (`GROUPID` `USERID`) ) ENGINE=InnoDB DEFAULT CHARSET=gbk COLLATE=gbk_bin compare failed . db : TableMeta [schema=imserver table=IM_GROUP_USER fileds= FieldMeta [columnName=GROUPID columnType=int(11) defaultValue=null nullable=false key=true] FieldMeta [columnName=USERID columnType=int(11) defaultValue=null nullable=false key=true] FieldMeta [columnName=MANA columnType=int(11) defaultValue=null nullable=true key=false] FieldMeta [columnName=NOTE columnType=varchar(200) defaultValue=null nullable=true key=false] ] mem : TableMeta [schema=imserver table=IM_GROUP_USER fileds= FieldMeta [columnName=GROUPID columnType=int(11) defaultValue=null nullable=false key=false] FieldMeta [columnName=USERID columnType=int(11) defaultValue=null nullable=false key=false] FieldMeta [columnName=MANA columnType=int(11) defaultValue=null nullable=true key=false] FieldMeta [columnName=NOTE columnType=varchar(200) defaultValue=null nullable=true key=false] ] You mysql is the 啥 version to look like UNIQUE KEY IM_GROUP_USER_INDEX (GROUPID USERID was used as a pk to send me a desc imserver.IM_GROUP_USER with show create table imserver.IM_GROUP_USER the result of I have reproduced locally.
506,mvn run Canal example project jar Packet loss Using mvn exec:java Dexec mainClass com alibaba otter canal example SimpleCanalClientTest When running the canal example project, the two jars are missing. [WARNING] The POM for com.alibaba.otter:canal.client:jar:1.0.26-SNAPSHOT is missing no dependency information available [WARNING] The POM for com.alibaba.otter:canal.protocol:jar:1.0.26-SNAPSHOT is missing no dependency information available mvn clean install eclipse:eclipse -Dmaven.test.skip
505,The local server is running normally and the database connection fails. Will the implementation of the CanalLauncher class group instance xml in the machine with the source code running normally on the centos server will report the following error configuration file is exactly the same and then the server uses instance properties to run the connection normal. How to fix it 2018-01-25 13:35:30.225 [destination = group_mysql address = /10.138.225.198:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - parse events has an error com.alibaba.otter.canal.parse.exception.CanalParseException: dump address /10.138.225.198:3306 has an error retrying. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: connect /10.138.225.198:3306 failure Caused by: java.io.IOException: connect /10.138.225.198:3306 failure at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:72) ~[canal.parse.driver-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:71) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:88) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:160) ~[canal.parse-1.0.25.jar:na] at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_102] Caused by: java.lang.InterruptedException: null at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1302) ~[na:1.8.0_102] at com.alibaba.otter.canal.common.utils.BooleanMutex$Sync.innerGet(BooleanMutex.java:123) ~[canal.common-1.0.25.jar:na] at com.alibaba.otter.canal.common.utils.BooleanMutex.get(BooleanMutex.java:53) ~[canal.common-1.0.25.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannelPool.open(SocketChannelPool.java:71) ~[canal.parse.driver-1.0.25.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:67) ~[canal.parse.driver-1.0.25.jar:na] ... 4 common frames omitted Put the source code directly on the wrong debug log 2018-01-26 15:44:43.706 [destination = group_mysql address = /10.138.225.198:3306 EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - the channel /10.138.225.198:3306 is not connected 2018-01-26 15:44:43.708 [destination = group_mysql address = /10.138.225.198:3306 EventParser] ERROR c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - java.lang.InterruptedException: AbstractBootstrap$PendingRegistrationPromise@6d9c2400(incomplete) at io.netty.util.concurrent.DefaultPromise.await(DefaultPromise.java:230) at io.netty.channel.DefaultChannelPromise.await(DefaultChannelPromise.java:129) at io.netty.channel.DefaultChannelPromise.await(DefaultChannelPromise.java:28) at io.netty.util.concurrent.DefaultPromise.sync(DefaultPromise.java:340) at io.netty.channel.DefaultChannelPromise.sync(DefaultChannelPromise.java:117) at io.netty.channel.DefaultChannelPromise.sync(DefaultChannelPromise.java:28) at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannelPool.open(SocketChannelPool.java:60) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:70) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:74) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:87) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:160) at java.lang.Thread.run(Thread.java:745) 2018-01-26 15:44:43.710 [destination = group_mysql address = /10.138.225.198:3306 EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - the channel /10.138.225.198:3306 is not connected 2018-01-26 15:44:43.710 [destination = group_mysql address = /10.138.225.198:3306 EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - the channel /10.138.225.198:3306 is not connected 2018-01-26 15:44:43.713 [destination = group_mysql address = /10.138.225.198:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - parse events has an error com.alibaba.otter.canal.parse.exception.CanalParseException: dump address /10.138.225.198:3306 has an error retrying. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: connect /10.138.225.198:3306 failure Caused by: java.io.IOException: connect /10.138.225.198:3306 failure at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:76) ~[class/:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:74) ~[class/:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:87) ~[class/:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:160) ~[class/:na] at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_102] Caused by: java.lang.InterruptedException: AbstractBootstrap$PendingRegistrationPromise@6d9c2400(incomplete) at io.netty.util.concurrent.DefaultPromise.await(DefaultPromise.java:230) ~[netty-all-4.1.6.Final.jar:4.1.6.Final] at io.netty.channel.DefaultChannelPromise.await(DefaultChannelPromise.java:129) ~[netty-all-4.1.6.Final.jar:4.1.6.Final] at io.netty.channel.DefaultChannelPromise.await(DefaultChannelPromise.java:28) ~[netty-all-4.1.6.Final.jar:4.1.6.Final] at io.netty.util.concurrent.DefaultPromise.sync(DefaultPromise.java:340) ~[netty-all-4.1.6.Final.jar:4.1.6.Final] at io.netty.channel.DefaultChannelPromise.sync(DefaultChannelPromise.java:117) ~[netty-all-4.1.6.Final.jar:4.1.6.Final] at io.netty.channel.DefaultChannelPromise.sync(DefaultChannelPromise.java:28) ~[netty-all-4.1.6.Final.jar:4.1.6.Final] at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannelPool.open(SocketChannelPool.java:60) ~[class/:na] at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:70) ~[class/:na] ... 4 common frames omitted 2018-01-26 15:44:43.713 [canal-instance-scan-0] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - disConnect MysqlConnection to /10.138.225.197:3306... 2018-01-26 15:44:43.713 [canal-instance-scan-0] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - the channel /10.138.225.197:3306 is not connected 2018-01-26 15:44:43.713 [destination = group_mysql address = /10.138.225.197:3306 EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - the channel /10.138.225.197:3306 is not connected 2018-01-26 15:44:43.713 [destination = group_mysql address = /10.138.225.197:3306 EventParser] ERROR c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - java.lang.InterruptedException at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:998) at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304) at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:231) at com.alibaba.otter.canal.parse.driver.mysql.socket.SocketChannelPool.open(SocketChannelPool.java:63) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:70) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:74) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:87) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:160) at java.lang.Thread.run(Thread.java:745) Found that starting with startup sh will have this problem, write a script to start it normally, continue to find the reason I found the DappName otter canal in the startup parameter and removed it. It’s strange that this parameter will have an effect.
504,ERROR ## findAsPerTimestampInSpecificLogFile has an error ![_qyd8 tx sr 6qc8 gb74g](https://user-images.githubusercontent.com/35767127/35331901-f8093d74-0143-11e8-9e2f-89528c232bc2.png) Configuration canal.instance.master.journal.name= canal.instance.master.position= There is also the same error ![kqa al2uno vd1 0u_3h](https://user-images.githubusercontent.com/35767127/35332342-a2deabb6-0145-11e8-8d54-ac1735efdce7.png) New version has been fixed
503,compile failed [ERROR] Failed to execute goal on project canal.parse: Could not resolve dependencies for project com.alibaba.otter:canal.parse:jar:1.0.26-SNAPSHOT: Failed to collect dependencies at com.alibaba:druid:jar:1.1.7-preview_0: Failed to read artifact descriptor for com.alibaba:druid:jar:1.1.7-preview_0: Could not transfer artifact com.alibaba:druid:pom:1.1.7-preview_0 from/to alibaba (http://code.alibabatech.com/mvn/releases/): Connect to code.alibabatech.com:80 [code.alibabatech.com/119.38.217.15] failed: Connection refused (Connection refused) -> [Help 1] Druid switched to 1 1 5
502,Merge pull request #1 from alibaba/master . [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=502) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=502) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=502) it.</sub> [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=502) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=502) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=502) it.</sub>
501,Group mode cursor problem In the case of 1 library, a total of 10 libraries are configured with group instance xml 2 There is a client to consume this data 3 After the consumption is finished, the server writes the cursor at this node. The canal destinations example 1001 cursor is formatted as follows type com alibaba otter canal protocol position LogPosition "identity":{"slaveId":-1 "sourceAddress":{"address":"10.20.144.15" "port":3306}} "postion":{"included":false "journalName":"mysql-bin.002253" "position":2574756 "timestamp":1363688722000}} 4 I have configured a total of 10 libraries to consume this. How is this stored in zk? Currently, the cursor node in zk seems to store only one. Store only timestamps
500,Restart the database and the canal service after the server is powered off. requested master to start replication from position > file size Restart canalServer has been reporting this error client requested master to start replication from position > file Size should be caused by data out of sync. How to solve this situation? This error is reported when the position in the canal instance master position or the meta dat in the instance xml is greater than the size of the actual binlog. LS Correct Answer
499,Run a period of time server thread hangs error as follows, how to solve ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - scheudle applySnapshotToDB faield com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: should execute connector.connect() first Caused by: java.io.IOException: should execute connector.connect() first at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.<init>(MysqlQueryExecutor.java:30) ~[canal.parse.driver-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:87) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.compareTableMetaDbAndMemory(DatabaseTableMeta.java:294) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.applySnapshotToDB(DatabaseTableMeta.java:251) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.access$100(DatabaseTableMeta.java:45) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta$2.run(DatabaseTableMeta.java:84) ~[canal.parse-1.0.25.jar:na] at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_144] at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) [na:1.8.0_144] at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) [na:1.8.0_144] at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) [na:1.8.0_144] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_144] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_144] Upgrade the new version
498,ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel 2018-01-19 17:31:34.593 [New I/O server worker #1-1] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x120de79e /127.0.0.1:51432 :> /127.0.0.1:11111] exception=java.nio.channels.ClosedChannelException at org.jboss.netty.channel.socket.nio.NioWorker.cleanUpWriteBuffer(NioWorker.java:649) at org.jboss.netty.channel.socket.nio.NioWorker.writeFromUserCode(NioWorker.java:370) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:137) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) at org.jboss.netty.channel.Channels.write(Channels.java:611) at org.jboss.netty.channel.Channels.write(Channels.java:578) at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:28) at com.alibaba.otter.canal.server.netty.NettyUtils.ack(NettyUtils.java:35) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:80) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.cleanup(ReplayingDecoder.java:542) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.channelDisconnected(ReplayingDecoder.java:450) at org.jboss.netty.channel.Channels.fireChannelDisconnected(Channels.java:360) at org.jboss.netty.channel.socket.nio.NioWorker.close(NioWorker.java:599) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:119) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) at org.jboss.netty.channel.Channels.close(Channels.java:720) at org.jboss.netty.channel.AbstractChannel.close(AbstractChannel.java:208) at org.jboss.netty.channel.ChannelFutureListener$1.operationComplete(ChannelFutureListener.java:46) at org.jboss.netty.channel.DefaultChannelFuture.notifyListener(DefaultChannelFuture.java:381) at org.jboss.netty.channel.DefaultChannelFuture.addListener(DefaultChannelFuture.java:148) at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:30) at com.alibaba.otter.canal.server.netty.NettyUtils.error(NettyUtils.java:51) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:200) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:444) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261) at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:350) at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Who knows what this error code is? ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400 Caused by : something goes wrong with channel:[id: 0x27361109 /172.50.3.52:37230 => /172.50.3.52:11111] exception=com.alibaba.otter.canal.server.exception.CanalServerException: ack error clientId:1001 batchId:6135 is not exist please check 2018-02-08 13:36:55.521 [New I/O server worker #1-7] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x27361109 /172.50.3.52:37230 :> /172.50.3.52:11111] exception=java.nio.channels.ClosedChannelException at org.jboss.netty.channel.socket.nio.NioWorker.cleanUpWriteBuffer(NioWorker.java:649) at org.jboss.netty.channel.socket.nio.NioWorker.writeFromUserCode(NioWorker.java:370) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:137) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) at org.jboss.netty.channel.Channels.write(Channels.java:611) at org.jboss.netty.channel.Channels.write(Channels.java:578) at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:28) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:144) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.cleanup(ReplayingDecoder.java:542) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.channelDisconnected(ReplayingDecoder.java:450) at org.jboss.netty.channel.Channels.fireChannelDisconnected(Channels.java:360) at org.jboss.netty.channel.socket.nio.NioWorker.close(NioWorker.java:599) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:119) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) at org.jboss.netty.channel.Channels.close(Channels.java:720) at org.jboss.netty.channel.AbstractChannel.close(AbstractChannel.java:208) at org.jboss.netty.channel.ChannelFutureListener$1.operationComplete(ChannelFutureListener.java:46) at org.jboss.netty.channel.DefaultChannelFuture.notifyListener(DefaultChannelFuture.java:381) at org.jboss.netty.channel.DefaultChannelFuture.addListener(DefaultChannelFuture.java:148) at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:30) at com.alibaba.otter.canal.server.netty.NettyUtils.error(NettyUtils.java:51) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:200) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:276) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:526) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:444) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261) at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:350) at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) ack Cannot confirm that batchId 1000 does not exist when writing to the client @FataliBud Is this problem solved? The client does not repeat ack but it does report this error. Is this problem solved? This I think is the client connection timeout caused by the extended client connection time in SIMPLECONNECTOR Is this problem solved? ack error clientId:1001 batchId:6135 is not exist please check。 The server did a batch rollback. @agapple Can you give a solution or this is a bug? When can I update it? Thank you. Refer to the client&#39;s demo, do a rollback and re-subscribe. Https github com alibaba canal issues 640 Take a look at this
497,Communication module log small optimization Take debug as an example. The format parameter output has the following overloads. public void debug(String format Object arg); public void debug(String format Object arg1 Object arg2); public void debug(String format Object... arguments); Have the following comments This form avoids superfluous string concatenation when the logger is disabled for the DEBUG level. However this variant incurs the hidden (and relatively small) cost of creating an Object[] before invoking the method even if this logger is disabled for DEBUG. **The variants taking one and two arguments exist solely in order to avoid this hidden cost.** The calls to get and ack are more frequent. Even if the log level is higher, there will be some performance impact on the overhead of creating an array. So for parameter 2, use overloaded function parameter 2 with logger isInfoEnabled Tks excellence
496,Merge pull request #1 from alibaba/master merge from master [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=496) <br/>All committers have signed the CLA. [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=496) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=496) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=496) it.</sub>
495,Write canal by example under example The client cannot subscribe to the binlog data. Write canal by example under example After the client subscribes to the binlog data, the IDE&#39;s console will continuously output DEBUG. org.apache.zookeeper.ClientCnxn - Got ping response for Logs such as sessionid can not output binglog related log data by performing DDL or DML operations under mysql. Look at the wiki first, there should be a place where the configuration is incorrect. @olin017 I also encountered the same problem. The version I am using is 1.0.25 Mysql version is mysql Ver 14.14 Distrib 5.7.21 for Linux (x86_64) using EditLine wrapper I have tried various methods and can&#39;t get the binlog changes. Can you solve your problem? Can you talk about the solution? Change to 1 0 20 The same configuration problem is solved In order to further determine the problem and then change to 1 0 25, still can not get any db change。 I don’t know the specific reason It is recommended not to use too new versions I think someone should also have encountered this problem just found here 1 0 25 version using various methods to throw can not get binlog update according to the upstairs said to change to 1 0 20 version to solve the problem thank you friends are not clear high version Why can&#39;t I get changes? Is the problem solved? Is it related to the name format of the binlog file? Related to the canal version | | Bugs Email aiyabeetle 163 com | Signature by NetEase Mailbox Master custom made On June 13, 2018 09:37，fanpeng Write Is the problem solved? Is it related to the name format of the binlog file? — You are receiving this because you commented. Reply to this email directly view it on GitHub or mute the thread. Suggested to change to 1 0 26
494,Error after starting normal update data Download v1 0 26 alpha 1 version according to the instructions on the website to modify the configuration file to create a database test Start the program in the test database in the test table to insert a data error as shown below in the error message inside the other library table why I update the test library will report other library errors [root@localhost canal]# tail -f logs/canal/canal.log 2018-01-17 23:05:53.957 [main] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## set default uncaught exception handler 2018-01-17 23:05:54.022 [main] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## load canal configurations 2018-01-17 23:05:54.023 [main] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## start the canal server. 2018-01-17 23:05:54.069 [main] INFO com.alibaba.otter.canal.deployer.CanalController - ## start the canal server[192.168.122.1:11111] 2018-01-17 23:05:54.591 [main] WARN o.s.beans.GenericTypeAwarePropertyDescriptor - Invalid JavaBean property 'connectionCharset' being accessed! Ambiguous write methods found next to actually used [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.lang.String)]: [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.nio.charset.Charset)] 2018-01-17 23:05:54.847 [main] ERROR com.alibaba.druid.pool.DruidDataSource - testWhileIdle is true validationQuery not set 2018-01-17 23:05:55.127 [main] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## the canal server is running now ...... 2018-01-17 23:05:55.968 [destination = example address = /127.0.0.1:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just show master status 2018-01-17 23:06:09.929 [destination = example address = /127.0.0.1:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `columns_priv` ( `Host` char(60) COLLATE utf8_bin NOT NULL DEFAULT '' `Db` char(64) COLLATE utf8_bin NOT NULL DEFAULT '' `User` char(16) COLLATE utf8_bin NOT NULL DEFAULT '' `Table_name` char(64) COLLATE utf8_bin NOT NULL DEFAULT '' `Column_name` char(64) COLLATE utf8_bin NOT NULL DEFAULT '' `Timestamp` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP `Column_priv` set('Select' 'Insert' 'Update' 'References') CHARACTER SET utf8 NOT NULL DEFAULT '' PRIMARY KEY (`Host` `Db` `User` `Table_name` `Column_name`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='Column privileges' com.alibaba.druid.sql.parser.ParserException: syntax error error in :'es') CHARACTER SET utf8 NOT NULL DE' expect RPAREN actual IDENTIFIER pos 479 line 8 column 62 token IDENTIFIER CHARACTER at com.alibaba.druid.sql.parser.SQLParser.printError(SQLParser.java:283) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] at com.alibaba.druid.sql.parser.SQLParser.accept(SQLParser.java:292) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:191) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.7.0_79] 2018-01-17 23:06:09.930 [destination = example address = /127.0.0.1:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `db` ( `Host` char(60) COLLATE utf8_bin NOT NULL DEFAULT '' `Db` char(64) COLLATE utf8_bin NOT NULL DEFAULT '' `User` char(16) COLLATE utf8_bin NOT NULL DEFAULT '' `Select_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Insert_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Update_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Delete_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Create_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Drop_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Grant_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `References_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Index_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Alter_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Create_tmp_table_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Lock_tables_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Create_view_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Show_view_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Create_routine_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Alter_routine_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Execute_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Event_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' `Trigger_priv` enum('N' 'Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N' PRIMARY KEY (`Host` `Db` `User`) KEY `User` (`User`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='Database privileges' com.alibaba.druid.sql.parser.ParserException: syntax error error in :''Y') CHARACTER SET utf8 NOT NULL DE' expect RPAREN actual IDENTIFIER pos 225 line 5 column 31 token IDENTIFIER CHARACTER at com.alibaba.druid.sql.parser.SQLParser.printError(SQLParser.java:283) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] at com.alibaba.druid.sql.parser.SQLParser.accept(SQLParser.java:292) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:191) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] Exactly the same mistake How to deal with it @kaikewang The cause of this problem is that the SQL statement that creates the table has caused the fields such as the enumerated type to be avoided as much as possible. Some fields in the system database are enumerated, and errors can be reported as long as they do not listen to changes in these libraries. You can modify all matching libraries below the instance to match only specific libraries. ``` # table regex canal.instance.filter.regex=test\\.* ``` The dependent version of druid is not updated or the tsdb feature can be turned off. Druid upgrade to 1 1 8 can solve the problem of set type @wingerx How to close Tsdb feature Encountered the same problem Just comment out the following line in the conf canal properties file and restart it. #canal.instance.tsdb.spring.xml=classpath:spring/tsdb/h2-tsdb.xml Upgraded druid version is close
493,1.0.25 Version setting timeout error #492 I am trying to solve the problem of reading the card master in this bug. The solution is to use the new mechanism reference in 1 0 25 #297 But I found that when I set the timeout, I will report an error without setting the timeout instead. ![image](https://user-images.githubusercontent.com/11556152/34980458-936b5baa-fadf-11e7-8497-7a00673f6810.png) ![image](https://user-images.githubusercontent.com/11556152/34981797-95c6a5f4-fae3-11e7-851f-4ec4359c51f1.png) Your timeout and 297 timeout are two different things. @agapple Thank you for your busy reply. The first question in the first question is to set the timeout but at 1 0 25 I just have to set it up Timeout time reported socketTimoutException But at 1 0 24 The version will not have this error Just want to ask if this behavior is an expected result. 1 0 25 changed canal The server-side mysql timeout mechanism defaults to 3 seconds, but the frequency of your timeout is too high.
492,Do you want to set the getWithoutAck timeout? Really, I was troubled by this timeout. I didn’t set the timeout, but it seems to be the card owner in the weak network environment. reference #297 I set the timeout later Later discovered that it is waiting for batchSize within the specified time. Fill up And when I tested it, I did see that I really waited for the timeout. ![image](https://user-images.githubusercontent.com/11556152/34977258-ca958ca0-fad4-11e7-9bc6-703c495e919f.png) ![image](https://user-images.githubusercontent.com/11556152/34976865-61b85a10-fad3-11e7-80ff-adc7770a9f4c.png) I just want to avoid the card owner. Is it necessary to set the timeout? in 1.0.25 Version setting timeout error ![image](https://user-images.githubusercontent.com/11556152/34979214-9c6dc91c-fadb-11e7-9d00-32884b26ecc1.png) It is recommended to set the timeout mainly to the backend. If there is no data change, the card owner will always be the owner. @agapple Ok Ok Probably understand so batchSize I still can&#39;t set a big pair.
491,log event entry exceeded max_allowed_packet It should be that when there is a large transaction, the following exception will occur and the canal will not resolve. 2018-01-12 12:45:20.537 [destination =xxxx address = /xxxxxx EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = log event entry exceeded max_allowed_packet; Increase max_allowed_packet on master; the first event 'mysql-bin.007177' at 998888207 the last event read from './mysql-bin.007177' at 123 the last byte read from './mysql-bin.007177' at 998888226. at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) ~[canal.parse-1.0.19.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:117) [canal.parse-1.0.19.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) [canal.parse-1.0.19.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.7.0_76] See an issue about https github com alibaba canal issues 85 on max_allowed_packet Looks like it is to solve this problem plus the logic of the package but it seems that there is no effect. Have you encountered this problem? You can first look at the binlog row event max size parameter setting is defined as follows https://dev.mysql.com/doc/refman/5.7/en/replication-options-binary-log.html#option_mysqld_binlog-row-event-max-size Binlog row event max size default is 8k max_allowed_packet default is 1M binlog will not combine the total binlog row event max size rows into an event unless the size of a single row has exceeded this limit and max_allowed_packet can only change max_allowed_packet at this time add another point com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection#updateSettings It seems that setting max_allowed_packet here requires author confirmation updateSettings has not done max Packet size adjustment How to solve it, I put max_allowed_packet 16mb Add to 64 or there will be an error It is recommended to modify the global max_allowed_packet on the mysql side. The canal itself supports multi-packet regrouping.
490,Obtain table Link failure destination of binlog dump drop out ``` 2018-01-12 00:49:37.659 [destination = test address = /192.168.1.0:3307 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:test[com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4832) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:160) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:759) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:428) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:120) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:66) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:337) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:184) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:145) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) at java.lang.Thread.run(Thread.java:745) Caused by: java.io.IOException: should execute connector.connect() first at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.<init>(MysqlQueryExecutor.java:30) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:87) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMetaByDB(TableMetaCache.java:80) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:30) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:55) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:50) at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) at com.google.common.cache.LocalCache.get(LocalCache.java:3937) at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941) at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824) at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830) ... 10 more ] ``` Obtain table When the information is reported, the link will be invalid. then binlog dump Just died Usually occurs after running for hours mysql Server version: 5.6.29 canal Version 1 0 25 The latest trunk fixes can try the latest package
489,Canal can&#39;t get any data Using canal 1 0 24 version Mysql version is 5 7 20 version After the portal starts, there are the following logs. WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just show master status There are no more logs, and there is no cursor node in the 1001 node under zookeeper. Open the log debug and keep outputting the log as follows getWithoutAck successfully clientId:1001 batchSize:1000 but result is null In addition mysql opened gtid_mode = ON， Connect the database instance to the show using the configured account on the machine where the canal is deployed. master Status gets +------------------+-----------+--------------+------------------+-------------------------------------------------------------------------------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +------------------+-----------+--------------+------------------+-------------------------------------------------------------------------------------------+ | mysql-bin.000506 | 428663296 | | | f6af278c-cf73-11e7-86ed-801844ea454c:1-6 fb35b001-cf73-11e7-9c62-801844e9f00c:1-87011966 | +------------------+-----------+--------------+------------------+-------------------------------------------------------------------------------------------+ Therefore, I have no problem with the account configuration. I should be able to get the binlog. Position The previous permission was set to GRANT SELECT REPLICATION SLAVE REPLICATION CLIENT ON *.* TO 'canal'@'%'; Now changed to RANT ALL PRIVILEGES ON *.* TO 'canal'@'%' ; Then restart the canal and the following log appears. 2018-01-12 11:57:30.620 [destination = 10_64_1_198_3318_instance address = /10.64.0.101:3318 EventParser] INFO c.a.otter.canal.parse.inbound.mysql.MysqlConnection - COM_BINLOG_DUMP with position:BinlogDumpCommandPacket[binlogPosition=523583203 slaveServerId=1234 binlogFileName=mysql-bin.000506 command=18] 2018-01-12 11:57:30.633 [destination = 10_64_1_198_3318_instance address = /10.64.0.101:3318 EventParser] INFO com.taobao.tddl.dbsync.binlog.LogEvent - common_header_len= 19 number_of_event_types= 38 2018-01-12 11:57:32.987 [New I/O server worker #1-2] INFO c.a.otter.canal.server.embedded.CanalServerWithEmbedded - subscribe successfully ClientIdentity[destination=10_64_1_198_3318_instance clientId=1001 filter=] with first position:null 2018-01-12 11:57:33.002 [New I/O server worker #1-2] DEBUG c.a.otter.canal.server.embedded.CanalServerWithEmbedded - getWithoutAck successfully clientId:1001 batchSize:1000 but result is null Looks like the position is obtained but the last is null filter with first position:null Tcpdump capture package has Binlog&#39;s package comes in It seems to be a problem in the canal processing process. If it is a binlog parsing failure, it should throw an exception. it&#39;s wired Debug code discovery canal.instance.memory.buffer.size = 1048576 canal.instance.memory.buffer.memunit = 1024 The multiplication of these two attributes cannot be greater than the maximum int. Otherwise, it will overflow and cause problems when calculating the current buffer size. EVENT。 It’s better to have this suggestion document and be able to throw an exception. This is a constraint that currently cannot exceed the maximum value of int
488,Network is not broken, can&#39;t link again hi Hello there We found that the problem is that when the server network is not good, the client will be disconnected, but we find that once we go offline, our client can no longer relink. I don&#39;t know if the reconnection mechanism is not working. ClosedChannelException Will it not be reconnected? The log is as follows 2018-01-10 12:01:38.228 [New I/O server worker #1-8] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x787f32e8 /*************** => /************] exception=java.io.IOException: Connection timed out at sun.nio.ch.FileDispatcherImpl.read0(Native Method) at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39) at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223) at sun.nio.ch.IOUtil.read(IOUtil.java:192) at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380) at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:321) at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:280) at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:200) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) 2018-01-10 12:01:38.230 [New I/O server worker #1-8] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x787f32e8 /************* :> /*****************] exception=java.nio.channels.ClosedChannelException at org.jboss.netty.channel.socket.nio.NioWorker.cleanUpWriteBuffer(NioWorker.java:623) at org.jboss.netty.channel.socket.nio.NioWorker.close(NioWorker.java:599) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:119) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) at org.jboss.netty.channel.Channels.close(Channels.java:720) at org.jboss.netty.channel.AbstractChannel.close(AbstractChannel.java:200) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.exceptionCaught(SessionHandler.java:216) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.exceptionCaught(ReplayingDecoder.java:461) at org.jboss.netty.channel.Channels.fireExceptionCaught(Channels.java:432) at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:331) at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:280) at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:200) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) with bug repeat #297
487,dump connection is disconnected For SocketChannel.cache instance when produced data (dump data) is faster than consume data(canal handled data) it will caused overflow issue. should be flow control. tks
486,canal.instance.filter.regex Table filtering problem
485,CanalParseException: parse row data failed（v1.0.25） canal The server can run normally after restarting. The following is the exception information ------------- 2018-01-08 09:36:30.435 [pool-4-thread-1] INFO com.alibaba.otter.canal.meta.FileMixedMetaManager - clientId:1001 cursor:[mysql-bin.000914 191343375 1515375389000] address[xxxxxx:3306] 2018/1/8 9 36 312018 01 08 09:36:31.435 [pool-4-thread-1] INFO com.alibaba.otter.canal.meta.FileMixedMetaManager - clientId:1001 cursor:[mysql-bin.000914 191344652 1515375390000] address[xxxxxx:3306] 2018/1/8 9 36 322018 01 08 09:36:32.435 [pool-4-thread-1] INFO com.alibaba.otter.canal.meta.FileMixedMetaManager - clientId:1001 cursor:[mysql-bin.000914 191346634 1515375391000] address[xxxxxx:3306] 2018/1/8 9 36 332018 01 08 09:36:33.435 [pool-4-thread-1] INFO com.alibaba.otter.canal.meta.FileMixedMetaManager - clientId:1001 cursor:[mysql-bin.000914 191348091 1515375392000] address[xxxxxx:3306] 2018/1/8 9 36 342018 01 08 09:36:34.435 [pool-4-thread-1] INFO com.alibaba.otter.canal.meta.FileMixedMetaManager - clientId:1001 cursor:[mysql-bin.000914 191349500 1515375393000] address[xxxxxx:3306] 2018/1/8 9 36 342018 01 08 09:36:34.475 [destination = xxxxxxx address = xxxxxxxxxxx: 3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - ERROR ## parse this event has an error last position : [EntryPosition[included=false journalName=mysql-bin.000914 position=191349531 serverId=200 timestamp=1515375394000]] 2018/1/8 上午9 36 34com alibaba otter canal parse exception CanalParseException  parse row data failed. 2018/1/8 9 36 34Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first 2018/1/8 9 36 34Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first 2018/1/8 9 36 34 AM at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4832) ~[guava-18.0.jar:na] 2018/1/8 9 36 34 AM at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:160) ~[canal.parse-1.0.25.jar:na] 2018/1/8 9 36 34 AM at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:759) ~[canal.parse-1.0.25.jar:na] 2018/1/8 9 36 34 AM at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:428) ~[canal.parse-1.0.25.jar:na] 2018/1/8 9 36 34 AM at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:114) ~[canal.parse-1.0.25.jar:na] 2018/1/8 9 36 34 AM at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:66) ~[canal.parse-1.0.25.jar:na] 2018/1/8 9 36 34 AM at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:337) ~[canal.parse-1.0.25.jar:na] 2018/1/8 9 36 34 AM at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:184) ~[canal.parse-1.0.25.jar:na] 2018/1/8 9 36 34 AM at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:145) [canal.parse-1.0.25.jar:na] 2018/1/8 9 36 34 AM at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) [canal.parse-1.0.25.jar:na] 2018/1/8 9 36 34 AM at java.lang.Thread.run(Thread.java:745) [na:1.8.0_111] 2018/1/8 9 36 34Caused by: java.io.IOException: should execute connector.connect() first 2018/1/8 9 36 34 AM at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.<init>(MysqlQueryExecutor.java:30) ~[canal.parse.driver-1.0.25.jar:na] 2018/1/8 9 36 34 AM at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:87) [canal.parse-1.0.25.jar:na] 2018/1/8 9 36 34 AM at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMetaByDB(TableMetaCache.java:80) ~[canal.parse-1.0.25.jar:na] 2018/1/8 9 36 34 AM at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:30) ~[canal.parse-1.0.25.jar:na] 2018/1/8 9 36 34 AM at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:55) ~[canal.parse-1.0.25.jar:na] 2018/1/8 9 36 34 AM at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:50) ~[canal.parse-1.0.25.jar:na] 2018/1/8 9 36 34 AM at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) ~[guava-18.0.jar:na] 2018/1/8 9 36 34 AM at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) ~[guava-18.0.jar:na] 2018/1/8 9 36 34 AM at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) ~[guava-18.0.jar:na] 2018/1/8 9 36 34 AM at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) ~[guava-18.0.jar:na] 2018/1/8 9 36 34 AM at com.google.common.cache.LocalCache.get(LocalCache.java:3937) ~[guava-18.0.jar:na] 2018/1/8 9 36 34 AM at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941) ~[guava-18.0.jar:na] 2018/1/8 9 36 34 AM at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824) ~[guava-18.0.jar:na] 2018/1/8 9 36 34 AM at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830) ~[guava-18.0.jar:na] 2018/1/8 9 36 34 AM ... 10 common frames omitted 2018/1/8 9 36 342018 01 08 09:36:34.479 [destination = weiming-test address = xxxxxx:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address xxxxxx:3306 has an error retrying. caused by 2018/1/8 上午9 36 34com alibaba otter canal parse exception CanalParseException  com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. 2018/1/8 9 36 34Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. 2018/1/8 9 36 34Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first 2018/1/8 9 36 34Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first 2018/1/8 9 36 34 AM at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4832) ~[guava-18.0.jar:na] 2018/1/8 9 36 34 AM at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:160) ~[canal.parse-1.0.25.jar:na] 2018/1/8 9 36 34 AM at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:759) ~[canal.parse-1.0.25.jar:na] 2018/1/8 9 36 34 AM at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:428) ~[canal.parse-1.0.25.jar:na] 2018/1/8 9 36 34 AM at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:114) ~[canal.parse-1.0.25.jar:na] 2018/1/8 9 36 34 AM at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:66) ~[canal.parse-1.0.25.jar:na] 2018/1/8 9 36 34 AM at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:337) ~[canal.parse-1.0.25.jar:na] 2018/1/8 9 36 34 AM at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:184) ~[canal.parse-1.0.25.jar:na] 2018/1/8 9 36 34 AM at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:145) ~[canal.parse-1.0.25.jar:na] 2018/1/8 9 36 34 AM at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) ~[canal.parse-1.0.25.jar:na] 2018/1/8 9 36 34 AM at java.lang.Thread.run(Thread.java:745) [na:1.8.0_111] 2018/1/8 9 36 34Caused by: java.io.IOException: should execute connector.connect() first 2018/1/8 9 36 34 AM at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.<init>(MysqlQueryExecutor.java:30) ~[canal.parse.driver-1.0.25.jar:na] 2018/1/8 9 36 34 AM at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:87) ~[canal.parse-1.0.25.jar:na] 2018/1/8 9 36 34 AM at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMetaByDB(TableMetaCache.java:80) ~[canal.parse-1.0.25.jar:na] 2018/1/8 9 36 34 AM at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:30) ~[canal.parse-1.0.25.jar:na] 2018/1/8 9 36 34 AM at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:55) ~[canal.parse-1.0.25.jar:na] 2018/1/8 9 36 34 AM at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:50) ~[canal.parse-1.0.25.jar:na] 2018/1/8 9 36 34 AM at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) ~[guava-18.0.jar:na] 2018/1/8 9 36 34 AM at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) ~[guava-18.0.jar:na] 2018/1/8 9 36 34 AM at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) ~[guava-18.0.jar:na] 2018/1/8 9 36 34 AM at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) ~[guava-18.0.jar:na] 2018/1/8 9 36 34 AM at com.google.common.cache.LocalCache.get(LocalCache.java:3937) ~[guava-18.0.jar:na] 2018/1/8 9 36 34 AM at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941) ~[guava-18.0.jar:na] 2018/1/8 9 36 34 AM at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824) ~[guava-18.0.jar:na] 2018/1/8 9 36 34 AM at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830) ~[guava-18.0.jar:na] 2018/1/8 9 36 34 AM ... 10 common frames omitted 2018/1/8 9 36 342018 01 08 09:36:34.480 [destination = weiming-test address = xxxxxx:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:weiming-test[com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. 2018/1/8 9 36 34Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. 2018/1/8 9 36 34Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first 2018/1/8 9 36 34Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first 2018/1/8 9 36 34 AM at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4832) 2018/1/8 9 36 34 AM at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:160) 2018/1/8 9 36 34 AM at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:759) 2018/1/8 9 36 34 AM at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:428) 2018/1/8 9 36 34 AM at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:114) 2018/1/8 9 36 34 AM at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:66) 2018/1/8 9 36 34 AM at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:337) 2018/1/8 9 36 34 AM at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:184) 2018/1/8 9 36 34 AM at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:145) 2018/1/8 9 36 34 AM at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) 2018/1/8 9 36 34 AM at java.lang.Thread.run(Thread.java:745) 2018/1/8 9 36 34Caused by: java.io.IOException: should execute connector.connect() first 2018/1/8 9 36 34 AM at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.<init>(MysqlQueryExecutor.java:30) 2018/1/8 9 36 34 AM at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:87) 2018/1/8 9 36 34 AM at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMetaByDB(TableMetaCache.java:80) 2018/1/8 9 36 34 AM at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:30) 2018/1/8 9 36 34 AM at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:55) 2018/1/8 9 36 34 AM at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:50) 2018/1/8 9 36 34 AM at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) 2018/1/8 9 36 34 AM at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) 2018/1/8 9 36 34 AM at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) 2018/1/8 9 36 34 AM at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) 2018/1/8 9 36 34 AM at com.google.common.cache.LocalCache.get(LocalCache.java:3937) 2018/1/8 9 36 34 AM at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941) 2018/1/8 9 36 34 AM at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824) 2018/1/8 9 36 34 AM at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830) 2018/1/8 9 36 34 AM ... 10 more 2018/1/8 9 36 34 AM reference #482
484,I hope there is a follow-up Docker Supported planning environment debugging is particularly inconvenient I originally planned to encapsulate the image myself. I found that I need to write it myself. `docker entrypoint.sh` Not to mention that the main log and configuration processing needs to do some work in the application layer, decisively give up hope that the official follow-up Docker Support If we have made a mirror for ourselves, we can exchange WeChat Gary0526. @freeme You are convenient to take your `dockerfile` Is the warehouse link sent? > BTW: Is your packaged image a modified official code? The mirroring function I expect here has the following two main features. * Logs can be output uniformly and effectively passed `docker logging` Control collection * able to pass `docker environment` Environment variable control core configuration Modify the startup sh file to become a foreground application. @pczhaoyun The script is only one of the applications, and the log and configuration of the application itself must be adaptively processed. Otherwise, it is still troublesome to manage. @freeme Can submit a dockerfile PR Yeah, this will be a lot easier. Docker already supports reference https github com alibaba canal wiki Docker QuickStart
483,It is recommended to add a ReportSlave message. Now connect to Mysql when the connection is successful, directly RequestDump, so hit the show on Mysql slave Hosts can&#39;t see the current Canal connection, although it does not affect data synchronization, but it is not comfortable for obsessive-compulsive disorder. The solution is to send a ReportSlave message before RequestDump to report itself and then reclaim an OK message. The message content code is as follows [RegisterSlaveCommandPacket.java.txt](https://github.com/alibaba/canal/files/1610961/RegisterSlaveCommandPacket.java.txt) Then add the following function in MysqlConnection java and call it in dump. private void sendRegisterSlave() throws IOException { RegisterSlaveCommandPacket cmd = new RegisterSlaveCommandPacket(); cmd.setReport_host(canalHost); cmd.setReport_port(canalPort); cmd.setReport_user(connector.getUsername()); cmd.setServerid(slaveId); byte[] cmdBody = cmd.toBytes(); logger.info("Register slave {}" cmd); HeaderPacket header = new HeaderPacket(); header.setPacketBodyLength(cmdBody.length); header.setPacketSequenceNumber((byte) 0x00); PacketManager.write(connector.getChannel() new ByteBuffer[] { ByteBuffer.wrap(header.toBytes()) ByteBuffer.wrap(cmdBody) }); ByteBuffer dest = ByteBuffer.allocate(1024); connector.getChannel().read(dest); } The variable I took from the configuration file by changing Spring&#39;s xml Very good optimization is easy to submit a code PR to me Latest release Didn&#39;t bring out this feature? I downloaded it and tried to find the show. slave hosts Still not found canal Server information 26 test version should have Pro test There is no 噢 in the latest version 1 10 Only show when you enter the dump phase slave Hosts for location-targeting phase
482,first parse row data failed should execute connector.connect() Recently reported in the early morning this caused the channel to hang mysql :5.6.35 canal:1.0.25 2018-01-07 00:00:00.085 [destination = sms_log_2 address = /127.0.0.1:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - ERROR ## parse this event has an error last position : [EntryPosition[included=false journalName=mysql-bin.000106 position=583737799 serverId=20563 timestamp=1515254400000]] com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4832) ~[guava-18.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:160) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:759) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:428) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:114) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:66) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:337) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:184) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:145) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) [canal.parse-1.0.25.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_151] Caused by: java.io.IOException: should execute connector.connect() first at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.<init>(MysqlQueryExecutor.java:30) ~[canal.parse.driver-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:87) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMetaByDB(TableMetaCache.java:80) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:30) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:55) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:50) ~[canal.parse-1.0.25.jar:na] at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache.get(LocalCache.java:3937) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830) ~[guava-18.0.jar:na] ... 10 common frames omitted 2018-01-07 00:00:00.087 [destination = sms_log_2 address = /127.0.0.1:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address 127.0.0.1/127.0.0.1:3306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4832) ~[guava-18.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:160) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:759) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:428) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:114) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:66) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:337) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:184) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:145) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) ~[canal.parse-1.0.25.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_151] Caused by: java.io.IOException: should execute connector.connect() first at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.<init>(MysqlQueryExecutor.java:30) ~[canal.parse.driver-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:87) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMetaByDB(TableMetaCache.java:80) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:30) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:55) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:50) ~[canal.parse-1.0.25.jar:na] at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache.get(LocalCache.java:3937) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830) ~[guava-18.0.jar:na] ... 10 common frames omitted 2018-01-07 00:00:00.088 [destination = sms_log_2 address = /127.0.0.1:3306 EventParser] WARN c.a.o.s.a.i.setl.zookeeper.termin.WarningTerminProcess - nid:3[3:canal:sms_log_2:com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4832) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:160) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:759) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:428) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:114) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:66) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:337) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:184) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:145) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) at java.lang.Thread.run(Thread.java:748) Caused by: java.io.IOException: should execute connector.connect() first at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.<init>(MysqlQueryExecutor.java:30) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:87) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMetaByDB(TableMetaCache.java:80) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:30) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:55) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:50) at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) at com.google.common.cache.LocalCache.get(LocalCache.java:3937) at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941) at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824) at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830) ... 10 more ] I have also encountered this problem. I also encountered a message that prevented monitoring I fix the following Does the same problem affect the use? `com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector .connect() first Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4832) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:160) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:759) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:428) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:114) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:66) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:337) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:184) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:145) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) at java.lang.Thread.run(Unknown Source)` @agapple
481,The value of the can read Integer field is null. Is it not? Now read the value of the Integer field. If it is null, it is the empty field of the character segment. Can it be read out to be null? Have a judgment of the isNull method
480,Canal start error com alibaba otter canal parse exception CanalParseException java.io.IOException: ErrorPacket [errorNumber=1142 fieldCount=-1 message=SHOW command denied to user 'shencesys'@'ip' for table 'slow_log' sqlState=42000 sqlStateMarker=#] Error after startup ``` 2018-01-05 15:36:23.539 [destination = example address = ip/ip:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just show master status 2018-01-05 15:36:30.095 [destination = example address = ip/ip:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address ip/ip:3306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: ErrorPacket [errorNumber=1142 fieldCount=-1 message=SHOW command denied to user 'shencesys'@'ip' for table 'slow_log' sqlState=42000 sqlStateMarker=#] with command: show create table `mysql`.`event`;show create table `mysql`.`func`;show create table `mysql`.`general_log`;show create table `mysql`.`help_category`;show create table `mysql`.`help_keyword`;show create table `mysql`.`help_relation`;show create table `mysql`.`help_topic`;show create table `mysql`.`proc`;show create table `mysql`.`slow_log`;show create table `mysql`.`time_zone`;show create table `mysql`.`time_zone_leap_second`;show create table `mysql`.`time_zone_name`;show create table `mysql`.`time_zone_transition`;show create table `mysql`.`time_zone_transition_type`; Caused by: java.io.IOException: ErrorPacket [errorNumber=1142 fieldCount=-1 message=SHOW command denied to user 'shencesys'@'ip' for table 'slow_log' sqlState=42000 sqlStateMarker=#] with command: show create table `mysql`.`event`;show create table `mysql`.`func`;show create table `mysql`.`general_log`;show create table `mysql`.`help_category`;show create table `mysql`.`help_keyword`;show create table `mysql`.`help_relation`;show create table `mysql`.`help_topic`;show create table `mysql`.`proc`;show create table `mysql`.`slow_log`;show create table `mysql`.`time_zone`;show create table `mysql`.`time_zone_leap_second`;show create table `mysql`.`time_zone_name`;show create table `mysql`.`time_zone_transition`;show create table `mysql`.`time_zone_transition_type`; at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.queryMulti(MysqlQueryExecutor.java:109) ~[canal.parse.driver-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.queryMulti(MysqlConnection.java:93) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:173) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) ~[canal.parse-1.0.25.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_45] 2018-01-05 15:36:30.096 [destination = example address = ip/ip:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: ErrorPacket [errorNumber=1142 fieldCount=-1 message=SHOW command denied to user 'shencesys'@'ip' for table 'slow_log' sqlState=42000 sqlStateMarker=#] with command: show create table `mysql`.`event`;show create table `mysql`.`func`;show create table `mysql`.`general_log`;show create table `mysql`.`help_category`;show create table `mysql`.`help_keyword`;show create table `mysql`.`help_relation`;show create table `mysql`.`help_topic`;show create table `mysql`.`proc`;show create table `mysql`.`slow_log`;show create table `mysql`.`time_zone`;show create table `mysql`.`time_zone_leap_second`;show create table `mysql`.`time_zone_name`;show create table `mysql`.`time_zone_transition`;show create table `mysql`.`time_zone_transition_type`; Caused by: java.io.IOException: ErrorPacket [errorNumber=1142 fieldCount=-1 message=SHOW command denied to user 'shencesys'@'ip' for table 'slow_log' sqlState=42000 sqlStateMarker=#] with command: show create table `mysql`.`event`;show create table `mysql`.`func`;show create table `mysql`.`general_log`;show create table `mysql`.`help_category`;show create table `mysql`.`help_keyword`;show create table `mysql`.`help_relation`;show create table `mysql`.`help_topic`;show create table `mysql`.`proc`;show create table `mysql`.`slow_log`;show create table `mysql`.`time_zone`;show create table `mysql`.`time_zone_leap_second`;show create table `mysql`.`time_zone_name`;show create table `mysql`.`time_zone_transition`;show create table `mysql`.`time_zone_transition_type`; at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.queryMulti(MysqlQueryExecutor.java:109) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.queryMulti(MysqlConnection.java:93) at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:173) at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) at java.lang.Thread.run(Thread.java:745) ``` Account permissions are as follows ``` mysql> show grants for shencesys; +---------------------------------------------------------------------------------------------------------------------------------------------------+ | Grants for shencesys@% | +---------------------------------------------------------------------------------------------------------------------------------------------------+ | GRANT PROCESS REPLICATION SLAVE REPLICATION CLIENT ON *.* TO 'shencesys'@'%' IDENTIFIED BY PASSWORD '***********************************' | | GRANT SELECT LOCK TABLES SHOW VIEW ON `xbdchainfin`.* TO 'shencesys'@'%' | | GRANT SELECT ON `performance_schema`.* TO 'shencesys'@'%' | | GRANT SELECT ON `mysql`.`func` TO 'shencesys'@'%' | | GRANT SELECT ON `mysql`.`help_relation` TO 'shencesys'@'%' | | GRANT SELECT ON `mysql`.`time_zone_leap_second` TO 'shencesys'@'%' | | GRANT SELECT ON `mysql`.`time_zone_transition` TO 'shencesys'@'%' | | GRANT SELECT ON `mysql`.`help_keyword` TO 'shencesys'@'%' | | GRANT SELECT ON `mysql`.`slow_log` TO 'shencesys'@'%' | | GRANT SELECT ON `mysql`.`event` TO 'shencesys'@'%' | | GRANT SELECT ON `mysql`.`proc` TO 'shencesys'@'%' | | GRANT SELECT ON `mysql`.`general_log` TO 'shencesys'@'%' | | GRANT SELECT ON `mysql`.`help_category` TO 'shencesys'@'%' | | GRANT SELECT ON `mysql`.`help_topic` TO 'shencesys'@'%' | | GRANT SELECT ON `mysql`.`time_zone` TO 'shencesys'@'%' | | GRANT SELECT ON `mysql`.`time_zone_name` TO 'shencesys'@'%' | | GRANT SELECT ON `mysql`.`time_zone_transition_type` TO 'shencesys'@'%' | +---------------------------------------------------------------------------------------------------------------------------------------------------+ ``` show create table `mysql`.`event`;show create table `mysql`.`func`;show create table `mysql`.`general_log`;show create table `mysql`.`help_category`;show create table `mysql`.`help_keyword`;show create table `mysql`.`help_relation`;show create table `mysql`.`help_topic`;show create table `mysql`.`proc`;show create table `mysql`.`slow_log`;show create table `mysql`.`time_zone`;show create table `mysql`.`time_zone_leap_second`;show create table `mysql`.`time_zone_name`;show create table `mysql`.`time_zone_transition`;show create table `mysql`.`time_zone_transition_type`; Execute the corresponding sql Has been resolved thank you @jinxiaoxin How to solve it? @jinxiaoxin Can you share the problem of how to solve it? Thank you. @millinchen Did you finally solve this problem? @agapple I already have permission, I have to do it again. aliyun The mysql library super account on rds does not necessarily have permission.
479,canal Server syncs the main library error com alibaba otter canal parse exception CanalParseException java.io.IOException: ErrorPacket [errorNumber=1146 fieldCount=-1 message=Table 'mysql.innodb_index_stats' doesn't exist sqlState=42S02 sqlStateMarker=#] I insert a data in the main library after the insert is a business table, the error is reported. The internal table restarts many cans of mysql. Server mysql reset position is invalid as long as the main library has insert update, etc. reported this error 2018-01-04 20:51:50.268 [destination = example address = /10.30.255.151:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /10.30.255.151:3306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: ErrorPacket [errorNumber=1146 fieldCount=-1 message=Table 'mysql.innodb_index_stats' doesn't exist sqlState=42S02 sqlStateMarker=#] with command: show create table `mysql`.`columns_priv`;show create table `mysql`.`db`;show create table `mysql`.`event`;show create table `mysql`.`func`;show create table `mysql`.`general_log`;show create table `mysql`.`help_category`;show create table `mysql`.`help_keyword`;show create table `mysql`.`help_relation`;show create table `mysql`.`help_topic`;show create table `mysql`.`innodb_index_stats`;show create table `mysql`.`innodb_table_stats`;show create table `mysql`.`ndb_binlog_index`;show create table `mysql`.`plugin`;show create table `mysql`.`proc`;show create table `mysql`.`procs_priv`;show create table `mysql`.`proxies_priv`;show create table `mysql`.`servers`;show create table `mysql`.`slave_master_info`;show create table `mysql`.`slave_relay_log_info`;show create table `mysql`.`slave_worker_info`;show create table `mysql`.`slow_log`;show create table `mysql`.`tables_priv`;show create table `mysql`.`time_zone`;show create table `mysql`.`time_zone_leap_second`;show create table `mysql`.`time_zone_name`;show create table `mysql`.`time_zone_transition`;show create table `mysql`.`time_zone_transition_type`;show create table `mysql`.`user`; Caused by: java.io.IOException: ErrorPacket [errorNumber=1146 fieldCount=-1 message=Table 'mysql.innodb_index_stats' doesn't exist sqlState=42S02 sqlStateMarker=#] with command: show create table `mysql`.`columns_priv`;show create table `mysql`.`db`;show create table `mysql`.`event`;show create table `mysql`.`func`;show create table `mysql`.`general_log`;show create table `mysql`.`help_category`;show create table `mysql`.`help_keyword`;show create table `mysql`.`help_relation`;show create table `mysql`.`help_topic`;show create table `mysql`.`innodb_index_stats`;show create table `mysql`.`innodb_table_stats`;show create table `mysql`.`ndb_binlog_index`;show create table `mysql`.`plugin`;show create table `mysql`.`proc`;show create table `mysql`.`procs_priv`;show create table `mysql`.`proxies_priv`;show create table `mysql`.`servers`;show create table `mysql`.`slave_master_info`;show create table `mysql`.`slave_relay_log_info`;show create table `mysql`.`slave_worker_info`;show create table `mysql`.`slow_log`;show create table `mysql`.`tables_priv`;show create table `mysql`.`time_zone`;show create table `mysql`.`time_zone_leap_second`;show create table `mysql`.`time_zone_name`;show create table `mysql`.`time_zone_transition`;show create table `mysql`.`time_zone_transition_type`;show create table `mysql`.`user`; at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.queryMulti(MysqlQueryExecutor.java:109) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.queryMulti(MysqlConnection.java:94) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:173) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_60] No permission I also encountered this problem. The account permissions are yes. +1
478,Canal read datagram IndexOutOfBoundsException Canal read datagram IndexOutOfBoundsException 2018-01-04 17:39:52.043 [pipelineId = 2 taskName = ProcessSelect] WARN c.a.o.s.a.i.setl.zookeeper.termin.WarningTerminProcess - nid:2[2:setl:com.alibaba.otter.node.etl.select.exceptions.SelectException: java.lang.IndexOutOfBoundsException: Index: 0 at com.alibaba.otter.node.etl.select.selector.MessageParser.parse(MessageParser.java:211) at com.alibaba.otter.node.etl.select.selector.canal.CanalEmbedSelector.selector(CanalEmbedSelector.java:258) at com.alibaba.otter.node.etl.select.SelectTask.processSelect(SelectTask.java:236) at com.alibaba.otter.node.etl.select.SelectTask.access$300(SelectTask.java:94) at com.alibaba.otter.node.etl.select.SelectTask$1.run(SelectTask.java:208) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Caused by: java.lang.IndexOutOfBoundsException: Index: 0 at java.util.Collections$EmptyList.get(Collections.java:4454) at com.alibaba.otter.canal.protocol.CanalEntry$RowChange.getRowDatas(CanalEntry.java:8107) at com.alibaba.otter.node.etl.select.selector.MessageParser.parse(MessageParser.java:109) ... 9 more Otter problem non-canal
477,There is a problem with integrating Springboot @Order(value=1) @Component public class SimpleCanalClientServer implements CommandLineRunner{ Exception in thread "restartedMain" java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:49) Caused by: java.lang.ClassCastException: org.springframework.boot.context.event.ApplicationStartedEvent cannot be cast to org.springframework.boot.context.event.ApplicationEnvironmentPreparedEvent at org.springframework.boot.context.config.AnsiOutputApplicationListener.onApplicationEvent(AnsiOutputApplicationListener.java:34) at org.springframework.context.event.SimpleApplicationEventMulticaster$1.run(SimpleApplicationEventMulticaster.java:78) at org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:50) at org.springframework.context.event.SimpleApplicationEventMulticaster.multicastEvent(SimpleApplicationEventMulticaster.java:76) at org.springframework.boot.context.event.EventPublishingRunListener.starting(EventPublishingRunListener.java:69) at org.springframework.boot.SpringApplicationRunListeners.starting(SpringApplicationRunListeners.java:48) at org.springframework.boot.SpringApplication.run(SpringApplication.java:292) at org.springframework.boot.SpringApplication.run(SpringApplication.java:1118) at org.springframework.boot.SpringApplication.run(SpringApplication.java:1107) at com.jorden.li.ApplicationStart.main(ApplicationStart.java:23) ... 5 more Your question has nothing to do with canal is to use spring Boot problem can add my WeChat Gary0526 to help you look
476,canal client Get db serverip+port canal Client can get db Server ip+port ? This time there is no corresponding serverId
475,MySQL After restarting Canal Infinite loop inside the SocketChannel read method Scenes canal Restart MySQL directly during dump phenomenon debug Can see that it has been circulating in this place all the time. ![image](https://user-images.githubusercontent.com/33280738/34552985-e07d80aa-f15f-11e7-806c-c3183b47050c.png) Console log ![image](https://user-images.githubusercontent.com/33280738/34553015-1ed73ab2-f160-11e7-9eb2-19acbf410a08.png) Unable to let Canal Re-dump After half an hour, the console displays the following log. ` 2018-01-04 15:01:05.400 [Hashed wheel timer #1] WARN c.a.o.c.server.netty.handler.ClientAuthenticationHandler - channel:[id: 0x15bf0ead /10.10.80.108:50423 :> /10.10.80.108:11111] idle timeout exceeds close channel to save server resources... 2018-01-04 15:01:05.403 [New I/O server worker #1-4] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x571b8f50 /10.10.80.108:58807 => /10.10.80.108:11111] exception=java.io.IOException: Connection reset by peer at sun.nio.ch.FileDispatcherImpl.read0(Native Method) at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39) at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223) at sun.nio.ch.IOUtil.read(IOUtil.java:192) at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380) at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:322) at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281) at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201) at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) ` Canal Still unable to enter re-dump status Has been mentioned PR 了 #473
474,jvm crash 1. Partial crash log ``` A fatal error has been detected by the Java Runtime Environment: SIGSEGV (0xb) at pc=0x00007f36d5052219 pid=15219 tid=139872797697792 JRE version: Java(TM) SE Runtime Environment (8.0_74-b02) (build 1.8.0_74-b02) Java VM: Java HotSpot(TM) 64-Bit Server VM (25.74-b02 mixed mode linux-amd64 compressed oops) Problematic frame: v ~StubRoutines::jshort_disjoint_arraycopy Stack: [0x00007f36ac67a000 0x00007f36ac6bb000] sp=0x00007f36ac6b94a0 free space=253k Native frames: (J=compiled Java code j=interpreted Vv=VM code C=native code) v ~StubRoutines::jshort_disjoint_arraycopy J 2004 C2 io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(ILio/netty/buffer/ByteBuf;II)Lio/netty/buffer/ByteBuf; (16 bytes) @ 0x00007f36d578d132 [0x00007f36d578cfe0+0x152] J 1955 C2 io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(Lio/netty/channel/AbstractChannelHandlerContext;Ljava/lang/Object;)V (53 bytes) @ 0x00007f36d574ef70 [0x00007f36d574e800+0x770] J 3093 C2 io.netty.channel.nio.NioEventLoop.processSelectedKeys()V (33 bytes) @ 0x00007f36d59a953c [0x00007f36d59a8f00+0x63c] j io.netty.channel.nio.NioEventLoop.run()V+126 j io.netty.util.concurrent.SingleThreadEventExecutor$5.run()V+44 j io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run()V+4 j java.lang.Thread.run()V+11 v ~StubRoutines::call_stub V [libjvm.so+0x68c616] JavaCalls::call_helper(JavaValue* methodHandle* JavaCallArguments* Thread*)+0x1056 V [libjvm.so+0x68cb21] JavaCalls::call_virtual(JavaValue* KlassHandle Symbol* Symbol* JavaCallArguments* Thread*)+0x321 V [libjvm.so+0x68cfc7] JavaCalls::call_virtual(JavaValue* Handle KlassHandle Symbol* Symbol* Thread*)+0x47 V [libjvm.so+0x723d80] thread_entry(JavaThread* Thread*)+0xa0 V [libjvm.so+0xa69dcf] JavaThread::thread_main_inner()+0xdf V [libjvm.so+0xa69efc] JavaThread::run()+0x11c V [libjvm.so+0x91d9d8] java_start(Thread*)+0x108 C [libpthread.so.0+0x7aa1] start_thread+0xd1 ``` 2. The 1 0 25 version startup parameter configuration is as follows ``` jvm_args: -Xms2048m -Xmx3072m -Xmn1024m -XX:SurvivorRatio=2 -XX:PermSize=96m -XX:MaxPermSize=256m -Xss256k -XX:-UseAdaptiveSizePolicy -XX:MaxTenuringThreshold=15 -XX:+DisableExplicitGC -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:+UseCMSCompactAtFullCollection -XX:+UseFastAccessorMethods -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -Djava.awt.headless=true -Djava.net.preferIPv4Stack=true -Dfile.encoding=UTF-8 -DappName=otter-canal -Dlogback.configurationFile=/opt/canal-server/bin/../conf/logback.xml -Dcanal.conf=/opt/canal-server/bin/../conf/canal.properties ``` 3. canal.properties ``` canal.id= 1 canal.ip= canal.port= 11111 canal.zkServers=172.42.11.9:2181 172.42.11.8:2181 172.42.11.7:2181 # flush data to zk canal.zookeeper.flush.period = 1000 # flush meta cursor/parse position to file canal.file.data.dir = ${canal.conf.dir} canal.file.flush.period = 1000 ## memory store RingBuffer size should be Math.pow(2 n) canal.instance.memory.buffer.size = 16384 ## memory store RingBuffer used memory unit size default 1kb canal.instance.memory.buffer.memunit = 1024 ## meory store gets mode used MEMSIZE or ITEMSIZE canal.instance.memory.batch.mode = MEMSIZE ## detecing config canal.instance.detecting.enable = true #canal.instance.detecting.sql = insert into retl.xdual values(1 now()) on duplicate key update x=now() canal.instance.detecting.sql = select 1 canal.instance.detecting.interval.time = 3 canal.instance.detecting.retry.threshold = 3 canal.instance.detecting.heartbeatHaEnable = false # support maximum transaction size more than the size of the transaction will be cut into multiple transactions delivery canal.instance.transaction.size = 1024 # mysql fallback connected to new master should fallback times canal.instance.fallbackIntervalInSeconds = 60 # network config canal.instance.network.receiveBufferSize = 16384 canal.instance.network.sendBufferSize = 16384 canal.instance.network.soTimeout = 30 # binlog filter config canal.instance.filter.query.dcl = false canal.instance.filter.query.dml = false canal.instance.filter.query.ddl = false canal.instance.filter.table.error = false canal.instance.filter.rows = false # binlog format/image check canal.instance.binlog.format = ROW STATEMENT MIXED canal.instance.binlog.image = FULL MINIMAL NOBLOB # binlog ddl isolation canal.instance.get.ddl.isolation = false ################################################# ######### destinations ############# ################################################# canal.destinations= example example2 # conf root dir canal.conf.dir = ../conf # auto scan instance dir add/remove and start/stop instance canal.auto.scan = true canal.auto.scan.interval = 5 canal.instance.global.mode = spring canal.instance.global.lazy = false #canal.instance.global.manager.address = 127.0.0.1:1099 #canal.instance.global.spring.xml = classpath:spring/memory-instance.xml #canal.instance.global.spring.xml = classpath:spring/file-instance.xml canal.instance.global.spring.xml = classpath:spring/default-instance.xml ``` 4. Canal often reports this error during the run but can automatically reconnect later ``` 2018-01-03 11:03:57.331 [destination = example address = /172.42.11.50:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4832) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:160) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:759) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:428) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:117) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:66) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:337) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:184) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:145) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) at java.lang.Thread.run(Thread.java:745) Caused by: java.io.IOException: should execute connector.connect() first at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.<init>(MysqlQueryExecutor.java:30) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:87) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMetaByDB(TableMetaCache.java:80) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:30) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:55) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:50) at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) at com.google.common.cache.LocalCache.get(LocalCache.java:3937) at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941) at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824) at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830) ... 10 more ] ``` 2018-01-03 11:03:57.331 [destination = example address = /172.42.11.50:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4832) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:160) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:759) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:428) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:117) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:66) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:337) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:184) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:145) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) at java.lang.Thread.run(Thread.java:745) Caused by: java.io.IOException: should execute connector.connect() first I also encountered a solution to this problem? reference #482
473,connect mysql server failed when reconnect mysql server frequently cannot start dump biz due to mysql connection is disconnected by timeout. root cause multi-thread issue (netty i/o thread vs. biz thread). when invoke connect() it will return a ChannelFuture instance and then the "addListener" method is invoked by biz thread however "channelRead" method in BusinessHandler is executed by netty I/O thread. ChannelFutureListener -> operationComplete() cannot guarantee that which is executed before than BusinessHandler -> channelRead() method. It will caused new connection is timeout when AbstractEventParser -> start() -> parseThread ->erosaConnection.reconnect() is invoked. ENV: canal: 1.0.25 open-jdk 1.8 OS: CentOS 6.5 -Dio.netty.eventLoopThreads: 1 (very important) another hot bug: mysql dump connection will be disconnected about 10' minutes. It's caused by using Netty4. I'm trying to figure it out. tks
472,CanalConnector checkValid improves check if the link is legal Added judgment on the following rules - Link canal Server failed has not been a link available to return false This PR feels a lot of other changes. I don’t understand why the changes to the parser search site are based on what considerations. I am very sorry for this pull. request Inadvertently submitted other local modifications not related to the trunk I withdraw it here.
471,Ask about the canal client report com alibaba otter canal protocol exception CanalClientException java.io.IOException: end of stream when reading Header error while (true) { Message message = connector.getWithoutAck(1); // Get the specified amount of data long batchId = message.getId(); int size = message.getEntries().size(); if (batchId == -1 || size == 0) { System.out.println("empty count : " + emptyCount); } else { System.out.printf("message[batchId=%s size=%s] \n" batchId size); } try { Thread.sleep(40000); } catch (InterruptedException e) { } connector.ack(batchId); // Submit confirmation } Above I took the data sleep 40 seconds canal connection will break the error of the report description But if I just have Thread sleep 5000 sleep for 5 seconds, there will be no problem for guidance. This is the reason. Based on the latest version 1 0 25 I have encountered the same problem. From the log, it is canal. The server actively closes the connection may be a strategy to avoid the synchronization interruption caused by the client blocking. Waiting for an authoritative answer The soTimeout set in SimpleCanalConnector will be passed to the server for timeout management default 60 seconds. This soTimeout time means that if the client does not process the batch of data, the server will terminal the client during this time period. In the program, the client takes 1000000 data processing time per batch for more than 1 minute and then causes this exception and this 1000000 The data will be processed repeatedly and then every time it is more than 1 minute, the loop will be reported. This exception will change the soTimeout to 120 seconds. How to solve this problem? Https github com alibaba canal issues 640 Take a look at this
470,The client of the canal always reports canal com alibaba otter canal protocol exception CanalClientException java.io.IOException: end of stream when reading Header error Generally requires client retry I also encountered the same problem, the key is handshake Coming out of time Behind ClientAuth Haven&#39;t sent it yet? Caused by: java.io.IOException: end of stream when reading header at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.read(SimpleCanalConnector.java:401) ~[classes/:na] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.readNextPacket(SimpleCanalConnector.java:392) ~[classes/:na] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.readNextPacket(SimpleCanalConnector.java:373) ~[classes/:na] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.doConnect(SimpleCanalConnector.java:140) ~[classes/:na] ... 4 common frames omitted
469,Canal support ddl? Wiki documents that support ddl parsing
468,CanalServerException: destination:example1 should start first How does the server confirm the instance startup? Turn on auto scan After Added a new instance of example1 under conf Can be seen in the log 2017-12-29 14:24:01.056 [canal-instance-scan-0] INFO c.a.o.canal.deployer.monitor.SpringInstanceConfigMonitor - auto notify start example1 successful. The log shows that the newly added instance is started. But the logs for client access and server records are only exception=com.alibaba.otter.canal.server.exception.CanalServerException: destination:example1 should start first This kind of exception is very strange. There is no log message about example1 on the server. The example is able to generate logs normally. So I would like to ask how to determine if the instance is started without neither the log output nor the service exception throwing only when access is made. should start First, it’s hard to let people track the problem. You set the lazy mode to false Enable instance automatic scanning If the configuration is true, the instance configuration change in the canal conf dir directory will be triggered automatically. a. Instance directory added Trigger instance configuration automatically starts when lazy is true b. The instance directory is deleted and the corresponding instance is configured. If it is started, it is closed. c. Instance properties file change reload Instance configuration, if started, automatic restart operation I followed this prompt to put lazy Set as The original default of true is false I am impressed that lazy false is the default value lazy true will be initialized when the first client accesses Oh, thank you
467,AbstractEventParser&#39;s buildLastPosition function offset is not the plus event length is more appropriate com.alibaba.otter.canal.parse.inbound.AbstractEventParser Canal Deployer 1.0.24 Original code protected LogPosition buildLastPosition(CanalEntry.Entry entry) { // Initialize LogPosition logPosition = new LogPosition(); EntryPosition position = new EntryPosition(); position.setJournalName(entry.getHeader().getLogfileName()); position.setPosition(entry.getHeader().getLogfileOffset()); position.setTimestamp(entry.getHeader().getExecuteTime()); // add serverId at 2016-06-28 position.setServerId(entry.getHeader().getServerId()); logPosition.setPostion(position); LogIdentity identity = new LogIdentity(runningInfo.getAddress() -1L); logPosition.setIdentity(identity); return logPosition; } But in the setPosition, the only part of the event is written here. When we save it into the ZK and other locations, we don&#39;t want this event to be processed again. Is it the way to change this? position.setPosition(entry.getHeader().getLogfileOffset()+entry.getHeader().getEventLength()); After I changed it, I measured it and finally saved it. It is the end of Event. I see that the code in the code has been very troublesome. The starting position of the restart may not be the end of a transaction. Now it should be handled much better. I also added an event such as XID DDL in the code to save the offset. The event is not saved. It is so big that it will not be reprocessed, but it should not happen again. The starting position is not at the beginning of the transaction. I received a reminder that these are only useful in the direct Ack mode. My scenario is equivalent to direct Ack. This change is only useful to me. It is useless to Canal.
466,canal报CanalParseException  parse row data failed. error I am based on https github com alibaba canal issues 161 Run the FileLogFetcherTest test java.io.IOException: Error binlog file header: [109 121 115 113] What is the problem?
465,Canal synchronization deadlock problem How to deal with deadlocks 2017-12-25 14:18:54 295 (main) [INFO - com.trcloud.hamal.stream.core.operator.impl.sink.GPSinkOperator.processAutoCommit(GPSinkOperator.java:138)] current sql is: INSERT INTO ods_fina_bj.account_user("account_type" "balance" "bank_frozen" "create_date" "cust_no" "del_flag" "id" "local_frozen" "parent_id" "remarks" "update_date" "user_id" "storm_time" "etl_time") VALUES('1' '0.0' '0.0' '2017-12-25 14:18:26' '201712251418330723046283' '0' '8a32f14f815a40f5868ba7f0c8286cfe' '0.0' null DTS borrower opens an account '2017-12-25 14:18:26' 'C33DBA5ADEB0415DBC4AE36CA3B66E9B' '2017-12-25 14:18:26' '2017-12-25 14:18:37'); 2017-12-25 14:19:04 297 (main) [ERROR - com.trcloud.hamal.stream.core.operator.impl.sink.GPSinkOperator.processAutoCommit(GPSinkOperator.java:159)] exec sql one by one failed: org.postgresql.util.PSQLException: ERROR: deadlock detected Detail: Process 71574 waits for RowExclusiveLock on relation 2388238 of database 23968; blocked by process 24634. Process 24634 waits for ExclusiveLock on relation 2388316 of database 23968; blocked by process 59646. Process 59646 waits for RowExclusiveLock on relation 2388238 of database 23968; blocked by process 35132. Process 35132 waits for ExclusiveLock on relation 2388238 of database 23968; blocked by process 71574. at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2182) at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:1911) at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:173) at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:622) at org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:458) at org.postgresql.jdbc.PgStatement.executeUpdate(PgStatement.java:406) at com.zaxxer.hikari.pool.ProxyStatement.executeUpdate(ProxyStatement.java:120) at com.zaxxer.hikari.pool.HikariProxyStatement.executeUpdate(HikariProxyStatement.java) at com.trcloud.hamal.stream.core.operator.impl.sink.GPSinkOperator.processAutoCommit(GPSinkOperator.java:139) at com.trcloud.hamal.stream.core.operator.impl.sink.GPSinkOperator.process(GPSinkOperator.java:92) at com.trcloud.hamal.stream.core.operator.impl.canal.CanalOperatorChain.start(CanalOperatorChain.java:49) at com.trcloud.hamal.stream.core.StreamJob.start(StreamJob.java:121) at com.trcloud.hamal.stream.core.JobRunner.main(JobRunner.java:46) This is not a native canal support. Look at the error should be extended to extract pg data. canal Is there a way to circumvent or solve this problem with wingerx For example, the following parameters are OK #canal batch size canal.batch.size=100
463,canal The ddl statement re-executes again from the time it was executed before. Where do I need to filter out the executions that have already been consumed? 2017-12-22 14:12:47 365 (main) [INFO - com.trcloud.hamal.stream.core.operator.impl.canal.CanalSource.parseColumns(CanalSource.java:238)] parse ddl sql : ALTER TABLE `account_user_info_stream_test` ADD COLUMN `test001` varchar(255) NOT NULL AFTER `modifiedon` 2017-12-22 14:12:47 375 (main) [INFO - com.trcloud.hamal.stream.core.operator.impl.canal.CanalSource.parseColumns(CanalSource.java:336)] ddl result map : {"test001":{"curName":"test001" "key":false "nulled":false "oldName":"" "operPimay":false "operType":"ADD" "type":"varchar(255)"}} 2017-12-22 14:12:47 390 (main) [INFO - com.trcloud.hamal.stream.core.operator.impl.canal.Canal2SqlOperator.transform(Canal2SqlOperator.java:77)] sqlDecorators: [{"eventType":"ALTER" "sql":"ALTER TABLE test.account_user_info_stream_test ADD COLUMN test001 varchar(255) NOT NULL DEFAULT '' ;" "storm_time":"2017-12-22 14:11:23" "tableName":"account_user_info_stream_test"}] 2017-12-22 14:12:47 432 (main) [WARN - com.trcloud.hamal.stream.core.operator.impl.sink.GPSinkOperator.process(GPSinkOperator.java:92)] batch exec error try exec one by one 2017-12-22 14:12:47 433 (main) [INFO - com.trcloud.hamal.stream.core.operator.impl.sink.GPSinkOperator.processAutoCommit(GPSinkOperator.java:141)] current sql is: ALTER TABLE test.account_user_info_stream_test ADD COLUMN test001 varchar(255) NOT NULL DEFAULT '' ; 2017-12-22 14:12:47 465 (main) [ERROR - com.trcloud.hamal.stream.core.operator.impl.sink.GPSinkOperator.processAutoCommit(GPSinkOperator.java:168)] exec sql one by one failed: org.postgresql.util.PSQLException: ERROR: column "test001" of relation "account_user_info_stream_test" already exists at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2182) at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:1911) at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:173) at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:622) at org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:458) at org.postgresql.jdbc.PgStatement.executeUpdate(PgStatement.java:406) at com.zaxxer.hikari.pool.ProxyStatement.executeUpdate(ProxyStatement.java:120) at com.zaxxer.hikari.pool.HikariProxyStatement.executeUpdate(HikariProxyStatement.java) at com.trcloud.hamal.stream.core.operator.impl.sink.GPSinkOperator.processAutoCommit(GPSinkOperator.java:142) at com.trcloud.hamal.stream.core.operator.impl.sink.GPSinkOperator.process(GPSinkOperator.java:95) at com.trcloud.hamal.stream.core.operator.impl.canal.CanalOperatorChain.start(CanalOperatorChain.java:49) at com.trcloud.hamal.stream.core.StreamJob.start(StreamJob.java:121) at com.trcloud.hamal.stream.core.JobRunner.main(JobRunner.java:46) 2017-12-22 14:12:47 468 (main) [ERROR - com.trcloud.hamal.stream.core.operator.impl.sink.GPSinkOperator.process(GPSinkOperator.java:97)] exec sql one by one failed: 200001: sink sql failed! 2017-12-22 14:12:49 165 (main) [ERROR - com.trcloud.hamal.stream.core.operator.impl.canal.CanalOperatorChain.start(CanalOperatorChain.java:51)] 200001: sink sql failed! Duplicate data can only be recorded and filtered by the client.
462,canal Timestamp specification Look at the documentation canal.instance.master.timestamp : Specify a timestamp canal will automatically traverse mysql Binlog finds the binlog bit corresponding to the timestamp and starts it So what is the format of the format must be written at the same time position and timestamp? canal.instance.standby.timestamp = 1513844601 Error instance standby timestamp = ‘2017-12-22 xx:xx:xx’ The same mistake The error is as follows ![canal_err02](https://user-images.githubusercontent.com/34462344/34283126-062ed3c2-e705-11e7-93d5-802275d5a35b.png) position info conf: canal.instance.master.address = xxxxxx:3306 canal.instance.master.journal.name = MariaDB-bin.000001 canal.instance.master.position = canal.instance.master.timestamp = 1513845143 canal.instance.standby.address = xxxxxxxxx:3306 canal.instance.standby.journal.name = MariaDB-bin.000004 canal.instance.standby.position = canal.instance.standby.timestamp = 1513844601 Write the corresponding timestamp
461,Change the configuration file of the destination. The entire Canal process is exited. Is it supposed to be restarted? Hello, everyone I am using 1 0 24 canal deployer。 Main configuration canal.destinations= example canal.conf.dir = ../conf canal.auto.scan = true canal.auto.scan.interval = 5 canal.instance.global.mode = spring canal.instance.global.lazy = false canal.instance.global.spring.xml = classpath:spring/default-instance.xml ———————————————————— Then I tried to change the example instance properties. I added something to the black regex and saw that the entire Canal exited. I simply looked at the code and didn&#39;t understand why I didn&#39;t restart the example. logs/example/example.log： 2017-12-20 15:31:15.241 [canal-instance-scan-0] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - stop CannalInstance for null-example 2017-12-20 15:31:15.241 [canal-instance-scan-0] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - disConnect MysqlConnection to edc0.ecloud.com/132.122.1.162:6606... 2017-12-20 15:31:15.246 [destination = example address = /132.122.1.162:6606 EventParser] INFO c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O interrupted while reading from client socket java.nio.channels.ClosedByInterruptException: null at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202) ~[na:1.8.0_66] at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:407) ~[na:1.8.0_66] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:154) ~[canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70) ~[canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122) [canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:222) [canal.parse-1.0.24.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_66] 2017-12-20 15:31:15.246 [destination = example address = /132.122.1.162:6606 EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - the channel edc0.ecloud.com/132.122.1.162:6606 is not connected 2017-12-20 15:31:15.246 [destination = example address = /132.122.1.162:6606 EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - disConnect MysqlConnection to edc0.ecloud.com/132.122.1.162:6606... 2017-12-20 15:31:15.247 [destination = example address = /132.122.1.162:6606 EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - connect MysqlConnection to edc0.ecloud.com/132.122.1.162:6606... 2017-12-20 15:31:15.247 [destination = example address = /132.122.1.162:6606 EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - handshake initialization packet received prepare the client authentication packet to send 2017-12-20 15:31:15.248 [destination = example address = /132.122.1.162:6606 EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - client authentication packet is sent out. 2017-12-20 15:31:15.248 [destination = example address = /132.122.1.162:6606 EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - disConnect MysqlConnection to edc0.ecloud.com/132.122.1.162:6606... 2017-12-20 15:31:15.249 [canal-instance-scan-0] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - stop successful.... 2017-12-20 15:31:15.249 [canal-instance-scan-0] INFO c.a.otter.canal.server.embedded.CanalServerWithEmbedded - stop CanalInstances[example] successfully logs/canal/canal.log： 2017-12-20 15:14:05.117 [main] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## the canal server is running now ..... . 2017-12-20 15:31:15.250 [Thread-4] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## stop the canal server 2017-12-20 15:31:15.254 [Thread-4] INFO com.alibaba.otter.canal.deployer.CanalController - ## stop the canal server[132.122.1 .162:11111] 2017-12-20 15:31:15.254 [Thread-4] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## canal server is down. How to close the example of this Destination leads to the exit of the JVM. canal.pid The file is still Sorry I have commented out the start and stop of the canalServer and restored it.
460,ddl Appears 305 241 305 202 342 211 240 305 233 304 226 303 241 Direct printing is garbled eventType: ALTER isDdl: true sql: "ALTER TABLE `account_user_info_stream_test`\r\nADD COLUMN `test001` varchar(255) NULL DEFAULT \'\' COMMENT \'\305\241\305\202\342\211\240\305\233\304\226\303\241\' AFTER `modifiedon`" ddlSchemaName: "test" parse ddl sql : ALTER TABLE `account_user_info_stream_test` ADD COLUMN `test001` varchar(255) NULL DEFAULT '' COMMENT 'šł≠śĖá' AFTER `modifiedon` @agapple How to transcode this? The default should be iso 8859 1 encoding according to this transcode look Tried new String(sql "ISO8859-1")... Useless From CanalEntry Entry Get in entry Entry result：header { version: 1 logfileName: "mysql-bin.006352" logfileOffset: 1073084521 serverId: 3 serverenCode: "UTF-8" executeTime: 1513863317000 sourceType: MYSQL schemaName: "test" tableName: "account_user_info_stream_test" eventLength: 203 eventType: ALTER } entryType: ROWDATA storeValue: "\020\005P\001Z\224\001alter table test.account_user_info_stream_test add `test00001_14` varchar(25) not null DEFAULT \'2\' comment \'\305\233\304\266\304\215\305\244\304\256\304\223\305\232\303\244\302\243\305\232\303\244\305\205\305\232\303\251\303\234\305\232\305\271\342\211\244\'r\004test" CanalEntry.RowChange rowChange = null; try { rowChange = CanalEntry.RowChange.parseFrom(entry.getStoreValue()); ............ RowChange result：eventType: ALTER isDdl: true sql: "alter table test.account_user_info_stream_test add `test00001_14` varchar(25) not null DEFAULT \'2\' comment \'\305\233\304\266\304\215\305\244\304\256\304\223\305\232\303\244\302\243\305\232\303\244\305\205\305\232\303\251\303\234\305\232\305\271\342\211\244\'" ddlSchemaName: "test" Is it here CanalEntry RowChange parseFrom has a problem @agapple CanalEntry.Column Chinese output is normal CanalEntry.RowChange Chinese seems to be directly above a string similar to octal @agapple You look at the debug mode from mysql The data received in binlog is 啥 QueryLogEvent Because there is no way to debug the code locally I tried to parse MySQL with OpenReplicator. binlog** 2017-12-25 19:03:29 469 [binlog-parser-1] INFO [com.taihe.cloud.binlog.InstanceListener] - cdcEvent:{ eventId:66 databaseName:test tableName:`account_user_info_stream_test` add eventType:2 timestamp:1514199796000 timestampReceipt:1514199809467 binlogName:null position:398723955 nextPostion:398724150 serverId:3 isDdl:true sql:ALTER TABLE `account_user_info_stream_test` ADD COLUMN `test002` varchar(255) NULL COMMENT Chinese AFTER `test00001_2` before:null after:null} { "eventId": 66 "databaseName": "test" "tableName": "`account_user_info_stream_test`\r\nadd" "eventType": 2 "timestamp": 1514199796000 "timestampReceipt": 1514199809467 "binlogName": null "position": 398723955 "nextPostion": 398724150 "serverId": 3 "before": null "after": null "isDdl": true "sql": "ALTER TABLE `account_user_info_stream_test`\r\nADD COLUMN `test002` varchar(255) NULL COMMENT U0027中文u0027 AFTER `test00001_2`" } Also use canal Then display parsing garbled Entry result：header { version: 1 logfileName: "mysql-bin.006360" logfileOffset: 398723955 serverId: 3 serverenCode: "UTF-8" executeTime: 1514199796000 sourceType: MYSQL schemaName: "test" tableName: "account_user_info_stream_test" eventLength: 195 eventType: ALTER } entryType: ROWDATA storeValue: "\020\005P\001Z\200\001ALTER TABLE `account_user_info_stream_test`\r\nADD COLUMN `test002` varchar(255) NULL COMMENT \'\305\241\305\202\342\211\240\305\233\304\226\303\241\' AFTER `test00001_2`r\004test" 2017-12-25 19:02:53 902 (main) [INFO - com.trcloud.hamal.stream.core.operator.impl.canal.CanalSource.fetchEvent(CanalSource.java:142)] RowChange result：eventType: ALTER isDdl: true sql: "ALTER TABLE `account_user_info_stream_test`\r\nADD COLUMN `test002` varchar(255) NULL COMMENT \'\305\241\305\202\342\211\240\305\233\304\226\303\241\' AFTER `test00001_2`" ddlSchemaName: "test" 2017-12-25 19:02:53 903 (main) [INFO - com.trcloud.hamal.stream.core.operator.impl.canal.CanalSource.buildEvent(CanalSource.java:179)] RowChange result：eventType: ALTER isDdl: true sql: "ALTER TABLE `account_user_info_stream_test`\r\nADD COLUMN `test002` varchar(255) NULL COMMENT \'\305\241\305\202\342\211\240\305\233\304\226\303\241\' AFTER `test00001_2`" ddlSchemaName: "test" 2017-12-25 19:02:53 904 (main) [INFO - com.trcloud.hamal.stream.core.operator.impl.canal.CanalSource.parseColumns(CanalSource.java:239)] parse ddl sql : ALTER TABLE `account_user_info_stream_test` ADD COLUMN `test002` varchar(255) NULL COMMENT 'šł≠śĖá' AFTER `test00001_2` 2017-12-25 19:02:53 914 (main) [INFO - com.trcloud.hamal.stream.core.operator.impl.canal.CanalSource.parseColumns(CanalSource.java:337)] ddl result map : {"test002":{"comment":"šł≠śĖá" "curName":"test002" "key":false "nulled":true "oldName":"" "operPimay":false "operType":"ADD" "type":"varchar(255)"}} 2017-12-25 19:02:53 954 (main) [INFO - com.trcloud.hamal.stream.core.operator.impl.canal.Canal2SqlOperator.transform(Canal2SqlOperator.java:77)] sqlDecorators: [{"eventType":"ALTER" "sql":"ALTER TABLE test.account_user_info_stream_test ADD COLUMN test002 varchar(255) NULL DEFAULT '' ;COMMENT ON COLUMN test.account_user_info_stream_test.test002 IS 'šł≠śĖá';" "storm_time":"2017-12-25 19:03:16" "tableName":"account_user_info_stream_test"}] ![image](https://user-images.githubusercontent.com/834743/34376917-6f694958-eb29-11e7-81b1-90119700a9dd.png) I verified here that I can get the correct code, please make sure to build the table binlog The code of dump is consistent. I set all of them to utf8 here. binlog Which code is consistent with the dump code? Built the table and looked at it It seems to be UTF8 mysql> show global variables like '%character%'; +--------------------------+-------------------------------------+ | Variable_name | Value | +--------------------------+-------------------------------------+ | character_set_client | utf8 | | character_set_connection | utf8 | | character_set_database | utf8 | | character_set_filesystem | binary | | character_set_results | utf8 | | character_set_server | utf8 | | character_set_system | utf8 | | character_sets_dir | /usr/share/percona-server/charsets/ | +--------------------------+-------------------------------------+ 8 rows in set (0.00 sec) mysql> canal client Configuration is also UTF 8 Through the official open source canal parse dbsync Unit test to measure binlog QueryLogEvent The output is garbled. The dml statement is normal. ddl has a problem Binlog also viewed Also Chinese 16:47:31.330 [main] INFO c.taobao.tddl.dbsync.binlog.LogEvent - common_header_len= 19 number_of_event_types= 35 sql : ALTER TABLE `account_user_info_stream_test` DROP COLUMN `test001_1` DROP COLUMN `testt223` sql : ALTER TABLE `account_user_info_stream_test` ADD COLUMN `test001` varchar(255) NULL COMMENT 'śąĎśėĮšł≠śĖáÔľĆšĹ†šĻĪÁ†ĀšļÜŚźóÔľü' AFTER `modifiedon` sql : ALTER TABLE `account_user_info_stream_test` ADD COLUMN `test002` varchar(255) NULL COMMENT 'śąĎŚįĪšłćšŅ°šĹ†šĻĪÁ†Ā' AFTER `test001` sql : ALTER TABLE `account_user_info_stream_test` ADD COLUMN `test003` varchar(255) NULL COMMENT 'śÄéšĻąŚŹĮšĽ•šĻĪÁ†ĀŚĎĘ' sql : ALTER TABLE `account_user_info_stream_test` ADD COLUMN `test004` varchar(255) NULL COMMENT 'šłļšĽÄšĻąšĻĪÁ†Ā' sql : ALTER TABLE `account_user_info_stream_test` ADD COLUMN `test005` varchar(255) NULL COMMENT 'šłļšĽÄšĻąšĻĪÁ†Ā112233' The final investigation can only be done here to confirm whether this piece has a bug. com.taobao.tddl.dbsync.binlog.LogBuffer: /** * Return next 16-bit unsigned int from buffer. (little-endian) * * @see mysql-5.1.60/include/my_global.h - uint2korr */ public final int getUint16() { if (position + 1 >= origin + limit) throw new IllegalArgumentException("limit excceed: " + (position - origin + 1)); byte[] buf = buffer; return (0xff & buf[position++]) | ((0xff & buf[position++]) << 8); } This code gets 45 Not 33 I want to ask the big god here to resolve the mysql version? Local use mysql version 5 6 27 charsetId 45 javaCharset "MacCentralEurope" (id=77) mysqlCharset "utf8mb4" (id=80) mysqlCollation "utf8mb4_general_ci" (id=81) I have the same phenomenon when I ask the next question.
459,How to restart the instance without restarting the server How can I restart the instance because I want to apply position? s position If stop Start again if Will cause other instances to also receive an impact Remove the folder and add it back @agapple Ok Ok I will try it out. Can you write this method to the wiki? inside Test valid Close issue
458,How to set position position I will set it in the instance again. position But what you actually get is Latest location Then looked at the log show master status This must be the latest log, then how to set the position ？ ![image](https://user-images.githubusercontent.com/11556152/34143020-7cd03918-e4c5-11e7-9d17-cf606f8bf555.png) ![image](https://user-images.githubusercontent.com/11556152/34143026-8cbb85f8-e4c5-11e7-8fbe-c2cb78226649.png) I set up timestmp All right I am jealous. There is no need to give position There are spaces behind I am just like this.
457,Client subscribe Screening has failed Hello, I am using canal 1.0.24 deployer with client Are this version The first time I set the client Subscribe It is valid to get the data can be filtered normally, but I changed the deployer Middle positon And deleted meta.dat I want to re-insert the data and find that I can download the data to other libraries. In fact, I only need two tables. What is the problem with God? Client filtering is dm_data person dm_data.student As a result, I downloaded data from other databases. Now meta Data can be seen I only need dm_data Library data ![image](https://user-images.githubusercontent.com/11556152/34139754-70e7f47e-e4b1-11e7-8a96-133c720f9a58.png) Now download the data to other libraries. ![image](https://user-images.githubusercontent.com/11556152/34140115-7e1973be-e4b3-11e7-9150-d4e5f4e83c99.png)dm_ I can try to reproduce it a few times. Delete conf Meta dat in the example Then restart Re-specify instance properties Position in It is invalid Workaround now Is in instance.properties Write another filter Is the condition of the client&#39;s subscribe sent every time? I encountered a similar problem. I can&#39;t monitor the data when I filter a large list of individual tables, but I can monitor the data smoothly when I only monitor this table. 参考 https github com alibaba canal wiki FAQ
456,server ha simulation Downtime something goes wrong when getWithoutAck data from server:null something goes wrong when getWithoutAck data from server:null How can I break thank you? Choose the latest version of canal1 0 25? Canal1 0 25 now also has this problem in the client of the canal error I also encountered this problem. 1.0.25 Look at the exception corresponding to the server Yesterday when testing abnormalities on the client side, no abnormality test on the server side was found. The client mainly had an exception in two aspects. The client started and connected to the server multiple times. 2 The client has this exception when the server performs HA switching.
455,mysql semi support and mariadb gtid parse Mysql semi-synchronous semi protocol packet parsing mariadb gtid Analysis If mysql Mariadb started semi plugin Ddb semi 1 when the program starts Enable semi parsing mode [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=455) <br/>All committers have signed the CLA. tks
454,cancal bind ip address error Something goes wrong when starting up the canal Server: org.jboss.netty.channel.ChannelException: Failed to bind to xxxx Can canal only bind this machine ip? If the station vm is assigned a mapped ip, specifying this IP will report the above error. You can specify the bound ip 1.0.25 Binding vm mapping IP fails can only bind the private network ip of the local vm ![canal_err01](https://user-images.githubusercontent.com/34462344/34280405-6547bbc0-e6f2-11e7-9f3f-a08c14c2ceec.png)
453,Whether the client side resolves the duplicate after the master standby switchover If the problem is added after the configuration of master standby is resolved, stop the master once. After mysql switch to standby parsing sql again appears normal ![tim 20171215145732](https://user-images.githubusercontent.com/34462344/34030567-942a6efa-e1a8-11e7-9943-604a372e450f.png) As the binlog is different, but the same sql is parsed, please check if it is normal. Sorry, please turn off the test is not normal.
452,Galera Mariadb cluster Configuring multi-node resolution configuration Excuse me, if you use galera mariadb The cluster words as the master of the canal according to the document I need to write the master and standby information, then only need to configure their respective binlog file name和position  Because canal does not support the GTID 3 binlog information is not unified can achieve the effect of normal switching Timestamp can be used
451,Canal, what is wrong, what did you understand? Canal version canal.deployer-1.0.23 Mysql version 5 7 18 log The other party has to set the log bin master bin is not log_bin mysql bin I don&#39;t know if this is caused by the error and then the error is 2017-12-14 18:25:40.835 [destination = cmp_source address = /100.100.30.203:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position {"identity":{"slaveId":-1 "sourceAddress":{"address":"100.100.30.203" "port":3306}} "postion":{"included":false "journalName":"master-bin.000001" "position":92307730 "serverId":312 "timestamp":1513246966000}} 2017-12-14 18:25:41.257 [destination = cmp_source address = /100.100.30.203:3306 EventParser] WARN com.taobao.tddl.dbsync.binlog.LogDecoder - Decoding Query failed from: master-bin.000001:92307826 java.io.IOException: Read Q_FLAGS2_CODE error: limit excceed: 71 at com.taobao.tddl.dbsync.binlog.event.QueryLogEvent.unpackVariables(QueryLogEvent.java:650) ~[canal.parse.dbsync-1.0.23.jar:na] at com.taobao.tddl.dbsync.binlog.event.QueryLogEvent.<init>(QueryLogEvent.java:477) ~[canal.parse.dbsync-1.0.23.jar:na] at com.taobao.tddl.dbsync.binlog.LogDecoder.decode(LogDecoder.java:154) ~[canal.parse.dbsync-1.0.23.jar:na] at com.taobao.tddl.dbsync.binlog.LogDecoder.decode(LogDecoder.java:106) ~[canal.parse.dbsync-1.0.23.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:123) [canal.parse-1.0.23.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) [canal.parse-1.0.23.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_111] Caused by: java.lang.IllegalArgumentException: limit excceed: 71 at com.taobao.tddl.dbsync.binlog.LogBuffer.getUint32(LogBuffer.java:561) ~[canal.parse.dbsync-1.0.23.jar:na] at com.taobao.tddl.dbsync.binlog.event.QueryLogEvent.unpackVariables(QueryLogEvent.java:563) ~[canal.parse.dbsync-1.0.23.jar:na] ... 6 common frames omitted Binlog parsing error upgrade to 1 0 25
449,Canal when the specified pos happens to be the next data of the rowdata TableMap server The end will fall into an infinite loop BUG Test version canal version 1.0.24 1. Canal implements this function via LogEventConvert parseRowsEvent Thrown when parsing event TableIdNotFoundException， But parseRowsEvent Captured all Exception directly and rethrows CanalParseException such parseThread of AbstractEventParser The worker thread gets an exception that is always a Throwable exception and cannot perceive a TableIdNotFoundException causing an infinite loop. 2. Modify LogEventConvert parseRowsEvent After it throws a TableIdNotFoundException, the next loop will perceive that the needTransactionPosition variable will be true. After entering the findTransactionBeginPosition method, the seek method will be called. However, the seek method only parses the header of the transaction. Then the event event of the sink implemented in the current seek method is not resolved because The rowdata data is filtered so reDump It will never be true, and it will not enter the loop of the main thread and enter the infinite loop of the main thread. PS: Personal simple analysis I don&#39;t know if there is a misunderstanding or findTransactionBeginPosition is not to solve the agapple of this scene. Repair suggestion 1 1. 将LogEventConvert parseRowsEvent In catch Add a catch before CanalParseException TableIdNotFoundException and throw it out to let the upper layer continue to perceive 2. Change the first mysqlConnection seek in the MysqlEventParser findTransactionBeginPosition method to mysqlConnection dump Can solve this bug Fix 2 because MysqlEventParser findTransactionBeginPosition is only called when needTransactionPosition is true, so delete the first mysqlConnection in the MysqlEventParser findTransactionBeginPosition method. Let the logic directly enter the reDump logic to solve this problem. Reduce the dump request. This 1 0 25 I remember that if I have fixed it, I can try it first. Ok, I switched to 1 0 25 and try again. @agapple I measured the code of 1 0 25 and it is still very easy to reproduce through the following scenarios. 1. Binlog file content ![image](https://user-images.githubusercontent.com/33280738/33978900-d4cbc89a-e0dc-11e7-9104-bd4b3c083a0b.png) The red on the map is rowdata Where the data is located 981 2. Designation The starting point of the canal is 981 this position 3. Start server The observation log is as follows ![image](https://user-images.githubusercontent.com/33280738/33979023-720fb1de-e0dd-11e7-8baf-0c29da157c32.png) Can see that it is always in the loop start state The first mysqlconnection seek method via findTransactionBeginPosition is not found 981 this event must let 981 This event The logic that reoccurs TableIdNotFoundException reDump can be executed I pay attention to this piece.
448,Will the mysql and mariadb binlog have a difference between canal and mysql and mariadb binlog analysis is different Such as the question asks mysql and mariadb binlog has a difference between canal and mysql and mariadb binlog analysis is different Canal dbsync src main java com taobao tddl dbsync binlog event mariadb BinlogCheckPointLogEvent java in this class public BinlogCheckPointLogEvent(LogHeader header LogBuffer buffer FormatDescriptionLogEvent descriptionEvent){ super(header buffer descriptionEvent); // do nothing just mariadb binlog checkpoint Moderate do nothing just mariadb binlog checkpoint What do you mean by this class? See more wikis have descriptions Can I send a link or screenshot every time I find it? Thank you. @agapple BinlogChange MariaDB5 amp 10 seems to have been deleted on the wiki Then do you still have the original version? Link fixed
447,Solve the bug of destination infinite connection waiting When there are multiple destinations that are connected to the database at the same time, I have more than 8 here. There will be a high probability that some of the destinations will not get the database feedback, so that I can&#39;t wait for the data synchronization. Therefore, when the destination is initialized, the timeout processing is added. If the timeout is currently 3 seconds, an exception retry is thrown to try to establish the connection again. [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=447) <br/>All committers have signed the CLA. tks
445,Does the current version of can support maridb10 0 2 or more GTID? Such as the current stable version of mariadb is 10 2 10 Can you use GTID and canal to do binlog analysis? Does not support GTID Then, can I support the GTID of mysql5 7? Gtid is supported in version 26 and can be tested.
444,canal server abnormal 'show binlog events limit 1’ Morning binlog log was deleted in the afternoon to restart the task canal client The log has no abnormal canal Server exception 2017-12-11 19:35:30.882 [destination = fengdai_mqnotify address = /10.20.21.11:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /10.20.21.11:3306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: command : 'show binlog events limit 1' has an error! Caused by: java.io.IOException: Unexpected End Stream at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.readBytesAsBuffer(PacketManager.java:22) ~[canal.parse.driver-1.0.24.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.readHeader(PacketManager.java:13) ~[canal.parse.driver-1.0.24.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.readNextPacket(MysqlQueryExecutor.java:104) ~[canal.parse.driver-1.0.24.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.query(MysqlQueryExecutor.java:77) ~[canal.parse.driver-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:73) ~[canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPosition(MysqlEventParser.java:610) ~[canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findByStartTimeStamp(MysqlEventParser.java:514) ~[canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPositionInternal(MysqlEventParser.java:358) ~[canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPosition(MysqlEventParser.java:315) ~[canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:161) ~[canal.parse-1.0.24.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_66] I want to confirm why the morning log can still be written normally and restarted. @agapple com.alibaba.otter.canal.parse.exception.CanalParseException: command : 'show binlog events limit 1' has an error! Caused by: java.io.IOException: Unexpected End Stream Restart mysql or re-open binlog.
443,Canal set ZooKeeperMetaManager 1 canal in the zookeeper when switching HA will have repeated consumption problems 2 canal can not be modified very well to position Based on the above two points, I want to switch to ZooKeeperMetaManager, so I am currently modifying XXX instance xml but will report org I0Itec zkclient exception ZkNoNodeException org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for Otter canal destinations example 1001 mark Even after manually adding the node, the metadata still does not exist only in the zokeeper or the memory exists first and then the time is swiped into the zk @agapple Is there a better way to solve the above two problems? How can I more elegantly specify the position of the position to consume? Complete error exception=org.I0Itec.zkclient.exception.ZkNoNodeException: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /otter/canal/destinations/example/1001/mark at org.I0Itec.zkclient.exception.ZkException.create(ZkException.java:47) at org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:685) at org.I0Itec.zkclient.ZkClient.getChildren(ZkClient.java:413) at org.I0Itec.zkclient.ZkClient.getChildren(ZkClient.java:409) at com.alibaba.otter.canal.meta.ZooKeeperMetaManager.clearAllBatchs(ZooKeeperMetaManager.java:231) at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.rollback(CanalServerWithEmbedded.java:406) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:182) at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:100) at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:783) at org.jboss.netty.channel.SimpleChannelHandler.messageReceived(SimpleChannelHandler.java:154) at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:100) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:783) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:275) at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80) at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:783) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndFireMessageReceived(ReplayingDecoder.java:525) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:506) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:443) at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80) at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261) at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:349) at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:280) at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:200) at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:44) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Caused by: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /otter/canal/destinations/example/1001/mark at org.apache.zookeeper.KeeperException.create(KeeperException.java:111) at org.apache.zookeeper.KeeperException.create(KeeperException.java:51) at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1468) at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1496) at com.alibaba.otter.canal.common.zookeeper.ZooKeeperx.getChildren(ZooKeeperx.java:107) at org.I0Itec.zkclient.ZkClient$2.call(ZkClient.java:416) at org.I0Itec.zkclient.ZkClient$2.call(ZkClient.java:413) at org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:675) ... 34 more There is no guarantee in theory design that it will not be repeated. @agapple If the metadata only exists in zk, it will be slightly better on the issue of repetition, but the overall efficiency of the program will be reduced. This is a trade-off problem. Also would like to ask canal 1.0.24 If you want to switch to ZooKeeperMetaManager, you can modify it by modifying XXX instance xml. Although this will reduce the efficiency of the program to a certain extent, it has failed but failed. In addition, is there a way to elegantly specify the location for consumption without restarting, such as just modifying the position in zk? Currently there is no such API that can directly specify a site to start.
442,table There is a problem with the meta generation snapshot comparison method. In the DatabaseTableMeta applySnapshotToDB method, first obtain a copy of memoryTableMeta, tmpMemoryTableMeta, and then go to the master library to compare and then persist the snapshot if everything is normal. But in contrast to the ddl in the master library, the meta information is from the memoryTableMeta instead of the tmpMemoryTableMeta. See the compareTableMetaDbAndMemory method. This can lead to data inconsistency in the case of multi-threading fix From the tmpMemoryTableMeta, the meta information is taken during the comparison. >>>>>> private boolean compareTableMetaDbAndMemory(MysqlConnection connection final String schema final String table) { TableMeta tableMetaFromMem = memoryTableMeta.find(schema table); <<<<<< private boolean compareTableMetaDbAndMemory(MysqlConnection connection final String schema final String table，MemoryTableMeta tmpMemoryTableMeta) { TableMeta tableMetaFromMem = tmpMemoryTableMeta.find(schema table); ...... Look very carefully about this TSDB comparative understanding will have a concurrency problem
441,The canal client does not prompt the error server-side exception batchId 414 is not the Firstly 404 What is the cause of this? @agapple I would like to ask what the following circumstances lead to help. 2017-12-07 12:11:57.887 [New I/O server worker #1-4] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - subscribe filter change to bank_supervised_account_10.account_item_detail_rel_user bank_supervised_account_10.account_itemized_err_user bank_supervised_account_10.account_detail_err_user bank_supervised_account_10.account_detail_user bank_supervised_account_10.account_frozen_user bank_supervised_account_10.account_itemized_user bank_supervised_account_10.account_user bank_supervised_account_10.bank_user bank_supervised_account_10.company_user_info 2017-12-07 12:12:04.964 [New I/O server worker #1-4] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400 Caused by : something goes wrong with channel:[id: 0x1fe5e36c /10.203.151.61:38324 => /10.203.151.17:11111] exception=com.alibaba.otter.canal.meta.exception.CanalMetaManagerException: batchId:414 is not the firstly:404 2017-12-07 12:12:04.965 [New I/O server worker #1-4] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x1fe5e36c /10.203.151.61:38324 :> /10.203.151.17:11111] exception=java.nio.channels.ClosedChannelException at org.jboss.netty.channel.socket.nio.NioWorker.cleanUpWriteBuffer(NioWorker.java:643) at org.jboss.netty.channel.socket.nio.NioWorker.writeFromUserCode(NioWorker.java:370) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:137) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) at org.jboss.netty.channel.Channels.write(Channels.java:611) at org.jboss.netty.channel.Channels.write(Channels.java:578) at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:28) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:144) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:275) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndFireMessageReceived(ReplayingDecoder.java:525) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:506) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.cleanup(ReplayingDecoder.java:541) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.channelDisconnected(ReplayingDecoder.java:449) at org.jboss.netty.channel.Channels.fireChannelDisconnected(Channels.java:360) at org.jboss.netty.channel.socket.nio.NioWorker.close(NioWorker.java:593) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:119) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) at org.jboss.netty.channel.Channels.close(Channels.java:720) at org.jboss.netty.channel.AbstractChannel.close(AbstractChannel.java:200) at org.jboss.netty.channel.ChannelFutureListener$1.operationComplete(ChannelFutureListener.java:46) at org.jboss.netty.channel.DefaultChannelFuture.notifyListener(DefaultChannelFuture.java:381) at org.jboss.netty.channel.DefaultChannelFuture.addListener(DefaultChannelFuture.java:148) at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:30) at com.alibaba.otter.canal.server.netty.NettyUtils.error(NettyUtils.java:51) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:200) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:275) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndFireMessageReceived(ReplayingDecoder.java:525) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:506) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:443) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261) at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:349) at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:280) at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:200) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) batchId:414 is not the Does firstly 404 have out of order consumption? How to deal with this situation Not much change with the previous code This problem occurs when you restart the job again. Already found the problem Thank you Own sql parsing code uploading empty problem
440,Tsdb is enabled by default System startup has an exception Used version 1 0 25 Which defaults to tsdb Will report an error after startup ``` 2017-12-07 17:16:23.254 [destination = example address = /127.0.0.1:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /127.0.0.1:3306 has an error retrying. caused by java.lang.NullPointerException: null at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPositionInternal(MysqlEventParser.java:428) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPosition(MysqlEventParser.java:348) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:164) ~[canal.parse-1.0.25.jar:na] at java.lang.Thread.run(Unknown Source) [na:1.8.0_144] 2017-12-07 17:16:23.256 [destination = example address = /127.0.0.1:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.lang.NullPointerException at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPositionInternal(MysqlEventParser.java:428) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPosition(MysqlEventParser.java:348) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:164) at java.lang.Thread.run(Unknown Source) ``` will canal.instance.tsdb.spring.xml=classpath:spring/tsdb/h2-tsdb.xml The comment is just fine. Canal instance tsdb enable set to false does not work Also hope to have a detailed description of these parameters The binlog site that you started initially specifies that you should not have an NPE, and you will not specify an offline 4. Configuration is as follows ``` canal.instance.master.journal.name= mysql-bin.000001 canal.instance.master.position= 154 canal.instance.master.timestamp= ``` See the code change above But only the canal compilation does not pass Have a preview version of druid Is it necessary to modify it? Inconvenient to test Https github com alibaba druid can temporarily download druid package to execute mvn clean install Dmaven test skip to generate preview version Based on table The configuration of tsdb will try to determine the timestamp through binlog search if there is no timestamp of the corresponding location. If it is not found, it is the NPE here. table The historical multi-version design of meta is mainly based on timestamps, so there must be a timestamp at any point in time when starting. Encountered the same problem v1.0.26 On the version OK
439,parse faield : CREATE TABLE `columns_priv` The example log found this error The problem that the set type that is fed back in the group is not supported has been reported to the druid for repair. https://github.com/alibaba/druid/commit/f8731f9182f01353e53331cf8ad590c6ca3db416 Okay, we have temporarily disabled this feature.
437,Adding an extension field in the LogEventConvert parseOneRow method appears duplicate data on the client side In the LogEventConvert parseOneRow method Add extension field rowDataBuilder addProps createSpecialPair testName "testValue")); client end CanalEntry RowData received a duplicate key testName of Pair This way, you can not repeat the Pair Builder. builder = rowDataBuilder.addPropsBuilder(); Did not understand the specific problem
436,1 0 24 version HA mode deployment adds instance after server-side exception client does not prompt error 2017-12-05 19:07:43.385 [New I/O server worker #1-10] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400 Caused by : something goes wrong with channel:[id: 0x4926cac6 /192.168.182.17:2025 => /192.168.182.13:11111] exception=com.alibaba.otter.canal.server.exception.CanalServerException: ack error clientId:1001 batchId:1 is not exist please check 2017-12-05 19:07:43.390 [New I/O server worker #1-10] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x4926cac6 /192.168.182.17:2025 :> /192.168.182.13:11111] exception=java.nio.channels.ClosedChannelException at org.jboss.netty.channel.socket.nio.NioWorker.cleanUpWriteBuffer(NioWorker.java:643) at org.jboss.netty.channel.socket.nio.NioWorker.writeFromUserCode(NioWorker.java:370) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:137) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) at org.jboss.netty.channel.Channels.write(Channels.java:611) at org.jboss.netty.channel.Channels.write(Channels.java:578) at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:28) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:144) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:275) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndFireMessageReceived(ReplayingDecoder.java:525) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:506) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.cleanup(ReplayingDecoder.java:541) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.channelDisconnected(ReplayingDecoder.java:449) at org.jboss.netty.channel.Channels.fireChannelDisconnected(Channels.java:360) at org.jboss.netty.channel.socket.nio.NioWorker.close(NioWorker.java:593) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:119) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) at org.jboss.netty.channel.Channels.close(Channels.java:720) at org.jboss.netty.channel.AbstractChannel.close(AbstractChannel.java:200) at org.jboss.netty.channel.ChannelFutureListener$1.operationComplete(ChannelFutureListener.java:46) at org.jboss.netty.channel.DefaultChannelFuture.notifyListener(DefaultChannelFuture.java:381) at org.jboss.netty.channel.DefaultChannelFuture.addListener(DefaultChannelFuture.java:148) at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:30) at com.alibaba.otter.canal.server.netty.NettyUtils.error(NettyUtils.java:51) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:200) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:275) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndFireMessageReceived(ReplayingDecoder.java:525) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:506) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:443) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261) at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:349) at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:280) at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:200) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745) clientId:1001 batchId:1 is not exist please check Estimate trigger instance Reload the client and try again Restart the client to solve the thank you
435,How to turn off zookeeperDebug mode after starting canal client If the question is configured, close the zookeeperDebug mode. Google search
434,org.jboss.netty.channel.ChannelException: Failed to bind to: 0.0.0.0/0.0.0.0:11111 After the canal starts, the canal log gives an error and I don’t know how to solve it. OpenJDK 64-Bit Server VM warning: ignoring option PermSize=96m; support was removed in 8.0 OpenJDK 64-Bit Server VM warning: ignoring option MaxPermSize=256m; support was removed in 8.0 org.jboss.netty.channel.ChannelException: Failed to bind to: 0.0.0.0/0.0.0.0:11111 at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:303) at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223) at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74) OpenJDK 64-Bit Server VM warning: ignoring option PermSize=96m; support was removed in 8.0 OpenJDK 64-Bit Server VM warning: ignoring option MaxPermSize=256m; support was removed in 8.0 OpenJDK 64-Bit Server VM warning: UseCMSCompactAtFullCollection is deprecated and will likely be removed in a future release. 2017-12-04 19:36:38.282 [main] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## start the canal server. 2017-12-04 19:36:38.340 [main] INFO com.alibaba.otter.canal.deployer.CanalController - ## start the canal server[10.108.211.136:11111] 2017-12-04 19:36:38.736 [main] ERROR com.alibaba.otter.canal.deployer.CanalLauncher - ## Something goes wrong when starting up the canal Server: org.jboss.netty.channel.ChannelException: Failed to bind to: 0.0.0.0/0.0.0.0:11111 at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:303) at com.alibaba.otter.canal.server.netty.CanalServerWithNetty.start(CanalServerWithNetty.java:79) at com.alibaba.otter.canal.deployer.CanalController.start(CanalController.java:418) at com.alibaba.otter.canal.deployer.CanalLauncher.main(CanalLauncher.java:35) Caused by: java.net.BindException: Address already in use at sun.nio.ch.Net.bind0(Native Method) at sun.nio.ch.Net.bind(Net.java:433) at sun.nio.ch.Net.bind(Net.java:425) at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223) at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.bind(NioServerSocketPipelineSink.java:148) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleServerSocket(NioServerSocketPipelineSink.java:100) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:74) at org.jboss.netty.channel.Channels.bind(Channels.java:468) at org.jboss.netty.channel.AbstractChannel.bind(AbstractChannel.java:192) at org.jboss.netty.bootstrap.ServerBootstrap$Binder.channelOpen(ServerBootstrap.java:348) at org.jboss.netty.channel.Channels.fireChannelOpen(Channels.java:176) at org.jboss.netty.channel.socket.nio.NioServerSocketChannel.<init>(NioServerSocketChannel.java:85) at org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory.newChannel(NioServerSocketChannelFactory.java:142) at org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory.newChannel(NioServerSocketChannelFactory.java:90) at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:282) ... 3 more Caused by: java.net.BindException: Address already in use Port conflict
433,canal client Repeated data cannal Client Take data The first time I took it batchId=1 size=1 msg=Message[id=1 exit After the second time, the data becomes batchId=2 size=2 msg=Message[id=2 And the second time I got it was Two repeated data Have the same logfileName: "mysql-bin.000012" logfileOffset: 3207300 Restart canal Start after server Canal client It’s the first time I get the data and then the data will be repeated. How can I solve this problem? Restart canal Server ``` CanalConnector connector = CanalConnectors.newSingleConnector(new InetSocketAddress("192.168.157.130" 11111) "example" "" ""); int batchSize = 10; int emptyCount = 0; try { connector.connect(); connector.subscribe(".*\\..*"); connector.rollback(); int totalEmtryCount = 120; while (emptyCount < totalEmtryCount) { Message message = connector.getWithoutAck(batchSize); // Get the specified amount of data long batchId = message.getId(); int size = message.getEntries().size(); if (batchId == -1 || size == 0) { emptyCount++; System.out.println("empty count : " + emptyCount); try { Thread.sleep(1000); } catch (InterruptedException e) { } } else { emptyCount = 0; System.out.printf("message[batchId=%s size=%s msg=%s] \n" batchId size message.toString()); System.exit(0); printEntry(message.getEntries()); } connector.ack(batchId); // Submit confirmation // connector.rollback(batchId); // Processing failure Rollback data } System.out.println("empty too many times exit"); } finally { connector.disconnect(); } ``` Duplicate data cannot be avoided
432,Join directly to db event Push to kafka the most convenient way to change which one is better Use canal as databus to write a simple canalClient connection canal Destination specified by server But such a client has a single point of risk I want to directly send the kafka message to the canal and use the existing zk. Hot standby mode avoids single point Cannal source code from which to make changes to insert this function is better canal The client can also be HA @wingerx Thank you, I went to see the client code and it’s the best.
431,The batchSize of the connector getWithoutAck method does not work During use, it is found that no matter how much batchSize passes, the buffer configured in canal properties is used. Size instead of method into the batchSize bufferSize is the maximum record value in the current memory. Are you setting the trapSize too small? > bufferSize @agapple Thanks Reply I tried to find out that batchSize is the actual function of getWithoutAck. batchSize * memunit。 such as canal.instance.memory.buffer.size=16384 canal.instance.memory.buffer.memunit = 1024 canal.instance.memory.batch.mode = MEMSIZE The ring in memory at this time Buffer size should be 16384 * 1024 B If the getWithoutAck is passed in the client code, then the amount of binlog data acquired in batches is no more than or slightly more than 2 * 1024 B is affected by the canal instance memory buffer memunit and canal instance memory batch mode configuration in the server configuration. I understand right? Yes, understand correctly
430,Cannal client blocking The client cannot obtain data after blocking the blocking when getting the message. The timeout period is set and the data can be obtained after restarting. Message message = connector.getWithoutAck(batchSize) May be a buffer that encounters a large transaction canal Not enough size Solved this way Estimated or tcp link is actively disconnected by mysql canal Server not aware of the problem try to upgrade the version to 26 Setting the timeout is fine.
429,Can you get binlog sql directly from canal? The example in example prints a shape similar to the following can be given to execute insert update delete create The specific sql statement of the table and other commands? delete from sharddb. tab_shard where id =5; BEGIN ----> Thread id: 18 ----------------> binlog[mysql-bin.000005:1200] name[sharddb tab_shard] eventType : DELETE executeTime : 1512015314000 delay : 709ms id : 5 type=int(10) unsigned randomstr : 14504 type=varchar(100) create_time : 2017-11-28 05:00:35 type=timestamp ---------------- END ----> transaction id: 20565 ================> binlog[mysql-bin.000005:1251] executeTime : 1512015314000 delay : 709ms Binlog format can be found online
428,cannel service can't find start position for XX I want to ask what is the cause of this. 2017-11-29 16:17:36.772 [canal-instance-scan-0] INFO c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-crm_underline_prod 2017-11-29 16:17:36.775 [canal-instance-scan-0] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - start successful.... 2017-11-29 16:17:41.864 [destination = crm_underline_prod address = /10.203.8.48:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position ::1502794800000 2017-11-29 16:17:42.170 [destination = crm_underline_prod address = /10.203.8.48:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - Didn't find the corresponding binlog files from binlog.000002 to binlog.000020 2017-11-29 16:17:42.171 [destination = crm_underline_prod address = /10.203.8.48:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /10.203.8.48:3306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: can't find start position for crm_underline_prod 2017-11-29 16:17:42.171 [destination = crm_underline_prod address = /10.203.8.48:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:crm_underline_prod[com.alibaba.otter.canal.parse.exception.CanalParseException: can't find start position for crm_underline_prod ] The timestamp of the site you specified could not find a suitable binlog. The initial suspected site was too long. @agepple The site is too old Is the timestamp under instance properties Still zookeeper get  otter canal destinations xxxx 1001 cursor的timestamp See if you use file instance xml or default instance xml. The former is the file and the latter is zk. @agapple I am using default-instance.xml How to deal with this situation where the zk site is too old is to change the get Otter canal destinations xxxx 1001 cursor timestamp? There is still a question to ask if the next timestamp must be after the binlog is opened. Currently, there is a problem that the time of the timestamp before the binlog is started will also indicate that the above problem is abnormal. Yes, the given timestamp must be a binlog after the time
427,Home readme md inside How canal works Image does not show complete I guess you need to look over the wall. like this ： Picture http ww4 sinaimg cn large 0060lm7Tly1fm5mvsvfizj31kw0xt459 jpg @agapple Http dl iteye com upload attachment 0080 3107 c87b67ba 394c 3086 9577 9db05be04c95 jpg Image Source Address
426,Alibaba Cloud RDS database will report an error when switching timed out） When synchronizing data sources If you are using MHA&#39;s database when switching between high availability The database main library has changed At this time Canal synchronization log Changed Will report an error . The error is as follows Caused by: java.io.IOException: connect rm-2zed22tz2539.mysql.rds.aliyuncs.com/100.114.42.154:3306 failure:java.net.ConnectException: Connection timed out at sun.nio.ch.Net.connect0(Native Method) at sun.nio.ch.Net.connect(Net.java:454) at sun.nio.ch.Net.connect(Net.java:446) at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:70) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:56) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:86) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:157) at java.lang.Thread.run(Thread.java:745) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:74) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:56) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:86) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:157) at java.lang.Thread.run(Thread.java:745) Can&#39;t connect to the database Connection timed out
425,canal binlog Lost The current network data changes greatly and the data is found to be missing. The location information is not received in the binlog. Contrast found binlog 29827255 Unknown information is lost dump binlog ![image](https://user-images.githubusercontent.com/8357717/33230017-72c7cf0e-d1a0-11e7-94e6-981ab2367e25.png) canal log ![image](https://user-images.githubusercontent.com/8357717/33230032-0865fa2c-d1a1-11e7-90ae-a62fc76fbbe5.png) ![image](https://user-images.githubusercontent.com/8357717/33230062-a4101d04-d1a1-11e7-83d1-c151ca6b771f.png) The problem of log loss is to see the source code to find the reason, but the binlog receiving office does not receive the log. Every time the received binlog will be printed out. 29827255 is not received. The processing code is as follows: ``` void execute() { long batchId; LOGGER.debug("execute destination : " + destination); while (true) { try { connector.connect(); connector.subscribe(filter); while (true) { Message message = connector.getWithoutAck(BATCH_SIZE); batchId = message.getId(); int size = message.getEntries().size(); if (batchId == -1 || size == 0) { try { Thread.sleep(SLEEP_TIME); } catch (InterruptedException ignored) { // ignored } } else { process(message.getEntries()); } connector.ack(batchId); } } catch (Exception e) { LOGGER.error("canal connect error destination : " + destination e); AlarmUtil.dcAlarm(App.DC_ID "canal_connect_error" "canal connect error destination : " + destination); } finally { connector.disconnect(); } try { Thread.sleep(1000 * 60); } catch (InterruptedException e) { // ignored } } } private void process(List<CanalEntry.Entry> entryList) { for (CanalEntry.Entry entry : entryList) { if (entry.getEntryType() == CanalEntry.EntryType.TRANSACTIONBEGIN || entry.getEntryType() == CanalEntry.EntryType.TRANSACTIONEND) { continue; } if (entry.getEntryType() != CanalEntry.EntryType.ROWDATA) { continue; } CanalEntry.RowChange rowChange; try { rowChange = CanalEntry.RowChange.parseFrom(entry.getStoreValue()); } catch (Exception e) { throw new RuntimeException("parse event has an error data:" + entry.toString() e); } CanalEntry.EventType eventType = rowChange.getEventType(); if (eventType == CanalEntry.EventType.DELETE) { return; } if (eventType != CanalEntry.EventType.INSERT && eventType != CanalEntry.EventType.UPDATE) { return; } List<CanalEntry.RowData> rowDataList = rowChange.getRowDatasList(); if (rowDataList == null || rowDataList.size() == 0) { continue; } List<BinlogColumnDTO> columns = new ArrayList<>(); try { for (CanalEntry.RowData rowData : rowDataList) { columns = convertColumnList(rowData.getAfterColumnsList()); long updatedCount = columns.stream().filter(BinlogColumnDTO::getUpdated).count(); if (updatedCount < 1) { return; } String binlogFileOffset = entry.getHeader().getLogfileName() + ":" + entry.getHeader().getLogfileOffset() + ":" + DateUtil.timestamp2DateTime(entry.getHeader().getExecuteTime()); LOGGER.log(ACCESS binlogFileOffset + " eventType : {} data : {}" eventType toJsonString(columns)); if (eventType == CanalEntry.EventType.INSERT) { syncService.insert(columns binlogFileOffset); } else if (eventType == CanalEntry.EventType.UPDATE){ syncService.update(columns binlogFileOffset); } } } catch (Exception e) { LOGGER.error("binlog handle error data : " + JSONArray.toJSONString(columns) e); AlarmUtil.dcAlarm(App.DC_ID "binlog_handle_error" e.toString()); } } } ``` Tracking source canal The server prints the debug log and finds that when the client sends a request, it has read the corresponding postion, but it does not return the event content. Screenshot of another case binlog file ![image](https://user-images.githubusercontent.com/8357717/33230711-53efdf28-d1ae-11e7-87ac-229d440857f0.png) canal server log ![image](https://user-images.githubusercontent.com/8357717/33230713-6129ee18-d1ae-11e7-899c-58cdc8d1e51c.png) 1. Your last log is not the data read log when the location is located. 2. canal The points recorded in the log are in batches and are not accurate to one record. Confirm that the filter conditions are ok. Only filter a table if it is a filter condition, then a record should not be obtained. In the current situation, part of the binlog is lost. instance.properties # table regex canal.instance.filter.regex = p2p.t_loan # table black regex canal.instance.filter.black.regex = Passed filter p2p t_loan Have you concluded that the latest version 26 of my side also found that the update record has a missing problem. @JasonHuangHuster Excuse me, I have solved the problem that I have lost one or two 1000 data. It is recommended that you first print the received data using the standard example project. If there is a way to reproduce it, it is best to provide it. Is there a solution? I also encountered a loss of binlog here. I am using canal 1 0 24 database using MariaDB 10.1.19 @agapple I found that the canal log reported an exception when the data was lost. My deployment situation 1. 2 canal HA mode 2. 2 instances monitor two mysql master 3. Use mycat to slice 8 slices per master 4. Mysql uses GTID ROW This is the error log 2018-09-12 17:08:12.989 [New I/O server worker #1-7] INFO c.a.otter.canal.instance.core.AbstractCan alInstance - subscribe filter change to immig([1-8]|1[0-8])\.immig_enc_text 2018-09-12 17:09:37.077 [destination = IMMIG_200 address = /172.16.40.200:3306 EventParser] ERRO R c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address 172.16.40.200/172.16.40.200:3 306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception .CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: java.util.ConcurrentModificationException: null at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:909) ~[na:1.8.0_171] at java.util.ArrayList$Itr.next(ArrayList.java:859) ~[na:1.8.0_171] at com.alibaba.otter.canal.parse.driver.mysql.packets.UUIDSet.toString(UUIDSet.java:125) ~[c anal.parse.driver-1.1.0.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.packets.MysqlGTIDSet.toString(MysqlGTIDSet.jav a:111) ~[canal.parse.driver-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.createHeader(LogEventC onvert.java:849) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEven tConvert.java:561) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$DmlParserStage.onE vent(MysqlMultiStageCoprocessor.java:302) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$DmlParserStage.onE vent(MysqlMultiStageCoprocessor.java:288) ~[canal.parse-1.1.0.jar:na] at com.lmax.disruptor.WorkProcessor.run(WorkProcessor.java:143) ~[disruptor-3.4.2.jar:na] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8. 0_171] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8. 0_171] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171] 2018-09-12 17:09:37.078 [destination = IMMIG_200 address = /172.16.40.200:3306 EventParser] ERRO R com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:IMMIG_200[com.alibaba.otter.can al.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: java.util.ConcurrentModificationException at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:909) at java.util.ArrayList$Itr.next(ArrayList.java:859) at com.alibaba.otter.canal.parse.driver.mysql.packets.UUIDSet.toString(UUIDSet.java:125) at com.alibaba.otter.canal.parse.driver.mysql.packets.MysqlGTIDSet.toString(MysqlGTIDSet.jav a:111) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.createHeader(LogEventC onvert.java:849) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEven tConvert.java:561) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$DmlParserStage.onE vent(MysqlMultiStageCoprocessor.java:302) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$DmlParserStage.onE vent(MysqlMultiStageCoprocessor.java:288) at com.lmax.disruptor.WorkProcessor.run(WorkProcessor.java:143) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) ] 2018-09-12 17:09:51.781 [destination = IMMIG_200 address = /172.16.40.200:3306 EventParser] WARN c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - find start position : EntryPosition[includ ed=false journalName=mysql-bin.000012 position=46893782 serverId=200 gtid=85506e6a-a0f8-11e8-96b3-00 5056b70bff:1-85344 timestamp=1536743377000] 2018-09-12 17:09:53.682 [New I/O server worker #1-9] INFO c.a.otter.canal.instance.core.AbstractCan alInstance - subscribe filter change to immig([1-8]|1[0-8])\.immig_enc_text This is my binlog 4757 that is missing ### INSERT INTO `immig2`.`immig_enc_text` ### SET ### @1='4756' ### @2='Name-20180912171327-3073F1CC4D611dd0b13565e94969af0b4fe0972d58bf' ### @3='DETAIL-20180912171327-73F1CC4D611dd0b13565e94969af0b4fe0972d58bf' ### @4='BIRTHDATE-20180912171327-1CC4D611dd0b13565e94969af0b4fe0972d58bf' ### @5='1dd0b13565e94969af0b4fe0972d58bf' ### @6='1' ### @7='00' ### @8='1' ### @9='1' ### @10='20180912171327' ### @11='CHN' ### @12=1536743376 # at 46871947 #180912 17:09:36 server id 200 end_log_pos 46872041 CRC32 0xf43de30e Table_map: `immig2`.`immig_enc_text` mapped to number 111 # at 46872041 #180912 17:09:36 server id 200 end_log_pos 46872344 CRC32 0x32dac44c Write_rows: table id 111 flags: STMT_END_F ### INSERT INTO `immig2`.`immig_enc_text` ### SET ### @1='4757' ### @2='Name-20180912171327-3073F1CC4D615b1e326471284984b522636e96b55fc3' ### @3='DETAIL-20180912171327-73F1CC4D615b1e326471284984b522636e96b55fc3' ### @4='BIRTHDATE-20180912171327-1CC4D615b1e326471284984b522636e96b55fc3' ### @5='5b1e326471284984b522636e96b55fc3' ### @6='1' ### @7='00' ### @8='1' ### @9='1' ### @10='20180912171327' ### @11='CHN' ### @12=1536743376 # at 46872344 #180912 17:09:36 server id 200 end_log_pos 46872438 CRC32 0x4dd666d7 Table_map: `immig2`.`immig_enc_text` mapped to number 111 # at 46872438 #180912 17:09:36 server id 200 end_log_pos 46872741 CRC32 0x86092308 Write_rows: table id 111 flags: STMT_END_F ### INSERT INTO `immig2`.`immig_enc_text` ### SET ### @1='4758' ### @2='Name-20180912171327-3073F1CC4D618b85988e6a054822800bf861691f6e13' ### @3='DETAIL-20180912171327-73F1CC4D618b85988e6a054822800bf861691f6e13' ### @4='BIRTHDATE-20180912171327-1CC4D618b85988e6a054822800bf861691f6e13' ### @5='8b85988e6a054822800bf861691f6e13' ### @6='1' ### @7='00' ### @8='1' ### @9='1' ### @10='20180912171327' ### @11='CHN' ### @12=1536743376 # at 46872741 #180912 17:09:36 server id 200 end_log_pos 46872835 CRC32 0xeb520dc4 Table_map: `immig2`.`immig_enc_text` mapped to number 111 # at 46872835 #180912 17:09:36 server id 200 end_log_pos 46873138 CRC32 0x31e40985 Write_rows: table id 111 flags: STMT_END_F @xesygao Problem with ConcurrentModificationException https://github.com/alibaba/canal/pull/902 @lcybo Since the performance of the serial is enough for me, I have temporarily tested the problem with missing data for a dozen times using serial parsing. tks You can try the alpha version of 1 1 1 first.
424,1 0 23 repaired inside KILL DUMP The exception problem was overwritten in 334. As the change in question 334 is more likely to be a misoperation tks
423,Support Mariadb 10 0 17? Support Mariadb 10 0 17? Theoretically support mariadb 10 x series can run test to give feedback
422,canal In Alibaba Cloud RDS-Mysql Running in the environment Since Alibaba Cloud&#39;s mysql exists, the binlog mechanism is automatically cleaned up. When the mechanism starts The current task ACK being processed will generate an exception because the log file no longer exists. Or the remaining unprocessed tasks will also be discarded. . For this scene Is there an optimization? java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file
421,A problem with multiple instances of destinations listening to the same library under a canal A canal under destinations wrote 2 instances of the listener library. I found that my client will receive two identical changes after the data changes in the library. What happened? Has anyone encountered it? An instance of data you said A data refers to a database? An instance corresponds to a database @loveluckystar I also wrote three instances corresponding to a library and then filtered by filter, there will be no duplicate data, but there will always be an instance. Client Cannot get the change data batchId is 1 size is 0
420,mysql5.7.20 Using canal 1.0.24 For mysql 5 7 20 monitoring There is no official canal on mysql when the table name library name is empty. 5 7 20 version monitored canal And mysql configuration Note the binlog format
419,Canal1 0 24 Before the recent addition of an instance, I have been reporting this error. The client can consume it without knowing if the data is missing. 2017-11-21 11:40:16.215 [destination = example31 address = /localhost:30701 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position {"identity":{"slaveId":-1 "sourceAddress":{"address":"localhost" "port":30701}} "postion":{"included":false "journalName":"mysql-bin.000220" "position":3614315 "serverId":1 "timestamp":1511235396000}} 2017-11-21 11:40:46.230 [destination = example31 address = /localhost:30701 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Unexpected End Stream at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) ~[canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70) ~[canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122) [canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) [canal.parse-1.0.24.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_111] 2017-11-21 11:40:46.230 [destination = example31 address = /localhost:30701 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address localhost/localhost:30701 has an error retrying. caused by java.io.IOException: Unexpected End Stream at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) ~[canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70) ~[canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122) ~[canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) ~[canal.parse-1.0.24.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_111] 2017-11-21 11:40:46.230 [destination = example31 address = /localhost:30701 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example31[java.io.IOException: Unexpected End Stream at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) at java.lang.Thread.run(Thread.java:745) ] Is it a slaveId that appears to be duplicated by the mysql end kill link can try the latest version to increase the ability of random slaveId
418,Compile error isHeartBeat this variable can not be found [ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.2:compile (default-compile) on project canal.parse: Compilation failure: Compilation failure: [ERROR] /Users/yes/Documents/workspace/idea/canal/parse/src/main/java/com/alibaba/otter/canal/parse/inbound/mysql/dbsync/LogEventConvert.java:[390 24] Symbol not found [ERROR] symbol variable isHeartBeat [ERROR] position class com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert [ERROR] /Users/yes/Documents/workspace/idea/canal/parse/src/main/java/com/alibaba/otter/canal/parse/inbound/mysql/dbsync/LogEventConvert.java:[390 23] Illegal type begins [ERROR] -> [Help 1] Ibid. ``` [INFO] ------------------------------------------------------------- [ERROR] /root/canal/parse/src/main/java/com/alibaba/otter/canal/parse/inbound/mysql/dbsync/LogEventConvert.java:[390 24] cannot find symbol symbol: variable isHeartBeat location: class com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert [ERROR] /root/canal/parse/src/main/java/com/alibaba/otter/canal/parse/inbound/mysql/dbsync/LogEventConvert.java:[390 23] illegal start of type [INFO] 2 errors ``` surroundings ``` ~/canal# mvn -version Apache Maven 3.3.9 Maven home: /usr/share/maven Java version: 1.8.0_151 vendor: Oracle Corporation Java home: /usr/lib/jvm/java-8-oracle/jre Default locale: en_US platform encoding: UTF-8 OS name: "linux" version: "4.4.0-62-generic" arch: "amd64" family: "unix" ~/canal# java -version java version "1.8.0_151" Java(TM) SE Runtime Environment (build 1.8.0_151-b12) Java HotSpot(TM) 64-Bit Server VM (build 25.151-b12 mixed mode) ~/canal# lsb_release -a LSB Version: core-9.20160110ubuntu0.2-amd64:core-9.20160110ubuntu0.2-noarch:security-9.20160110ubuntu0.2-amd64:security-9.20160110ubuntu0.2-noarch Distributor ID: Ubuntu Description: Ubuntu 16.04.2 LTS Release: 16.04 Codename: xenial ``` Revert the latest commit Should be missing the code when submitting The following command can solve the compilation problem ``` git reset --hard a3b9f6f1ebb21dd528effcceba2ac207f40b15e8 mvn clean install -Dmaven.test.skip -Denv=release ``` A9284b1b3967917ee6cef0e85787575e9b121e1e has been fixed
417,Canal resolves library A, but when another library B deletes a large amount of data in the same instance, it causes the canal to die. A canal server An instance resolves the change of the library A. However, the library B does a lot of deletion operations with the same instance. The canal parsing library A is stuck. The stuck log may not be displayed normally. However, the change of the library A has not been resolved and the log is in the middle. Information no longer changes Mysql version 5 6 Canal version 1 0 16 Ps otter encountered the same problem log without exception but could not parse the data of the source library Has anyone encountered this problem how to solve it? It is recommended to upgrade the canal version.
416,cannal mysql to hive Can cannal directly synchronize mysql data to hive Need to receive the binlog change message and write the code to hive
415,Help start canal Error after server 2017-11-13 10:41:41.588 [destination = example address = /127.0.0.1:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : flush privileges com.alibaba.druid.sql.parser.ParserException: syntax error expect TABLES actual IDENTIFIER pos 16 line 1 column 6 token IDENTIFIER privileges at com.alibaba.druid.sql.parser.SQLParser.acceptIdentifier(SQLParser.java:60) ~[druid-1.1.5.jar:1.1.5] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseFlush(MySqlStatementParser.java:915) ~[druid-1.1.5.jar:1.1.5] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseStatementListDialect(MySqlStatementParser.java:762) ~[druid-1.1.5.jar:1.1.5] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:388) ~[druid-1.1.5.jar:1.1.5] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.5.jar:1.1.5] at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:461) ~[druid-1.1.5.jar:1.1.5] at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.5.jar:1.1.5] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:67) ~[canal.parse-1.0.25-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.applyHistoryOnMemory(DatabaseTableMeta.java:387) [canal.parse-1.0.25-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:121) [canal.parse-1.0.25-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.25-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.25-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_121] 2017-11-13 10:41:41.590 [destination = example address = /127.0.0.1:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : flush privileges com.alibaba.druid.sql.parser.ParserException: syntax error expect TABLES actual IDENTIFIER pos 16 line 1 column 6 token IDENTIFIER privileges at com.alibaba.druid.sql.parser.SQLParser.acceptIdentifier(SQLParser.java:60) ~[druid-1.1.5.jar:1.1.5] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseFlush(MySqlStatementParser.java:915) ~[druid-1.1.5.jar:1.1.5] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseStatementListDialect(MySqlStatementParser.java:762) ~[druid-1.1.5.jar:1.1.5] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:388) ~[druid-1.1.5.jar:1.1.5] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.5.jar:1.1.5] at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:461) ~[druid-1.1.5.jar:1.1.5] at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.5.jar:1.1.5] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:67) ~[canal.parse-1.0.25-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.applyHistoryOnMemory(DatabaseTableMeta.java:387) [canal.parse-1.0.25-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:121) [canal.parse-1.0.25-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.25-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.25-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_121] It looks like a question of permissions I started the canal first. Server then I re-executed a flush The privileges have been given to the canal user and this error has been reported. Lou brother The activity of this project is still quite high. I am going to look at the source implementation in detail. Temporary solution com alibaba otter canal parse inbound mysql tsdb apply will alter the ddl user 、flush The privileges statement can be filtered out. This block will only record the exception log itself is try Catch does not affect the function druid 1 1 6 does not support flush temporarily The analysis of privileges temporarily bypasses the author who has already fed back to druid
414,Optimization of the heartbeat mechanism Now canal and mySQL Heartbeat mechanism between databases When the detection mechanism is turned on, the sql statement is sent to detect whether the remote end is alive. Is always being sent cyclically at regular intervals And did not decide to send according to whether there is traffic now. That is the same as the heartbeat mechanism of binlog Send heartbeat messages only after not having traffictic for a while Will send heartbeat messages at a fixed frequency
413,ErrorPacket What is the error? INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - KILL DUMP 151432475 failure:java.io.IOException: ErrorPacket [errorNumber=1094 fieldCount=-1 message=Unknown thread id: 151432475 sqlState=HY000 sqlStateMarker=#] with command: KILL CONNECTION 151432475 at com.alibaba.otter.canal.parse.driver.mysql.MysqlUpdateExecutor.update(MysqlUpdateExecutor.java:49) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.disconnect(MysqlConnector.java:104) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.disconnect(MysqlConnection.java:64) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:241) at java.lang.Thread.run(Thread.java:748) Upgrade the canal version
412,After the master/slave database switch occurs Exception will occur in disconnect After the master/slave database switch occurs Because the address has changed So looking for start Timestamp Find the corresponding position according to timestamp from the binlog file At this time, the connection&#39;s dumping flag will be set to true. Reconnect at the back Disconnect inside because it is now dumping state So KILL will be executed once the channel is turned off. CONNECTION" And the connection used at this time Id is the previous one that has been closed So it will throw an exception It is recommended to execute KILL first. CONNECTION 再close channel public void disconnect() throws IOException { if (connected.compareAndSet(true false)) { try { if (channel != null) { channel.close(); } logger.info("disConnect MysqlConnection to {}..." address); } catch (Exception e) { throw new IOException("disconnect " + this.address + " failure:" + ExceptionUtils.getStackTrace(e)); } // Execute a quit if (dumping && connectionId >= 0) { MysqlConnector connector = null; try { connector = this.fork(); connector.connect(); MysqlUpdateExecutor executor = new MysqlUpdateExecutor(connector); executor.update("KILL CONNECTION " + connectionId); } catch (Exception e) { throw new IOException("KILL DUMP " + connectionId + " failure:" + ExceptionUtils.getStackTrace(e)); } finally { if (connector != null) { connector.disconnect(); } } dumping = false; } } else { logger.info("the channel {} is not connected" this.address); } } Has been fixed in the new version Please close Thank you
411,Canal and mysql connection how to get the current operation mysql user No record in binlog
410,How can Canal monitor its running status Sometimes the canal is not developed, it is not clear that the business data is wrong. I just want to put a good suggestion on the canal monitoring system. Monitor canal processes or zookeeper&#39;s business consumption sites
409,InetAddress serialization problem in JsonUtils The InetAddressSerializer in JsonUtils serializes the InetAddress object. ``` java InetAddress address = (InetAddress)object; serializer.write(address.getHostName()) ``` Is to get the getHostName to get the host name that is resolved to this value if the host name is configured in the hosts Suggested to change to Address getHostAddress Get IP directly PS JsonUtils ``` java static { SerializeConfig.getGlobalInstance().put(InetAddress.class JsonUtils.InetAddressSerializer.instance); SerializeConfig.getGlobalInstance().put(Inet4Address.class JsonUtils.InetAddressSerializer.instance); SerializeConfig.getGlobalInstance().put(Inet6Address.class JsonUtils.InetAddressSerializer.instance); } ``` Is this method too dark? If the entire fastJson framework parses the InetAddress object, it is serialized in this way and affects the application embedded in the canal. I want to keep the hostname instead of ip. The domain name will remain the same when it is the domain name vip mode.
408,Canal monitoring mysql primary and backup services Canal monitoring mysql master and backup services how to configure if you only configure the primary database address in the case of the primary database server hangs, you can continue to collect binlog? Need to configure the information of the main and standby libraries
407,In the manager management interface, you can&#39;t see the node node hanging under Zookeeper but it is running normally. If the problem is solved, I can see the number of nodes hanging under Zookeeper. Ask questions to otter
406,binlog-format in MIXED How to get tableName If the company has been operating for a long time and has modified it, I don’t know how many problems will be solved. The service restarts from the database update. MIXED The information below is really too small. I can only get the original SQL statement. Does it mean that only the original SQL statement is parsed? tableName？ Can need to rely on a sql Parser The latest version 26 has been fully obtained through the full SQL parsing of the corresponding table. thanks Thanks answer
405,ClusterCanalClient For a canal server Cluster Deploy multiple canal client client Listening data will monitor the same update data on both platforms. How do you do cluster consumption? Cluster consumption is recommended to be delivered to the MQ system for processing
404,canal HA Zookeeper does not update position, please advise As the title tried a lot of ways, although I don&#39;t update zookeeper, but I don&#39;t switch HA when restarting the client side can be executed in order. But when I stop a switch server, the client automatically runs from the previous position. I don&#39;t understand how it is going to ask God. Will not look at the offset changes on zk It doesn&#39;t change without changing the configuration. server I have restarted, I just started this problem and I didn’t have it.
403,When is the release of the next version expected?
402,otter The node process fullgc causes the synchronization point to not update and the infinite loop We are now using otter to encounter a problem we put otter Deploying to Alibaba Cloud ECS machine Synchronous RDS found a weird problem when the table has millions of data added after adding a field and updating the default value of this field 1 update statement updates millions of data found that otter has been updating in an infinite loop Millions of data to view select The load detailed log also sees the update all the time and the point of the infinite loop interface synchronization progress has been fixed at a point that does not advance. In this process, many times, fullgc does not know whether it has an effect. Look at the FAQ
401,Can&#39;t get ROWDATA record @agapple canal server: 1.0.24 Using the Alibaba Cloud RDS deployment structure ``` master --> slave --> canal server --> Consumer program ``` ``` MySQL [zz]> show variables like '%binlog_format%'; +---------------+-------+ | Variable_name | Value | +---------------+-------+ | binlog_format | ROW | +---------------+-------+ 1 row in set (0.00 sec) ``` Consumer program logs are basically like this ``` [2017-10-18 11:51:18 398] (CanalMessageHandlerONSImpl.java:35) DEBUG - handleMessage batchId[46] entrys[2] [2017-10-18 11:51:18 398] (CanalMessageHandlerONSImpl.java:42) DEBUG - entryType[TRANSACTIONBEGIN] schema[] tablename[] [2017-10-18 11:51:18 399] (CanalMessageHandlerONSImpl.java:42) DEBUG - entryType[TRANSACTIONEND] schema[] tablename[] ``` Consumer program ``` connector.connect(); connector.subscribe(null); ``` instance.properties ``` ## mysql serverId canal.instance.mysql.slaveId = 42341 # position info canal.instance.master.address = xxxxx canal.instance.master.journal.name = canal.instance.master.position = canal.instance.master.timestamp = #canal.instance.standby.address = #canal.instance.standby.journal.name = #canal.instance.standby.position = #canal.instance.standby.timestamp = # username/password canal.instance.dbUsername = xx canal.instance.dbPassword = xx canal.instance.defaultDatabaseName = zz canal.instance.connectionCharset = UTF-8 # table regex canal.instance.filter.regex = .*\\..* #canal.instance.filter.regex = .*order_info.* # table black regex canal.instance.filter.black.regex = ``` canal.properties ``` ################################################# ######### common argument ############# ################################################# canal.id= 101 canal.ip= canal.port= 11111 canal.zkServers= 127.0.0.1:2181 # flush data to zk canal.zookeeper.flush.period = 1000 # flush meta cursor/parse position to file canal.file.data.dir = ${canal.conf.dir} canal.file.flush.period = 1000 ## memory store RingBuffer size should be Math.pow(2 n) canal.instance.memory.buffer.size = 16384 ## memory store RingBuffer used memory unit size default 1kb canal.instance.memory.buffer.memunit = 1024 ## meory store gets mode used MEMSIZE or ITEMSIZE canal.instance.memory.batch.mode = MEMSIZE ## detecing config canal.instance.detecting.enable = false #canal.instance.detecting.sql = insert into retl.xdual values(1 now()) on duplicate key update x=now() canal.instance.detecting.sql = select 1 canal.instance.detecting.interval.time = 3 canal.instance.detecting.retry.threshold = 3 canal.instance.detecting.heartbeatHaEnable = false # support maximum transaction size more than the size of the transaction will be cut into multiple transactions delivery canal.instance.transaction.size = 1024 # mysql fallback connected to new master should fallback times canal.instance.fallbackIntervalInSeconds = 60 # network config canal.instance.network.receiveBufferSize = 16384 canal.instance.network.sendBufferSize = 16384 canal.instance.network.soTimeout = 30 # binlog filter config canal.instance.filter.query.dcl = false canal.instance.filter.query.dml = false canal.instance.filter.query.ddl = false canal.instance.filter.table.error = false canal.instance.filter.rows = false # binlog format/image check canal.instance.binlog.format = ROW STATEMENT MIXED canal.instance.binlog.image = FULL MINIMAL NOBLOB # binlog ddl isolation canal.instance.get.ddl.isolation = false ################################################# ######### destinations ############# ################################################# canal.destinations= zz # conf root dir canal.conf.dir = ../conf # auto scan instance dir add/remove and start/stop instance canal.auto.scan = true canal.auto.scan.interval = 5 canal.instance.global.mode = spring canal.instance.global.lazy = false #canal.instance.global.manager.address = 127.0.0.1:1099 #canal.instance.global.spring.xml = classpath:spring/memory-instance.xml #canal.instance.global.spring.xml = classpath:spring/file-instance.xml canal.instance.global.spring.xml = classpath:spring/default-instance.xml ``` Pay attention to whether there is an open log Slave parameter @agapple Is it the parameter log_slave_updates ``` MySQL [zaozuo]> show variables like '%slave%'; +---------------------------------------+-----------------------+ | Variable_name | Value | +---------------------------------------+-----------------------+ | init_slave | | | log_slave_updates | ON | | log_slow_slave_statements | OFF | | pseudo_slave_mode | OFF | | rpl_semi_sync_master_wait_no_slave | ON | | rpl_semi_sync_slave_delay_master | OFF | | rpl_semi_sync_slave_enabled | ON | | rpl_semi_sync_slave_kill_conn_timeout | 5 | | rpl_semi_sync_slave_trace_level | 1 | | rpl_stop_slave_timeout | 31536000 | | slave_allow_batching | OFF | | slave_checkpoint_group | 512 | | slave_checkpoint_period | 300 | | slave_compressed_protocol | OFF | | slave_exec_mode | STRICT | | slave_load_tmpdir | | | slave_max_allowed_packet | 1073741824 | | slave_net_timeout | 60 | | slave_parallel_workers | 16 | | slave_pending_jobs_size_max | 167772160 | | slave_pr_mode | TABLE | | slave_rows_search_algorithms | TABLE_SCAN INDEX_SCAN | | slave_skip_errors | OFF | | slave_sql_verify_checksum | ON | | slave_transaction_retries | 10 | | slave_type_conversions | | | sql_slave_skip_counter | 0 | +---------------------------------------+-----------------------+ 27 rows in set (0.00 sec) ``` ``` MySQL [zaozuo]> show variables like '%log%'; +-----------------------------------------+----------------------+ | Variable_name | Value | +-----------------------------------------+----------------------+ | back_log | 3000 | | binlog_cache_size | 131072 | | binlog_checksum | CRC32 | | binlog_direct_non_transactional_updates | OFF | | binlog_format | ROW | | binlog_order_commits | OFF | | binlog_row_image | FULL | | binlog_rows_query_log_events | OFF | | binlog_stmt_cache_size | 32768 | | expire_logs_days | 0 | | failover_log_verbose | | | general_log | OFF | | general_log_file | | | innodb_api_enable_binlog | OFF | | innodb_flush_log_at_timeout | 1 | | innodb_flush_log_at_trx_commit | 2 | | innodb_locks_unsafe_for_binlog | OFF | | innodb_log_buffer_size | 8388608 | | innodb_log_compressed_pages | OFF | | innodb_log_file_size | 1048576000 | | innodb_log_files_in_group | 2 | | innodb_log_group_home_dir | | | innodb_mirrored_log_groups | 1 | | innodb_online_alter_log_max_size | 134217728 | | innodb_undo_logs | 128 | | log_bin | ON | | log_bin_basename | | | log_bin_index | | | log_bin_trust_function_creators | ON | | log_bin_use_v1_row_events | ON | | log_error | | | log_output | TABLE | | log_queries_not_using_indexes | OFF | | log_slave_updates | ON | | log_slow_admin_statements | ON | | log_slow_slave_statements | OFF | | log_throttle_queries_not_using_indexes | 0 | | log_warnings | 2 | | max_binlog_cache_size | 18446744073709547520 | | max_binlog_size | 524288000 | | max_binlog_stmt_cache_size | 18446744073709547520 | | max_relay_log_size | 0 | | relay_log | | | relay_log_basename | | | relay_log_index | | | relay_log_info_file | | | relay_log_info_repository | TABLE | | relay_log_purge | ON | | relay_log_recovery | OFF | | relay_log_space_limit | 0 | | slow_query_log | ON | | slow_query_log_file | | | sql_log_bin | ON | | sql_log_off | OFF | | sync_binlog | 1000 | | sync_relay_log | 10000 | | sync_relay_log_info | 10000 | | tokudb_checkpoint_on_flush_logs | OFF | | tokudb_fsync_log_period | 0 | | tokudb_log_buffer_size | 16777216 | | tokudb_log_dir | | | tokudb_log_file_size | 104857600 | +-----------------------------------------+----------------------+ 62 rows in set (0.00 sec) ``` @agapple The strange thing is that mysql slow log statement can actually come in ``` [2017-10-19 01:47:52 343] (CanalMessageHandlerONSImpl.java:42) DEBUG - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 02:05:15 708] (CanalMessageHandlerONSImpl.java:42) DEBUG - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 02:19:55 133] (CanalMessageHandlerONSImpl.java:42) DEBUG - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 02:37:49 861] (CanalMessageHandlerONSImpl.java:42) DEBUG - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 02:48:20 351] (CanalMessageHandlerONSImpl.java:42) DEBUG - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 03:07:13 337] (CanalMessageHandlerONSImpl.java:42) DEBUG - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 03:17:14 421] (CanalMessageHandlerONSImpl.java:42) DEBUG - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 03:35:21 371] (CanalMessageHandlerONSImpl.java:42) DEBUG - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 03:45:55 885] (CanalMessageHandlerONSImpl.java:42) DEBUG - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 04:04:42 615] (CanalMessageHandlerONSImpl.java:42) DEBUG - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 04:23:22 967] (CanalMessageHandlerONSImpl.java:42) DEBUG - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 05:07:06 185] (CanalMessageHandlerONSImpl.java:42) DEBUG - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 05:17:21 915] (CanalMessageHandlerONSImpl.java:42) DEBUG - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 05:36:32 541] (CanalMessageHandlerONSImpl.java:42) DEBUG - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 05:46:33 606] (CanalMessageHandlerONSImpl.java:42) DEBUG - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 06:04:54 056] (CanalMessageHandlerONSImpl.java:42) DEBUG - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 06:17:09 120] (CanalMessageHandlerONSImpl.java:42) DEBUG - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 06:37:21 586] (CanalMessageHandlerONSImpl.java:42) DEBUG - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 06:45:09 582] (CanalMessageHandlerONSImpl.java:42) DEBUG - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 06:49:18 785] (CanalMessageHandlerONSImpl.java:42) DEBUG - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 07:04:10 957] (CanalMessageHandlerONSImpl.java:42) DEBUG - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 07:13:34 243] (CanalMessageHandlerONSImpl.java:42) DEBUG - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 07:35:16 863] (CanalMessageHandlerONSImpl.java:42) DEBUG - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 07:43:58 415] (CanalMessageHandlerONSImpl.java:42) DEBUG - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 08:05:37 009] (CanalMessageHandlerONSImpl.java:42) DEBUG - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 08:23:29 424] (CanalMessageHandlerONSImpl.java:42) DEBUG - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 09:06:56 832] (CanalMessageHandlerONSImpl.java:42) DEBUG - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 09:18:18 433] (CanalMessageHandlerONSImpl.java:42) DEBUG - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 09:36:09 787] (CanalMessageHandlerONSImpl.java:42) DEBUG - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 09:48:05 170] (CanalMessageHandlerONSImpl.java:42) DEBUG - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 10:07:39 216] (CanalMessageHandlerONSImpl.java:42) DEBUG - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 10:19:40 631] (CanalMessageHandlerONSImpl.java:42) DEBUG - entryType[ROWDATA] schema[mysql] tablename[slow_log] [2017-10-19 10:37:33 833] (CanalMessageHandlerONSImpl.java:42) DEBUG - entryType[ROWDATA] schema[mysql] tablename[slow_log] ``` Directly connected to the main library problem Tiali Clouds, they said that the official does not support the independent establishment of a replication system to synchronize RDS recommendations to use DTS Speculation may be what restrictions Alibaba Cloud has made Well, connect the main library directly to the link. The DTS principle is the same as the main library principle. x > Well, connect the main library directly to the link. The DTS principle is the same as the main library principle. Now I am having problems with canal [v.1.1.0] Connected to Ali&#39;s rds logs/**-instance/meta.log The data that has been in the new log but the database has not received the data is always like this. ` Message[id=444 entries=[header { version: 1 logfileName: "*****" logfileOffset: ***** serverId: ***** serverenCode: "UTF-8" executeTime: 1539572438000 sourceType: MYSQL schemaName: "" tableName: "" eventLength: 76 } entryType: TRANSACTIONBEGIN storeValue: " \242\206\356\001" header { version: 1 logfileName: "*****" logfileOffset: ***** serverId: ***** serverenCode: "UTF-8" executeTime: 1539572438000 sourceType: MYSQL schemaName: "" tableName: "" eventLength: 31 } entryType: TRANSACTIONEND storeValue: "\022\n7768450739" ]] ` The type of data is entry is entryType TRANSACTION Everyone is not using RDS read-only instance. RDS read-only instance is not open log_update_slave. There will be no binlog of the main library. So you can open the cascading synchronization and directly operate the RDS main instance.
400,Can can support grpc? Canal can support grpc so that you can write client and canal interaction in any language. Can submit a PR to me
399,Help 2017-10-16 10:44:47.424 [destination = example address = /127.0.0.1:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just show master status 2017-10-16 10:44:47.427 [destination = example address = /127.0.0.1:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /127.0.0.1:3306 has an error retrying. caused by java.lang.NullPointerException: null at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.loadBinlogChecksum(MysqlConnection.java:284) ~[canal.parse-1.0.23.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) ~[canal.parse-1.0.23.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) ~[canal.parse-1.0.23.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_111] 2017-10-16 10:44:47.427 [destination = example address = /127.0.0.1:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.lang.NullPointerException at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.loadBinlogChecksum(MysqlConnection.java:284) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) at java.lang.Thread.run(Thread.java:745) ] What caused this error? Upgrade to version 1 0 24 has been fixed
398,LOST_EVENTS stand by The version of Canal I use is 1.0.22。 We are analyzing MySQL LOST_EVENTS that may appear during replication. I checked the source code and Canal does not support the processing of LOST_EVENTS. Instead, I lost it when I converted it to Entry after reading this event. Is Canal designed specifically for this kind of behavior? We understand the mistakes The following code we analyzed In LogDecoder java INCIDENT_EVENT Is to be read case LogEvent.INCIDENT_EVENT: { IncidentLogEvent event = new IncidentLogEvent(header buffer descriptionEvent); /* updating position in context */ logPosition.position = header.getLogPos(); return event; } But when LogEventConvert.java Moderate parse Method does not handle this INCIDENT_EVENT public Entry parse(LogEvent logEvent) throws CanalParseException { if (logEvent == null || logEvent instanceof UnknownLogEvent) { return null; } int eventType = logEvent.getHeader().getType(); switch (eventType) { ...... ( Nothing here case LogEvent.INCIDENT_EVENT) } } is not it Canal I thought that even if I read this event, the consumer could do nothing, so I took the initiative to discard it. If you can report an alarm automatically, it will help. Thank you Best Regards Stone When the moment is not added to the canal design is to extract as much as possible DML DDL DCL part of the content does not fully handle all mysql events @agapple Thank you for your instant reply. Are there any considerations for expanding Canal to cover this or as many events as possible? Best Regards Stone Interface protocol design is not supported, so it will not be considered for the time being. thank you I will close this question first. If there are new problems, I will open a new ticket. Thank you Best Regards Stone
397,Problem with isDdl field value in RowChange Use the officially provided clent example，RowChange.parseFrom(entry.getStoreValue()); Update insert delete statement conversion result The value of isDdl is true and the length of the list of rowChage getRowDatasList is 0. The current version of canal is 1 0 24 Mysql version is 5 6 26 log What is the cause of the parsing problem? Confirm that the use of ROW mode and a little mysql 5 6 will have rows The query object can be changed to canal instance filter query dml
396,group-instance.xml How to configure I want to use the group mode to handle the problem of multi-library merge, but the wiki can&#39;t find the configuration method. canal.properties Only changed canal.destinations= food mdm ``` ################################################# ######### common argument ############# ################################################# canal.id= 1 canal.ip= canal.port= 11111 canal.zkServers= # flush data to zk canal.zookeeper.flush.period = 1000 # flush meta cursor/parse position to file canal.file.data.dir = ${canal.conf.dir} canal.file.flush.period = 1000 ## memory store RingBuffer size should be Math.pow(2 n) canal.instance.memory.buffer.size = 16384 ## memory store RingBuffer used memory unit size default 1kb canal.instance.memory.buffer.memunit = 1024 ## meory store gets mode used MEMSIZE or ITEMSIZE canal.instance.memory.batch.mode = MEMSIZE ## detecing config canal.instance.detecting.enable = false #canal.instance.detecting.sql = insert into retl.xdual values(1 now()) on duplicate key update x=now() canal.instance.detecting.sql = select 1 canal.instance.detecting.interval.time = 3 canal.instance.detecting.retry.threshold = 3 canal.instance.detecting.heartbeatHaEnable = false # support maximum transaction size more than the size of the transaction will be cut into multiple transactions delivery canal.instance.transaction.size = 1024 # mysql fallback connected to new master should fallback times canal.instance.fallbackIntervalInSeconds = 60 # network config canal.instance.network.receiveBufferSize = 16384 canal.instance.network.sendBufferSize = 16384 canal.instance.network.soTimeout = 30 # binlog filter config canal.instance.filter.query.dcl = false canal.instance.filter.query.dml = false canal.instance.filter.query.ddl = false canal.instance.filter.table.error = false canal.instance.filter.rows = false # binlog format/image check canal.instance.binlog.format = ROW STATEMENT MIXED canal.instance.binlog.image = FULL MINIMAL NOBLOB # binlog ddl isolation canal.instance.get.ddl.isolation = false ################################################# ######### destinations ############# ################################################# canal.destinations= food mdm # conf root dir canal.conf.dir = ../conf # auto scan instance dir add/remove and start/stop instance canal.auto.scan = true canal.auto.scan.interval = 5 canal.instance.global.mode = spring canal.instance.global.lazy = false #canal.instance.global.manager.address = 127.0.0.1:1099 #canal.instance.global.spring.xml = classpath:spring/memory-instance.xml canal.instance.global.spring.xml = classpath:spring/file-instance.xml #canal.instance.global.spring.xml = classpath:spring/default-instance.xml ``` group-instance.xml Use default content without modification ``` <?xml version="1.0" encoding="UTF-8"?> <beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:tx="http://www.springframework.org/schema/tx" xmlns:aop="http://www.springframework.org/schema/aop" xmlns:lang="http://www.springframework.org/schema/lang" xmlns:context="http://www.springframework.org/schema/context" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.0.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-2.0.xsd http://www.springframework.org/schema/lang http://www.springframework.org/schema/lang/spring-lang-2.0.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-2.0.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-2.5.xsd" default-autowire="byName"> <!-- properties --> <bean class="com.alibaba.otter.canal.instance.spring.support.PropertyPlaceholderConfigurer" lazy-init="false"> <property name="ignoreResourceNotFound" value="true" /> <property name="systemPropertiesModeName" value="SYSTEM_PROPERTIES_MODE_OVERRIDE"/><!-- Allow system coverage --> <property name="locationNames"> <list> <value>classpath:canal.properties</value> <value>classpath:${canal.instance.destination:}/instance.properties</value> </list> </property> </bean> <bean id="socketAddressEditor" class="com.alibaba.otter.canal.instance.spring.support.SocketAddressEditor" /> <bean class="org.springframework.beans.factory.config.CustomEditorConfigurer"> <property name="propertyEditorRegistrars"> <list> <ref bean="socketAddressEditor" /> </list> </property> </bean> <bean id="instance" class="com.alibaba.otter.canal.instance.spring.CanalInstanceWithSpring"> <property name="destination" value="${canal.instance.destination}" /> <property name="eventParser"> <ref local="eventParser" /> </property> <property name="eventSink"> <ref local="eventSink" /> </property> <property name="eventStore"> <ref local="eventStore" /> </property> <property name="metaManager"> <ref local="metaManager" /> </property> <property name="alarmHandler"> <ref local="alarmHandler" /> </property> </bean> <!-- Alarm processing class --> <bean id="alarmHandler" class="com.alibaba.otter.canal.common.alarm.LogAlarmHandler" /> <bean id="metaManager" class="com.alibaba.otter.canal.meta.MemoryMetaManager" /> <bean id="eventStore" class="com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer"> <property name="bufferSize" value="${canal.instance.memory.buffer.size:16384}" /> <property name="bufferMemUnit" value="${canal.instance.memory.buffer.memunit:1024}" /> <property name="batchMode" value="${canal.instance.memory.batch.mode:MEMSIZE}" /> <property name="ddlIsolation" value="${canal.instance.get.ddl.isolation:false}" /> </bean> <bean id="eventSink" class="com.alibaba.otter.canal.sink.entry.EntryEventSink"> <property name="eventStore" ref="eventStore" /> </bean> <bean id="eventParser" class="com.alibaba.otter.canal.parse.inbound.group.GroupEventParser"> <property name="eventParsers"> <list> <ref bean="eventParser1" /> <ref bean="eventParser2" /> </list> </property> </bean> <bean id="eventParser1" class="com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser"> <property name="destination" value="${canal.instance.destination}" /> <property name="slaveId" value="${canal.instance.mysql.slaveId:1234}" /> <!-- Heartbeat configuration --> <property name="detectingEnable" value="${canal.instance.detecting.enable:false}" /> <property name="detectingSQL" value="${canal.instance.detecting.sql}" /> <property name="detectingIntervalInSeconds" value="${canal.instance.detecting.interval.time:5}" /> <property name="haController"> <bean class="com.alibaba.otter.canal.parse.ha.HeartBeatHAController"> <property name="detectingRetryTimes" value="${canal.instance.detecting.retry.threshold:3}" /> <property name="switchEnable" value="${canal.instance.detecting.heartbeatHaEnable:false}" /> </bean> </property> <property name="alarmHandler" ref="alarmHandler" /> <!-- Analytic filtering --> <property name="eventFilter"> <bean class="com.alibaba.otter.canal.filter.aviater.AviaterRegexFilter" > <constructor-arg index="0" value="${canal.instance.filter.regex:.*\..*}" /> </bean> </property> <property name="eventBlackFilter"> <bean class="com.alibaba.otter.canal.filter.aviater.AviaterRegexFilter" > <constructor-arg index="0" value="${canal.instance.filter.black.regex:}" /> <constructor-arg index="1" value="false" /> </bean> </property> <!-- After the maximum transaction resolution size exceeds this size, the transaction will be split into multiple transaction delivery --> <property name="transactionSize" value="${canal.instance.transaction.size:1024}" /> <!-- Network link parameter --> <property name="receiveBufferSize" value="${canal.instance.network.receiveBufferSize:16384}" /> <property name="sendBufferSize" value="${canal.instance.network.sendBufferSize:16384}" /> <property name="defaultConnectionTimeoutInSeconds" value="${canal.instance.network.soTimeout:30}" /> <!-- Analytic coding --> <!-- property name="connectionCharsetNumber" value="${canal.instance.connectionCharsetNumber:33}" /--> <property name="connectionCharset" value="${canal.instance.connectionCharset:UTF-8}" /> <!-- Parsing site record --> <property name="logPositionManager"> <bean class="com.alibaba.otter.canal.parse.index.MemoryLogPositionManager" /> </property> <!-- Time to roll back when failover switches --> <property name="fallbackIntervalInSeconds" value="${canal.instance.fallbackIntervalInSeconds:60}" /> <!-- Parsing database information --> <property name="masterInfo"> <bean class="com.alibaba.otter.canal.parse.support.AuthenticationInfo"> <property name="address" value="${canal.instance.master1.address}" /> <property name="username" value="${canal.instance.dbUsername:retl}" /> <property name="password" value="${canal.instance.dbPassword:retl}" /> <property name="defaultDatabaseName" value="${canal.instance.defaultDatabaseName:retl}" /> </bean> </property> <property name="standbyInfo"> <bean class="com.alibaba.otter.canal.parse.support.AuthenticationInfo"> <property name="address" value="${canal.instance.standby1.address}" /> <property name="username" value="${canal.instance.dbUsername:retl}" /> <property name="password" value="${canal.instance.dbPassword:retl}" /> <property name="defaultDatabaseName" value="${canal.instance.defaultDatabaseName:retl}" /> </bean> </property> <!-- Analytical start site --> <property name="masterPosition"> <bean class="com.alibaba.otter.canal.protocol.position.EntryPosition"> <property name="journalName" value="${canal.instance.master1.journal.name}" /> <property name="position" value="${canal.instance.master1.position}" /> <property name="timestamp" value="${canal.instance.master1.timestamp}" /> </bean> </property> <property name="standbyPosition"> <bean class="com.alibaba.otter.canal.protocol.position.EntryPosition"> <property name="journalName" value="${canal.instance.standby1.journal.name}" /> <property name="position" value="${canal.instance.standby1.position}" /> <property name="timestamp" value="${canal.instance.standby1.timestamp}" /> </bean> </property> <property name="filterQueryDml" value="${canal.instance.filter.query.dml:false}" /> <property name="filterQueryDcl" value="${canal.instance.filter.query.dcl:false}" /> <property name="filterQueryDdl" value="${canal.instance.filter.query.ddl:false}" /> <property name="filterTableError" value="${canal.instance.filter.table.error:false}" /> <property name="supportBinlogFormats" value="${canal.instance.binlog.format}" /> <property name="supportBinlogImages" value="${canal.instance.binlog.image}" /> </bean> <bean id="eventParser2" class="com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser"> <property name="destination" value="${canal.instance.destination}" /> <property name="slaveId" value="${canal.instance.mysql.slaveId:1234}" /> <!-- Heartbeat configuration --> <property name="detectingEnable" value="${canal.instance.detecting.enable:false}" /> <property name="detectingSQL" value="${canal.instance.detecting.sql}" /> <property name="detectingIntervalInSeconds" value="${canal.instance.detecting.interval.time:5}" /> <property name="haController"> <bean class="com.alibaba.otter.canal.parse.ha.HeartBeatHAController"> <property name="detectingRetryTimes" value="${canal.instance.detecting.retry.threshold:3}" /> <property name="switchEnable" value="${canal.instance.detecting.heartbeatHaEnable:false}" /> </bean> </property> <property name="alarmHandler" ref="alarmHandler" /> <!-- Analytic filtering --> <property name="eventFilter"> <bean class="com.alibaba.otter.canal.filter.aviater.AviaterRegexFilter" > <constructor-arg index="0" value="${canal.instance.filter.regex:.*\..*}" /> </bean> </property> <property name="eventBlackFilter"> <bean class="com.alibaba.otter.canal.filter.aviater.AviaterRegexFilter" > <constructor-arg index="0" value="${canal.instance.filter.black.regex:}" /> <constructor-arg index="1" value="false" /> </bean> </property> <!-- After the maximum transaction resolution size exceeds this size, the transaction will be split into multiple transaction delivery --> <property name="transactionSize" value="${canal.instance.transaction.size:1024}" /> <!-- Network link parameter --> <property name="receiveBufferSize" value="${canal.instance.network.receiveBufferSize:16384}" /> <property name="sendBufferSize" value="${canal.instance.network.sendBufferSize:16384}" /> <property name="defaultConnectionTimeoutInSeconds" value="${canal.instance.network.soTimeout:30}" /> <!-- Analytic coding --> <!-- property name="connectionCharsetNumber" value="${canal.instance.connectionCharsetNumber:33}" /--> <property name="connectionCharset" value="${canal.instance.connectionCharset:UTF-8}" /> <!-- Parsing site record --> <property name="logPositionManager"> <bean class="com.alibaba.otter.canal.parse.index.MemoryLogPositionManager" /> </property> <!-- Time to roll back when failover switches --> <property name="fallbackIntervalInSeconds" value="${canal.instance.fallbackIntervalInSeconds:60}" /> <!-- Parsing database information --> <property name="masterInfo"> <bean class="com.alibaba.otter.canal.parse.support.AuthenticationInfo"> <property name="address" value="${canal.instance.master2.address}" /> <property name="username" value="${canal.instance.dbUsername:retl}" /> <property name="password" value="${canal.instance.dbPassword:retl}" /> <property name="defaultDatabaseName" value="${canal.instance.defaultDatabaseName:retl}" /> </bean> </property> <property name="standbyInfo"> <bean class="com.alibaba.otter.canal.parse.support.AuthenticationInfo"> <property name="address" value="${canal.instance.standby2.address}" /> <property name="username" value="${canal.instance.dbUsername:retl}" /> <property name="password" value="${canal.instance.dbPassword:retl}" /> <property name="defaultDatabaseName" value="${canal.instance.defaultDatabaseName:retl}" /> </bean> </property> <!-- Analytical start site --> <property name="masterPosition"> <bean class="com.alibaba.otter.canal.protocol.position.EntryPosition"> <property name="journalName" value="${canal.instance.master2.journal.name}" /> <property name="position" value="${canal.instance.master2.position}" /> <property name="timestamp" value="${canal.instance.master2.timestamp}" /> </bean> </property> <property name="standbyPosition"> <bean class="com.alibaba.otter.canal.protocol.position.EntryPosition"> <property name="journalName" value="${canal.instance.standby2.journal.name}" /> <property name="position" value="${canal.instance.standby2.position}" /> <property name="timestamp" value="${canal.instance.standby2.timestamp}" /> </bean> </property> <property name="filterQueryDml" value="${canal.instance.filter.query.dml:false}" /> <property name="filterQueryDcl" value="${canal.instance.filter.query.dcl:false}" /> <property name="filterQueryDdl" value="${canal.instance.filter.query.ddl:false}" /> <property name="filterRows" value="${canal.instance.filter.rows:false}" /> <property name="filterTableError" value="${canal.instance.filter.table.error:false}" /> <property name="supportBinlogFormats" value="${canal.instance.binlog.format}" /> <property name="supportBinlogImages" value="${canal.instance.binlog.image}" /> </bean> </beans> ``` How to configure the destination when the client accesses? It is not possible to use a comma to separate it. ``` String destination = "food"; String ip = "127.0.0.1"; CanalConnector connector = CanalConnectors.newSingleConnector(new InetSocketAddress(ip 11111) destination "" ""); ``` # In addition, can the QQ group handle 161559791? ？ @xiaopan0513 I also want to know this, how can I write the corresponding code? @xiaopan0513 I also face the same problem. I would like to ask how to configure this piece.
395,Solution for inaccurate synchronization of data changes when using GroupEventSink hi，agapple： When using GroupEventSink, if there are one or more sub-libraries that have not been binlog, then all paser threads will be stuck on the doSink method and the sub-database data generated by binlog will not be consumed. The reason for designing from the code comments is to prevent each other. When a certain database is in trouble, it can be dropped. This can avoid the loss of data when the instance is restarted, but it also increases the limitation of the Group mode. Now we have a similar scenario. It is expected that there will be a large amount of data in advance. But the library has not yet been promoted in large quantities. The amount of data per day is only a few thousand. Some libraries will have no data for a long time. In order to solve this problem, I have two options. 1 One is to use heartbeat Sql like insert into retl.xdual values(1 now()) on duplicate key update x now guarantees that there are continuous binlogs generated by the library, so it will not get stuck. The disadvantage is that the business system has to invade and must create a supplementary table on the business library. 2 Another solution that comes to mind is to modify MysqlEventParser in the internal class MysqlDetectingTimeTask&#39;s run method to construct a Heartbeat type Event when there is no exception and then call the consumeTheEventAndProfilingIfNecessary method similar to AbstractEventParser&#39;s buildHeartBeatTimeTask method Do you see if there is a better plan? The design of the group mode was not very good. The performance based on the lock is too poor. How do you use this solution internally? The performance is also relatively poor. This method is still very convenient. The previous 8 sub-libraries need to create 8 instances. Then each instance of a bunch of mapping configuration and operation is still very troublesome. If you have time, then you I can think about it over there and see if there is a better solution to this kind of synergistic problem. @agapple Is there any good idea for group mode? For MySQL The scene of sub-database sub-tables is still quite a lot, but for the application side, I don’t really care how many libraries are in the bottom. I don’t know if there are any good suggestions for this. 参考 https github com alibaba canal pull 522 Upgrade canal Try version 26 Thank you for your concern about when the official version of 26 will be released. Currently we solve the deadlock problem by patch on 1 0 24 ps Previously, 1 0 25 will always have a thread stuck in the connection on reconnect. It seems to be solved in the 26 version.
394,TimelineTransactionBarrier has a deadlock bug The following code in the isPermit method of TimelineTransactionBarrier `if (isTransactionBegin(event)) { if (txState.compareAndSet(0 1)) { inTransaction.set(true); return true; // Transaction allowed } } else if (txState.compareAndSet(0 2)) { // Non-transactional protection return true; // DDL DCL allows passage }` For the last else The if branch not only ddl and dcl scenes will enter TransactionEnd but also enter the first event type that is obtained when the EventParser is started based on the last Position is TransactionEnd Look at the clear method of TimelineTransactionBarrier `public void clear(Event event) { super.clear(event); if (isTransactionEnd(event)) { inTransaction.set(false); // The transaction ends and the store has been successfully written. The cleanup flag goes into requeuing to allow new transactions to enter. txState.compareAndSet(1 0); // if (txState.compareAndSet(1 0) == false) { // throw new // CanalSinkException("state is not correct in transaction"); // } } else if (txState.intValue() == 2) {// Non-transaction txState.compareAndSet(2 0); // if (txState.compareAndSet(2 0) == false) { // throw new // CanalSinkException("state is not correct in non-transaction"); // } } }` The first if judgment should add one more condition txState.intValue() != 2 In response to the TransactionEnd problem described above, the state of txState will always be 2. The isPermit method always returns false. The system has a deadlock. Or swap the order of the two ifs in the clear method. Convenient to submit a pull Request me Good two days to finish
393,Table meta support [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=393) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=393) before we can accept your contribution.<br/>**0** out of **2** committers have signed the CLA.<br/><br/>:x: KaimingWan<br/>:x: wanshao<br/><hr/>**wanshao** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=393) it.</sub>
392,table meta support support table meta [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=392) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=392) before we can accept your contribution.<br/>**0** out of **2** committers have signed the CLA.<br/><br/>:x: KaimingWan<br/>:x: wanshao<br/><hr/>**wanshao** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=392) it.</sub>
391,The main library is mysql 5 7 main library code UTF8MB4 With canal 1 0 24 this version of the initial value of the site is 123 Execution time 1506482123 start error reporting The main library is mysql 5 7 main library code UTF8MB4 With canal 1 0 24 this version of the initial value of the site is 123 Execution time 1506482123 canal configured timestamp No configuration point prompts the following exception 2017-09-29 14:20:32.614 [main] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - start successful.... 2017-09-29 14:20:32.892 [destination = example_wxprober address = /XXXXXXX:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position mysql-bin.000013::1506482123000 2017-09-29 14:20:32.934 [destination = example_wxprober address = /XXXXXXX:3306 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file Could not find first log file name in binary log index File location is wrong
390,ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:xxx[java.io.IOException: Unexpected End Stream Occasionally report this error in the canal instance log. How to solve it? Mysql chain-breaking problem is currently a known issue. The trunk version has been submitted for repair. @agapple ``` 2017-11-06 15:00:03.937 [destination = action_log address = /127.0.0.2:2020 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address 127.0.0.2/127.0.0.2:2020 has an error retrying. caused by java.io.IOException: Unexpected End Stream at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) ~[canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70) ~[canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122) ~[canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) ~[canal.parse-1.0.24.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.7.0_80] 2017-11-06 15:00:03.938 [destination = action_log address = /127.0.0.2:2020 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:action_log[java.io.IOException: Unexpected End Stream at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) at java.lang.Thread.run(Thread.java:745) ] 2017-11-06 15:00:22.587 [destination = action_log address = /127.0.0.2:2020 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position {"identity":{"slaveId":-1 "sourceAddress":{"address":"127.0.0.2" "port":2020}} "postion":{"included":false "journalName":"mysql-bin.001213" "position":638501339 "serverId":1037 "timestamp":1509951603000}} 2017-11-06 17:00:22.588 [destination = action_log address = /127.0.0.2:2020 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Unexpected End Stream at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) ~[canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70) ~[canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122) [canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) [canal.parse-1.0.24.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.7.0_80] 2017-11-06 17:00:22.588 [destination = action_log address = /127.0.0.2:2020 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address 127.0.0.2/127.0.0.2:2020 has an error retrying. caused by java.io.IOException: Unexpected End Stream at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) ~[canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70) ~[canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122) ~[canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) ~[canal.parse-1.0.24.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.7.0_80] 2017-11-06 17:00:22.588 [destination = action_log address = /127.0.0.2:2020 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:action_log[java.io.IOException: Unexpected End Stream at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) at java.lang.Thread.run(Thread.java:745) ] 2017-11-06 17:00:34.936 [destination = action_log address = /127.0.0.2:2020 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position {"identity":{"slaveId":-1 "sourceAddress":{"address":"127.0.0.2" "port":2020}} "postion":{"included":false "journalName":"mysql-bin.001214" "position":269841413 "serverId":1037 "timestamp":1509958822000}} (END) ``` I report it every two hours, but it seems that I have no data. Does it affect the online business? Is the new version of the canal fixed? Pro, this problem has been solved. @tdytaylor I haven’t solved it yet if you solved it. mysql The server server actively disconnects the link and tries to use the latest version. This problem will cause the loss of data? Is there any easy way to upgrade the version?
389,canal is not run any in node failed to connect to:/1.0.0.1:7536 after retry 1 times 2017-09-27/14:33:30.751||||^_^|uid:|clientIp:|deviceId:|^_^|[binlog main] ERROR c.a.o.c.client.impl.running.ClientRunningMonitor 137 - There is an error when execute initRunning method with destination [es1]. 434654-com.alibaba.otter.canal.protocol.exception.CanalClientException: java.net.ConnectException: Connection refused 434655- at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.doConnect(SimpleCanalConnector.java:171) 434656- at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.access$000(SimpleCanalConnector.java:48) 434657- at com.alibaba.otter.canal.client.impl.SimpleCanalConnector$1.processActiveEnter(SimpleCanalConnector.java:396) 434658- at com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor.processActiveEnter(ClientRunningMonitor.java:207) 434659- at com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor.initRunning(ClientRunningMonitor.java:118) 434660- at com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor.start(ClientRunningMonitor.java:92) 434661- at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.connect(SimpleCanalConnector.java:93) 434662- at com.alibaba.otter.canal.client.impl.ClusterCanalConnector.connect(ClusterCanalConnector.java:63) 434663- at com.alibaba.otter.canal.client.impl.ClusterCanalConnector.restart(ClusterCanalConnector.java:275) 434664- at com.alibaba.otter.canal.client.impl.ClusterCanalConnector.getWithoutAck(ClusterCanalConnector.java:187) 434665- at com.domain.es.canal.Consumer$HandlerThread.run(BinLogConsumer.java:194) 434666-Caused by: java.net.ConnectException: Connection refused 434667- at sun.nio.ch.Net.connect0(Native Method) 434668- at sun.nio.ch.Net.connect(Net.java:454) 434669- at sun.nio.ch.Net.connect(Net.java:446) 434670- at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) 434671- at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.doConnect(SimpleCanalConnector.java:132) 434672- ... 10 common frames omitted 434673-2017-09-27/14:33:30.759||||^_^|uid:|clientIp:|deviceId:|^_^|[binlog main] WARN c.a.otter.canal.client.impl.ClusterCanalConnector 66 - failed to connect to:/1.0.0.1:7536 after retry 1 times 434674-2017-09-27/14:33:30.761||||^_^|uid:|clientIp:|deviceId:|^_^|[binlog main] WARN c.a.o.c.client.impl.running.ClientRunningMonitor 174 - canal is not run any in node Caused by: java.net.ConnectException: Connection refused
388,Canal will report irregularly no Data situation We use canal to get binlog data like otter, but after running for a while, there are individual channel tasks that can report long time no. Data situation alarm content such as pid 24 nid:4 exception:mainstem:pid:24 canal elapsed 5000 seconds no data Switching mainstem can&#39;t solve the problem. Restarting the entire service will recover for a while. I don&#39;t know if the big gods have encountered similar problems or have a solution. We also have the same problem but it doesn&#39;t seem to affect synchronization. Should be the early version of the canal source library no data problems try to upgrade the otter and canal version
387,canal Will the server restart be resolved from the last location? If HA is turned on, the last parsed site should exist. The zk will be resolved from the last site. I am using file persistence and do not support HA. If I restart, I can resolve from the last location. In addition, I use Otter with the current Otter. It seems to only support the memory mode. Will I lose the data during the service disconnection after reboot? Don&#39;t lose too much to see WIKI and FAQ @zhumt The file mode will produce a meta dat file record in the instance configuration.
386,Binlog parsing slows down under high pressure During the test, it was found that when the binlog was generated more than 30,000 qps, the event generated by the canal parsing changed to a particularly slow server with a cpu running to 100. What is the situation? 参考 https github com alibaba canal wiki FAQ
384,Startup bat startup error in windows Error: missing `server' JVM at `C:\Program Files (x86)\Java\jre1.8.0_111\bin\server\jvm.dll'. Please install or use the JRE or JDK that contains these missing components. Check your own jvm and try using the 1 6 1 7 version
383,canal Postion reset problem @agapple @lulu2panpan Please have two free time to see thank you. - Last successful ack log [ batchId: 906915 position: PositionRange[ start=LogPosition[ identity=LogIdentity[ sourceAddress=192.168.10.180/192.168.10.180: 3306 slaveId=-1 ] postion=EntryPosition[ included=false journalName=mysql-bin.000940 position=150869837 serverId=180 timestamp=1505671087000 ] ] ack=<null> end=LogPosition[ identity=LogIdentity[ sourceAddress=192.168.10.180/192.168.10.180: 3306 slaveId=-1 ] postion=EntryPosition[ included=false journalName=mysql-bin.000940 position=151977852 serverId=180 timestamp=1505671093000 ] ] ] ] - Rollback failed ack log e[ start=LogPosition[ identity=LogIdentity[ sourceAddress=192.168.10.180/192.168.10.180: 3306 slaveId=-1 ] postion=EntryPosition[ included=false journalName=mysql-bin.000940 position=151979096 serverId=180 timestamp=1505671093000 ] ] ack=LogPosition[ identity=LogIdentity[ sourceAddress=192.168.10.180/192.168.10.180: 3306 slaveId=-1 ] postion=EntryPosition[ included=false journalName=mysql-bin.000940 position=138659218 serverId=180 timestamp=1505670995000 ] ] end=LogPosition[ identity=LogIdentity[ sourceAddress=192.168.10.180/192.168.10.180: 3306 slaveId=-1 ] postion=EntryPosition[ included=false journalName=mysql-bin.000940 position=138740531 serverId=180 timestamp=1505670996000 ] ] ] We compare the two logs and find that the contents of the ack node have changed. Normally, the value of ack is null, but the information is attached in the abnormal case and the position of position is higher than start. with The end node must be in front of it. I don’t know why this happens. The online environment has been encountered many times. Because the position change data will repeat the primary key conflict. The insert operation cannot be manually modified. Our current solution is 1. Compare the position of zk is the latest. If it is, then it can be read normally after restarting the server. 2. If the position of zk is also the previous site, you can only manually modify the post of zk to the latest site to solve I haven’t found any specific reasons so far and I don’t know what caused it. We are a multi-instance way to deploy jdk 1.7 canal It is inevitable that the parser will have repeated consumption when the active/standby switch or the link is disconnected.
382,canal Specified timestamp 1 Why do you only need to specify the timestamp when you can&#39;t find the binlog location when you start it? Filename is specified together to find the location. I see the document is written like this. canal.instance.master.timestamp : Specify a timestamp canal will automatically traverse mysql Binlog finds the binlog bit corresponding to the timestamp and starts it 2 and I use binlog filename+ When position is specified, he will only traverse this binlog. Will it not execute the next binlog? Please help me to answer timestamp Not used binlog filename+ Position will also automatically remove a binlog LS Correct Answer Simply use timestamp I tried it, but I used binlog. filename+ Position no problem
381,I always get an error when starting a channel. pid:9 nid:1 exception:canal:tdesk_test_canal:java.lang.NoClassDefFoundError: Could not initialize class com.alibaba.otter.canal.common.utils.JsonUtils at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPositionInternal(MysqlEventParser.java:415) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPosition(MysqlEventParser.java:315) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:161) at java.lang.Thread.run(Thread.java:748) What is the reason why there is no reason for the location? It may have been wrong for a while. Please use the latest version of the test
380,A bug in which multiple ddl statements cause a schema mismatch under delay When there are multiple ddl statements If there is a delay in binlog parsing Will cause the following error com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: column size is not match for table:`xxdb`.`xxxtable` 39 vs 37。 Here&#39;s why There are two ddl statements in the binlog before and after the parsing of the a and b. The parse is resolved to a. The latest schema is obtained from the db. The table structure of the message between the a and b after the b operation does not match the latest schema. I also encountered the same problem and asked God to answer The next version will be resolved According to the source code canal take the meta structure of the table structure is not parsed from the binlog but from the mysql database using desc Db1 table1 to get this must have hidden dangers Use the following case to reproduce the problem Stop canal first Then run the following sql statement on the mysql main library use dbt1; CREATE TABLE Usert1 ( `userId` BIGINT NOT NULL AUTO_INCREMENT `userNickName` varchar(20) NOT NULL DEFAULT '' `isSystem` TINYINT NOT NULL DEFAULT 0 `updateTime` INT NOT NULL DEFAULT 0 PRIMARY KEY (`userId`) ) ENGINE=InnoDB AUTO_INCREMENT=100000; CREATE INDEX IDX_Usert1_mobile on `Usert1` (`mobile`); INSERT INTO Usert1(`userId` `userNickName` `isSystem` `updateTime`) VALUES (101 'u101' 1 UNIX_TIMESTAMP()); INSERT INTO Usert1(`userId` `userNickName` `isSystem` `updateTime`) VALUES (201 'u201' 1 UNIX_TIMESTAMP()); UPDATE Usert1 SET isSystem=0 WHERE userId=201; DELETE FROM Usert1 WHERE userId=101; SELECT * FROM Usert1; ALTER TABLE Usert1 ADD COLUMN age INT DEFAULT 0; ALTER TABLE Usert1 DROP COLUMN updateTime; ALTER TABLE Usert1 change userNickName nickName varchar(20); INSERT INTO Usert1(`userId` `nickName` `isSystem` `age`) VALUES (301 'u301' 1 301); INSERT INTO Usert1(`userId` `nickName` `isSystem` `age`) VALUES (302 'u302' 1 302); UPDATE Usert1 SET isSystem=3 WHERE userId=301; DELETE FROM Usert1 WHERE userId=302; SELECT * FROM Usert1; Restart canal.deployer-1.0.24 with canal.example-1.0.24 Then you can see the following output from the log output of the canal example 1 0 24. The output below the nickName is already the new column name and the value of the updateTime column becomes the value of the age column. ================> binlog[log-bin.000007:5281] executeTime : 1507805075000 delay : -17643ms BEGIN ----> Thread id: 17 ----------------> binlog[log-bin.000007:5407] name[dbt1 Usert2] eventType : INSERT executeTime : 1507805075000 delay : -17638ms userId : 101 type=bigint(20) update=true nickName : u101 type=varchar(20) update=true isSystem : 1 type=tinyint(4) update=true age : 1507805075 type=int(11) update=true ---------------- END ----> transaction id: 105 The new version will support ddl version changes. You can freely switch back to any location. The DDL table structure will also be rolled back synchronously. The new version of the code has been submitted to the master @agapple The master branch compiles a bit of a problem, when the new version 1 0 25 ``` [ERROR] Failed to execute goal on project canal.parse: Could not resolve dependencies for project com.alibaba.otter:canal.parse:jar:1.0.25-SNAPSHOT: Failed to collect dependencies at com.alibaba:druid:jar:1.1.5-preview_14: Failed to read artifact descriptor for com.alibaba:druid:jar:1.1.5-preview_14: Could not transfer artifact com.alibaba:druid:pom:1.1.5-preview_14 from/to alibaba (http://code.alibabatech.com/mvn/releases/): Connect to code.alibabatech.com:80 [code.alibabatech.com/119.38.217.15] failed: Refuse to connect (Connection refused) -> [Help 1] [ERROR] [ERROR] To see the full stack trace of the errors re-run Maven with the -e switch. [ERROR] Re-run Maven using the -X switch to enable full debug logging. [ERROR] [ERROR] For more information about the errors and possible solutions please read the following articles: [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException ``` Switch to druid 1 1 5 official version master Compilation cannot succeed } else if (isHeartBeat) { // Handling test heartbeat heartbeat data in alisql mode already fixed How can the old version skip this question? Older versions can only skip data. Try 26 latest version
379,Is there an event that considers the transaction rollback? Mysql binlog document has such a paragraph when introducing XID_EVENT > XID_EVENT > Generated for a commit of a transaction that modifies one or more tables of an XA-capable storage engine. Normal transactions are implemented by sending a QUERY_EVENT containing a BEGIN statement and a QUERY_EVENT containing a COMMIT statement (or a ROLLBACK statement if the transaction is rolled back). But I see that the QUERY_EVENT in the canal1 24 code only considers BEGIN and COMMIT and does not consider ROLLBACK. What is the reason for this? https://dev.mysql.com/doc/refman/5.7/en/innodb-and-mysql-replication.html There is another narrative here. Transactions that fail on the master do not affect replication at all. MySQL replication is based on the binary log where MySQL writes SQL statements that modify data. A transaction that fails (for example because of a foreign key violation or because it is rolled back) is not written to the binary log It seems that the binlog that does not record the rollback is not particularly clear. Rollback does not log binlog only committed transactions will be logged
378,Configurable mysql Ssl connection? see title Temporarily not supported
377,BINARY type resolution If there is a column in the database that uses the BINARY type column getValue to get garbled, how to deal with the BINARY in the database is treated with hex column.getValue().getBytes("ISO-8859-1")
376,Specify the starting position of the binlog Hello, how are you? I now want to start the canal and read all the existing binlogs so I need to specify the location of the binlog but I can&#39;t use show because of permission issues. binary Logs statement, is there any way to make canal automatically dump from the first binlog file? @agapple @luyee @agapple May be busy Say the used posture Need to keep all binlog files from the first one This is not realistic. Used plan 1 No update time, such as 3:00 in the morning, directly import the automatic acquisition of the site. 2 Time period with data update 2.1 Now generally small table direct mysqldump mydumper myloader + Specified site increment 2.2 xbackup The specified site increment can specify the table or db for partial recovery. However, this stuff is a bit pit. It is not necessary to specify some table recovery. It seems that db level recovery is required. The server&#39;s datadir must be empty 2.3 3 4 points direct etl tool pumping full automatic acquisition of sites 3 Recently discovered debezium https github com debezium debezium This support full increment is directly imported into kafka is under study About permission permissions, this stuff is looking for dba Operation and maintenance can not be completed, in the recovery, or what is generally a site full export import Let the dba operation provide Pull binlog This is a required reference document to open permissions. ``` GRANT SELECT REPLICATION SLAVE REPLICATION CLIENT ON *.* TO 'canal'@'%'; ``` Well, it’s really unrealistic to decide to use the direct full import to automatically get the site. Thanks answer
375,Master branch source code compilation error Looked at the master branch and put netty Version changed from 3 x to 4 x netty package name changed from org jboss netty io netty but canal used to netty The api&#39;s local package name has not been changed to cause an error. 1.023 Version of netty4 This version is unstable 1 0 24 version has retreated currently Netty3 and netty4 are dependent on
374,try to avoid use multiline exception directly I want to put the canal into the docker. Now the canal print exception method makes the fluker of the docker not easy to collect many abnormal log prints. It can be used directly by the logback function. There is no need to explicitly convert it. This also makes the canal and docker. It&#39;s easier to combine @agapple [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=374) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=374) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=374) it.</sub> tks
373,meta.bat s position meta.bat Where is the storage location? canal Service connect zookeeper using spring xml classpath:spring/default-instance.xml If using default data is stored in zookeeper
372,canal The amount of data in the subscription mode is too large to explode the memory. Is there a best practice for canal? canal The amount of data in the subscription mode is too large to explode the memory. Is there a best practice for canal? batchsize Too big Adjust the size of the ringbuffer on the server
371,Spring version 2 5 6 and the version of the application conflict, is there any way to be compatible? The application uses the canal client method is 4 x spring And canal is using 2 5 6 spring Boot will report ``` java.lang.NoSuchMethodError: org.springframework.core.annotation.AnnotationAwareOrderComparator.sort ``` Is there a way to get around? Self-answering exclusions Should be able to solve @liutaihua Will you use a different version of the spring canal after removing the dependencies, will it not be an inexplicable bug? @agapple Will you use a different version of the spring canal after removing the dependencies, will it not be an inexplicable bug? There is very little spring function for canal
370,Does canal support regular consumption? Does canal support regular consumption? no Timing rocketmq can
369,Canal consumer does not consume at a certain point in time Canal canal deployer 1 0 24 The latest version of The subscribed mysql version is mariadb 10 0 22 Discover that the consumer is running to a certain moment Zk&#39;s location and timestamp are not changing And the consumer process has not hanged No error log found Restart process is not good Time stamp must be reset to instance properties Delete the site in zk Re-consumption Hope for reply Thank you No log allowed Is it blocked? Solving the same problem No log is very annoying, you can be sure Direct problem with client and server +1
367,About zk storage of resolution and consumption sites, data loss to flush time 在default instance xml Configured as a periodic input zk. If the server hangs up another standby machine before the flush time, the location of the read is behind the latest location. Because the main server is not flushed to zk, it will cause repeated reading of data. Distributed high availability systems are not designed to ensure data is not duplicated
366,This is BUG? Canal parsing error fetch failed by table meta:`schemeName`.`tableName` This problem is similar to 358 If the canal reads from the first record of a database binglog, the canal may be sent during the parsing process because the table corresponding to the record is deleted when parsing into a binlog record. Desc  schemeName tableName 的sql  private TableMeta getTableMeta0(String fullname) throws IOException { ResultSetPacket packet = connection.query("desc " + fullname); return new TableMeta(fullname parserTableMeta(packet)); } And the corresponding table has been deleted. This time, this exception will be thrown and the binlog reading will not continue. I don&#39;t know if this is a bug or a solution to the problem. The specific log is like this 2017-08-10 14:26:56.593 [destination = 10.160.246.137-1379 address = /10.160.246.137:1379 EventParser]369 WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - ERROR ## parse this event has an error last position : [EntryPosition[included=false journalName= mysql-bin.000003 position=46788142 serverId=1092 timestamp=1493132987000]] com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.collect.ComputationException: com.alibaba.otter.canal.parse.exception.CanalParseException: fetch failed by table meta:`expert`.`bak_TB_ZXJ_ADVICE_COLLECT_ACTIVITY` Caused by: com.google.common.collect.ComputationException: com.alibaba.otter.canal.parse.exception.CanalParseException: fetch failed by table meta:`expert`.`bak_TB_ZXJ_ADVICE_COLLECT_ACTIVITY` at com.google.common.collect.MapMaker$ComputingMapAdapter.get(MapMaker.java:889) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:78) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:677) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:362) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:108) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:62) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:326) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:176) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:130) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) at java.lang.Thread.run(Thread.java:745) Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: fetch failed by table meta:`expert`.`bak_TB_ZXJ_ADVICE_COLLECT_ACTIVITY` Caused by: java.io.IOException: ErrorPacket [errorNumber=1146 fieldCount=-1 message=Table 'expert.bak_TB_ZXJ_ADVICE_COLLECT_ACTIVITY' doesn't exist sqlState=42S02 sqlStateMarker=#] with command: desc `expert`.`bak_TB_ZXJ_ADVICE_COLLECT_ACTIVITY` at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.query(MysqlQueryExecutor.java:60) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:73) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta0(TableMetaCache.java:105) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:26) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.apply(TableMetaCache.java:51) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.apply(TableMetaCache.java:42) at com.google.common.collect.ComputingConcurrentHashMap$ComputingValueReference.compute(ComputingConcurrentHashMap.java:356) at com.google.common.collect.ComputingConcurrentHashMap$ComputingSegment.compute(ComputingConcurrentHashMap.java:182) at com.google.common.collect.ComputingConcurrentHashMap$ComputingSegment.getOrCompute(ComputingConcurrentHashMap.java:151) at com.google.common.collect.ComputingConcurrentHashMap.getOrCompute(ComputingConcurrentHashMap.java:67) at com.google.common.collect.MapMaker$ComputingMapAdapter.get(MapMaker.java:885) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:78) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:677) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:362) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:108) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:62) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:326) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:176) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:130) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) at java.lang.Thread.run(Thread.java:745) what is the problem this is already settled Now solve the problem that you can continue to run by configuring filterTableError true to ignore this exception. I also encountered this problem. Will this filterTableError be true? Where is it added? Add filterTableError true directly @liutizhong I am using the CanalInstanceWithManager as long as it is set in its configuration CanalParameter. The code is basically like this CanalParameter parameter = new CanalParameter; parameter.setFilterTableError(true); Canal canal = new Canal(); canal.setCanalParameter(parameter); Finally in new CanalInstance new CanalInstanceWithManager(canal filter); @liutizhong Can be configured directly in the configuration file canal.instance.filter.table.error =true 6666666666😁 2017-11-22 11:41 GMT+08:00 yuxie <notifications@github.com>: > @liutizhong <https://github.com/liutizhong> Can be configured directly in the configuration file > canal.instance.filter.table.error =true > > — > You are receiving this because you commented. > Reply to this email directly view it on GitHub > <https://github.com/alibaba/canal/issues/366#issuecomment-346234373> or mute > the thread > <https://github.com/notifications/unsubscribe-auth/AWRZFY2hQZ-z-4GZKhxsDL64GgwgKF5Dks5s45fSgaJpZM4OzTks> > . > Niubi I also encountered this problem. Where should I add the otter configuration file? canal.instance.filter.table.error True to ignore table errors Thank you for sharing the direct modification configuration file canal instance filter table error =true Ignore the exception
365,heartbeatHaEnable is set to true but successfully started without the standby mysql configuration heartbeatHaEnable is set to true but there is no standby mysql configuration when it starts successfully but does mysql stop & When the start action is done, the parser will do the switch operation. This operation will cause the null pointer to exit. Since the null pointer is stopped after the heartbeat thread and the parsing thread, the new heartbeat and the parsing thread are started, so the canal cannot continue the dump message. What is the version?
364,maven update Missing artifact com.alibaba.otter:canal.instance.core:jar:1.0.25-SNAPSHOT I would like to ask how to solve this problem when I grab the latest code from the master to update the maven Missing artifact com.alibaba.otter:canal.instance.core:jar:1.0.25-SNAPSHOT Missing artifact com.alibaba.otter:canal.instance.manager:jar:1.0.25-SNAPSHOT Missing artifact com.alibaba.otter:canal.instance.spring:jar:1.0.25-SNAPSHOT Originally 1 0 25 SNAPSHOT How come Miss First in canal Root directory run ``` mvn clean mvn install ```
363,Can Canal have plans to support similar functionality in groups like mysql5 7? After mysql upgrade 5 7 due to improved master-slave synchronization performance The limitation of the main library write qps is also reduced. This brings up a problem that the speed of business writes may be faster than the speed of our collection. This may not be possible to collect the real-time collection of canal plans to support the group submission mentioned in mysql5 7? Canal is submitted after the binlog is written and does not care about mysql.
362,Maven view source code garbled All jar packages through maven to view the source code, its Chinese comments all garbled can solve this garbled problem? Utf8 encoding
361,Get the increment of other databases from the client even if defaultDatabaseName is set defaultDatabaseName is not a parameter that is currently used to filter data.
360,SocketChannelPool bug Please follow the last line of the running log ```` 2017-08-08 09:59:07.224 [destination = example address = /127.0.0.1:3306 EventParser] DEBUG io.netty.util.NetUtil - -Djava.net.preferIPv4Stack: false 2017-08-08 09:59:07.224 [destination = example address = /127.0.0.1:3306 EventParser] DEBUG io.netty.util.NetUtil - -Djava.net.preferIPv6Addresses: false 2017-08-08 09:59:07.313 [destination = example address = /127.0.0.1:3306 EventParser] DEBUG io.netty.util.NetUtil - Loopback interface: lo (Software Loopback Interface 1 127.0.0.1) 2017-08-08 09:59:07.317 [destination = example address = /127.0.0.1:3306 EventParser] DEBUG io.netty.util.NetUtil - \proc\sys\net\core\somaxconn: 200 (non-existent) 2017-08-08 09:59:07.403 [destination = example address = /127.0.0.1:3306 EventParser] DEBUG io.netty.channel.DefaultChannelId - -Dio.netty.machineId: 00:00:00:00:00:00:00:e0 (auto-detected) 2017-08-08 09:59:07.404 [destination = example address = /127.0.0.1:3306 EventParser] DEBUG io.netty.util.internal.ThreadLocalRandom - -Dio.netty.initialSeedUniquifier: 0xfe50d10b389fdd4b 2017-08-08 09:59:07.482 [destination = example address = /127.0.0.1:3306 EventParser] DEBUG io.netty.buffer.ByteBufUtil - -Dio.netty.allocator.type: pooled 2017-08-08 09:59:07.483 [destination = example address = /127.0.0.1:3306 EventParser] DEBUG io.netty.buffer.ByteBufUtil - -Dio.netty.threadLocalDirectBufferSize: 65536 2017-08-08 09:59:07.483 [destination = example address = /127.0.0.1:3306 EventParser] DEBUG io.netty.buffer.ByteBufUtil - -Dio.netty.maxThreadLocalCharBufferSize: 16384 2017-08-08 09:59:17.547 [destination = example address = /127.0.0.1:3306 EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - connect MysqlConnection to /127.0.0.1:3306... ```` ````java public static SocketChannel open(SocketAddress address) throws Exception { final SocketChannel socket = new SocketChannel(); boot.connect(address).addListener(new ChannelFutureListener() { @Override public void operationComplete(ChannelFuture arg0) throws Exception { if (arg0.isSuccess()) socket.setChannel(arg0.channel()); synchronized (socket) { socket.notify(); // <-------------------- } } }); synchronized (socket) { socket.wait(); // <-------------------- } if (null == socket.getChannel()) { throw new IOException("can't create socket!"); } chManager.put(socket.getChannel() socket); return socket; } ```` Notify is a suggested optimization that may be executed ahead of wait
359,About mysql cut the main problem We have a scene here mysql is a master two from canal from the main dump Binlog, but the main words, we are cutting the domain name domain name, the domain name hangs the ip change in this case canal Is to restart Need mysql Master-slave Site file name Different needs human modification to From the library file name Pay attention to the site canal Connected to New ip Ensure the main library No write And sync to From the library Talent Switch a lot of details to pay attention to The new version already supports the single vip mode. It will record the last serverId. When the database is switched between active and standby, it will judge whether the serverId is the same or not. @agapple How can the canal automatically find the correct binlog in the case of non-gtid? file&pos？ Time stamp based What is the principle? The timestamp of the same event on the new master is different from that of the old master. Only the serverid of evnet remains unchanged. I also asked how the timestamp on the new master is different from the old master. How can I determine the log? position？ The timestmap will be used to find the corresponding binlog on the standby database. The binlog event closest to the timestamp is used as the starting site for the standby storage. Here one assumes that the time of the active and standby libraries is guaranteed by NTP synchronization and will not differ by more than 60 seconds by default. I see the code master-storage library switch based on the timestmap pushed 60s, this may be repeated consumption Whether to record the latest entry when you can consume, push 60s before the main and standby library switch, and then find the entry of the latest consumption of the standby database through MD5 or hash entry, and then save the consumption, then it will not repeat consumption.
358,Canal parsing error fetch failed by table meta:`test`.`test` Is there any way to solve this kind of error? Thank you. com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.collect.ComputationException: com.alibaba.otter.canal.parse.exception.CanalParseException: fetch failed by table meta:`test`.`test` Caused by: com.google.common.collect.ComputationException: com.alibaba.otter.canal.parse.exception.CanalParseException: fetch failed by table meta:`test`.`test` at com.google.common.collect.MapMaker$ComputingMapAdapter.get(MapMaker.java:889) ~[guava-18.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:78) ~[canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:677) ~[canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:362) ~[canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:108) ~[canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:62) ~[canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:326) ~[canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:176) ~[canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:130) [canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) [canal.parse-1.0.24.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_91] I also encountered this error. I found the reason. This is because the table corresponding to the record is deleted when parsing a binlog record. Canal will send a desc during the binlog parsing process. Sql language of schemeName tableName private TableMeta getTableMeta0(String fullname) throws IOException { ResultSetPacket packet = connection.query("desc " + fullname); return new TableMeta(fullname parserTableMeta(packet)); } And the corresponding table has been deleted, this time will throw this exception But I didn&#39;t find out how to solve this method. I don&#39;t know if this is a bug. Is there any Ali’s big answer? Try canal/conf Under canal.instance.filter.black.regex = test.* Try the latest version of TSDB to solve this problem.
357,Multi-client subscription questions Multiple canal clients subscribe to a canal service. One of the clients performs the unsubscribe operation. Other canal client throw exceptions will not affect the subscription message reception of other clients. @agapple It is recommended that only one client consumes or the data sequence cannot be guaranteed. @agapple If the client can guarantee the sequential consumption for multiple clients to subscribe to the same service, one of the clients calls unsubscribe Does the other client throw an exception, will it affect the subscription and message acquisition of other clients? Currently affecting other clients @agapple The actual measurement has really affected the thanks. @agapple Can can get data from RDS? Can subscribe to RDS @agapple Thanks to 161559791, this group can&#39;t join my QQ 295826395. Can you pull me in? Thank you. @NPCSun The group can already be added The way I use it now is to write first. Redis Consumer subscription
356,Canal anomaly com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.collect.ComputationException: com.alibaba.otter.canal.parse.exception.CanalParseException: fetch failed by table meta:`mysql`.`ha_health_check` Caused by: com.google.common.collect.ComputationException: com.alibaba.otter.canal.parse.exception.CanalParseException: fetch failed by table meta:`mysql`.`ha_health_check` at com.google.common.collect.MapMaker$ComputingMapAdapter.get(MapMaker.java:889) ~[guava-18.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:70) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:645) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:357) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:111) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:62) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:327) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:177) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:121) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210) ~[canal.parse-1.0.22.jar:na] at java.lang.Thread.run(Thread.java:722) [na:1.7.0_10] The canal version is 1 0 22 I don&#39;t know what this anomaly means to ask God to help answer Solving this problem is also encountered by me. Upgraded version has been fixed I use the latest version and report such an error. Is there any solution?
355,Canal parsing slows when the database has a large number of deletes When the database has a large number of deletes, the canal parsing becomes slower before each hour can be resolved 50,000. There are a lot of deletions every hour after 100. Is there any solution? Thank you. This and 267 Is the same type of problem Mysql configuration parameters as innodb_flush_log_at_trx_commit Whether to submit a buffer when submitting a transaction There are 3 values 0 does not actively trigger log buffer write to disk 1 By default, the log buffer is flushed to disk at the same time each transaction is committed. 2 Every time a transaction is committed, the log buffer is flushed to disk but he is not doing it at the same time. But refresh every second Recommended to be configured as 2 Great impact on performance Can try this
354,Start cancl connection database connection is not available 2017-08-01 09:27:36.766 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2017-08-01 09:27:36.770 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties] 2017-08-01 09:27:36.850 [main] WARN org.springframework.beans.TypeConverterDelegate - PropertyEditor [com.sun.beans.editors.EnumEditor] found through deprecated global PropertyEditorManager fallback - consider using a more isolated form of registration e.g. on the BeanWrapper/BeanFactory! 2017-08-01 09:27:36.971 [main] INFO c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-example 2017-08-01 09:27:36.987 [main] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - start successful.... 2017-08-01 09:27:44.004 [destination = example address = /192.168.**.**:**** EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /192.168.30.46:3306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: connect /192.168.**.**:**** failure:java.net.ConnectException: Connection timed out at sun.nio.ch.Net.connect0(Native Method) at sun.nio.ch.Net.connect(Net.java:364) at sun.nio.ch.Net.connect(Net.java:356) at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:623) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:70) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:56) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:86) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:157) at java.lang.Thread.run(Thread.java:722) I haven&#39;t known if this is the problem. But I didn&#39;t see the source code in the configuration file. I don&#39;t know why it is asking for help. Thank you very much. This is definitely a problem with your own connection. 1 0 24 Still don&#39;t use it well
353,How to configure a table under multiple databases How can I specify how to subscribe to several databases in several databases? For example, I only subscribe. Order table under oschina library and user table under oschina_user Overturning the previous issue The correct posture should be the client write filter see https://github.com/alibaba/canal/issues/311 Multiple libraries That is canal server Configuration Multiple instances
352,canal Report an error Read Q_FLAGS2_CODE error: limit excceed: 63 Bulk insertion of large data volumes Similar select into Sql 10w records canal Report an error pid:1 nid:2 exception:canal:canal_master:java.io.IOException: Read Q_FLAGS2_CODE error: limit excceed: 63 at com.taobao.tddl.dbsync.binlog.event.QueryLogEvent.unpackVariables(QueryLogEvent.java:650) at com.taobao.tddl.dbsync.binlog.event.QueryLogEvent.(QueryLogEvent.java:477) at com.taobao.tddl.dbsync.binlog.LogDecoder.decode(LogDecoder.java:154) at com.taobao.tddl.dbsync.binlog.LogDecoder.decode(LogDecoder.java:106) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:123) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) at java.lang.Thread.run(Thread.java:745) Caused by: java.lang.IllegalArgumentException: limit excceed: 63 at com.taobao.tddl.dbsync.binlog.LogBuffer.getUint32(LogBuffer.java:561) at com.taobao.tddl.dbsync.binlog.event.QueryLogEvent.unpackVariables(QueryLogEvent.java:563) ... 6 more Report the mysql and canal versions I also have this problem here. Canal is 1 0 22 Mysql is 5 7 12 5 log @jwcjlu Try testing the 1 0 26 version first. After testing 1 0 25, there is no problem.
351,What does batchSize mean? Hello, I tried it. Blocked getWithoutAck finds that the example is printed when the update is updated many times but the batchSize is set to 1 I don’t quite understand what batchSize means. I thought it was a modification. Batch Id: [3] count : [33] memsize : [1050] Time : 2017-07-29 23:52:08 * Start : [mysql-bin.000003:17576:1501343167000(2017-07-29 23:46:07)] * End : [mysql-bin.000003:19943:1501343529000(2017-07-29 23:52:09)] This is the printed log trouble batchSize Is the parameter set on the client side is the control The size of the get data each time from the server side of the store That does not mean A modification of the database is to have 33 entries for the time seen above. Under what circumstances is the server having a data? Yeah, I don&#39;t really understand the specific role of batchSize. When I set batchSize 1, I have multiple entries in the Message that I pull each time. batchSize is the most appropriate number of records just above this size The minimum is 1 Message @agapple Uh, thank you.
350,Canal parsing json type error Same as the problem mentioned in 330. The mysql version is 5 7 Sql is as follows a='"a"'+' "a"'*30000 sql="insert into canal_json_test(j_json) values('[%s]')" % (a) That is, the value of a json field is a. "a" "a"……]' A total of 30001 a can be inserted into mysql normally but canal parsing this error Consistent with the issues mentioned in 330 Sql is as follows a='"a"'+' "a"'*30000 sql="insert into canal_json_test(j_json) values('[%s]')" % (a) That is, the value of a json field is a. "a" "a"……]' A total of 30001 a can be inserted into mysql normally but canal parsing this error I should fix it as soon as possible. I have tried to fix it. It should be the length of the json value. When using getUnit16, it should be using getUnit32.
349,Some questions about parsing the json type in Mysql5 7 case LogEvent.MYSQL_TYPE_JSON: { len = buffer.getUint16(); buffer.forward(meta - 2); int position = buffer.position(); JsonConversion.Json_Value jsonValue = JsonConversion.parse_value(buffer.getUint8() buffer len - 1 charsetName); StringBuilder builder = new StringBuilder(); jsonValue.toJsonString(builder charsetName); value = builder.toString(); buffer.position(position + len); // byte[] binary = new byte[len]; // buffer.fillBytes(binary 0 len); // value = binary; javaType = Types.VARCHAR; length = len; break; } This part is to parse the json type of code which does not understand 1.buffer.forward(meta - 2 Why do you want to go back here? 2.JsonConversion.Json_Value jsonValue = JsonConversion.parse_value(buffer.getUint8() buffer len - 1 charsetName); Why is len 1 here? Hope God answers @agapple
348,canal Re-enable error after server is deactivated for one month canal After the server is deactivated for one month, re-enable the error. The location of the parsed file cannot be found. Can the canal not be parsed from the latest location when the file location cannot be found? Thank you. [destination = xxx address = /1.1.1.1:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:xxx [java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) at java.lang.Thread.run(Thread.java:745) If you specify a site, you can try to delete the site directly from the latest consumption. Or not specifying a location is starting from the latest No specific site, how to delete the site, thank you My problem should be that the binlog cleans up the previous file and then it can&#39;t find the problem of parsing the binlog from the location. Yes Specifying a site will not specify that it should be consumed from the latest. Uh-huh, thank you for modifying the location, can you only modify the data stored in the zookeeper?
347,Mysql uses utf8mb4 to appear Chinese garbled ![image](https://user-images.githubusercontent.com/5357638/28571083-c7fade36-7173-11e7-8cef-2484de5b1b04.png) The specific garbled part is like this because there are some business sensitive fields, so I am sorry for the code. Found the problem is not a problem with canal What is the reason? I also encountered similar problems. I don’t remember clearly, it seems that the version of druid we used is too low or what I think about it.
346,AbstractEventParser sets the parseThread setName when it is empty parseThread.setName(String.format("destination = %s address = %s EventParser" destination runningInfo == null ? null : runningInfo.getAddress().toString())); The toString method should be removed submitted
345,Why can&#39;t get the tableName and schemaName empty? header { version: 1 logfileName: "mysql-bin.000068" logfileOffset: 719945754 serverId: 125 serverenCode: "UTF-8" executeTime: 1500862055000 sourceType: MYSQL schemaName: "" tableName: "" eventLength: 217 eventType: QUERY } entryType: ROWDATA storeValue: "\020\aZ\301\001insert into productinfotemp(SysId Price RealityPrice SkuCount Name Code SaleDate Status) values (\'eab6d30a-2436-4c3b-b493-3c997fd8d8fb\' 10.00 10.00 1.00 \'\' \'E0205004\' \'2017-07-24 00:00:00 \' 99)" The QUERY statement is not a row record of the ROW mode. I also have a lot of such data. Insert update has 2017-09-12 14:09:57 417[INFO]yike.canal.AbstractCanalClientTest.printEntry(AbstractCanalClientTest.java:206) ----------------> binlog[mysql-bin.002059:3086158] name[xmall ] eventType : UPDATE executeTime : 1505196597000 delay : 417ms 2017-09-12 14:09:57 417[INFO]yike.canal.AbstractCanalClientTest.printEntry(AbstractCanalClientTest.java:206) ----------------> binlog[mysql-bin.002059:3086397] name[xmall ] eventType : INSERT executeTime : 1505196597000 delay : 417ms 2017-09-12 14:09:57 606[INFO]yike.canal.AbstractCanalClientTest.printSummary(AbstractCanalClientTest.java:155) The new version of canal has completely extracted the tableName schemaName through the SQL parsing tool. There is no record in the binlog.
344,How can only register the insert event update delete select does not care does not need to send to the client How can only register the insert event update delete select does not care does not need to send to the client ``` if (!(eventType == EventType.DELETE|EventType.SELECT|EventType.UPDATE)) {} ```
343,Bugfix when filterqueryddl If ddl is filtered out of meta Cache will not change The added field name will be confused. [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=343) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=343) before we can accept your contribution.<br/>**1** out of **2** committers have signed the CLA.<br/><br/>:white_check_mark: toruneko<br/>:x: jianhao.dai<br/><hr/>**jianhao.dai** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=343) it.</sub> tks. First merge ps. Will be in the near future Cache to do a relatively large adjustment to support the table structure version of the archive storage to solve the problem of deleting the deleted table
342,How can I synchronize data to the slave library? I am a newcomer, I have a question, I would like to ask you. Environment v1 0 24 mysql：5.6 The problem client has run a function to capture the binary data of the main data but how to restore the binary file to the data from the library I understand that you need to parse the read entry yourself and then convert it to SQL in the library. Canal Does not provide data storage Ready-made is otter db-->db
341,Zookeeper hangs after restarting canal Server synchronization stopped Canal running zookeeper does not know why hangs and then canal The server started to fail to report the error of zookeeper. So stopped canal Server and then restart zookeeper canal Server and canal_client After restarting, I found that canal_client stuck in the getWithoutAck place and never got the message so the synchronization Also stuck the check the main library log has been sent to binlog 000446 and canal_server from zookeeper The obtained log is binlog 000440. It has been worse than 6 logs and has not been synchronized. Later, I found that cannal will stop after each synchronization for a while and then continue to synchronize and then repeat. The reason for the card owner&#39;s exception is to see if it is caused by the deletion of the table structure.
339,the max size of buffer must not more than 1G!!! if buffer = bufferSize * bufferMemUnit = 2G than (bufferSize * bufferMemUnit) = 2097152*1024 > max length of int; cause endless loop in MemoryEventStoreWithBuffer.tryPut & checkFreeSlotAt so that the max size of buffer = 1G!!! why not use long type? bufferSize = 2097152 ? Excessive bufferSize does not help a lot of performance. It is recommended that the bufferSize control has tens of thousands of TPS at 32768.
338,rowChage getRowDatasList data is empty Mysql version 5 6 19 Open binlog ROW Types of Parsing file event types are eventType QUERY no update Insert event type May I ask what is the reason canal.zkServers=192.168.0.14:2181 canal.instance.global.spring.xml Configuration file-instance.xml Memory file has data canal.instance.global.spring.xml Use default instance xml HA No data
337,How to get sql statement in mixed mode How can I get the sql statement in mixed mode?
336,Asynchronous ack when ack thread appears null pointer exception get thread blocking in BooleanMutex with setting ring buffer Size related The client side ack thread has a null pointer exception. The get thread is blocked at BooleanMutex. `"SyncThread5" #27 prio=5 os_prio=0 tid=0x00007fc880f62800 nid=0x549 waiting on condition [0x00007fc854302000] java.lang.Thread.State: WAITING (parking) at sun.misc.Unsafe.park(Native Method) - parking to wait for <0x0000000735ee1df0> (a com.alibaba.otter.canal.common.utils.BooleanMutex$Sync) at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175) at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836) at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997) at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304) at com.alibaba.otter.canal.common.utils.BooleanMutex$Sync.innerGet(BooleanMutex.java:123) at com.alibaba.otter.canal.common.utils.BooleanMutex.get(BooleanMutex.java:53) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.waitClientRunning(SimpleCanalConnector.java:425) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:256) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:252) at com.alibaba.otter.canal.client.impl.ClusterCanalConnector.getWithoutAck(ClusterCanalConnector.java:180) at com.dfire.obd.core.Launcher$SyncThread.run(Launcher.java:105)` Server side appears 2017 07 04 14:28:15.068 ERROR com.alibaba.otter.canal.server.netty.NettyUtils [New I/O server worker #1-4]；ErrotCode:400 Caused by : something goes wrong with channel:[id: 0x2ce9c057 /10.26.0.247:41053 :> /10.26.0.9:11111] exception=com.alibaba.otter.canal.server.exception.CanalServerException: ack error clientId:1001 batchId:14800952 is not exist please check Try a new version of canal
335,a canal Sever on the instance too much affects performance how to do Because the instance may be a lot Can can achieve multiple sets of master and backup? Can spread the instance to multiple servers
334,Optimization for socketchannel implementation using netty instead of JDK 1 SocketChannel is replaced by wait time 2 read and write directly replace the previous ByteBuff to byte with byte 3 Modify the PacketManager method and adjust the related references 4 As for the use of Jboss netty can be excluded, no submission here does not affect [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=334) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=334) before we can accept your contribution.<br/>**1** out of **2** committers have signed the CLA.<br/><br/>:white_check_mark: agapple<br/>:x: luoyaogui<br/><hr/>**luoyaogui** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=334) it.</sub> Tks, I will verify it.
333,fix Bug abnormal network connection leads to canal server Unable to close In reality, the client does not exist but the server-side connection is still alive. Stop in this network abnormal state canal Server will always live Cause Abnormal network connection due to no data transfer io Worker thread has been loop Selector cannot exit Solution 1 Set keepalive By os monitor 2. Set netty idle Mechanism timing detection and active close socket。(fix Time unit bug 3. Newly created socket each time Channel is stored in a container at stop canal Server automatically releases all sockets from the container channel。 [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=333) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=333) before we can accept your contribution.<br/><hr/>**Jason Huang** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=333) it.</sub> tks
332,HA mode kill process can not switch Hi HA mode when I manually kiil off the running process client program can not switch in zookeeper view already service has been switched, what is going on? Thank you The program can switch services when the service is normally closed. DEBUG org.apache.zookeeper.ClientCnxn - Got ping response for sessionid: 0x35ca6b0127d780e after 0ms Always print such a statement This is the DEBUG output ignore
331,update zkclient to 0.10 I didn&#39;t make too many changes and found that the ZkConnection provided in the original package of zkclient has a lot of code overlap with the ZooKeeperx that canal itself. Because it is not too clear about the history of this part of the code, it simply inherits ZkConnection. Or leave it to the canal maintainer to decide to leave the code here. @agapple ## Update: The first commit is still buggy Modified and then raised a second commit There is no problem with the current test. [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=331) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=331) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=331) it.</sub> tks @agapple When is the next release?
330,MySQL5.7 JSON parsing problem canal Version 1 0 24 JSON data written to MySQL failed to resolve 2017-06-22 11:39:25.104 [destination = example address = /127.0.0.1:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: java.lang.IllegalArgumentException: illegal json data at com.taobao.tddl.dbsync.binlog.JsonConversion.parse_array_or_object(JsonConversion.java:81) at com.taobao.tddl.dbsync.binlog.JsonConversion.parse_value(JsonConversion.java:61) at com.taobao.tddl.dbsync.binlog.event.RowsLogBuffer.fetchValue(RowsLogBuffer.java:955) at com.taobao.tddl.dbsync.binlog.event.RowsLogBuffer.nextValue(RowsLogBuffer.java:99) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseOneRow(LogEventConvert.java:495) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:376) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:108) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:62) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:326) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:176) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:130) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) at java.lang.Thread.run(Thread.java:745) In addition, if json contains double quotes canal parsing, the escape symbol is lost. ![image](https://user-images.githubusercontent.com/5357638/27416733-e8f73ba8-5741-11e7-96c0-2a0de5ddc8af.png) Provide some test SQL The first question of the two questions has not found the problem SQL The second one is canal has been parsed and canal Client can get the data +-------+-------------+------+-----+-------------------+-------+ | Field | Type | Null | Key | Default | Extra | +-------+-------------+------+-----+-------------------+-------+ | id | int(11) | NO | PRI | NULL | | | dt | timestamp | NO | | CURRENT_TIMESTAMP | | | json1 | json | YES | | NULL | | | aa | varchar(50) | YES | | NULL | | +-------+-------------+------+-----+-------------------+-------+ `insert into t1(id json1 aa) values(3 "{\"test\":\"aa\\\"aa\"}" "a");` Check back is like this +----+---------------------+--------------------+------+ | id | dt | json1 | aa | +----+---------------------+--------------------+------+ | 3 | 2017-06-22 14:34:19 | {"test": "aa\\\"aa"} | a | +----+---------------------+--------------------+------+ canal Client fetched json Become a test aa aa Found leading to illegal json data SQL Json is subsidized by about 25,000 bytes. Will it be because json is too large, causing byte and len length calculation errors Found leading to illegal json data SQL Json is subsidized by about 25,000 bytes. Will it be because json is too large, causing byte and len length calculation errors ---------------------------------------------------------------- @toruneko How do you solve this problem? @lan1994 Finally, we did not use json to change the program. Encounter the same problem I should fix it as soon as possible. @agapple Technical experts, have a question, please ask Recently, our project transformation database postgresql switched to mysql used json type field mysql version upgrade to 5 7 to support want to ask canal upgrade to v1 1 0 whether to support json type field Canal is forward compatible with support for json parsing @agapple I didn’t expect to reply so soon, thank you big coffee.
329,Can the can skip this error in the case of synchronous error reporting? According to the test canal in the synchronous sql error will always repeat the execution of this sql causes the position to stop. If there is a binlog sql due to a hand error, such as the table does not exist, this will cause the canal to stagnate. I have to skip the synchronization error. Method? At present, only the site can be reset. Support for adding DDL change capability can be considered.
328,Can Canal support MySQL5 7? Canal support for MySQL5 7? Thank you There may be partial defects in addition to json object support. The current trunk has fixed the parsing problem of json object too large
327,Canal server will inexplicably hang up the kind of log without any log My canal server is configured with four destinations. After the inexplicable hangs, all the canal clients start to brush like this. 2017-06-17 00:21:19.110 [Thread-1] ERROR com.alibaba.otter.canal.example.AbstractCanalClientTest - process error! com.alibaba.otter.canal.protocol.exception.CanalClientException: failed to subscribe with reason: something goes wrong with channel:[id: 0x1e023246 /**.**.**.**:39396 => /**.**.**.**:1 1111] exception=com.alibaba.otter.canal.server.exception.CanalServerException: destination:*** should start first Then the load on the machine is extremely high. Does the god have encountered similar problems?
326,If I want to synchronize the data of several databases under different addresses, I want a canal server and multiple canal clients. canal server It doesn&#39;t matter, canalclient One-on-one with destination otter+canal LS Correct Answer
325,Can the canal server directly access the MQ to send the message directly without passing through the canal client I hope that by directly modifying the canal server to directly access the company&#39;s message queue binlog directly through the message load to the cluster node canal only need the server side, how should this be done? [maxwell](https://github.com/zendesk/maxwell) Debezium https github com debezium debezium can support full increments
324,Upgrade zkclient to 0 10 @agapple I hope to upgrade zkclient to 0 10 # advantage 1. In the 0 10 version Switch from Log4j to Slf4j, this is convenient for us to debug Log open debugging 1. Some bugs fix # Disadvantage 1. 0.10 Add some methods to some interfaces. Canal should make some adjustments. If it is convenient to submit a PR for me #142 ， If you don&#39;t upgrade, you will still encounter problems with 142. #331 ， Submitted a PR, please review it. @agapple
323,Connection canal is too slow in ClusterCanalConnector zookeeper mode Prerequisite cluster construction has no problem. Java directly connects to zk speed and also uses Simple mode very quickly. CanalConnector connector = CanalConnectors.newClusterConnector("200.200.200.64:2181" "es" "" ""); int batchSize = 1000; int emptyCount = 0; try { System.out.println("start connect"); connector.connect(); System.out.println("connect successed!"); connector.subscribe("\\..*"); System.out.println("subscribeed !!"); ... 在connector subscribe  Very slow sometimes needs to be restarted twice to connect properly Debug view is public void subscribe(String filter) throws CanalClientException { this.waitClientRunning(); Is there any way to solve the problem? You are based on zookeeper&#39;s cluster mode. When you start the last abnormal exit, try to use smooth exit without killing. 9 correctly call disconnect
322,How to start the canal server in the docker container How to start the canal server in the docker container for guidance in a convenient and simple way
321,Specified site startup error Version 1 0 24 problem Hello canal itself supports the specified site to start parsing, but will report TableIdNotFoundException when parsing. I see that the source has handled this exception but the exception thrown is a CanalParseException that caused the capture to be unsuccessful. Such as a screenshot ![rtx](https://user-images.githubusercontent.com/29394152/27125044-9910f768-5125-11e7-9971-9309d016f512.jpg) The type throwing exception is CanalParseException Its cause is TableIdNotFoundException
320,About analytical sites and consumer sites Canal is so good to thank you for your contribution, but I found a problem I don&#39;t know if it is really a problem. Scenes canal Both the server and the client are running. At this point, the database changes a lot of data and assumes 100,000. Now suppose that the parser has parsed the location in the latest position of the binlog but the client consumes very slowly and only consumes 10,000 data. At this time, if the server hangs and then restarts, read the resolution location on the zk. The latest location of the binlog is now on the server. The binlog data is only the latest and the client has not consumed it. At this time, if the client obtains the data again, it will not get the 90,000 unconsumed data. https://github.com/alibaba/canal/blob/master/parse/src/main/java/com/alibaba/otter/canal/parse/inbound/mysql/MysqlEventParser.java#L314 https://github.com/alibaba/canal/blob/master/parse/src/main/java/com/alibaba/otter/canal/parse/inbound/mysql/MysqlEventParser.java#L330 @luyee Hello, are you posting this, not taking the resolution site? @xzhhh After the canal restart, the binlog location information is stored in the conf instance meta dat file from the last reading site. `{"clientDatas":[{"clientIdentity":{"clientId":1001 "destination":"example_db" "filter":""} "cursor":{"identity":{"slaveId":-1 "sourceAddress":{"address":"127.0.0.1" "port":3306}} "postion":{"included":false "journalName":"mysql-bin.000032" "position":18169170 "serverId":1 "timestamp":1497426548000}}}] "destination":"trc_account_db"}` Where binlog location field For position However, there may be such a problem that the server does not update the binlog location for a long time, such as downtime or long-term consumption of the mysql server. For some reason, the previous binlog file is cleared, which will cause the read exception to be relocated. The binlog location needs to be relocated from the specified location. @mfkugergvh I know that the canal restart will start reading from the previous site. But the problem is that if the resolved site is behind the consumption site, the canal is hung up and the unconsumed data is not already in the canal because the current eventStore. Only the unexpended events that existed in memory after the restart based on the memory version are gone. Every time you get a binlog from mysql, you don&#39;t have to start from the specified starting position. So what I posted is that the parsing start site code is probably every time canal server Start will go to get the positon of the last client consumption ack. If there is going to mysql to get the binlog So even canal server Got 10w Record canal client Only consumed 1000 after canal Server hangs If ack successfully updated the 1000 records, the next time you start spending from 1000. If you don&#39;t have ack success, you should start from the beginning and you will have the same repetition. As you said, the consumption is not less unless the journalname postion timestamp Not configured is from mysql binlog(show master Status Recently started spending LS understands correctly
319,fixbug: soTimeout doesn't work. description When the network is not good or other circumstances, the server closes the connection but the client does not receive the Fin packet connection will be in a semi-connected state for a long time. the reason SocketChanal Set timeout is not effective, use socket to read, see http bugs java com bugdatabase view_bug do bug_id 4614802 [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=319) <br/>All committers have signed the CLA. Your soTimeout parameters are not seen to be set at the client level. @agapple Set but no effect needs to use socket processing Do you confirm that all are submitted? I feel that only the client code is changed between the server and mysql soTimeout socket replacement does not seem to be submitted? tks Hi Thanks for the commit. I have met the same issue but it happens between canal server instance and mysql replication master. Any plan to fix it? Thanks mailzyok
318,canal Example error something goes wrong when getWithoutAck data from server:null canal example Started two Equivalent to master and slave When one is hung, it will automatically switch to another one. The following exception occurred during operation 2017-06-08 02:39:42.964 [Thread-3] WARN c.alibaba.otter.canal.client.impl.ClusterCanalConnector - something goes wrong when getWithoutAck data from server:null com.alibaba.otter.canal.protocol.exception.CanalClientException: java.nio.channels.AsynchronousCloseException at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:281) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:252) at com.alibaba.otter.canal.client.impl.ClusterCanalConnector.getWithoutAck(ClusterCanalConnector.java:180) at com.alibaba.otter.canal.example.AbstractCanalClientTest.process(AbstractCanalClientTest.java:131) at com.alibaba.otter.canal.example.AbstractCanalClientTest$2.run(AbstractCanalClientTest.java:98) at java.lang.Thread.run(Thread.java:748) Caused by: java.nio.channels.AsynchronousCloseException at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:205) at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:407) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.read(SimpleCanalConnector.java:376) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.readNextPacket(SimpleCanalConnector.java:366) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.receiveMessages(SimpleCanalConnector.java:286) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:279) ... 5 more 2017-06-08 02:39:43.088 [ZkClient-EventThread-17-10.1.102.181:3001] ERROR c.a.otter.canal.client.impl.running.ClientRunningMonitor - There is an error when execute initRunning method with destination [example]. com.alibaba.otter.canal.protocol.exception.CanalClientException: java.nio.channels.ClosedChannelException at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.doConnect(SimpleCanalConnector.java:171) ~[canal.client-1.0.24.jar:na] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.access$000(SimpleCanalConnector.java:48) ~[canal.client-1.0.24.jar:na] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector$1.processActiveEnter(SimpleCanalConnector.java:396) ~[canal.client-1.0.24.jar:na] at com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor.processActiveEnter(ClientRunningMonitor.java:207) ~[canal.client-1.0.24.jar:na] at com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor.initRunning(ClientRunningMonitor.java:118) ~[canal.client-1.0.24.jar:na] at com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor$1.handleDataDeleted(ClientRunningMonitor.java:71) [canal.client-1.0.24.jar:na] at org.I0Itec.zkclient.ZkClient$6.run(ZkClient.java:549) [zkclient-0.1.jar:na] at org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71) [zkclient-0.1.jar:na] Caused by: java.nio.channels.ClosedChannelException: null at sun.nio.ch.SocketChannelImpl.ensureReadOpen(SocketChannelImpl.java:257) ~[na:1.8.0_131] at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:300) ~[na:1.8.0_131] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.read(SimpleCanalConnector.java:376) ~[canal.client-1.0.24.jar:na] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.readNextPacket(SimpleCanalConnector.java:369) ~[canal.client-1.0.24.jar:na] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.doConnect(SimpleCanalConnector.java:133) ~[canal.client-1.0.24.jar:na] ... 7 common frames omitted 2017-06-08 02:39:43.114 [ZkClient-EventThread-17-10.1.102.181:3001] WARN c.a.otter.canal.client.impl.running.ClientRunningMonitor - canal is not run any in node 2017-06-08 02:39:48.103 [Thread-3] INFO c.alibaba.otter.canal.client.impl.ClusterCanalConnector - restart the connector for next round retry. 2017-06-08 11:34:56.529 [main-SendThread(10.1.102.181:3001)] INFO org.apache.zookeeper.ClientCnxn - Unable to read additional data from server sessionid 0x15c83dad6dc0000 likely server has closed socket closing socket connection and attempting reconnect 2017-06-08 11:34:59.029 [main-SendThread(10.1.102.181:3001)] INFO org.apache.zookeeper.ClientCnxn - Opening socket connection to server 10.1.102.181/10.1.102.181:3001. Will not attempt to authenticate using SASL (unknown error) 2017-06-08 11:34:59.310 [main-SendThread(10.1.102.181:3001)] INFO org.apache.zookeeper.ClientCnxn - Socket connection established to 10.1.102.181/10.1.102.181:3001 initiating session 2017-06-08 11:35:00.488 [main-SendThread(10.1.102.181:3001)] WARN org.apache.zookeeper.ClientCnxn - Session 0x15c83dad6dc0000 for server 10.1.102.181/10.1.102.181:3001 unexpected error closing socket connection and attempting reconnect java.io.IOException: Broken pipe at sun.nio.ch.FileDispatcherImpl.write0(Native Method) ~[na:1.8.0_131] at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47) ~[na:1.8.0_131] at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93) ~[na:1.8.0_131] at sun.nio.ch.IOUtil.write(IOUtil.java:65) ~[na:1.8.0_131] at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:471) ~[na:1.8.0_131] at org.apache.zookeeper.ClientCnxnSocketNIO.doIO(ClientCnxnSocketNIO.java:117) ~[zookeeper-3.4.5.jar:3.4.5-1392090] at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:355) ~[zookeeper-3.4.5.jar:3.4.5-1392090] at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1068) ~[zookeeper-3.4.5.jar:3.4.5-1392090] This I also found in 1 0 24 the canal during the execution process Server stopped and then canal Client has always had this problem putting canal Server restart boot canal The client still reports there and will not choose the new server to reconnect the specific problem. I am still debugging. @agapple I debugged it because of my own usage, I created the cluster I created. The client is placed in a pool, pay attention to the following code ``` CanalConnector connector = CanalConnectors.newClusterConnector(connectionPath destination "" ""); connector.connect(); connector.subscribe(); ``` This will throw a CanalClientException This is a runtime exception So when connect throws an exception, remember to capture it and then disconnect it. ``` try { connector.connect(); connector.subscribe(); } catch (Throwable t) { logger.error("failed to connect to canal server" t); connector.disconnect(); throw t; } ``` @xuliangyong ， I don&#39;t know if your question is the same as me. I hope to help you. @alexandnpu I have also encountered this situation and found a lot of things in the CLOSE_WAIT state when checking the TCP connection. Finally, it is solved by catch exception and then disconnect.
317,What to modify when moving a configuration file to another directory Hello, may I ask if I want to move the configuration file in the conf directory to other directories, what needs to be modified? Thank you. Focus on instance properties
316,MySQL5 6 10 connection failed Exception in thread "main" com.alibaba.otter.canal.protocol.exception.CanalClientException: something goes wrong with reason: something goes wrong with channel:[id: 0x6ac75c6a /127.0.0.1:51803 => /127.0.0.1:11111] exception=java.lang.NullPointerException at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.calculateSize(MemoryEventStoreWithBuffer.java:535) at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.doGet(MemoryEventStoreWithBuffer.java:312) at com.alibaba.otter.canal.store.memory.MemoryEventStoreWithBuffer.tryGet(MemoryEventStoreWithBuffer.java:251) at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getEvents(CanalServerWithEmbedded.java:460) at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:296) at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.getWithoutAck(CanalServerWithEmbedded.java:259) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:123) at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:100) at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:783) at org.jboss.netty.channel.SimpleChannelHandler.messageReceived(SimpleChannelHandler.java:154) at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:100) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:783) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:275) at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80) at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:783) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndFireMessageReceived(ReplayingDecoder.java:525) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:506) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:443) at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80) at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261) at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:349) at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:280) at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:200) at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:44) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:748) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.receiveMessages(SimpleCanalConnector.java:302) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:279) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:252) The specific version is
315,canal Can the client do cluster load balancing? Hello, please ask Can the client do the cluster load balancing method? Is there an example to explain it briefly? Thank you. ClientRunningMonitor / ServerRunningMonitors maybe useful You said it should be HA way, but I want to implement, for example, when a node is overloaded and then it will be automatically assigned to another node to run this. Thank you. Does not support LB is actually standby mode see Questions about canal high availability https github com alibaba canal issues 147 Thank you
314,ClientRunningMonitor ConnectException Running on the CentOS7 2 server, I found that a ConnectException was thrown. I was prompted to connect the exception. What is wrong with this part? ``` [2017-06-02 17:01:03.295] [ERROR] [ZkClient-EventThread-20-10.10.10.81:2181 10.10.10.82:2181 10.10.10.33:2181] [c.a.o.c.client.impl.running.ClientRunningMonitor] >>> There is an error when execute initRunning method with destination [db10101067002]. com.alibaba.otter.canal.protocol.exception.CanalClientException: java.net.ConnectException: Connection timed out at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.doConnect(SimpleCanalConnector.java:171) ~[canal.client-1.0.23.jar:na] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.access$000(SimpleCanalConnector.java:48) ~[canal.client-1.0.23.jar:na] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector$1.processActiveEnter(SimpleCanalConnector.java:396) ~[canal.client-1.0.23.jar:na] at com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor.processActiveEnter(ClientRunningMonitor.java:207) ~[canal.client-1.0.23.jar:na] at com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor.initRunning(ClientRunningMonitor.java:118) ~[canal.client-1.0.23.jar:na] ``` Self-checking environmental issues
313,Can I connect to the instance list? canal.destinations=example，example2，...... The server can configure multiple instance lists And only one single instance is supported when connecting, why not support multiple String destination=example; connector = CanalConnectors.newClusterConnector(addresses destination username password); connector = CanalConnectors.newSingleConnector(new InetSocketAddress(ip port) destination username password); Why can&#39;t the destination parameter support multiples like the configuration parameters? destination=example，example2，...... It seems that the document says it. A client only supports a single instance @liyong2008 In this case Assume 20 instances Need to deploy at least 20 Client Group instance xml introduction Mainly for the need to merge multiple physical entities into a logical instance for multi-library merge to provide client access Scene sub-library business For example, the product data is split into 4 libraries. Each library will have an instance. If you do not need to consume data on the group business, you need to start 4 clients to link 4 instance instances. After using group, you can use it in canal. Merging into a logical instance on the server, you only need to start a logical link of 1 client link.
312,Can Canal support MySQL5 7? Canal version 1 0 24 MySQL version 5 7 18 Not supported for the above version Currently known for mysql 5 7json type has some bugs on the escape character. The version on my side is ` mysql Ver 14.14 Distrib 5.7.17 for osx10.12 (x86_64) using EditLine wrapper ` Seen from the log inside `subscribe successfully ClientIdentity[destination=task-4 clientId=1001 filter=] with first position:null` But the update of the mysql data table canal can not receive data @alexandnpu Check if the Canal configuration is correct. Check if MySQL is open binlong. If there are no problems and I can&#39;t monitor the changes in the data table, I have encountered such a situation to download a clean canal version and then configure the MySQL address port and destination. Do not use example. @kervin521 Thank you for your reply My current problem is to test on the mac to start the vagrant used as a test environment. I am worried that the can use the virtual machine ip when logging to mysql to do slave registration and mysql send binglog to this ip is not found. @alexandnpu You don&#39;t have to worry about this because the canal service port is certain even if you borrow virtual machine IP. The same problem I use Canal version 1 0 24 MySQL version 5 7 16 Does Canal support? @shijiebei2009 If you can&#39;t configure your own destination, don&#39;t use the sample and delete the log and data records in the Canal installation directory.
311,How can can only listen to a database? 1 If there are multiple databases in mysql, only listen to the test database how to configure 2 If you can, how to configure a table under this library 3.canal.instance.defaultDatabaseName = how to use Set in the configuration file There are 2 configuration options in conf example instance properties canal.instance.filter.regex = .*\\..* This is a white list For example, test only listens to the test database. Test test monitors the test table in the test library Multiple words separated by commas But note that this configuration is invalid. Must be configured in the client subscribe method Because it will cover canal.instance.filter.black.regex = This is a blacklist Exclude library table Same as above This configuration is valid Specific configuration examples refer to the filter section of the source directory canal.instance.defaultDatabaseName = This configuration item is also invalid. No match, no relationship
310,Some binlogs are not synchronized when canal synchronization I am using the cluster mode to synchronize the data of a database with the updated binlog comrades syncing but the data of my own modified database is not synchronized. I don&#39;t know the situation. Pay attention to whether your operation binlog is generated. Well, it’s my point of synchronization. I’m always thinking that I’m not syncing. Thank you very much.
309,ERROR ## parse this event has an error last position : [mysql-bin.000001 1199597] java.lang.NullPointerException: null ``` 2017-05-27 16:53:34.167 [destination = example address = /127.0.0.1:3306 EventParser] INFO c.a.otter.canal.parse.inbound.mysql.MysqlConnection - COM_BINLOG_DUMP with position:BinlogDumpCommandPacket[binlogPosition=1199597 slaveServerId=1234 binlogFileName=mysql-bin.000001 command=18] 2017-05-27 16:53:34.168 [destination = example address = /127.0.0.1:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - ERROR ## parse this event has an error last position : [mysql-bin.000001 1199597] java.lang.NullPointerException: null at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:178) ~[canal.parse-1.0.25-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:130) [canal.parse-1.0.25-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210) [canal.parse-1.0.25-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_77] 2017-05-27 16:53:34.168 [destination = example address = /127.0.0.1:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /127.0.0.1:3306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: java.lang.NullPointerException Caused by: java.lang.NullPointerException: null at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:178) ~[canal.parse-1.0.25-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:130) ~[canal.parse-1.0.25-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210) ~[canal.parse-1.0.25-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_77] 2017-05-27 16:53:34.169 [destination = example address = /127.0.0.1:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: java.lang.NullPointerException Caused by: java.lang.NullPointerException at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:178) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:130) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210) at java.lang.Thread.run(Thread.java:745) ] 2017-05-27 16:53:34.169 [destination = example address = /127.0.0.1:3306 EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - disConnect MysqlConnection to /127.0.0.1:3306... 2017-05-27 16:53:34.169 [destination = example address = /127.0.0.1:3306 EventParser] INFO c.alibaba.otter.canal.pars ``` show binlog events; ``` | mysql-bin.000001 | 1199597 | Xid | 1 | 1199628 | COMMIT /* xid=5909 */ ```
308,How can there be no canal properties in the configuration file? How can there be no canal properties in the configuration file?
307,The ddl statement sql obtained in the client example is garbled Looked at the bottom of the implementation is to first turn into utf 8 encoding but it is still garbled That is your mysql client The code is not caused by utf8 Is this your problem solved? What is the reason? I have encountered similar problems now. https://github.com/alibaba/canal/issues/610
306,How to parse the date type How to parse the date type client Are strings, you can only convert java by yourself. type？ Yes, the time type range of mysql will be better than java The Date object is defined to be large, so the generic type of string is used. Canal Available JDBC Many types of general conversion functions? no Jdbc Can look at the otter project com.alibaba.otter.node.etl.common.db.utils.SqlUtils.java
305,Canal mysql5 5 35 error can t find start position pid:1 nid:2 exception:canal:CTSDB-CANAL-master:com.alibaba.otter.canal.parse.exception.CanalParseException: can't find start position for CTSDB-CANAL-master The target database is mysql community server 5 7 18 1 The database of the official business environment is mysql 5 5 35 Before the test environment source and purpose are mysql community server 5 7 18 1 when successfully changed to the official business environment mysql5 5 error canal.deployer-1.0.24 node.deployer-4.2.14 manager.deployer-4.2.14 zookeeper-3.4.10 aria2-1.19.0 +1 can't find start position for CTSDB CANAL master clears the site or sets an existing site
304,Canal destinations support two CanalConnectors newClusterConnector destinations only support one is it? RT CanalConnectors.newClusterConnector Two calls are supported twice.
303,I haven&#39;t been pulling the binlog for a while until I get an error. 18 23 55 605 received the last binlog request The next log is 2017 05 18 20 35 27 539 log Two questions 1 Why is the binlog in the middle of two hours not pulled? 2、2017-05-18 20:35:27.539 The error is not like the binlog file can not find this error, what experience can you share? Below is the log of last night 2017-05-18 18:23:55.605 [New I/O server worker #1-6] INFO c.a.otter.canal.server.embedded.CanalServerWithEmbedded - ack successfully clientId:1001 batchId:7193361 position:PositionRange[start=LogPosition[identity=LogIdentity[sourceAddress=x/y:3306 slaveId=-1] postion=EntryPosition[included=false journalName=mysql-bin.006011 position=409700943 serverId=262651658 timestamp=1495103035000]] ack=LogPosition[identity=LogIdentity[sourceAddress=x:3306 slaveId=-1] postion=EntryPosition[included=false journalName=mysql-bin.006011 position=409702008 serverId=262651658 timestamp=1495103035000]] end=LogPosition[identity=LogIdentity[sourceAddress=x/10.157.81.57:3306 slaveId=-1] postion=EntryPosition[included=false journalName=mysql-bin.006011 position=409702008 serverId=262651658 timestamp=1495103035000]]] 2017-05-18 20:35:27.539 [destination = db50-59 address = x/y:3306 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Connection timed out at sun.nio.ch.FileDispatcherImpl.read0(Native Method) ~[na:1.7.0_45] at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39) ~[na:1.7.0_45] at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223) ~[na:1.7.0_45] at sun.nio.ch.IOUtil.read(IOUtil.java:197) ~[na:1.7.0_45] at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379) ~[na:1.7.0_45] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:154) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) [canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210) [canal.parse-1.0.22.jar:na] at java.lang.Thread.run(Thread.java:744) [na:1.7.0_45] 2017-05-18 20:35:27.599 [destination = db50-59 address = x/y:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address x/y:3306 has an error retrying. caused by java.io.IOException: Connection timed out at sun.nio.ch.FileDispatcherImpl.read0(Native Method) ~[na:1.7.0_45] at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39) ~[na:1.7.0_45] at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223) ~[na:1.7.0_45] at sun.nio.ch.IOUtil.read(IOUtil.java:197) ~[na:1.7.0_45] at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379) ~[na:1.7.0_45] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:154) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210) ~[canal.parse-1.0.22.jar:na] 2017-05-18 20:35:27.599 [destination = db50-59 address = mangoerpdb1.mysql.rds.aliyuncs.com/10.157.81.57:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address mangoerpdb1.mysql.rds.aliyuncs.com/10.157.81.57:3306 has an error retrying. caused by java.io.IOException: Connection timed out Will it be that the database connection is abnormal during this time? No log in the middle does not mean that there is no binlog.
302,How to parse data from mysql&#39;s slave library There are two mysql ones in the hand as the master one as the slave. I want to connect the canal to the slave. However, in order to synchronize with the master machine, the slave machine generates the log named by the hostname relay bin. How can the canal be docked with the slave machine? In the document, only the way to naming the mysql bin can be adjusted by itself. Look at the following paragraph to help you? Slave configuration is similar to master. You also need to restart slave MySQL as follows: log_bin = mysql-bin server_id = 2 relay_log = mysql-relay-bin log_slave_updates = 1 read_only = 1 Server_id is required and the only slave does not need to open the binary log but in some cases must be set, for example, if the slave is the master of the other slaves, you must set the bin_log. Here we turn on the binary log and display the named default name as hostname but if the hostname changes There will be problems Relay_log configuration relay log log_slave_updates indicates that the slave will write the replication event into its own binary log and will see its usefulness. Some people open the slave&#39;s binary log but don&#39;t set log_slave_updates and then check if the slave&#39;s data is changed. This is a wrong configuration. So try to use read_only. It prevents changing data except for special threads but read_only is very useful especially those that need to be in slave Create an application on the table canal Is the master of MySQL simulation You are from slave Medium synchronization ，slave Relative to canal Is the master Parsing binary files in slave Thank you for your attention. How is it done?
301,Unable to use stop sh ``` Distributor ID: Ubuntu Description: Ubuntu 16.04.2 LTS Release: 16.04 Codename: xenial ``` ``` stop.sh: 47: [: 2211: unexpected operator -e xiaojie-VirtualBox: stopping canal 2211 ... stop.sh: 58: [: unexpected operator stop.sh: 63: stop.sh: let: not found stop.sh: 58: [: unexpected operator stop.sh: 63: stop.sh: let: not found stop.sh: 58: [: unexpected operator stop.sh: 63: stop.sh: let: not found ``` Is this my problem reference https://github.com/alibaba/canal/issues/228 Should be similar
299,Log exception dataType DB_BATCH solution Log error message pid:5 nid:1 exception:setl:load miss data with keys:[MemoryPipeKey[identity=Identity[channelId=2 pipelineId=5 processId=702747] time=1493745579135 dataType=DB_BATCH]] pid:5 nid:1 exception canal canal_题库 java io IOException  Connection reset by peer at sun.nio.ch.FileDispatcherImpl.read0(Native Method) at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39) at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223) at sun.nio.ch.IOUtil.read(IOUtil.java:197) at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:154) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:121) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) at java.lang.Thread.run(Thread.java:744) Connection reset by The peer network is disconnected by mysql, just try again.
298,The startup sh script has an error When executing the startup command in the Linux environment, the following error can be speculated that the startup sh script has an error. [root@localhost bin]# sh startup.sh : command not found 'tartup.sh: line 4: syntax error near unexpected token `in 'tartup.sh: line 4: `case "`uname`" in [root@localhost bin]# Everyone&#39;s environment is different. Try to adjust the shell script to submit a PR to me. :set ff=unix
297,Canalserver will get stuck after network jitter occurs and cannot be automatically restored. Problem Description When the network jitter occurs, the canalserver will be stuck in the network and resume normal. The canalserver cannot be automatically restored. You need to manually restart the instance to recover. The heartbeat check is the abnormal stack that is turned on as follows 2017-04-21 00:29:16.425 [destination = xxxx address = xxxx HeartBeatTimeTask] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - connect failed by java.io.IOException: Connection reset by peer at sun.nio.ch.FileDispatcherImpl.read0(Native Method) at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39) at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223) at sun.nio.ch.IOUtil.read(IOUtil.java:197) at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379) at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.readBytesAsBuffer(PacketManager.java:20) at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.readHeader(PacketManager.java:13) at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.readNextPacket(MysqlQueryExecutor.java:104) at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.query(MysqlQueryExecutor.java:55) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:68) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser$MysqlDetectingTimeTask.run(MysqlEventParser.java:191) at java.util.TimerThread.mainLoop(Timer.java:555) at java.util.TimerThread.run(Timer.java:505) There is no other exception information other than this. @agapple Either mysql The tcp link of binlogdump was abnormally disconnected and the canal was not perceived. There is an exception in the code that should be caught and the exception will be reconnected but I have not retryed here. You said that the canal is not perceived to mean that the exception is not captured. It should be that the socket has been blocked for no abnormality on the read operation. @lan1994 This situation has also been encountered today and the sotimeout default is 30 seconds. When the network is unstable, the consumption of binlog is slowed down. @agapple I don&#39;t know what is the 30 seconds here. My stack doesn&#39;t seem to be the same canal 1.0.24 ``` 2017-06-27 19:50:47.285 [destination = xxxxx address = xxxxx EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Connection reset by peer at sun.nio.ch.FileDispatcherImpl.read0(Native Method) ~[na:1.8.0_131] at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39) ~[na:1.8.0_131] at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223) ~[na:1.8.0_131] at sun.nio.ch.IOUtil.read(IOUtil.java:197) ~[na:1.8.0_131] at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380) ~[na:1.8.0_131] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:154) ~[canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70) ~[canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122) [canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) [canal.parse-1.0.24.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131] ``` I have encountered this sotimeout on this stack of yours. 30s is useless because this setting has no effect on nio. This problem should be caused by a network failure. The parsing thread has been in the read without perceiving the fault We may need a similar sotimeout Timeout reconnection function @alexandnpu The current master version has been provided to follow @agapple ， Is there a related PR? An MR submitted by someone else is mainly based on netty transformation. Added timeout reading. You can pay attention to the SocketChannel in the code. Good looking back, just think you can provide a pr number or a commit Id will be easier https://github.com/alibaba/canal/pull/334 Thanks
296,In the case of a single server, the ClusterCanalConnector disconnects from the server and the NullPointException causes the restart to fail to execute. Take the getWithoutAck method as an example ```java while (times < retryTimes) { try { Message msg = currentConnector.getWithoutAck(batchSize); return msg; } catch (Throwable t) { logger.warn("something goes wrong when getWithoutAck data from server:{}\n{}" currentConnector.getAddress() ExceptionUtils.getFullStackTrace(t)); times++; restart(); logger.info("restart the connector for next round retry."); } } ``` When the connection to the server is disconnected from the network or the server is stopped, the currentConnector getWithoutAck throws an exception into the restart method. The currentConnector is set to null when the connect method still fails to connect. The Exception is thrown after the number of retries. At this point, try again to call the getWithoutAck method. When the currentConnector is null currentConnector getWithoutAck throws a NullPotionException and is caught by catch The logger warn uses the currentConnector when printing the log and throws a NullPotionException, which causes the restart method to not be executed. After the server is restored, the client still cannot connect to the server for the same reason. 1.0.26-SNAPSHOT The version of getWithoutAck does not modify this question. What is the consideration? Estimated missing
295,The canal client or server ran for a while based on zookeeper Direct restart invalid must delete the canal related nodes in the zookeeper and restart the canal server and client to run normally. I also opened the HA mode but only run a canal server for testing. Look at the JVM memory and GC situation
294,Exception in thread "main" java.lang.UnsupportedOperationException: This is supposed to be overridden by subclasses. Run ClientSample prompt This is supposed to be overridden by subclasses. surroundings canal Server in linux machine man monitor remote linux machine mysql binlog Using ClientSample to report an error on win7 machine Protobuf version conflict
293,fix bug of repeat log manager [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=293) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=293) before we can accept your contribution.<br/><hr/>**yinxiu** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=293) it.</sub>
292,How to start canal in docker server How to start canal in docker server The problem is solved
291,Refactor LogPositionManager and adapt XML configuration The main content of the modification is to use the combination method instead of the original MixedLogPositionManager to inherit the MemoryLogPositionManager. The core logic remains unchanged. The modification is just to use the combination instead of inheritance to make the logic clearer. [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=291) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=291) before we can accept your contribution.<br/><hr/>**yinxiu** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=291) it.</sub>
290,Production configuration consultation The online environment is used in canal embedded mode. Now I want to configure CanalLogPositionManager as FailbackLogPositionManager mode. CanalMetaManager is configured as PeriodMixedMetaManager. Is there a recurring consumption problem? For example, the PeriodMixedMetaManager client has not updated the cursor after the consumption of a certain data, such as the ZooKeeper task. The client will follow the zookeeper. Recurring consumption in the cursor In order to pursue consistency, it is more appropriate to configure CanalMetaManager as ZooKeeperMetaManager. Repeated consumption of this piece can not avoid the repetition of the site. On the one hand, there is a batch of data processing. If the process is hung up, the entire batch will be processed next time. Such a transformation does not have much meaning.
289,something goes wrong with channel ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x68949ed6 /10.253.0.5:59418 => /10.124.134.173:11111] exception=java.io.IOException: Connection reset by peer Network reset client to increase the reconnection mechanism This is the server&#39;s error exception. The client has no exceptions. This happens when the server opens the HA default instance xml. I am here, multiple clients connected to the server reported that the wrong monitoring will have this problem only one is just fine. Thank you, the big cows have solved the problem. It is the problem of zookeeper. It’s a good idea. @agapple I also have this situation how the client adds reconnection mechanism. Take a look at the client code for example
288,With guava 20 compatible issues with guava 18 MapMaker makeComputingMap method is obsolete Exception in thread "main" java.lang.NoSuchMethodError: com.google.common.collect.MapMaker.makeComputingMap(Lcom/google/common/base/Function;)Ljava/util/concurrent/ConcurrentMap; at com.google.common.collect.MigrateMap.makeComputingMap(MigrateMap.java:17) at com.alibaba.otter.canal.common.zookeeper.ZkClientx.<clinit>(ZkClientx.java:26) at com.alibaba.otter.canal.client.CanalConnectors.newClusterConnector(CanalConnectors.java:66) Consider submitting a PR to me to put guava 20 is currently not used internally. I implemented the CanalNodeAccessStrategy solution. @Luckywb How to achieve @Luckywb Can you post the principle of implementation and encounter the same problem?
287,Json type field bug There is a bug under 1 0 23 com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: java.lang.IllegalArgumentException: illegal json data at com.taobao.tddl.dbsync.binlog.JsonConversion.parse_scalar(JsonConversion.java:150) at com.taobao.tddl.dbsync.binlog.JsonConversion.parse_value(JsonConversion.java:67) Database version mysql-5.7.11-winx64 Database structure CREATE TABLE customer_data ( id bigint(19) unsigned NOT NULL AUTO_INCREMENT supplier_id bigint(19) unsigned NOT NULL COMMENT Business id customer_id bigint(19) unsigned NOT NULL customer_data json DEFAULT NULL COMMENT Customer standard attribute customer_ext_data json DEFAULT NULL COMMENT Customer extended attribute PRIMARY KEY (id) UNIQUE KEY uk_cust (customer_id) ) ENGINE=InnoDB AUTO_INCREMENT=26036 DEFAULT CHARSET=utf8mb4; After debugging, it is 149 lines str_len 0 @agapple Give the json string you have the problem gender male phone number "1860652" "cardNumber": "" WeChat nickname "Peter"}`
286,refactoring log position manager module Refactoring log The position module does not change the core logic to use a combined pattern to make the code logic clearer. [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=286) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=286) before we can accept your contribution.<br/><hr/>**yinxiu** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=286) it.</sub>
285,Canal newspaper can not find binlog file 2017-03-17 15:17:27.000 [destination = qbo address = /10.0.0.68:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - **Didn't find the corresponding binlog files from mysql-bin.000001 to mysql-bin.000004** 2017-03-17 15:17:27.005 [destination = qbo address = /10.0.0.68:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /10.0.0.68:3306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: can't find start position for qbo 2017-03-17 15:17:27.008 [destination = qbo address = /10.0.0.68:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:qbo[com.alibaba.otter.canal.parse.exception.CanalParseException: can't find start position for qbo ] canal version 1.0.24 Mysql version 5.6.33 Main library status mysql> show master status; +------------------+----------+--------------+------------------+-------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +------------------+----------+--------------+------------------+-------------------+ | mysql-bin.000004 | 1507 | | | | +------------------+----------+--------------+------------------+-------------------+ 1 row in set (0.00 sec) The full log is in the attachment [qbo.log.txt](https://github.com/alibaba/canal/files/849900/qbo.log.txt) This exception is that the timestamp you specified is adjusted earlier than any of the current binlog files.
284,here comes an MySQL bug between MySQL 5.6.20 & MySQL5.6.34 for MySQL5.6.20 have not compatible with MySQL5.5 so it will no kill zombie thread without the slave uuid ![mysql5 6 20](https://cloud.githubusercontent.com/assets/22068461/24030521/6586639a-0b18-11e7-8014-068073ee5408.png) ![mysql5 6 34](https://cloud.githubusercontent.com/assets/22068461/24030527/6efc0060-0b18-11e7-8484-037403014128.png) fix this bug for using set @slave_uuid="${uuid}" command before dump I think it may be because of a problem that many user feedback links are disconnected abnormally. Can refer to the implementation of MySQL source code When building an IO thread to the Master Will set uuid > /** > Set user variables after connecting to the master. > > @param mysql MYSQL to request uuid from master. > @param mi Master_info to set master_uuid > > @return 0: Success 1: Fatal error 2: Network error. > */ > int io_thread_init_commands(MYSQL *mysql Master_info *mi) > { > char query[256]; > int ret= 0; > > sprintf(query "SET @slave_uuid= '%s'" server_uuid); > if (mysql_real_query(mysql query strlen(query)) > && !check_io_slave_killed(mi->info_thd mi NULL)) > goto err; Looked at the source code of mysql5 6 it should be compatible with mysql5 5 The client&#39;s serverId dump will not be set based on the serverId if the slave_uuid is not set by default. Increase the strong dependency on slaveId after adding slave_uuid
283,upgrade fastjson for security [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=283) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=283) before we can accept your contribution.<br/><hr/>**fengyong** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=283) it.</sub> tks
282,Update JsonUtils.java Fix the problem of JSONUtils error after upgrading fastjson to 1 2 28 [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=282) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=282) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=282) it.</sub> The version number in pom should also be changed. tks
281,Fastjson high risk vulnerability I don&#39;t know if it has any effect https://github.com/alibaba/fastjson/wiki/security_update_20170315 see #283 I pay attention to it
280,[bug]TableMetaCache The reconnect function is invalid after the connection is broken. Source code on master ``` public TableMetaCache(MysqlConnection con){ this.connection = con; tableMetaCache = MigrateMap.makeComputingMap(new Function<String TableMeta>() { public TableMeta apply(String name) { try { return getTableMeta0(name); } catch (IOException e) { // Try to do a retry operation try { connection.reconnect(); System.out.println("cclehui Renew connect master"); return getTableMeta0(name); } catch (IOException e1) { throw new CanalParseException("fetch failed by table meta:" + name e1); } } } }); ``` Can see that IOException was caught After that will automatically reconnect once and then get the table structure but the problem is if mysql Master actively closes the connection and will not throw this IOException. Instead, what is thrown is a RuntimeException the corresponding exception throws the code at MysqlQueryExecutor Line 29 ``` public MysqlQueryExecutor(MysqlConnector connector) throws IOException { if (!connector.isConnected()) This logic throws an exception after the server closes the connection. throw new RuntimeException("should execute connector.connect() first"); } this.channel = connector.getChannel(); } ``` Reconnection logic does not take effect as described above Bug reproduction method Put master mysql of wait_timeout Set to 20 seconds Wait 20 seconds after starting canal Go to the master to do some insert or update operations canal The slave gets the binlog and parses it. If there is no local structure information, it will go to the master to get the table structure information through a non-slave connection. But after 20 seconds, the master has closed the connection and this time throws a RuntimeException. ， Then the binlog event failed to parse. My solution is to runtimeException Change to IOException Throw but don&#39;t know if there are any other problems Of course I test it myself. ``` public MysqlQueryExecutor(MysqlConnector connector) throws IOException { if (!connector.isConnected()) { // throw new RuntimeException("should execute connector.connect() first"); throw new IOException("should execute connector.connect() first"); } this.channel = connector.getChannel(); } ``` I also encountered the same problem and can be reproduced according to the method of the landlord. There is another information theoretically canal After the server encounters an exception, it will retry as follows ![retry](https://cloud.githubusercontent.com/assets/3198806/23950250/4cf424ba-09c5-11e7-8439-a683e064a3ee.png) From the log point of view, it did retry and returned to normal. ![retry_log_1](https://cloud.githubusercontent.com/assets/3198806/23950630/937491f8-09c6-11e7-83c6-bd15f16b34a7.png) But after a while, the DB link is broken again after a while, but the success log is not retried. ![retry_log_2](https://cloud.githubusercontent.com/assets/3198806/23950657/b04fb302-09c6-11e7-919a-61e1f6b488cf.png) Then manually stop the canal Server will report the following error ![retry_log_3](https://cloud.githubusercontent.com/assets/3198806/23950700/d4f989a8-09c6-11e7-9e19-ed071a0caf11.png) Looking forward to the perfect solution
279,Running the client batchId occasionally conflicts batchId:2176966 is not the firstly:2176965 at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.receiveMessages(SimpleCanalConnector.java:302) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:279) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:252) at com.yagou.yggx.provider.datasync.start.AbstractCanalStart.process(AbstractCanalStart.java:115) Below is the code int batchSize = 5 * 1024; while (running) { try { MDC.put("destination" destination); connector.connect(); connector.subscribe(); while (running) { // Get the specified amount of data Message message = connector.getWithoutAck(batchSize); long batchId = message.getId(); int size = message.getEntries().size(); List<Entry> entrys = message.getEntries(); if (batchId == -1 || size == 0) { // try {Thread.sleep(1000); // } catch (InterruptedException e) { } } else { DataSourceListenerInformation.comeOutEntry(entrys); } connector.ack(batchId); // Submit confirmation I also encountered this question. How do you solve this problem? a destination Corresponding to the consumption of the same destination by multiple consumers at the same end, it will report that the batchId does not exist or is not a location.
278,canal server cpu 100% @agapple The version is the latest master. I am a binlog from 4 days ago. The pos point starts to synchronize and the synchronization speed is much slower than before. server Cpu has been 100 positioned to find the following stack ![cpu](https://cloud.githubusercontent.com/assets/3198806/23615030/0adf5ef2-02c0-11e7-9334-fc3c3e399d6e.png) Memory was at the time total used free shared buffers cached Mem: 32047 24349 7698 0 274 13740 -/+ buffers/cache: 10333 21713 Swap: 511 189 322 This piece is the implementation of netty4 submitted by new students. There may be some problems. Can try to optimize with the latest 1 1 1 version
277,Configuring automatic update is unstable ``` scan reload found[local] but reload failed ``` Restart it Clear the complete exception
276,canal The server displays a successful configuration but the client is abnormal. ``` 2017-03-06 15:19:16.800 [canal-instance-scan-0] INFO c.a.o.canal.deployer.monitor.SpringInstanceConfigMonitor - auto notify reload local successful. ``` Log display local This instance succeeds but the client reports an exception. ``` com.alibaba.otter.canal.protocol.exception.CanalClientException: something goes wrong with reason: something goes wrong with channel:[id: 0x3f655fa5 /127.0.0.1:65443 => /127.0.0.1:11111] exception=com.alibaba.otter.canal.server.exception.CanalServerException: destination:local should start first at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.receiveMessages(SimpleCanalConnector.java:302) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:279) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:252) at com.alibaba.otter.canal.example.RockyDemo.main(RockyDemo.java:38) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147) ``` The server may have restarted the client multiple test
275,canal.instance.filter.regex Parameter does not work canal.instance.filter.regex = demodbname\\\\.user After this setting, the client will still receive the binlog log information of the non-user table. Use version 1.0.23 [zk: localhost:2181(CONNECTED) 44] get /otter/canal/destinations/example/1001/filter .*\..* cZxid = 0x1000013e8 ctime = Fri Mar 03 11:26:12 CST 2017 ..................... .................... [zk: localhost:2181(CONNECTED) 48] set /otter/canal/destinations/example/1001/filter demodbname\\.user Try zk It seems that the initial value is modified after the configuration file is restarted. The service looks like the value in zk is not modified. The parameter does not work personally guessed
274,Parsing the binlog log failed An error message appears when parsing the binlog log after the canal run Q_SQL_MODE_CODE error: limit excceed: 67 I saw a part of the canal source code seems to be sql_mode set is not right SQL_mode defined in the canal is 64-bit, so use getLong64 but actually 67 caused the cross-border did not see the database data is a problem PS: Mysql version is 5 7 14 Mysqlbinlog log ------------------------------------------------------------------- see attached ![binlog](https://cloud.githubusercontent.com/assets/2059502/23546331/104ef6f0-003a-11e7-8f0c-40f50a85356c.png) [binlog.txt](https://github.com/alibaba/canal/files/816403/binlog.txt) Canal instance log ----------------------------------------------------------------- 2017-03-03 17:17:19.952 [destination = gene_cachenotice_play3 address = /172.16.10.213:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position {"identity":{"slaveId":-1 "sourceAddress":{"address":"172.16.10.213" "port":3306}} "postion":{"included":false "journalName":"mysql-bin.000016" "position":24856433 "serverId":213 "timestamp":1488484784000}} 2017-03-03 17:17:19.959 [destination = gene_cachenotice_play3 address = /172.16.10.213:3306 EventParser] WARN com.taobao.tddl.dbsync.binlog.LogDecoder - Decoding Query failed from: mysql-bin.000016:24858343 java.io.IOException: Read Q_SQL_MODE_CODE error: limit excceed: 67 at com.taobao.tddl.dbsync.binlog.event.QueryLogEvent.unpackVariables(QueryLogEvent.java:650) ~[canal.parse.dbsync-1.0.23.jar:na] at com.taobao.tddl.dbsync.binlog.event.QueryLogEvent.<init>(QueryLogEvent.java:477) ~[canal.parse.dbsync-1.0.23.jar:na] at com.taobao.tddl.dbsync.binlog.LogDecoder.decode(LogDecoder.java:154) ~[canal.parse.dbsync-1.0.23.jar:na] at com.taobao.tddl.dbsync.binlog.LogDecoder.decode(LogDecoder.java:106) ~[canal.parse.dbsync-1.0.23.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:123) [canal.parse-1.0.23.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) [canal.parse-1.0.23.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_25] Caused by: java.lang.IllegalArgumentException: limit excceed: 67 at com.taobao.tddl.dbsync.binlog.LogBuffer.getLong64(LogBuffer.java:873) ~[canal.parse.dbsync-1.0.23.jar:na] at com.taobao.tddl.dbsync.binlog.event.QueryLogEvent.unpackVariables(QueryLogEvent.java:566) ~[canal.parse.dbsync-1.0.23.jar:na] ... 6 common frames omitted 2017-03-03 17:17:19.960 [destination = gene_cachenotice_play3 address = /172.16.10.213:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address 172.16.10.213/172.16.10.213:3306 has an error retrying. caused by java.io.IOException: Read Q_SQL_MODE_CODE error: limit excceed: 67 at com.taobao.tddl.dbsync.binlog.event.QueryLogEvent.unpackVariables(QueryLogEvent.java:650) ~[canal.parse.dbsync-1.0.23.jar:na] at com.taobao.tddl.dbsync.binlog.event.QueryLogEvent.<init>(QueryLogEvent.java:477) ~[canal.parse.dbsync-1.0.23.jar:na] at com.taobao.tddl.dbsync.binlog.LogDecoder.decode(LogDecoder.java:154) ~[canal.parse.dbsync-1.0.23.jar:na] at com.taobao.tddl.dbsync.binlog.LogDecoder.decode(LogDecoder.java:106) ~[canal.parse.dbsync-1.0.23.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:123) ~[canal.parse-1.0.23.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) ~[canal.parse-1.0.23.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_25] Caused by: java.lang.IllegalArgumentException: limit excceed: 67 at com.taobao.tddl.dbsync.binlog.LogBuffer.getLong64(LogBuffer.java:873) ~[canal.parse.dbsync-1.0.23.jar:na] at com.taobao.tddl.dbsync.binlog.event.QueryLogEvent.unpackVariables(QueryLogEvent.java:566) ~[canal.parse.dbsync-1.0.23.jar:na] ... 6 common frames omitted 2017-03-03 17:17:19.960 [destination = gene_cachenotice_play3 address = /172.16.10.213:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:gene_cachenotice_play3[java.io.IOException: Read Q_SQL_MODE_CODE error: limit excceed: 67 at com.taobao.tddl.dbsync.binlog.event.QueryLogEvent.unpackVariables(QueryLogEvent.java:650) at com.taobao.tddl.dbsync.binlog.event.QueryLogEvent.<init>(QueryLogEvent.java:477) at com.taobao.tddl.dbsync.binlog.LogDecoder.decode(LogDecoder.java:154) at com.taobao.tddl.dbsync.binlog.LogDecoder.decode(LogDecoder.java:106) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:123) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) at java.lang.Thread.run(Thread.java:745) Caused by: java.lang.IllegalArgumentException: limit excceed: 67 at com.taobao.tddl.dbsync.binlog.LogBuffer.getLong64(LogBuffer.java:873) at com.taobao.tddl.dbsync.binlog.event.QueryLogEvent.unpackVariables(QueryLogEvent.java:566) ... 6 more ] mysql 5 7 14 version has not been tested, there may be a protocol change Changed a code to help me verify mysql 5 7 14 Whether the environment can be parsed normally. Thank you for verifying the ok and replying to the information.
273,Error reporting exception java.lang.ClassCastException: com.alibaba.otter.canal.parse.inbound.AbstractEventParser$4 cannot be cast to com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser$MysqlDetectingTimeTask at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.stopHeartBeat(MysqlEventParser.java:186) ~[canal.parse-1.0.23.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.stop(AbstractEventParser.java:282) ~[canal.parse-1.0.23.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.stop(MysqlEventParser.java:165) ~[canal.parse-1.0.23.jar:na] at com.alibaba.otter.canal.instance.core.AbstractCanalInstance.stop(AbstractCanalInstance.java:104) ~[canal.instance.core-1.0.23.jar:na] at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.stop(CanalServerWithEmbedded.java:85) ~[canal.server-1.0.23.jar:na] at com.alibaba.otter.canal.server.netty.CanalServerWithNetty.stop(CanalServerWithNetty.java:95) [canal.server-1.0.23.jar:na] at com.alibaba.otter.canal.deployer.CanalController.stop(CanalController.java:422) [canal.deployer-1.0.23.jar:na] at com.alibaba.otter.canal.deployer.CanalLauncher$1.run(CanalLauncher.java:42) [canal.deployer-1.0.23.jar:na] New master code has been fixed
272,After the server is started example instance And master Inexplicably disconnected for a while ， No log I am a branch compiled from the master on 2017 02 27 Mysql version 5 5 32 Start service is normal ``` 2017-03-02 17:27:58.627 [main] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## start the canal server. 2017-03-02 17:27:58.924 [main] INFO com.alibaba.otter.canal.deployer.CanalController - ## start the canal server[my ip :11111] 2017-03-02 17:28:00.401 [main] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## the canal server is running now ...... ``` example instance Startup failed The instance starts to connect to mysql Then inexplicably made a disConnect Broke the link ， No, then there is no log. ， The service is still running but the instance is not up ， The link to mysql is not established. And very strange is the connection twice mysql 。 The startup log of the instance is as follows ``` 2017-03-02 17:27:59.019 [main] INFO o.s.context.support.ClassPathXmlApplicationContext - Refreshing org.springframework.context.support.ClassPathXmlApplicationContext@68bbe345: display name [org.springframework.context.support.ClassPathXmlApplicationContext@68bbe345]; startup date [Thu Mar 02 17:27:59 CST 2017]; root of context hierarchy 2017-03-02 17:27:59.133 [main] INFO o.s.beans.factory.xml.XmlBeanDefinitionReader - Loading XML bean definitions from class path resource [spring/file-instance.xml] 2017-03-02 17:27:59.516 [main] INFO o.s.context.support.ClassPathXmlApplicationContext - Bean factory for application context [org.springframework.context.support.ClassPathXmlApplicationContext@68bbe345]: org.springframework.beans.factory.support.DefaultListableBeanFactory@78186a70 2017-03-02 17:27:59.642 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2017-03-02 17:27:59.643 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties] 2017-03-02 17:27:59.697 [main] INFO o.s.beans.factory.support.DefaultListableBeanFactory - Pre-instantiating singletons in org.springframework.beans.factory.support.DefaultListableBeanFactory@78186a70: defining beans [com.alibaba.otter.canal.instance.spring.support.PropertyPlaceholderConfigurer#0 socketAddressEditor org.springframework.beans.factory.config.CustomEditorConfigurer#0 instance alarmHandler metaManager eventStore eventSink eventParser]; root of factory hierarchy 2017-03-02 17:27:59.851 [main] WARN org.springframework.beans.TypeConverterDelegate - PropertyEditor [com.sun.beans.editors.EnumEditor] found through deprecated global PropertyEditorManager fallback - consider using a more isolated form of registration e.g. on the BeanWrapper/BeanFactory! 2017-03-02 17:28:00.131 [main] INFO c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-example 2017-03-02 17:28:00.158 [main] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - start successful.... 2017-03-02 17:28:00.158 [main] INFO c.a.otter.canal.server.embedded.CanalServerWithEmbedded - start CanalInstances[example] successfully 2017-03-02 17:28:00.177 [destination = example address = /127.0.0.1:3306 EventParser] INFO c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - start heart beat.... 2017-03-02 17:28:00.936 [destination = example address = /127.0.0.1:3306 EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - connect MysqlConnection to /127.0.0.1:3306... 2017-03-02 17:28:00.944 [destination = example address = /127.0.0.1:3306 EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - handshake initialization packet received prepare the client authentication packet to send 2017-03-02 17:28:00.973 [destination = example address = /127.0.0.1:3306 EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - client authentication packet is sent out. 2017-03-02 17:28:00.989 [destination = example address = /127.0.0.1:3306 EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - connect MysqlConnection to /127.0.0.1:3306... 2017-03-02 17:28:00.991 [destination = example address = /127.0.0.1:3306 EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - handshake initialization packet received prepare the client authentication packet to send 2017-03-02 17:28:00.991 [destination = example address = /127.0.0.1:3306 EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - client authentication packet is sent out. 2017-03-02 17:28:00.993 [destination = example address = /127.0.0.1:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just show master status 2017-03-02 17:28:01.013 [destination = example address = /127.0.0.1:3306 EventParser] INFO c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=mysql-bin.000004 position=121250 serverId=<null> timestamp=<null>] 2017-03-02 17:28:01.023 [destination = example address = /127.0.0.1:3306 EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - disConnect MysqlConnection to /127.0.0.1:3306... ``` mysql of Tcp connection situation Just started ``` tcp 0 0 127.0.0.1:3306 127.0.0.1:60383 ESTABLISHED 18565/mysqld tcp 0 0 127.0.0.1:60383 127.0.0.1:3306 ESTABLISHED 1273/java tcp 0 0 127.0.0.1:3306 127.0.0.1:60381 ESTABLISHED 18565/mysqld tcp 0 0 127.0.0.1:60381 127.0.0.1:3306 ESTABLISHED 1273/java ``` After a while, both connections are broken. This problem has been found out of my own test mysql server configuration interactive_timeout with wait_timeout 30 20 respectively ， After 20 seconds, mysql Proactively close inactive connections
271,fetch failed by table meta:`mysql`.`user` Suddenly after 11 50 last night, the problem suddenly appeared to be related to permissions. Online use is RDS Test line own machine canal is not abnormal 2017-02-28 23:49:51.829 [destination = example address = /XXX.178.61.230:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEvent Parser - ERROR ## parse this event has an error last position : [EntryPosition[included=false journalName=mysql-bin.000302 position=5492156 serve rId=3071795599 timestamp=1488296991000]] com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.collect.ComputationException: com.alibaba.otter.canal.parse.exception.CanalParseException: fetch failed by table meta:`mysql`.`user` Caused by: com.google.common.collect.ComputationException: com.alibaba.otter.canal.parse.exception.CanalParseException: fetch failed by table meta: `mysql`.`user` at com.google.common.collect.MapMaker$ComputingMapAdapter.get(MapMaker.java:889) ~[guava-18.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:78) ~[canal.parse-1.0.23-SNAPSHOT.jar :na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:677) ~[canal.parse-1.0.23-SNAPSHOT. jar:na] ........... at java.lang.Thread.run(Thread.java:745) [na:1.8.0_102] Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: fetch failed by table meta:`mysql`.`user` Caused by: java.io.IOException: connect XXX.178.61.230/118.178.61.230:3306 failure:java.io.IOException: Error When doing Client Authentication:ErrorPacket [errorNumber=2811 fieldCount=-1 message=Authentication Failed For RDS backend sqlState=RDS00 sqlStateMarker=#] at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:208) ........... Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: fetch failed by table meta:`mysql`.`user` Caused by: java.io.IOException: ErrorPacket [errorNumber=1142 fieldCount=-1 message=SELECT command denied to user 'app_prod_crawler'@'116.62.43.4 1' for table 'user' sqlState=42000 sqlStateMarker=#] with command: desc `mysql`.`user` at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.query(MysqlQueryExecutor.java:60) ~[canal.parse.driver-1.0.23-SNAPSHOT.jar:na] I also used a similar error when I deleted a table while using the canal listener library. Has this problem been solved? Before listening to all the libraries, the documentation can be configured to listen only to the specified library canal instance filter regex Filter the mysql user table The latest version 24 is already supported
270,Not receiving all insert events My company will issue coupons from time to time. From hundreds to tens of thousands of coupons, the coupon data is inserted into the database in batches. However, the client can only receive some insert events as follows. 2017-02-28 11 08 47 inserted 400 records into the database but the client only received 4 insert events. I don&#39;t know what the reason was. I thought it was memory. Mysql version 5 6 Canal version 1 0 20 Canal configuration ################################################# ######### common argument ############# ################################################# canal.id= 1 canal.ip= ******** canal.port= 11111 canal.zkServers= # flush data to zk canal.zookeeper.flush.period = 1000 # flush meta cursor/parse position to file canal.file.data.dir = ${canal.conf.dir} canal.file.flush.period = 1000 ## memory store RingBuffer size should be Math.pow(2 n) canal.instance.memory.buffer.size = 1048576 ## memory store RingBuffer used memory unit size default 1kb canal.instance.memory.buffer.memunit = 1024 Here started to change to 4096 server and client are started normally, but also can be connected normally, but there is no event generated or error returned to normal after 1024 ## meory store gets mode used MEMSIZE or ITEMSIZE canal.instance.memory.batch.mode = MEMSIZE ## detecing config canal.instance.detecting.enable = false #canal.instance.detecting.sql = insert into retl.xdual values(1 now()) on duplicate key update x=now() canal.instance.detecting.sql = select 1 canal.instance.detecting.interval.time = 3 canal.instance.detecting.retry.threshold = 3 canal.instance.detecting.heartbeatHaEnable = false # support maximum transaction size more than the size of the transaction will be cut into multiple transactions delivery canal.instance.transaction.size = 16384 # mysql fallback connected to new master should fallback times canal.instance.fallbackIntervalInSeconds = 60 # network config canal.instance.network.receiveBufferSize = 104857600 canal.instance.network.sendBufferSize = 104857600 canal.instance.network.soTimeout = 30 # binlog filter config canal.instance.filter.query.dcl = false canal.instance.filter.query.dml = false canal.instance.filter.query.ddl = false canal.instance.filter.table.error = false # binlog format/image check canal.instance.binlog.format = ROW STATEMENT MIXED canal.instance.binlog.image = FULL MINIMAL NOBLOB # binlog ddl isolation canal.instance.get.ddl.isolation = false ################################################# ######### destinations ############# ################################################# canal.destinations= ******** # conf root dir canal.conf.dir = ../conf # auto scan instance dir add/remove and start/stop instance canal.auto.scan = true canal.auto.scan.interval = 5 canal.instance.global.mode = spring canal.instance.global.lazy = false #canal.instance.global.manager.address = 127.0.0.1:1099 #canal.instance.global.spring.xml = classpath:spring/memory-instance.xml canal.instance.global.spring.xml = classpath:spring/file-instance.xml #canal.instance.global.spring.xml = classpath:spring/default-instance.xml There is no such problem when synchronizing other tables. Those tables do not insert a lot of data at the same time. An event event may contain multiple change records The reason for tracking this problem recently is that RowChange returns a list that contains multiple records but only processes the last record and seriously despise the person who wrote it.
269,canal The server runs for a period of time and reports the following error. 2017-02-28 17:12:37.470 [destination = example address = /10.10.10.193:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.collect.ComputationException: java.lang.RuntimeException: should execute connector.connect() first Caused by: com.google.common.collect.ComputationException: java.lang.RuntimeException: should execute connector.connect() first at com.google.common.collect.MapMaker$ComputingMapAdapter.get(MapMaker.java:889) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:78) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:677) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:362) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:108) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:62) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:326) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:176) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:129) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) at java.lang.Thread.run(Thread.java:724) Caused by: java.lang.RuntimeException: should execute connector.connect() first at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.<init>(MysqlQueryExecutor.java:29) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:71) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta0(TableMetaCache.java:105) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:26) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.apply(TableMetaCache.java:46) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.apply(TableMetaCache.java:42) at com.google.common.collect.ComputingConcurrentHashMap$ComputingValueReference.compute(ComputingConcurrentHashMap.java:356) at com.google.common.collect.ComputingConcurrentHashMap$ComputingSegment.compute(ComputingConcurrentHashMap.java:182) at com.google.common.collect.ComputingConcurrentHashMap$ComputingSegment.getOrCompute(ComputingConcurrentHashMap.java:151) at com.google.common.collect.ComputingConcurrentHashMap.getOrCompute(ComputingConcurrentHashMap.java:67) at com.google.common.collect.MapMaker$ComputingMapAdapter.get(MapMaker.java:885) ... 10 more ] canal The server restarts the estimated client and does more retry.
268,Can I only sync specific types of logs, such as adding new data? RT only synchronizes new data, can&#39;t you do it? You first understand mysql The basic concept of binlog
267,Canal collection Update multiple rows by an update statement The resulting binlog is very slow What is the reason? We have a scene update statement to update 3000 rows update tablename set x='xxx' where id>0 and id 3001 then submits that the canal collection speed is significantly slower than normal View binlog from mysql found that a transaction is divided into 47 update_row Event therefore an update_row Event contains 64 row changes When we consume in our consumption scenario, we find that there are 64 BinlogRow 47 64 in an entry rowDatas that corresponds to 3000 rows. I did some tests and found that when the size of the updated mysql line is small, when a statement updates multiple lines, this situation will be slow. But when the mysql line is relatively large, there are 3000 update_rows in the 3000 line binlog original file. Event This time the collection speed is normal. I want to know now. 1 Is it because the binlog content generated by updating multiple rows contains one entry containing multiple row change information? Subsequent parsing is slow and the collection speed is slow 2 What is the specific circumstances of an update_row Event will contain multiple row changes 3 Is there any way to improve the processing performance for a scenario where multiple rows of sql statements are updated? This has not been deliberately concerned about your attention to the slowness of some specific data, such as assessing the tps is probably how much In the case of the same configuration, a SQL statement can only affect 7MB of raw binlog files per second. This sql statement affects multiple rows. It is especially slow. I measured a sql statement to update 3000 rows. 0 5MBbinlog file When I tested it, I found that only when the original mysql single record is small, this batch update will occur. This is a big operation for batch updating a single mysql record. I tried that each field adds up to more than 1KB. The collection speed is almost the same as normal. @lan1994 Did your problem be solved? We also encountered similar problems. @pan289091315 Not solved Mysql configuration parameters as innodb_flush_log_at_trx_commit Whether to submit a buffer when submitting a transaction There are 3 values 0 does not actively trigger log buffer write to disk 1 By default, the log buffer is flushed to disk at the same time each transaction is committed. 2 Every time a transaction is committed, the log buffer is flushed to disk but he is not doing it at the same time. But refresh every second Recommended to be configured as 2 Great impact on performance Can try this @wufengbin The parameter you mentioned is to solve the problem of slow mysql batch write. But the problem mentioned here is that the canal parsing is slow. After the pre-write mysql is completed, the parsing performance is tested. @lan1994 Is it solved? Instead, you can execute a large transaction first and then canal back to the transaction before performing the verification. From the test on my side, basically, the basic performance can be satisfied. @lan1994 The cause of the problem was found no brother Reference documentation https github com alibaba canal wiki Performance
266,canal 1.0.16 Error in batch brushing database data canal 1.0.16 There is an error in the batch brush database data. What is the bug in this version? Need to describe the specific error content clearly version change you pay attention to release note
265,ErrotCode:400 Caused by : packet type=CLIENTAUTHENTICATION is NOT supported! Start canal report such an exception but the database I use msyql client account password can log in normally mysql from the library and canal is deployed on the same server, what is the reason for encountering similar problems? 2017-02-22 10:53:15.093 [New I/O server worker #1-2] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400 Caused by : packet type=CLIENTAUTHENTICATION is NOT supported! 2017-02-22 10:53:45.014 [Thread-3] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## stop the canal server 2017-02-22 10:53:45.315 [Thread-3] INFO com.alibaba.otter.canal.deployer.CanalController - ## stop the canal server[192.168.1.20:11111] 2017-02-22 10:53:45.315 [Thread-3] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## canal server is down. OpenJDK 64-Bit Server VM warning: ignoring option PermSize=96m; support was removed in 8.0 OpenJDK 64-Bit Server VM warning: ignoring option MaxPermSize=256m; support was removed in 8.0 OpenJDK 64-Bit Server VM warning: UseCMSCompactAtFullCollection is deprecated and will likely be removed in a future release. 2017-02-22 10:53:59.987 [main] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## start the canal server. 2017-02-22 10:54:00.151 [main] INFO com.alibaba.otter.canal.deployer.CanalController - ## start the canal server[192.168.1.20:11111] 2017-02-22 10:54:01.278 [main] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## the canal server is running now ...... Look at the last line of the log and use the ps query canal is started but no events occur but the binlog log is always increasing How can you solve this? 2462962670
264,Binlog parsing problem canal Parse the binlog log Sometimes it is not possible to get the primary key. mysql Version mysql 5 7 14 Have not encountered user feedback, it is best to clear the scene of recurring, such as the mysql table canal version and other information
263,Updated binlog analysis question Can can resolve the Percona branch in mysql now? If not Is there a supported plan? Supported However, if you try this kind of problem, you will not know what to ask. At least one day will be answered. I am not clear enough Is such that Source library by percona Synchronize to the destination library of the mysql master branch The source table to be synchronized has both innodb There are also tokudb The destination table to be synchronized is innodb There are no large field types such as blob in the source table. A parsing error occurred while synchronizing. A more detailed error stack was not printed. pid:17 nid:8 exception:canal:test_canal:com.alibaba.otter.canal.parse.exception.CanalParseException: Unsupported BinlogImage NOBLOB Configuration is not modified to use the default configuration That is, the supported BinlogImage is FULL. MINIMAL NOBLOB] @agapple Seeking answers Otter does not support image mode as NOBLOB
262,on SlaveId Collision problem Mysql Will think that they have the same SlaveId of connection Both come from the same slave library and thus have the same in both SlaveId of canal instance For the same Mysql Master initiate dump There must be one when requesting instance Will fail && Repeatedly try again This error is in Mysql The report came out 1236 but can&#39;t find it by binlog Exception to solve In this case, only new ones can be regenerated. SlaveId is like this canal.parse: MysqlEventParser.java ``` @Override protected void processDumpError(Throwable e) { if (e instanceof IOException) { String message = e.getMessage(); if (StringUtils.contains(message "errno = 1236")) { // 1236 errorCode representative ER_MASTER_FATAL_ERROR_READING_BINLOG // But it may also be slaveId conflict Pre-remediation here if (StringUtils.contains(message "errmsg = A slave with the same server_uuid/server_id as this slave has connected to the master")) { // regenerate slaveId long new_slave_id = new Random().nextLong(); logger.warn("SlaveId Conflict is detected: Old slave id = [{}] new slave id = [{}]" this.slaveId new_slave_id); this.slaveId = new_slave_id; return; } dumpErrorCount++; } } super.processDumpError(e); } ``` May be able to reduce the amount of manual configuration work Can you submit a pull Request to me The latest version has already set slave_uuid
261,Java client CanalConnector subscribe uses the following second regular filter is invalid Java using canal api， Why use the third Four valid second regular invalid filters all ps:canal Server conf example instance properties canal instance filter regex not configured 1. All tables or .*\\..* 2. canal All tables under the schema canal\\..* 3. Canal canal with canal under canal 4. canal a table under the schema canal test1 5. Multiple rules combined with canal mysql.test1 mysql.test2 Comma separated Friend, this one Issue What conclusions did you have later?
260,Production environment version considerations We are ready to use the canal in the production environment before the test environment colleagues built the 1 0 22 version of the test using some bugs such as kill The connection problem is currently basically fixed at 1 0 23. Later, I noticed that the official wiki recommended the 1 0 19 version. Does agapple recommend the 1 0 19 version based on stability? It is estimated that the document has not been updated in time to use the latest version. Thank you, thank you. We also selected the latest version and are testing these days. It’s better to update the documentation in time.
259,fix bug for #202（canal.instance.filter.regex Not valid after modification This can not be added to the meta configuration changes because this file location will continue to update will lead to frequent reload No meta, only compare the instance properties file in the corresponding directory. Subscribe automatically generates meta files
258,fix bug for: #202 canal.instance.filter.regex Not valid after modification Previous issue 202 canal.instance.filter.regex This is not valid after the modification. This is the patch I played. Sorry to see the reply [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=258) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=258) before we can accept your Contribution br hr seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).
257,Found a bug in MysqlEventParser 1 0 23 version of MysqlEventParser&#39;s stopHeartBeat method does not determine the type of heartBeatTimerTask directly cast to cause the following java.lang.ClassCastException: com.alibaba.otter.canal.parse.inbound.AbstractEventParser$4 cannot be cast to com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser$MysqlDetectingTimeTask at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.stopHeartBeat(MysqlEventParser.java:186) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.stop(AbstractEventParser.java:282) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.stop(MysqlEventParser.java:165) at com.alibaba.otter.canal.instance.core.AbstractCanalInstance.stop(AbstractCanalInstance.java:104) at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.stop(CanalServerWithEmbedded.java:85) @agapple The big god this bug will cause the eventparser thread to not release, does it mean that canalserver stop In fact, a lot of stop things have not been able to do it. 1 0 24 version to the mvn warehouse, sit and wait for the god
256,Refactoring socketChannel using netty jdk has built-in classes that modify the PacketManager implementation and other related references. [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=256) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=256) before we can accept your contribution.<br/>**1** out of **2** committers have signed the CLA.<br/><br/>:white_check_mark: agapple<br/>:x: luoyaogui<hr/>**luoyaogui** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user). tks @luoyaogui User feedback is more network problems. I will temporarily roll back the code and then update the verification stability before updating.
255,BUG in Mysql 5 5 28 version under the error Log 2017-01-12 12:06:16.107 [destination = example address = /127.0.0.1:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /127.0.0.1:3306 has an error retrying. caused by java.lang.NullPointerException: null at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.loadBinlogChecksum(MysqlConnection.java:309) ~[canal.parse-1.0.24-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:120) ~[canal.parse-1.0.24-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) ~[canal.parse-1.0.24-SNAPSHOT.jar:na] at java.lang.Thread.run(Unknown Source) [na:1.7.0_80] Cause program BUG lacks null judgment if (columnValues != null && columnValues.size() >= 1 && columnValues.get(0).toUpperCase().equals("CRC32")) Change to if (columnValues != null && columnValues.size() >= 1 && columnValues.get(0) != null Lack of judgment && columnValues.get(0).toUpperCase().equals("CRC32")) Symptom elimination Please update the current master version I feel that this problem is more serious. It is recommended to issue a release. It is not good for the low version of mysql canal.
254,ll
253,prepare to find start position just last position Deployed two machines A machine And B Machine A The machine starts successfully and then looks at the log. Always in prepare to find start position just last Position after I put A The machine is turned off and then go to observe B The machine then quickly Get to postion Is this the problem? Do you want to shut down one machine at a time? I have encountered a similar problem. I only deployed a canal node. Always stuck in the positioning step @boboChina You don&#39;t have to use zk for a single machine. I can use it, I suspect it is zk. The problem log above shows pinging all the time zk I suspect that I have tried several times in the background. 1 first stopped the canal and deleted the data above zk. This is not possible 2 Set the binlog file location timestamp No According to the printed log, compare the configuration items and the source code to the if Else branch is not right Do not try zk also no effect doubt and mysql settings related to other mysql some soon hate to locate the card is dead but specifically do not know which configuration items will have an impact According to your logic, it should be a database problem. But I turned Zk off and then the stand-alone mode can change the database. prepare to find start position just last position This is the meaning that the last site has been successfully launched. However, after the startup is successful, it has been stopped there and then the client will always be stuck when it goes to consume.
252,com.alibaba.otter.canal.common.utils.JsonUtils Related Client startup throws an exception Speculation is a problem with the FastJSON version <img width="1051" alt="2017-01-06 14 32 56" src="https://cloud.githubusercontent.com/assets/22702893/21709589/0e616c68-d41d-11e6-9af3-88e79a5af79b.png">
251,May I support MariaDB with version 10 0 10? > ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com. alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: java.lang.IllegalArgumentException: bad format x exceed: 1207926783 999999999 Mysql uses the 10 0 10 version of MariaDB canal to resolve the binlog after the start will report the above error test 1 0 22 1 0 23 version of the canal will not work This estimate is to see what mariab changed on 10 0 10 @minotaursu You can try maxscale&#39;s binlogserver component. @zhongjimax binlog The server component just solves the binlog remote download. The specific parsing work has to do it yourself. I tried the 10 0 10 version that cannot parse mariadb
250,[BUGFIX] File garbled repair that occurs when MySQL5 6 is doing a master-slave switchover Multiple tests found that the problem appeared in the seek method. Compared to the dump, the checksum is missing. The filename obtained when searching for the binlog site is four bytes larger. [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=250) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=250) before we can accept your contribution.<br/><hr/>**jianhao.dai** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user). tks
249,Active/standby switchover The error log occurred during the active/standby switchover. Note the following as mysql bin 000029 73⁄4 and Could not find first log file name in binary log index） 2017-01-03 22:09:16.930 [destination = xiaopang3 address = /192.168.6.123:3306 EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - client authentication packet is sent out. 2017-01-03 22:09:16.934 [destination = xiaopang3 address = /192.168.6.123:3306 EventParser] INFO c.a.otter.canal.parse.inbound.mysql.MysqlConnection - COM_BINLOG_DUMP with position:BinlogDumpCommandPacket[binlogPosition=21725485 slaveServerId=1237 binlogFileName=mysql-bin.000029=7¾ command=18] 2017-01-03 22:09:16.935 [destination = xiaopang3 address = /192.168.6.123:3306 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) ~[canal.parse-1.0.23.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:121) [canal.parse-1.0.23.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) [canal.parse-1.0.23.jar:na] at java.lang.Thread.run(Thread.java:744) [na:1.7.0_45] 2017-01-03 22:09:16.935 [destination = xiaopang3 address = /192.168.6.123:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /192.168.6.123:3306 has an error retrying. caused by java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) ~[canal.parse-1.0.23.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:121) ~[canal.parse-1.0.23.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) ~[canal.parse-1.0.23.jar:na] at java.lang.Thread.run(Thread.java:744) [na:1.7.0_45] 2017-01-03 22:09:16.935 [destination = xiaopang3 address = /192.168.6.123:3306 EventParser] ERROR com.alibaba.otter.canal.common. I feel that the binlog name is garbled and try the current trunk. @agapple This question still exists on the 1 0 26 version. What is the reason? I also encountered the same problem canal version v1.0.26 alpha 1， mariadb Version 10 2 11 When switching Found binlog file name garbled @lijie1992066 Provide me the corresponding binlog file I also encountered the same problem when the master-slave switch is always reported binlog can not find the mysql binlog log is the existence of breakpoint debugging as follows ![image](https://user-images.githubusercontent.com/20380664/38121113-d6675676-33ff-11e8-9341-6703fa332bd6.png) Feel garbled on the suffix Can debug to see where this garbled location is obtained from ZhiXingHeYiApple
248,Fix mysql5 6 below java lang NullPointerException Fix mysql5 6 below java lang NullPointerException [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=248) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=248) before we can accept your contribution.<br/> tks
247,Example cannot link to mysql report java lang NullPointerException 2017-01-03 16:58:21.099 [destination = example address = /127.0.0.1:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /127.0.0.1:3306 has an error retrying. caused by java.lang.NullPointerException: null at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.loadBinlogChecksum(MysqlConnection.java:284) ~[canal.parse-1.0.23.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) ~[canal.parse-1.0.23.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) ~[canal.parse-1.0.23.jar:na] at java.lang.Thread.run(Unknown Source) [na:1.8.0_111] 2017-01-03 16:58:21.100 [destination = example address = /127.0.0.1:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.lang.NullPointerException at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.loadBinlogChecksum(MysqlConnection.java:284) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) at java.lang.Thread.run(Unknown Source) ] Locate the MysqlConnection java 284 code as if (columnValues != null && columnValues.size() >= 1 && columnValues.get(0).toUpperCase().equals("CRC32")) MySQL version 5 5 14 Previous query select @master_binlog_checksum Is null Added a judgment before upgrading to the latest version code The current backbone has been raised by the PR. Hello, have you solved it? I also encountered the same problem with 1 0 23 Is it already the latest version? @CONANLMN I have already fixed this problem and submitted the Release. It should be repackaged. You can replace the MysqlConnection class with the latest source code.
246,UPDATE How to get SQL directly from the event UPDATE How to get SQL directly from the event Currently getting updated field values Can only fight sql
245,issues#244 [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=245) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=245) before we can accept your contribution.<br/> tks
244,Can&#39;t close Mysql heartbeat connection normally ## MysqlEventParser.stopHeartBeat ![screenshot_20161230_112811](https://cloud.githubusercontent.com/assets/1852661/21559222/32776da4-ce83-11e6-9cdb-e0671014de96.png) ## AbstractEventParser.stopHeartBeat ![screenshot_20161230_112756](https://cloud.githubusercontent.com/assets/1852661/21559229/5bae9b34-ce83-11e6-8de1-258aac961da3.png) tks
243,The startup bat script in the original example directly runs the runtime syntax error. [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=243) <br/>All committers have signed the CLA. tks
242,Client requested master to start replication from impossible position Complete error message 2016-12-22 00:00:54.195 [destination = ixtrade address = /172.30.10.109:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position 2016-12-22 00:00:54.199 [destination = ixtrade address = /172.30.10.109:3306 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Client requested master to start replication from impossible position; the first event 'mysql-bin.000024' at 533920026 the last event read from 'mysql-bin.000024' at 4 the last byte read from 'mysql-bin.000024' at 4. at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) [canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210) [canal.parse-1.0.22.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_25] 2016-12-22 00:00:54.199 [destination = ixtrade address = /172.30.10.109:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /172.30.10.109:3306 has an error retrying. caused by java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Client requested master to start replication from impossible position; the first event 'mysql-bin.000024' at 533920026 the last event read from 'mysql-bin.000024' at 4 the last byte read from 'mysql-bin.000024' at 4. at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210) ~[canal.parse-1.0.22.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_25] 2016-12-22 00:00:54.200 [destination = ixtrade address = /172.30.10.109:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:ixtrade[java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Client requested master to start replication from impossible position; the first event 'mysql-bin.000024' at 533920026 the last event read from 'mysql-bin.000024' at 4 the last byte read from 'mysql-bin.000024' at 4. at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210) at java.lang.Thread.run(Thread.java:745) ] 2016-12-22 00:00:54.202 [destination = ixtrade address = /172.30.10.109:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - disconnect address /172.30.10.109:3306 has an error retrying. caused by java.io.IOException: KILL DUMP 7726 failure:java.io.IOException: ErrorPacket [errorNumber=1094 fieldCount=-1 message=Unknown thread id: 7726 sqlState=HY000 sqlStateMarker=#] with command: KILL CONNECTION 7726 at com.alibaba.otter.canal.parse.driver.mysql.MysqlUpdateExecutor.update(MysqlUpdateExecutor.java:49) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.disconnect(MysqlConnector.java:104) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.disconnect(MysqlConnection.java:60) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:242) at java.lang.Thread.run(Thread.java:745) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.disconnect(MysqlConnector.java:106) ~[canal.parse.driver-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.disconnect(MysqlConnection.java:60) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:242) ~[canal.parse-1.0.22.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_25] This exception should be generated after mysql is abnormally closed. What should I do? sqlstate = HY000 errmsg = Client requested master to start replication from impossible position; the first event 'mysql-bin.000024' at 533920026 the last event read from 'mysql-bin.000024' at 4 the last byte read from 'mysql-bin.000024' at 4. Estimated site error Clear content in meta dat Or clear zk Postion wrong information Restart the server You can do it.
241,After the ClusterCanalClientTest runs, the server side frequently scans the logs. [New I/O server worker #1-2] INFO c.a.otter.canal.server.netty.handler.SessionHandler - message receives in session handler Clone&#39;s latest canal project can use the ClusterCanalClientTest to get the DB binlog normally. But the server side crazy brush log is INFO but it doesn&#39;t feel very normal. I opened the appender ref on the logback side. Ref STDOUT this appender 2016-12-22 18:07:31.810 [New I/O server worker #1-2] INFO c.a.otter.canal.server.netty.handler.SessionHandler - message receives in session handler... 2016-12-22 18:07:31.810 [New I/O server worker #1-2] INFO c.a.otter.canal.server.netty.handler.SessionHandler - message receives in session handler... 2016-12-22 18:07:31.810 [New I/O server worker #1-2] INFO c.a.otter.canal.server.netty.handler.SessionHandler - message receives in session handler... 2016-12-22 18:07:31.810 [New I/O server worker #1-2] INFO c.a.otter.canal.server.netty.handler.SessionHandler - message receives in session handler... 2016-12-22 18:07:31.810 [New I/O server worker #1-2] INFO c.a.otter.canal.server.netty.handler.SessionHandler - message receives in session handler... 2016-12-22 18:07:31.810 [New I/O server worker #1-2] INFO c.a.otter.canal.server.netty.handler.SessionHandler - message receives in session handler... 2016-12-22 18:07:31.810 [New I/O server worker #1-2] INFO c.a.otter.canal.server.netty.handler.SessionHandler - message receives in session handler... 2016-12-22 18:07:31.810 [New I/O server worker #1-2] INFO c.a.otter.canal.server.netty.handler.SessionHandler - message receives in session handler... 2016-12-22 18:07:31.810 [New I/O server worker #1-2] INFO c.a.otter.canal.server.netty.handler.SessionHandler - message receives in session handler... 2016-12-22 18:07:31.810 [New I/O server worker #1-2] INFO c.a.otter.canal.server.netty.handler.SessionHandler - message receives in session handler... Opened this STDOUT appender INFO log just printed the log Oh, I know that the long connection server in netty has been polling the message for a long time, so the sessionHandler is called frequently.
240,Switch to group instance xml Consumption less than data 2016-12-19 17:28:58.734 [main] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## start the canal server. log4j:WARN No appenders could be found for logger (org.I0Itec.zkclient.ZkEventThread). log4j:WARN Please initialize the log4j system properly. 2016-12-19 17:28:58.991 [main] INFO com.alibaba.otter.canal.deployer.CanalController - ## start the canal server[10.97.14.145:11111] 2016-12-19 17:28:59.527 [main] ERROR c.a.o.c.common.zookeeper.running.ServerRunningMonitor - processActiveEnter failed java.lang.NullPointerException: null at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.start(AbstractEventParser.java:274) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.start(MysqlEventParser.java:145) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.group.GroupEventParser.start(GroupEventParser.java:24) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.instance.core.AbstractCanalInstance.start(AbstractCanalInstance.java:91) ~[canal.instance.core-1.0.22.jar:na] at com.alibaba.otter.canal.instance.spring.CanalInstanceWithSpring.start(CanalInstanceWithSpring.java:30) ~[canal.instance.spring-1.0.22.jar:na] at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.start(CanalServerWithEmbedded.java:102) ~[canal.server-1.0.22.jar:na] at com.alibaba.otter.canal.deployer.CanalController$2$1.processActiveEnter(CanalController.java:121) ~[canal.deployer-1.0.22.jar:na] at com.alibaba.otter.canal.common.zookeeper.running.ServerRunningMonitor.processActiveEnter(ServerRunningMonitor.java:234) [canal.common-1.0.22.jar:na] at com.alibaba.otter.canal.common.zookeeper.running.ServerRunningMonitor.initRunning(ServerRunningMonitor.java:138) [canal.common-1.0.22.jar:na] at com.alibaba.otter.canal.common.zookeeper.running.ServerRunningMonitor.start(ServerRunningMonitor.java:98) [canal.common-1.0.22.jar:na] at com.alibaba.otter.canal.deployer.CanalController.start(CanalController.java:396) [canal.deployer-1.0.22.jar:na] at com.alibaba.otter.canal.deployer.CanalLauncher.main(CanalLauncher.java:35) [canal.deployer-1.0.22.jar:na] java.lang.NullPointerException: null at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.start(AbstractEventParser.java:274) ~ To learn to see the log Follow the latest version 1 0 23 Follow the latest version 1 0 23
239,Reporting with CanalConnectors newClusterConnector Use zk to get canal connection error Statement CanalConnector simpleCanalConnector= CanalConnectors.newClusterConnector("10.100.103.19:2181" "example" "" ""); Error message Exception in thread "main" java.lang.IllegalAccessError: tried to access method com.google.common.collect.MapMaker.makeComputingMap(Lcom/google/common/base/Function;)Ljava/util/concurrent/ConcurrentMap; from class com.alibaba.otter.canal.common.zookeeper.ZkClientx at com.alibaba.otter.canal.common.zookeeper.ZkClientx.<clinit>(ZkClientx.java:26) at com.alibaba.otter.canal.client.CanalConnectors.newClusterConnector(CanalConnectors.java:60) at com.foundation.test.canal.ClientCluster.main(ClientCluster.java:25) Solved the dependency update to the latest 1 0 22 ok the old version of the parameter is inconsistent <dependency> <groupId>com.alibaba.otter</groupId> <artifactId>canal.client</artifactId> <version>1.0.22</version> </dependency>
238,java.io.IOException: end of stream when reading header abnormal The first two days were still normal, and I suddenly reported this abnormality today. 18:08:13.923 [im_message-****] ERROR ***.mysql.binlog.CanalClient - process error! com.alibaba.otter.canal.protocol.exception.CanalClientException: java.io.IOException: end of stream when reading header at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:281) ~[canal.client.jar:na] at cn.ishow.db.mysql.binlog.CanalClient.process(CanalClient.java:160) [Subscriber.jar:na] at cn.ishow.db.mysql.binlog.CanalClient.access$100(CanalClient.java:27) [Subscriber.jar:na] at cn.ishow.db.mysql.binlog.CanalClient$2.run(CanalClient.java:101) [Subscriber.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_74] Caused by: java.io.IOException: end of stream when reading header at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.read(SimpleCanalConnector.java:378) ~[canal.client.jar:na] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.readNextPacket(SimpleCanalConnector.java:369) ~[canal.client.jar:na] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.receiveMessages(SimpleCanalConnector.java:286) ~[canal.client.jar:na] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:279) ~[canal.client.jar:na] ... 4 common frames omitted 18:08:13.923 [im_message-****] INFO ***.mysql.binlog.CanalClient - disconnect Then canal can&#39;t even get on the error. com.alibaba.otter.canal.protocol.exception.CanalClientException: java.net.ConnectException: Connection refused: connect at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.doConnect(SimpleCanalConnector.java:171) ~[canal.client.jar:na] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.connect(SimpleCanalConnector.java:97) ~[canal.client.jar:na] at cn.ishow.db.mysql.binlog.CanalClient.process(CanalClient.java:151) [Subscriber.jar:na] at cn.ishow.db.mysql.binlog.CanalClient.access$100(CanalClient.java:27) [Subscriber.jar:na] at cn.ishow.db.mysql.binlog.CanalClient$2.run(CanalClient.java:101) [Subscriber.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_74] Caused by: java.net.ConnectException: Connection refused: connect at sun.nio.ch.Net.connect0(Native Method) ~[na:1.8.0_74] at sun.nio.ch.Net.connect(Net.java:454) ~[na:1.8.0_74] at sun.nio.ch.Net.connect(Net.java:446) ~[na:1.8.0_74] at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[na:1.8.0_74] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.doConnect(SimpleCanalConnector.java:132) ~[canal.client.jar:na] ... 5 common frames omitted sorry Own question What happened to me? What problem I have encountered suddenly has this problem @AnakinSkyW
237,About data loss after canal embedded mode restart Great god I used the canal 1 0 22 version with reference to Otter to write a data subscription platform due to the current canal event The store only has the memory mode. When the data in the eventstore has not been available for consumption, the canal will reload the data from the last parsed location. This causes the data loss of the eventstore not consumed before the restart. Is there any good problem for this problem? Processing ideas or new version planning The data will not be lost. The next time you restart, you will re-bind the binlog with the location of your last ack of ack, including the data that was not consumed in the previous memory. Looked at the code ack just emptied the data in the store did not see write zookeeper, read the zookeeper after the next reboot The binlog location of the parse node causes data loss You have to choose the default instance xml mode to ensure that you can see the adminGuide wiki Probably found the problem I used in the initialization of the CanalLogPositionManager ZOOKEEPER mode does not resolve MetaLogPosition. My MetaManager is also MEMORY mode and does not write MetaLogPosition Thank you
236,Why did you run well and suddenly gave an error? not find first log file name in binary log index file One-way synchronization that also affects one-way synchronization Specifies that the site sync looks normal but the data is always out of sync I am also running here. Suddenly reported this canal 1.0.22 rds I remember that canal1 0 22 already supports the problem of automatic binlog switching when rds myql is switched between active and standby, but there will still be this problem. ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) [canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210) [canal.parse-1.0.22.jar:na] at java.lang.Thread.run(Thread.java:744) [na:1.7.0_45] Rds will delete the binlog operation. If the consumption is delayed, it is easy to see the location is absent. How can I handle this?
235,Multi-table data aggregation The id of the source library table appears Will cover Destination table library id Lead to multiple records What is the solution? Source and target set different id segments This is very unreasonable Set different id segments Is the source 1 100 target 100 200? I have set up to map id to mid Then there will be more records Instead of covering where the values ​​are the same Can&#39;t integrate in terms of aggregation? 2016-12-20 22:45 GMT+08:00 agapple <notifications@github.com>: > Source and target set different id segments > > — > You are receiving this because you authored the thread. > Reply to this email directly view it on GitHub > <https://github.com/alibaba/canal/issues/235#issuecomment-268260730> or mute > the thread > <https://github.com/notifications/unsubscribe-auth/AILd__h0spaA-5aLl4sX14smS1DpcYa6ks5rJ-obgaJpZM4LGToY> > . > In the distributed scenario, the ids of different partitions are required to be unique or fall on a library. How to judge whether it is a record?
234,Merge pull request #1 from alibaba/master full pull [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=234) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=234) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/alibaba/canal?pullRequest=234) it.</sub> Mainly changed, did not understand the submitted content @agapple This is a merged build from the upstream on the old version. git no-ff In a node, the contents are not turned off.
233,Canal based on timestamp site Canal version 1 0 22 During the test, based on the timestamp location information, the data is re-extracted and the error is continuously reported in the log. What is the reason for how to solve the error message as follows: 2016-12-05 19:24:29.049 [destination = oms_orders address = /172.172.230.36:3307 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position ::1480780800000 2016-12-05 19:24:49.062 [destination = oms_orders address = /172.172.230.36:3307 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - ERROR ## findAsPerTimestampInSpecificLogFile has an error java.io.IOException: KILL DUMP 19915370 failure:java.io.IOException: ErrorPacket [errorNumber=1094 fieldCount=-1 message=Unknown thread id: 19915370 sqlState=HY000 sqlStateMarker=#] with command: KILL CONNECTION 19915370 at com.alibaba.otter.canal.parse.driver.mysql.MysqlUpdateExecutor.update(MysqlUpdateExecutor.java:49) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.disconnect(MysqlConnector.java:104) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.reconnect(MysqlConnector.java:82) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.reconnect(MysqlConnection.java:56) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findAsPerTimestampInSpecificLogFile(MysqlEventParser.java:674) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findByStartTimeStamp(MysqlEventParser.java:519) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPositionInternal(MysqlEventParser.java:356) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPosition(MysqlEventParser.java:313) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:162) at java.lang.Thread.run(Thread.java:745) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.disconnect(MysqlConnector.java:106) ~[canal.parse.driver-1.0.22.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.reconnect(MysqlConnector.java:82) ~[canal.parse.driver-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.reconnect(MysqlConnection.java:56) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findAsPerTimestampInSpecificLogFile(MysqlEventParser.java:674) [canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findByStartTimeStamp(MysqlEventParser.java:519) [canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPositionInternal(MysqlEventParser.java:356) [canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPosition(MysqlEventParser.java:313) [canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:162) [canal.parse-1.0.22.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101] ![image](https://cloud.githubusercontent.com/assets/7370509/20909493/1b10a274-bb97-11e6-8098-6fa85a4ab56e.png) The code in the 1 0 22 version I used found that the exception is not logging. Modified to try using the current master to make a package Re-compilation based on master replaced the mysqlconnector class under the driver does not affect the extraction process.
232,ErrorPacket [errorNumber=1094 fieldCount=-1 message=Unknown thread id: 335944433 sqlState=HY000 sqlStateMarker=#] Canal version 1 0 22 Mysql version Alibaba Cloud rds mysql5.6.16-log Report the following exception 2016-12-05 10:05:19.828 [destination = example address = xxx.mysql.rds.aliyuncs.com/xxx:3306 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Unexpected End Stream at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) [canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210) [canal.parse-1.0.22.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_111] 2016-12-05 10:05:19.837 [destination = example address = xxx.mysql.rds.aliyuncs.com/xxx:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address xxx.mysql.rds.aliyuncs.com/xxx:3306 has an error retrying. caused by java.io.IOException: Unexpected End Stream at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210) ~[canal.parse-1.0.22.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_111] 2016-12-05 10:05:19.841 [destination = example address = xxx.mysql.rds.aliyuncs.com/xxx:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: Unexpected End Stream at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210) at java.lang.Thread.run(Thread.java:745) ] 2016-12-05 10:05:19.858 [destination = example address = xxx.mysql.rds.aliyuncs.com/xxx:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - disconnect address xxx.mysql.rds.aliyuncs.com/xxx:3306 has an error retrying. caused by java.io.IOException: KILL DUMP 335944433 failure:java.io.IOException: with command: KILL CONNECTION 335944433 at com.alibaba.otter.canal.parse.driver.mysql.MysqlUpdateExecutor.update(MysqlUpdateExecutor.java:49)ErrorPacket [errorNumber=1094 fieldCount=-1 message=Unknown thread id: 335944433 sqlState=HY000 sqlStateMarker=#] at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.disconnect(MysqlConnector.java:104) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.disconnect(MysqlConnection.java:60) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:242) at java.lang.Thread.run(Thread.java:745) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.disconnect(MysqlConnector.java:106) ~[canal.parse.driver-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.disconnect(MysqlConnection.java:60) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:242) ~[canal.parse-1.0.22.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_111] 2016-12-05 10:05:33.210 [destination = example address = xxx.mysql.rds.aliyuncs.com/xxx:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position 2016-12-05 10:05:33.249 [destination = example address = xxx.mysql.rds.aliyuncs.com/xxx:3306 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) [canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210) [canal.parse-1.0.22.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_111] 2016-12-05 10:05:33.250 [destination = example address = xxx.mysql.rds.aliyuncs.com/xxx:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address xxx.mysql.rds.aliyuncs.com/xxx:3306 has an error retrying. caused by java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210) ~[canal.parse-1.0.22.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_111] 2016-12-05 10:05:33.250 [destination = example address = xxx.mysql.rds.aliyuncs.com/xxx:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210) at java.lang.Thread.run(Thread.java:745) ] destination:example[java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file Is a java io IOException Unexpected End Send KILL after Stream DUMP 335944433 after that Then received [errorNumber=1094 fieldCount=-1 message=Unknown thread id: 335944433 sqlState=HY000 sqlStateMarker reconnects and appears. not find first log file name in binary log index File want to ask if it is the java io IOException Unexpected End Caused by Stream Could not find first log file name in binary log index File This is the main cause
231,fixed issue #227 fixed Mysql5.1.73 throw NullPointerException `select @master_binlog_checksum` Return null in mysql5 1 version Will trigger NullPointerException [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=231) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=231) before we can accept your contribution.<br/> tks
230,Canal listening problem The canal service I set up to listen to a database tjxt busi cs table but the program is connected to this can but can print out other databases tjxt busi ln table data changes two databases in the same machine mysql This is my canal Instance configuration information ``` canal.instance.mysql.slaveId = 2283306 canal.instance.master.address = 192.168.10.100:3306 canal.instance.master.journal.name = canal.instance.master.position = canal.instance.master.timestamp = #canal.instance.standby.address = #canal.instance.standby.journal.name = #canal.instance.standby.position = #canal.instance.standby.timestamp = canal.instance.dbUsername = canal canal.instance.dbPassword = canal canal.instance.defaultDatabaseName = tjxt-busi-cs canal.instance.connectionCharset = UTF-8 canal.instance.filter.regex = .*\\..* canal.instance.filter.black.regex = ``` I connected this caanl print binlog information in my program and actually listened to the binlog log of other databases. c.f.s.c.listener.BusiAnscWatcherThread - ================> binlog[mysqld-bin.000015:687343087] name[**tjxt-busi-ln** busi_wife_b_value] eventType : INSERT I don&#39;t know why canal.instance.defaultDatabaseName = tjxt-busi-cs This property is not used to specify that canal only listens to this database. You think that binlog is for a DB instance and not for a database. @AllenRay Well, I know the principle. Thank you, I think about solving it in another way.
229,java.io.IOException: Unexpected End Stream exception Canal version 1.0.16 The sporadic report is as follows 2016-11-30 12:56:17.588 [destination = example address = /xxx:3306 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Unexpected End Stream at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) ~[canal.parse-1.0.16.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:78) ~[canal.parse-1.0.16.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:116) [canal.parse-1.0.16.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:208) [canal.parse-1.0.16.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.7.0_67] 2016-11-30 12:56:17.588 [destination = example address = /xxx:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /172.16.81.97:23306 has an error retrying. caused by java.io.IOException: Unexpected End Stream at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) ~[canal.parse-1.0.16.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:78) ~[canal.parse-1.0.16.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:116) ~[canal.parse-1.0.16.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:208) ~[canal.parse-1.0.16.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.7.0_67] 2016-11-30 12:56:17.588 [destination = example address = /xxx:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: Unexpected End Stream at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:78) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:116) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:208) at java.lang.Thread.run(Thread.java:745) ] And with the loss of binlog sporadic time, there is no regularity, have you encountered this problem? Several suggestions 1. Upgrade the canal version to the latest version 2. Turn on heartbeat SQL Timed heartbeat Ensure that TCP links are consistent with packet delivery I also encountered this problem. The specific scenario is a table with more than 600,000 records. There is a batch update operation. The update statement has no where condition. The binlog based on the row mode is especially reported every time it is executed to 30w data. I am using it now. The latest version and the addition of heartbeat sql still have the same problem, there are no other solutions. Attached to the exception log 2017-03-03 09:14:29.255 [destination = shuangshi address = /10.3.250.93:3306 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Unexpected End Stream at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) ~[canal.parse-1.0.23.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:78) ~[canal.parse-1.0.23.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:121) [canal.parse-1.0.23.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) [canal.parse-1.0.23.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_111] 2017-03-03 09:14:29.255 [destination = shuangshi address = /10.3.250.93:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address 10.3.250.93/10.3.250.93:3306 has an error retrying. caused by java.io.IOException: Unexpected End Stream at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) ~[canal.parse-1.0.23.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:78) ~[canal.parse-1.0.23.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:121) ~[canal.parse-1.0.23.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) ~[canal.parse-1.0.23.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_111] 2017-03-03 09:14:29.256 [destination = shuangshi address = /10.3.250.93:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:shuangshi[java.io.IOException: Unexpected End Stream at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:78) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:121) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) at java.lang.Thread.run(Thread.java:745) ] Tested it seems that the client processing time is too long, if the client processing is not fast, so my current solution is to receive the message in the client and process the asynchronous processing through the file queue but this brings a trouble Is the logic of failure to be implemented once on the client side. I have a practical solution to the problems of my predecessors from time to time. Thank you. I don&#39;t know if you solved it. I have the same problem data piled up in the master and I get an error. @tutulvlv
228,Start startup sh error Downloaded master version 1 0 23 SNAPSHOT Start sh according to QuickStart bin/startup.sh The error is as follows : not found 2: startup.sh: startup.sh: 4: startup.sh: Syntax error: word unexpected (expecting "in") Estimated to be incompatible with all the shell environments in your environment, so you can test and fix it by submitting a PR. I also encountered the same problem, but the last check was caused by a newline. Download a dos2unix conversion and you&#39;re done. @keyganker My reason is not caused by incompatibility of the shell environment. My linux version is ubuntu 4 4 0 36 generic This version comes with a sh estimate that is different from the official one. Change the command to bash Stop sh is no problem :set ff=unix
227,After the canal repository code is compiled Does not support mysql5 1 problem hi Canal development team I compile code using the master branch in the canal repository. Mysql Server version: 5.1.73 Error message > ERROR: [destination = example address = test/192.168.10.16:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlConnection - java.io.IOException: ErrorPacket [errorNumber=1193 fieldCount=-1 message=Unknown system variable 'binlog_checksum' sqlState=HY000 sqlStateMarker=#] with command: set @master_binlog_checksum= @@global.binlog_checksum changed file:https://github.com/alibaba/canal/commit/0c9eecd758a8cba0f218a121f83d3e8f01c4dee8 ![image](https://cloud.githubusercontent.com/assets/9983697/20620006/5bffa586-b332-11e6-80b6-3dd104c87c00.png) This answer says that the binlog_checksum variable is only supported from mysql5 6 http://ba stackexchange com questions 66226 mysql slave database wont start if it is like this Time means that canal will not support mysql5 6 or later. thx Can increase try Lock mechanism ignores exception settings Mysql Server version: 5.1.73 [MysqlConnection.java#L282](https://github.com/alibaba/canal/blob/3f38ad6f4927e1d3bcf43faca8501df1e706405c/parse/src/main/java/com/alibaba/otter/canal/parse/inbound/mysql/MysqlConnection.java#L282 ) Throw NullPointerException Encountered the same problem with the old version of 5 1 73 The ultimate solution upstairs Try using the current trunk
226,Cleaned up The main mysql master binlog log and then restart the canal canal Server startup error I cleaned up The main mysql master&#39;s binlog log and then restarted the canal canal Server startup error ``` 2016-11-22 15:41:55.513 [destination = tj-172.18.100.97 address = /172.18.100.97:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - disconnect address /172.18.100.97:3306 has an error retrying. caused by java.io.IOException: KILL DUMP 1299 failure:java.io.IOException: ErrorPacket [errorNumber=1094 fieldCount=-1 message=Unknown thread id: 1299 sqlState=HY000 sqlStateMarker=#] with command: KILL CONNECTION 1299 at com.alibaba.otter.canal.parse.driver.mysql.MysqlUpdateExecutor.update(MysqlUpdateExecutor.java:49) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.disconnect(MysqlConnector.java:104) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.disconnect(MysqlConnection.java:60) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:242) at java.lang.Thread.run(Thread.java:745) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.disconnect(MysqlConnector.java:106) ~[canal.parse.driver-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.disconnect(MysqlConnection.java:60) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:242) ~[canal.parse-1.0.22.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.7.0_79] ``` What is the reason for this? I mainly cleaned the binlog log of the main mysql from the beginning of the binlog, but the meta dat recorded it is not so the canal is reported incorrectly cleaned up Instance meta dat The file is fine.
225,canal The server will run for a period of time and will report the following error. Do not know how to solve it. My program starts to connect canal Report an error something goes wrong with reason: something goes wrong with channel com.alibaba.otter.canal.server.exception.CanalServerException: ack error clientId:1001 batchId:47 is not exist please check How to solve the canal server, I also restarted and ran for a while, the program started or will continue to report an error. Below is my code ``` while (true) { Message message = connector.getWithoutAck(1000); // Get the specified amount of data long batchId = message.getId(); int size = message.getEntries().size(); if (batchId == -1 || size == 0) { try { Thread.sleep(1000); } catch (InterruptedException e) {} } else { long beginTime = DateUtils.nowDateToTimestamp(); Logic 1 first determines the insertion and update 2, if the state is completed 4, then the data is saved and saved. try { switchHandle(message.getEntries()); connector.ack(batchId); // Submit confirmation } catch (Exception e) { System out println synchronization error needs to be rolled back + e.getMessage()); Logger error sync error needs to be rolled back e); connector.rollback(batchId); continue; } long endTime = DateUtils.nowDateToTimestamp(); Logger info This synchronization time is + (endTime - beginTime) + " ms"); } } ``` This problem is suspected to be an instance restart on the server side, causing the batchId in memory to be lost. Check the server&#39;s log @agapple It’s really this problem. Scany scan is enabled on the server side I also encountered this problem, how can I solve it? Client adds retry or server closes scan false @SHUlibee One of the two ideas is to confirm whether it is canal The server has been restarted to confirm whether the mysql binlog has been cleaned up. Can the meta data of the canal be cleaned up. At the same time, if optimization is done, you can add the retry mechanism to the client side and shut down the canal as the agepple said. Server scan is set to false The same wrong solution I am switching the version of kakfa here. Modified the address of kafka and zookeeper I have been reporting this error since then. Ask how to deal with ``` 2018-05-29 11:00:08.799 [Thread-4] WARN c.alibaba.otter.canal.client.impl.ClusterCanalConnector - something goes wrong when rollbacking data from server:null com.alibaba.otter.canal.protocol.exception.CanalClientException: java.io.IOException: Broken pipe at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.rollback(SimpleCanalConnector.java:342) at com.alibaba.otter.canal.client.impl.ClusterCanalConnector.rollback(ClusterCanalConnector.java:216) at com.xiaomatech.dbevent.canal.dbkafka.AbstractCanalClient.process(AbstractCanalClient.java:203) at com.xiaomatech.dbevent.canal.dbkafka.AbstractCanalClient$2.run(AbstractCanalClient.java:99) at java.lang.Thread.run(Thread.java:745) Caused by: java.io.IOException: Broken pipe at sun.nio.ch.FileDispatcherImpl.write0(Native Method) at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47) at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93) at sun.nio.ch.IOUtil.write(IOUtil.java:65) at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:470) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.writeWithHeader(SimpleCanalConnector.java:359) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.rollback(SimpleCanalConnector.java:336) ... 4 more 2018-05-29 11:00:13.869 [Thread-4] INFO c.alibaba.otter.canal.client.impl.ClusterCanalConnector - restart the connector for next round retry. 2018-05-29 11:00:13.902 [Thread-4] WARN c.alibaba.otter.canal.client.impl.ClusterCanalConnector - something goes wrong when getWithoutAck data from server:null com.alibaba.otter.canal.protocol.exception.CanalClientException: something goes wrong with reason: something goes wrong with channel:[id: 0x513ca1d3 /10.20.105.201:39029 => /10.20.105.201:11112] exception=com.alibaba.otter.canal.server.exception.CanalServerException: rollback error clientId:1001 batchId:3 is not exist please check at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.receiveMessages(SimpleCanalConnector.java:302) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:279) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:252) at com.alibaba.otter.canal.client.impl.ClusterCanalConnector.getWithoutAck(ClusterCanalConnector.java:180) at com.xiaomatech.dbevent.canal.dbkafka.AbstractCanalClient.process(AbstractCanalClient.java:186) at com.xiaomatech.dbevent.canal.dbkafka.AbstractCanalClient$2.run(AbstractCanalClient.java:99) at java.lang.Thread.run(Thread.java:745) ``` @agapple The same error, turn off scan false.
224,Client does not get update event The binlog corresponding to the update operation is as follows ![image](https://cloud.githubusercontent.com/assets/3926181/20453252/ab62af26-ae58-11e6-8c27-2c14ea9a441b.png) Connector getWithoutAck batchSize can only get TRANSACTIONBEGIN and TRANSACTIONEND as shown ![image](https://cloud.githubusercontent.com/assets/3926181/20453255/c911f216-ae58-11e6-90a4-e77bfee52348.png) Db configuration is as follows ``` [mysqld] pid-file = /var/run/mysqld/mysqld.pid socket = /var/run/mysqld/mysqld.sock datadir = /data/mysql log-error = /var/log/mysql/error.log # By default we only accept connections from localhost #bind-address = 127.0.0.1 # Disabling symbolic-links is recommended to prevent assorted security risks symbolic-links=0 Custom parameter explicit_defaults_for_timestamp lower_case_table_names=1 max_connections = 2000 # Recommended in standard MySQL setup sql_mode=NO_ENGINE_SUBSTITUTION STRICT_TRANS_TABLES innodb_open_files=65535 max_connect_errors=3500 max_connections=3100 max_user_connections=3000 open_files_limit=65535 #connect_timeout=30 #interactive_timeout=200 #wait_timeout=200 log-bin=mysql-bin Add this line and ok binlog-format=ROW Select row mode server_id=1 Configuring mysql Replaction needs to be defined and cannot be repeated with canal&#39;s slaveId #skip-name-resolve innodb_log_file_size=1000M innodb_buffer_pool_size=2g innodb_log_buffer_size=10M innodb_sync_array_size=16 thread_cache_size=256 max_binlog_size=500M binlog_cache_size=1M key_buffer_size=8M Turn on slow query Slow_query_log value is 1 or on means open to 0 or off is off slow_query_log=on Set where the slow query log is placed slow_query_log_file=mysql-slow Set the length of sql execution time to slow query long_query_time=2 Indicates that the sql query that does not use the index will also be recorded. #log-queries-not-using-indexes Support emoji character-set-server=utf8mb4 collation-server=utf8mb4_unicode_ci ``` It is verified that can be obtained when canal is deployed to the same machine in mysql. ROWDATA事件 canal instance master address = use 127 0 0 1 3306 or The machine real address 3306 can be What to do if you use Alibaba Cloud rds - No such thing need to be deployed to the same mysql Check the filter conditions will be subject to the client&#39;s submission The situation in the red box is very strange. Checked the environment configuration of the two canal is the same The way the client is the same test is to switch the canal address through the same client. Debug to see the filter conditions on the server side can try to use RDS Filter conditions are the same for both environments ``` #canal.instance.filter.regex = test\\..* This is before canal.instance.filter.regex = Switching to this remote canal can work but the canal deployed by the machine will be affected # table black regex canal.instance.filter.black.regex = ``` Such a format Remove the filter condition from the blank remote deployment canal to get the normal Event, but the canal with mysql is not affected by this. Further comparison test found that the problem of canal instance filter regex is summarized as follows > 1 does not hit the canal instance filter regex SQL operation regardless of the canal deployment where the client can only get TRANSACTIONBEGIN and TRANSACTIONEND Why do you get these two events without hitting? Also very puzzled > 2 Deployed in DB native and remote canal for regex parsing is different > > 2 1 and DB deploy the canal regx test and Test can get row event > > 2 2 and DB remote deployment of cancal Regx test can&#39;t get row in this way event but Regx test works fine The above phenomenon is the same in both rds and mysql. Otter has a filter expression when configuring canal but where I filled it out test\\\\..* This format Synchronizing data seems to be no problem otter Node node and DB are not the same machine This expression also depends on the parameters specified in your client call subscriber.
223,Canal instance master configuration is not in effect Hello, please ask I configured the parameter conf example instance properties canal.instance.master.journal.name canal.instance.master.position canal.instance.master.timestamp operating ./bin/stop.sh ./bin/start.sh Not effective For example, I configured Timestamp 1479370130 The update before this timestamp will also get the message after the update does not know what the reason is. Timestamp positioning defaults back to 60 seconds
222,RDS mysql.mysql Ha_health_check table could not be found - When the canal starts, it reports mysql mysql Ha_health_check table can not find this error - Go to binlog to see that events do have a record of ha_health_check this table. - However through the show Tables and other sql statements can not find this table Estimated to be a table permission problem 2016-11-14 10:07 GMT+08:00 iadmirezhe notifications@github.com: > - > > When the canal starts, it reports mysql mysql Ha_health_check table can not find this error > - > > Go to binlog to see that events do have a record of ha_health_check this table. > - > > However through the show Tables and other sql statements can not find this table > > — > You are receiving this because you are subscribed to this thread. > Reply to this email directly view it on GitHub > https://github.com/alibaba/canal/issues/222 or mute the thread > https://github.com/notifications/unsubscribe-auth/AAy8txbjG_zt6LSby8EsUa3SQeXKXCOmks5q98JugaJpZM4Kw46d > . https://github.com/alibaba/canal/commit/96dca0b9e6b6dcc12fc439d2f17d85a4656485df Special support for the RDS of Alibaba Cloud Is this not a filter? Yes, the default recognition now adds filtering to RDS. Well added filtering to filter mysql
221,canal The client obtains the field value as an empty string itself should be null I found a problem in the process of using canal. I inserted a piece of data on the master machine. Like insert into test1 values(17 NULL); Get canal parsing results The value of column is an empty string The method used is String columnValue=column.getValue(); This is a problem with the canal framework. Or my own method of getting the field value is wrong. The column object in column getValue is import com alibaba otter canal protocol CanalEntry Column对象 Column isNull Use this to determine if it is null
220,canal Entry in the client does not include the operation user I want to ask a question canal The reason that the Entry in the client does not include the operation user is because there is no operator in the binlog itself. User or canal not qe to package parsing mysql The specific content contained in the binlog has relevant information. It is not complete on the Internet. If I want to try to increase it. Personally feel that this part of the information should not be placed in the binlog for the backup is completely redundant mysql should have permission to manage the relevant processing operation log, etc. Not in the binlog
219,How Canal supports Alibaba Cloud RDS subscription Currently using canal to subscribe to the incremental change data of Ali RDS, the following error occurs. com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.collect.ComputationException: com.alibaba.otter.canal.parse.exception.CanalParseException: fetch failed by table meta:`mysql`.`ha_health_check` Caused by: com.google.common.collect.ComputationException: com.alibaba.otter.canal.parse.exception.CanalParseException: fetch failed by table meta:`mysql`.`ha_health_check` Already solved Alibaba Cloud RDS account is missing permission to read mysql ha_health_check This should be a heartbeat table that Ali used to monitor. Upgrade the account to a high-rights account and give the canal account SELECT. Mysql ha_health_check permission is no longer reported error resolution One problem derived from this is that it seems that when parsing a row fails and throwing an exception will affect subsequent normal line parsing, causing the canal client to fail to receive the data. This is my guess and did not peruse the source code if the author saw this Article issue can pay attention to it Your guess is right, but you don&#39;t have to get so many permissions. You can filter that table off. Yes, you can set the whitelist table for the blacklist table. The binlog protocol only looks at one header and judges the miss. https://github.com/alibaba/canal/commit/96dca0b9e6b6dcc12fc439d2f17d85a4656485df Special support for the RDS of Alibaba Cloud
218,Update twice for consumption Hello question is as follows Recurring step + Existing two libraries USERS_01 USERS_02 need to subscribe to the DML of the tables users in the two libraries + Canal properties profile canal.id= 1 canal.ip=127.0.0.1 canal.port= 11111 canal.zkServers=localhost:2181 canal.destinations= users_building canal.instance.global.spring.xml = classpath:spring/group-instance.xml # Other configuration default + group-instance.xml： <!-- Other configurations remain the same --> <bean id="eventParser1" class="com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser"> <property name="eventFilter"> <bean class="com.alibaba.otter.canal.filter.aviater.AviaterRegexFilter" > <constructor-arg index="0" value="${canal.instance.master1.filter.regex:.*\..*}" /> </bean> </property> <!-- Other configurations remain the same --> </bean> <bean id="eventParser2" class="com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser"> <property name="eventFilter"> <bean class="com.alibaba.otter.canal.filter.aviater.AviaterRegexFilter" > <constructor-arg index="0" value="${canal.instance.master2.filter.regex:.*\..*}" /> </bean> </property> <!-- Other configurations remain the same --> </bean> + users_building instance properties配置  canal.instance.mysql.slaveId = 1000 canal.instance.master1.address = 127.0.0.1:3306 canal.instance.master1.journal.name = canal.instance.master1.position = canal.instance.master1.timestamp = canal.instance.master2.address = 127.0.0.1:3306 canal.instance.master2.journal.name = canal.instance.master2.position = canal.instance.master2.timestamp = canal.instance.master1.filter.regex = USERS_01.users # Canal instance master1 filter regex of eventParser1 corresponding to group instance xml canal.instance.master2.filter.regex = USERS_02.users # Canal instance master2 filter regex of eventParser2 corresponding to group instance xml # Other configuration default + Test code connector = CanalConnectors.newClusterConnector("127.0.0.1:2181" "users_building" "" ""); // The other code is ClusterCanalClientTest + When in mysql Perform an update operation update in terminal users set sex = 1 where id = 1 console will have two consecutive subscription messages like ![users_update](https://cloud.githubusercontent.com/assets/8662695/19843667/b9e8fc24-9f5f-11e6-910f-c2755de0461d.png) Is there any configuration problem or other questions? Thank you. The test found that only the client can set the filter expression to filter the deployer settings. understood Set the filter in the client subcribe in the demo
217,canal The client receives the blob field and parses it into Google Protocol Buffer protocol failed The database table contains Google. Protocol The Buffer protocol, however, resolves the error after receiving the canal. I would like to ask if the canal does not support non-basic type resolution. Blob type is new String ISO 8859 1 The processed string can be restored by string getBytes ISO 8859 1 Thank you for being resolved
216,How can support sql server At present, we have a project to monitor the binlog changes to update the index before the online customer found that the database is using sql Server canal does not currently support sql Server does not know if the author has ideas to support sql server？ Online looking for sql Server has a cdc mode https msdn microsoft com en us library cc645937 aspx Open this mode to track change records For sql Server did not study sorry
215,Canal server1 0 22 based on mariadb10 1 14 test HA function BUG Test description canalserver1 0 22 based on mariadb10 1 14 HA function is not good to deploy two canaserver When one of the servers hangs, zookeeper has switched to the new canalserver but the new canalserver obtained binlogname is incorrect, resulting in reading mariadb&#39;s binlog error The name of the binlog in mariadb (canal@127.0.0.1) [canal_test]> show binary logs; +------------------+-----------+ | Log_name | File_size | +------------------+-----------+ | mysql-bin.000001 | 25487 | | mysql-bin.000002 | 358 | | mysql-bin.000003 | 1518 | | mysql-bin.000004 | 29137967 | +------------------+-----------+ The binlogname binlog mysql bin 000004 Nk U 0081 29138083 obtained by canalserver has garbled characters Solution Modify canalserver1 0 22 source com alibaba otter canal protocol CanalEntry to filter the obtained binlogname add the following code 【public String filterLogFileName(String logfileName) { System out println starts filtering logfileName logfileName Pattern p = Pattern.compile("(mysql-bin\.[0-9]_)._"); Matcher m = p.matcher(logfileName); while(m.find()){ if(m.groupCount() >= 1) { System out println filter logfileName to m group 1 return m.group(1); } } return logfileName; } 】 Modify code 【public String getLogfileName() { java.lang.Object ref = logfileName_; if (ref instanceof String) { return filterLogFileName((String) ref); } else { com.google.protobuf.ByteString bs = (com.google.protobuf.ByteString) ref; String s = bs.toStringUtf8(); if (com.google.protobuf.Internal.isValidUtf8(bs)) { logfileName_ = s; } return filterLogFileName(s); } } 】 Repackaging test verifies that the HA function can be automatically switched but there is a problem of repeated consumption by the client consumer when the canalserver is switched. But this problem did not occur in the test in mysql5 6 Now there is a verified version of the test mariadb10 1 14? Look at the source code seems to be read from the protocolbuffer when there is a problem with the binglogname. The binlog protocol format is not very well understood. Is this a protocol format problem defined by the protocolbuffer? The actual code analysis appears in com alibaba otter canal parse inbound mysql dbsync int netlen = getUint24 PACKET_LEN_OFFSET code obtained by the length of the package is inconsistent caused by the maraidb and mysql get the length of 47 and 43. Mariadb gets 4 bytes more, causing subsequent processing logfilename to read more than 4 bytes. Hello, ask the author what causes the inconsistency of netlen acquisition. The last 4 digits may be checksum. You try to find mariadb to close the mariadb binlog. Checksum mechanism to see ps. Previously, there is support for mysql5 6 checksum. Maybe the mariadb protocol is a bit different. It is not completely compatible. First, help check if it is a checksum problem. if (FormatDescriptionLogEvent.versionProduct(versionSplit) >= FormatDescriptionLogEvent.checksumVersionProduct) { Before the canal code is limited to 5 6 1 version number will open the checksum check. This is the previous mysql5 6 mechanism mariab estimated to have different behavior Has been identified as a checksum problem but the test 1 0 22 stable version to check the source code is also looking at the stable code of 1 0 22 later saw canal1 0 23 snapshot version of the code has fixed this problem in the mariadb10 1 14 verification by thank you ps: xiaolinziemail@163.com Sender agapple Sending time 2016-11-03 13:49 Recipient alibaba/canal Cc linwenxue; Author theme Re: [alibaba/canal] Canal server1 0 22 based on mariadb10 1 14 test HA function BUG (#215) The last 4 digits may be checksum. You try to find mariadb to close the mariadb binlog. Checksum mechanism to see ps. Previously, there is support for mysql5 6 checksum. Maybe the mariadb protocol is a bit different. It is not completely compatible. First, help check if it is a checksum problem. — You are receiving this because you authored the thread. Reply to this email directly view it on GitHub or mute the thread.
214,canal How can a server listen to different tables of a database? canal How can a server monitor a different table of a database to copy multiple copies of the example folder and then configure different connection table information in the configuration file. The authors have trouble to see if there is a good solution. Matching regularity for writing multiple tables in one configuration
213,Fix issue #212 1 BinLogFileQueue&#39;s listBinlogFiles method for more stringent validation of the binlog file name must end with a number 2 The following bug is resolved in the LocalBinLogConnection class. 2 1 Add the code decoder handle LogEvent FORMAT_DESCRIPTION_EVENT in the second dump method Solve the QueryLogEvent event getQuery statement garbled problem BEGIN followed by 4 bytes The occurrence of garbled characters does not enter the if statement, resulting in inaccurate binlog sites found. 2 2 The first decoded code in the first dump method is correct. Unlike DirectLogFetcher, you need to construct a sub-loop to traverse. 2 3 The first dump method sets the first event of each binlog file to RotateLogEvent. Otherwise, all event binlogFileName will be the default mysql bin 000001. After this change, the behavior of DirectLogFetcher is matched with the latter in each binlogfile. The first event is RotateLogEvent 3 LocalBinlogEventParser adds metadata processing related code, if all the events are not listed, basically can&#39;t use [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=213) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=213) before we can accept your contribution.<br/><hr/>**lubiao** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user). Supplementary repair 206 Corresponding class file SessionHandler CanalServerWithEmbedded MysqlEventParser About 206, I didn’t see your reply later, but it’s definitely a problem. We’ve been online two or three times online. The main reason for this problem is that when there is no running node at a certain point in time, the client may get the future standby ip through the nextNode method of ClusterNodeAccessStrategy. If this is the case, it may appear on the standby machine that subscribe succeeds and getWithoutAck fails. The scene then the client eventually switches to the host but if there is an instance of the instance switch to the standby, then there is a problem because the standby machine&#39;s metaManger has been initialized and the previous position is cached. 205 did not fix our internal direct disable lazy function so the code is not submitted tks
212,About the bug of binlog offline consumption The original intention of designing binlog offline consumption LocalBinlogEventParser is that there is no large-scale use of this function in Ali. We found many problems in the process of trying to use 1 tableMetaCache is null when offline consumption The event constructed without metadata does not work like this. 2 event parsing also has bugs \* For example, the code in the dump method of LocalBinLogConnection is the correct code. \* The second dump method in LocalBinLogConnection should add decoder handle LogEvent FORMAT_DESCRIPTION_EVENT statement or QueryLogEvent Event getQuery returns a string with garbled characters \* The local type can not get the ROTATE_EVENT type event, the binlogFileName of the constructed event is always mysql bin 000001 LocalBinlogEventParser is mainly used for internal re-use of log retransmission. Someone has submitted a PR solution
211,The canal client thinks that insert update delete is a DDL statement. Such as the question today made a test to connect the native mysql test insert statement etc. After the client example execution, the insert update delete and other statements are treated as DDL statements but the mysql on the development server is normal. Why is this? Estimated to be mysql5 6 rows query Log canal will not be converted to sql Have specific information and then reopen
210,Bad design MysqlEventParser Yes AbstractEventParser How can an abstract class implementation be a dependent self-class that can be displayed in a parent class? lock In addition, it is not clear whether the class lock of the abstract class can [![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/alibaba/canal?pullRequest=210) <br/>All committers have signed the CLA. tks
209,Fix a minor fault: Canal instance reload again after startup. Say the general scene The user will perform full initialization before using the incremental synchronization. During the initialization process, the user can interrupt and reinitialize at any time. We will delete the original instance config folder and regenerate the instance folder and the configuration file content is the same. A scan interval of 5s will be reloaded again because the initialization transaction is relatively large, so it will synchronize to the client some duplicate binlog logs are as follows 2016-10-02 13:28:35.103 [canal-instance-scan-0] INFO c.a.o.canal.deployer.monitor.SpringInstanceConfigMonitor - auto notify start example successful. 2016-10-02 13:28:40.138 [canal-instance-scan-0] INFO c.a.o.canal.deployer.monitor.SpringInstanceConfigMonitor - auto notify reload example successful. So after the success of the stop, should you remove the remove files in lastFiles? If there is something wrong, please advise. Thank you. You repeatedly delete and add files, there is indeed a problem tks
208,Mysql5 6 29 global binlog_checksum with single quotes causes master to quit Optimize binlogFormat judgment Mysql5 6 29 global binlog_checksum with single quotes causes master to quit Optimize binlogFormat judgment Assertion failed: (ret <= BINLOG_CHECKSUM_ALG_CRC32) function get_binlog_checksum_value_at_connect [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=208) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=208) before we can accept your contribution.<br/> tks
207,a log question First look at the log `2016-09-27 11:01:06:com.alibaba.otter.canal.client.impl.ClusterCanalConnector.subscribe(ClusterCanalConnector.java:110) - something goes wrong when subscribing from server:null com.alibaba.otter.canal.protocol.exception.CanalClientException: failed to subscribe with reason: something goes wrong with channel:[id: 0x111fe04e /10.204.12.79:47450 => /10.204.246.102:11111] exceptio n=com.alibaba.otter.canal.server.exception.CanalServerException: destination:ucarobd should start first ``` at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.subscribe(SimpleCanalConnector.java:203) at com.alibaba.otter.canal.client.impl.ClusterCanalConnector.subscribe(ClusterCanalConnector.java:106) at com.alibaba.otter.canal.client.impl.ClusterCanalConnector.subscribe(ClusterCanalConnector.java:99) at com.ucar.dep.exchange.binlog.AbstractCanalClient$ProcessThread.process(AbstractCanalClient.java:142) at com.ucar.dep.exchange.binlog.AbstractCanalClient$ProcessThread.run(AbstractCanalClient.java:134)` ``` By analyzing the code should start The exception information of first is not triggered when subscribing, but there is such a log in my log file. I analyzed the code for one day. I am puzzled. Can the author provide some ideas? This exception is usually caused by a restart on the server side. For example, scan true is turned on. Scanning to file changes automatically reloads the client and adds a retry mechanism. The reason for this anomaly is clear to me. I am surprised that by looking at the stack information is the exception triggered in the subscribe method. I checked the client and server code when there is no checkStart in the subscribe. That is, this should start First exception should not appear here The first subscription of the client triggers the server to start the operation.
206,SUBSCRIPTION has a bug Directly on the snippet of the SessionHandler `case SUBSCRIPTION: Sub sub = Sub.parseFrom(packet.getBody()); if (StringUtils.isNotEmpty(sub.getDestination()) && StringUtils.isNotEmpty(sub.getClientId())) { clientIdentity = new ClientIdentity(sub.getDestination() Short.valueOf(sub.getClientId()) sub.getFilter()); MDC.put("destination" clientIdentity.getDestination()); embeddedServer.subscribe(clientIdentity); ``` // Try to start if it has been started to ignore if (!embeddedServer.isStart(clientIdentity.getDestination())) { ServerRunningMonitor runningMonitor = ServerRunningMonitors.getRunningMonitor(clientIdentity.getDestination()); if (!runningMonitor.isStart()) { runningMonitor.start(); } } ctx.setAttachment(clientIdentity);// Set status data NettyUtils.ack(ctx.getChannel() null); } else { NettyUtils.error(401 MessageFormatter.format("destination or clientId is null" sub.toString()).getMessage() ctx.getChannel() null); } break; ``` ` The problem code is embeddedServer subscribe clientIdentity Instance has not started yet can execute subscribe Personally think that the correct logic should be to start the runningMonitor and then judge whether the instance is started and then execute the subscribe. According to the current code logic, the following problems will occur. 1 two canalservers and two canalclients are running, one of which has an instance name of xxx 2 Set xxx Active to false xxx will be released and trigger re-preemption 3 In the process of preemption, the registration of xxx cluster node and xxx running node on zk is time-interval. 4 Because there is a time interval, according to the logicCanalclient of ClusterNodeAccessStrategy, the ip obtained when connecting is not necessarily the IP of the canalserver that will seize the running node in the future. 5 Even if there is a problem with ip, you can still perform connect and subscribe operations, but when you get, you will get an error and reconnect. 6 Although reconnected, the dirty data position information generated by subscribe is cached to the metaManager. 7 After a few days, the canal in which the xxx is in the running state is closed. At this time, the slave canal takes over the instance, but the metaManager has already started the postion and positioned it a few days ago. 8 tragedy Remarks 1 Here the purpose of runningMonitor is to trigger lazy start but lazy start is not valid in HA mode. https://github.com/alibaba/canal/issues/205 2 It is strongly recommended to modify the ClusterNodeAccessStrategy HA mode to take the Address when only the running node will support lazy. According to the current design, in order to support the lazy increase complexity, the client&#39;s high-availability switching time is sometimes very long. I looked at the code embeddedServer subscribe clientIdentity subscription will start metaManager in advance to record the subscription information and then start CanalInstance when I get the data to verify whether I got the lock and started the service. Even if it is not lazy mode 1 When Active is set to false 2 or instance of autoboot when restarting There will be only the cluster node without the running node. At this time, the client will randomly take an ip under the cluster node to reconnect. This ip is probably the slave that is preempted. Then the subscriber will be triggered. Need to change two codes 1 SessionHander changes the order in which the runningmonitor is started and the subscribe is called. 2 The first line in the Subscribe method of CanalServerWithEmbedded adds the checkStart method. If the checkStart method is added, there is a risk that it will fail. Even if your SessionHander calls the start method and goes to the subscribe method, it may not be initialized. I remember that I was deliberately removed before. I don’t know if this has no checkstart verification.
205,The lazy startup of instance is not established in HA mode. Lazy startup of instance is not established in HA mode First look at the code of the CanalController for (Map.Entry<String InstanceConfig> entry : instanceConfigs.entrySet()) { final String destination = entry.getKey(); InstanceConfig config = entry.getValue(); // Create a work node for the destination if (!config.getLazy() && !embededCanalServer.isStart(destination)) { // HA mechanism starts ServerRunningMonitor runningMonitor = ServerRunningMonitors.getRunningMonitor(destination); if (!runningMonitor.isStart()) { runningMonitor.start(); } } ``` if (autoScan) { instanceConfigMonitors.get(config.getMode()).register(destination defaultAction); } } ``` ServerRunningMonitor will not start in lazy mode, then there will be no data running node under zookeeper&#39;s otter canal destinations destination cluster node. Look at the code for ClusterNodeAccessStrategy public SocketAddress nextNode() { if (runningAddress != null) {// If the service has been started, directly select the node that is currently working. return runningAddress; } else if (!currentAddress.isEmpty()) { // If there is no service that has already started, the service may be a lazy startup randomly selecting a trigger server to start return currentAddress.get(0);// Shuffle has been done before returning to the first node by default } else { throw new CanalClientException("no alive canal server"); } } Since the cluster and running nodes are not present, the client will enter the else branch. The client will always report an error and there is no way to trigger the laze startup. Personally think that lazy startup is a more ribbed feature that increases the complexity of the code and is not cost effective. This mode is completely shielded from our internal use and the code in the ClusterNodeAccessStrategy class is directly commented out. `else if (!currentAddress.isEmpty()) { // If there is no service that has already started, the service may be a lazy startup randomly selecting a trigger server to start return currentAddress.get(0);// Shuffle has been done before returning to the first node by default }` Can not find the running node, directly throwing the error to eliminate the unnecessary network interaction to make the switching time in the high-availability scene as small as possible One idea that comes to mind when the server is remodeled is Add one to the start method of ServerRunningMonitor boolean lazy parameter ServerRunningMonitor just detects changes in the running node. Regardless of whether lazy mode will create a cid node for each instance, the currentAddress here should be this list. Looked at the code below, the registration of the cid node is dependent on the ServerRunningMonitor start method. There is indeed a problem here. Improved method ServerRunningMonitor can increase the init method init method call to create cid init method is not controlled by lazy mode Yep This feasible Monitor life cycle and instance are not in a line Init is only responsible for starting monitor This problem is not so serious. Lazy is rarely used. Subscribe the question suggestion or timely processing we have encountered two problems in the production environment, maintenance, stop a canal server, another takeover instance and then the position is reset to a few days ago Position is 啥 will be the first time a few days ago subscrite got the history store data updated If there is new information then reopen the issue
204,replace into The statement of the operation filter is invalid. replace into table You can also receive messages without subscribing to this table client. The query statement is not strictly filtered by table
203,fixed issue # 201 [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=203) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=203) before we can accept your contribution.<br/><hr/>**zikaifeng** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user). tks
202,canal.instance.filter.regex Not valid after modification We hereby automatically add a new synchronization table through the program to modify the canal instance filter under the instance properties regex configuration item found that sometimes the canal instance filter regex modified does not take effect corresponding to the destination and reload View com alibaba otter canal deployer monitor SpringInstanceConfigMonitor found reload logic vulnerable > Since the configuration file information is not recorded at the first startup, the configuration file change will not be recognized until 5 seconds after the next scan. Naturally, the reload operation cannot be performed. Here is the patch I played. ``` Index: deployer/src/main/java/com/alibaba/otter/canal/deployer/monitor/SpringInstanceConfigMonitor.java IDEA additional info: Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP <+>UTF-8 =================================================================== --- deployer/src/main/java/com/alibaba/otter/canal/deployer/monitor/SpringInstanceConfigMonitor.java (revision f6cf88a6bd887afc56203bf9c4fa2aaccfd76e85) +++ deployer/src/main/java/com/alibaba/otter/canal/deployer/monitor/SpringInstanceConfigMonitor.java (revision ) @@ -49 6 +49 8 @@ private ScheduledExecutorService executor = Executors.newScheduledThreadPool(1 new NamedThreadFactory("canal-instance-scan")); + private volatile boolean isFirst = true; + public void start() { super.start(); Assert.notNull(rootConf "root conf dir is null!"); @@ -58 6 +60 7 @@ public void run() { try { scan(); + if (isFirst) isFirst = false; } catch (Throwable e) { logger.error("scan failed" e); } @@ -122 13 +125 18 @@ if (!actions.containsKey(destination) && instanceConfigs.length > 0) { // There is a legal instance property and it is started when it is added for the first time. - notifyStart(instanceDir destination); + notifyStart(instanceDir destination instanceConfigs); } else if (actions.containsKey(destination)) { // History has been launched if (instanceConfigs.length == 0) { // If there is no legal instance property notifyStop(destination); } else { InstanceConfigFiles lastFile = lastFiles.get(destination); + // TODO History started So the configuration file information must exist. + if (!isFirst && CollectionUtils.isEmpty(lastFile.getInstanceFiles())) { + logger.error("### [{}] is started but not found instance file info." destination); + } + boolean hasChanged = judgeFileChanged(instanceConfigs lastFile.getInstanceFiles()); // Notification change if (hasChanged) { @@ -161 10 +169 19 @@ } } - private void notifyStart(File instanceDir String destination) { + private void notifyStart(File instanceDir String destination File[] instanceConfigs) { try { defaultAction.start(destination); actions.put(destination defaultAction); + + // TODO Record profile information after successful startup + InstanceConfigFiles lastFile = lastFiles.get(destination); + List<FileInfo> newFileInfo = new ArrayList<FileInfo>(); + for (File instanceConfig : instanceConfigs) { + newFileInfo.add(new FileInfo(instanceConfig.getName() instanceConfig.lastModified())); + } + lastFile.setInstanceFiles(newFileInfo); + logger.info("auto notify start {} successful." destination); } catch (Throwable e) { logger.error("scan add found[{}] but start failed" destination ExceptionUtils.getFullStackTrace(e)); ``` The patch you hit can submit a PR to me, I am good to merge into the master. How do you feel that the canal instance filter regex is not in effect? ​​Any table updates will be consumed by all the tables. Table data filtering is only valid for row mode Hello for CanalServerWithEmbedded_StandaloneTest CanalServerWithEmbedded_StandbyTest has the following problem - Modify MetaMode to MetaMode ZOOKEEPER Found that both data will be updated at the same time, two instances will get update notification or embedded does not support Server mode. An instance will only be monitored by one instance of Server. - Using embedded for configuring multi-library configuration settings group is more troublesome than xml - Is there a specific case test and reference benchmark for the Server mode of the production environment? 1. The mechanism of HA is to solve HA when the deployer implements the immigration. 2. Group mode can choose the way the API is created to see the part of the combination of otter and canal. 3. The approximate performance of the canal TPS is around 2 5w and there are some slight changes in the machine environment and network of different users. @ihaolin Check whether the CanalConnector calls the subscribe filter method. If there is a filter, the filter needs to be consistent with the canal instance filter regex of the instance property. Otherwise, the subscribe filter will overwrite the instance configuration. If the subscribe filter is equivalent to all the updated data you consume.
201,MysqlConnection&#39;s seek method and dump method will report NullPointerException The seek method and the dump method both initialize the LogContext ``` java LogContext context = new LogContext(); ``` But when decoding, it will report NullPointerException It is recommended to add separately ``` java context.setLogPosition(new LogPosition(binlogfilename)); ``` Can give a more detailed exception stack not I am running the test_timestamp method of MysqlEventParserTest ``` java 16:06:16.145 [destination = null address = /10.4.0.20:3306 EventParser] INFO c.a.o.c.p.i.mysql.MysqlConnection - COM_BINLOG_DUMP with position:BinlogDumpCommandPacket[binlogPosition=4 slaveServerId=3344 binlogFileName=mysql-bin.000047 command=18] 16:06:16.146 [destination = null address = /10.4.0.20:3306 EventParser] WARN c.a.o.c.p.i.mysql.MysqlEventParser - the binlogfile:mysql-bin.000047 doesn't exist to continue to search the next binlogfile caused by java.lang.NullPointerException at com.taobao.tddl.dbsync.binlog.LogDecoder.decode(LogDecoder.java:162) at com.taobao.tddl.dbsync.binlog.LogDecoder.decode(LogDecoder.java:106) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.seek(MysqlConnection.java:96) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findAsPerTimestampInSpecificLogFile(MysqlEventParser.java:1101) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findByStartTimeStamp(MysqlEventParser.java:539) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPositionInternal(MysqlEventParser.java:375) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPosition(MysqlEventParser.java:310) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:162) at java.lang.Thread.run(Thread.java:745) ``` https://github.com/alibaba/canal/pull/203
200,The project uses log4j2 to conflict with the logback of the canal `SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/Users/raoshaoquan/.m2/repository/org/slf4j/slf4j-log4j12/1.6.1/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/Users/raoshaoquan/.m2/repository/ch/qos/logback/logback-classic/1.1.3/logback-classic-1.1.3.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory] log4j:WARN No appenders could be found for logger (cn.mwee.search.job.misc.MyApplicationContext). log4j:WARN Please initialize the log4j system properly. log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.` Canal default depends on slf4j. You can exclude logback to enable your own logger selection. Thank you solved
199,upgrade protobuf version The version is too old. When it relies on canal for secondary development, it conflicts with other open source project-dependent versions to improve the version of canal&#39;s protobuf. [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=199) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=199) before we can accept your contribution.<br/><hr/>**yinxiu** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user). tks
198,mysql 5 5 52 canal par analysis bin log problem mysql 5.5 My cnf configuration is as follows [mysqld] log-bin=mysql-bin Add this line and ok binlog-format=ROW Select row mode server_id=1 Configuring mysql Replaction needs to be defined and cannot be repeated with canal&#39;s slaveId Canal log 2016-09-12 16:22:04.994 [destination = example address = localhost/127.0.0.1:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - Didn't find the corresponding binlog files from mysql-bin.000001 to mysql-bin.000009 2016-09-12 16:22:04.994 [destination = example address = localhost/127.0.0.1:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address localhost/127.0.0.1:3306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: can't find start position for example 2016-09-12 16:22:04.995 [destination = example address = localhost/127.0.0.1:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: can't find start position for example mysql 5 6 is normal, can you help me see the problem? can't find start position for example Cannot find time before the specified timestamp I would like to ask how to solve it. I also have this problem. Thank you. updated: Found that deleting the canal deployer 1 0 22 conf example meta dat is fine. CanalParseException: can't find start position for example The above error occurred because the timestamp problem may have caused several reasons. - conf/${instance}/meta.dat The file records the timestamp and deletes it to try - In addition, it may be that the previous instance name already has a binlog timestamp record in Zookeeper, but the instance after you changed the database connection but the instance name is still the same, which will cause this problem. I can modify the instance name.
197,Unexpected End Stream Exception Alibaba Cloud rds for database Canal version 1 0 22 In the last few days, the following error will be reported and the binlog will not be received. Can you see that it is a problem? 2016-08-30 11:00:27.364 [destination = prodb2 address = x/y:3306 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Unexpected End Stream at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) [canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210) [canal.parse-1.0.22.jar:na] at java.lang.Thread.run(Thread.java:744) [na:1.7.0_45] 2016-08-30 11:00:27.370 [destination = prodb2 address = x/y:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address x/y:3306 has an error retrying. caused by java.io.IOException: Unexpected End Stream at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210) ~[canal.parse-1.0.22.jar:na] at java.lang.Thread.run(Thread.java:744) [na:1.7.0_45] 2016-08-30 11:00:27.371 [destination = prodb2 address = x/y:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:prodb2[java.io.IOException: Unexpected End Stream at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:210) at java.lang.Thread.run(Thread.java:744) ] 2016-08-30 11:00:28.554 [destination = prodb2 address = x/y:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - disconnect address x/y:3306 has an error retrying. caused by java.io.IOException: KILL DUMP 369615002 failure:java.io.IOException: ErrorPacket [errorNumber=1094 fieldCount=-1 message=Unknown thread id: 369615002 sqlState=HY000 sqlStateMarker=#] with command: KILL CONNECTION 369615002 at com.alibaba.otter.canal.parse.driver.mysql.MysqlUpdateExecutor.update(MysqlUpdateExecutor.java:49) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.disconnect(MysqlConnector.java:104) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.disconnect(MysqlConnection.java:60) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:242) at java.lang.Thread.run(Thread.java:744) KILL CONNECTION It is recommended to use trunk packaging to try @agapple Hello, I also found this problem recently. What do you mean by the main package? I don’t understand this question. The solution you proposed in another same issue is to open the heartbeat sql. Is there a concrete and feasible solution? @tutulvlv Try using the new version of 1 0 24 The 1 0 24 used is also occasionally reported this error. `2017-09-15 16:33:17.786 [destination = action_log address = /127.0.0.2:2020 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address 127.0.0.2/127.0.0.2:2020 has an error retrying. caused by java.io.IOException: Unexpected End Stream at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) ~[canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70) ~[canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122) ~[canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) ~[canal.parse-1.0.24.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.7.0_80] 2017-09-15 16:33:17.786 [destination = action_log address = /127.0.0.2:2020 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:action_log[java.io.IOException: Unexpected End Stream at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) at java.lang.Thread.run(Thread.java:745) ] 2017-09-15 16:33:34.137 [destination = action_log address = /127.0.0.2:2020 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position {"identity":{"slaveId":-1 "sourceAddress":{"address":"127.0.0.2" "port":2020}} "postion":{"included":false "journalName":"mysql-bin.000876" "position":1064558972 "serverId":1037 "timestamp":1505464397000}} 2017-09-15 18:33:34.137 [destination = action_log address = /127.0.0.2:2020 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Unexpected End Stream at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) ~[canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70) ~[canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122) [canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) [canal.parse-1.0.24.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.7.0_80] 2017-09-15 18:33:34.137 [destination = action_log address = /127.0.0.2:2020 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address 127.0.0.2/127.0.0.2:2020 has an error retrying. caused by java.io.IOException: Unexpected End Stream at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) ~[canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70) ~[canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122) ~[canal.parse-1.0.24.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) ~[canal.parse-1.0.24.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.7.0_80] 2017-09-15 18:33:34.137 [destination = action_log address = /127.0.0.2:2020 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:action_log[java.io.IOException: Unexpected End Stream at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:156) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:122) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) at java.lang.Thread.run(Thread.java:745) ] 2017-09-15 18:33:46.970 [destination = action_log address = /127.0.0.2:2020 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position {"identity":{"slaveId":-1 "sourceAddress":{"address":"127.0.0.2" "port":2020}} "postion":{"included":false "journalName":"mysql-bin.000877" "position":459898074 "serverId":1037 "timestamp":1505471614000}}` It will reappear in a few hours. I don’t know if the client is causing the client to pull it once and confirm the recycling. If the data is accumulated in the master, is there a problem? @agapple
196,Eclipse import project compilation error Problem Description Code version 1.0.22 maven version 3.3.3 Jdk version 1.7.0_80 Execute mvn in the root directory eclipse:eclipse After the command, the project is imported into eclipse. The reason for the large area error is that the driver project depends on canal common. But the project name of the common project imported in eclipse is common. The project name does not match. Do you have any similar problems? ![qq20160828-1](https://cloud.githubusercontent.com/assets/13866684/18031262/31068e26-6d0a-11e6-8257-f8b21a2e4681.png) My project dependency tree definition is the full name of canal xxx. You can see the pom xml definition of other projects. @agapple I looked at other projects&#39; pom xml definition dependencies are also full name dependencies but the dependent project is not fully named after importing eclipse. See the screenshot in the problem common Is the project required for the maven version or the mvn I am executing? The eclipse eclipse command is missing parameters I changed the project name manually in eclipse. Now it is no problem compiling.
195,Positioning error caused by competition in position memory Premise the use of PeriodMixedMetaManager java as a metaManager Problem Description There is such a piece of code in the subscribe method of CanalServerWithEmbedded java ``` Position position = canalInstance.getMetaManager().getCursor(clientIdentity); if (position == null) { position = canalInstance.getEventStore().getFirstPosition();// Get the first item in the store if (position != null) { canalInstance.getMetaManager().updateCursor(clientIdentity position); // Update the cursor } logger.info("subscribe successfully {} with first position:{} " clientIdentity position); ``` According to the context, when the position in the memory is null, this code will first take the position in the eventStore and then update the cursor position in the zk according to this position. However, the code for updating the method in the PeriodMixedMetaManager is as follows: ``` public void updateCursor(ClientIdentity clientIdentity Position position) throws CanalMetaManagerException { updateCursorTasks.add(clientIdentity);// Add to the task queue for triggering super.updateCursor(clientIdentity position); } ``` This method will first add the scheduled task to the task queue and then update the position in memory. The scheduling thread will first take the position in memory and then update the cursor in zookeeper. The two threads are in concurrent competition and may read and write. Problem solution suggestion If you agree that there may be a risk here, change the two sentences in updateCursor. Problem background In the case of switching canal, the cursor positioning error starts from 0. I am holding the problematic mentality to find the problem. I will locate the problem here. If there is a risk, please refer to the author. updateCursorTasks.add(clientIdentity);// Add to the task queue for triggering This is just a Peugeot bit and has been specifically updated to get the following current in-memory values ​​to write out without reading the old version of the data. It is also recommended that subscribe is a single client call to avoid concurrency. I am not talking about the problem of multiple clients calling subscribe but the problems that may occur when a single subscribe is executed. Timing pushing memory points to zk and updating bits to memory are two parallel processes and the startup of scheduled push tasks. It may be earlier than the update site to the memory operation is completed, so it is possible to read the old data who is who depends first and then depends entirely on the scheduling situation The detailed requirements for the code modify the risk controllable
194,High availability issues with server and client deployed on the same machine Problem scenario High availability issues with server and client deployed on the same machine canal The server version is 1 0 21 client version is 1 0 22 Problem Description There are two machines pc1 pc2 Each server and client are named s1 c1 s2 C2 s1 and c1 run normally on pc1 s2 and c2 on pc2 as backup hanging Now pc1 is s2 on reboot pc2 can switch to normal operation but c2 will be stuck on connector subscribe method problem analysis Subscribe in the internal SimpleCanalConnector java implementation will always wait to get the lock mutex get and does not use a timeout to return a TimeoutException method mutex get long timeout TimeUnit unit))。 When only the server is switched, the original client will throw an exception when the server updates the location, and then jump out of the loop of receiving and receiving the binlog to re-subscribe to the server node. When the server and the client switch at the same time, the backup client is always in the waiting lock state. Failed to update the parameters for getting node or call the subscribe method again Problem hack I modified some source code to expose the subscribe method with TimeoutException and hacked this bug with retry. There are still the following problems. 1、 Since the timeout mechanism join and disconnect methods are also thrown at the bottom layer, a timeout exception is thrown, so there is no guarantee that the connection will be opened correctly. 2、 Other potential risks, I just took over this piece of source code and read it deeply enough. Hope to get the author&#39;s advice one or two thanks Look down https://github.com/alibaba/canal/issues/171， If the card is checked by the subscribe method, the initRunning method will trigger a callback update mutex after successful execution. If the initRunning method is successful, the mutex subscribe method will be modified to return a retry. However, according to the log situation, initRunning only has a log record 15 minutes before the reboot. It may be because the client still retains the black hole status of the original server and the client list when the machine is rebooted. It’s not the same as cutting the blockage. I suggest that the author try to basically reproduce my side and say it is not clear.
193,canal KILL DUMP # position info canal.instance.master.address = xx.xx.xx.xx:3306 canal.instance.master.journal.name = canal.instance.master.position = canal.instance.master.timestamp = 1471076485000 2016-08-13 16:23:09.821 [main] WARN org.springframework.beans.TypeConverterDelegate - PropertyEditor [com.sun.beans.editors.EnumEditor] found through deprecated global PropertyEditorManager fallback - consider using a more isolated form of registration e.g. on the BeanWrapper/BeanFactory! 2016-08-13 16:23:09.978 [main] INFO c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-example 2016-08-13 16:23:10.000 [main] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - start successful.... 2016-08-13 16:23:10.030 [destination = example address = /xxxxxxxxx:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position ::1471076485000 2016-08-13 16:23:34.010 [destination = example address = /xxxxxxxxxxx:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /xxxxxxxxxx:3306 has an error retrying. caused by java.io.IOException: KILL DUMP 459561 failure:java.io.IOException: ErrorPacket [errorNumber=1094 fieldCount=-1 message=Unknown thread id: 459561 sqlState=HY000 sqlStateMarker=#] with command: KILL CONNECTION 459561 at com.alibaba.otter.canal.parse.driver.mysql.MysqlUpdateExecutor.update(MysqlUpdateExecutor.java:49) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.disconnect(MysqlConnector.java:104) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.reconnect(MysqlConnector.java:82) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.reconnect(MysqlConnection.java:56) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:169) at java.lang.Thread.run(Thread.java:745) ``` at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.disconnect(MysqlConnector.java:106) ~[canal.parse.driver-1.0.22.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.reconnect(MysqlConnector.java:82) ~[canal.parse.driver-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.reconnect(MysqlConnection.java:56) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:169) ~[canal.parse-1.0.22.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.7.0_80] ``` 2016-08-13 16:23:34.010 [destination = example address = /xxxxxxxx:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: KILL DUMP 459561 failure:java.io.IOException: ErrorPacket [errorNumber=1094 fieldCount=-1 message=Unknown thread id: 459561 sqlState=HY000 sqlStateMarker=#] with command: KILL CONNECTION 459561 at com.alibaba.otter.canal.parse.driver.mysql.MysqlUpdateExecutor.update(MysqlUpdateExecutor.java:49) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.disconnect(MysqlConnector.java:104) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.reconnect(MysqlConnector.java:82) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.reconnect(MysqlConnection.java:56) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:169) at java.lang.Thread.run(Thread.java:745) ``` at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.disconnect(MysqlConnector.java:106) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.reconnect(MysqlConnector.java:82) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.reconnect(MysqlConnection.java:56) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:169) at java.lang.Thread.run(Thread.java:745) ``` ] 2016-08-13 16:23:46.004 [destination = example address = /xxxxxxxx:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position ::1471076485000 1.0.22 version Configure the start time information after the client is started. logs example example log Medium record kill dump The configuration start position also reported the same error. Thank you Great gods help I also encountered such a problem and later deleted the conf example meta dat and restarted it. @whlgh I tried this method, I didn’t know if I had any other places that were not set. I use windows Running configuration canal instance master timestamp No problem configuration canal.instance.master.journal.name with canal.instance.master.position Report the above mistake Running under Linux 2 configuration mode Reported an error ################################################# ## mysql serverId canal.instance.mysql.slaveId = 1234 # position info canal.instance.master.address = 127.0.0.1:3306 canal.instance.master.journal.name = canal.instance.master.position = canal.instance.master.timestamp = #canal.instance.standby.address = #canal.instance.standby.journal.name = #canal.instance.standby.position = #canal.instance.standby.timestamp = # username/password canal.instance.dbUsername =root canal.instance.dbPassword =root canal.instance.defaultDatabaseName =test canal.instance.connectionCharset = UTF-8 # table regex canal.instance.filter.regex = ._\.._ # table black regex canal.instance.filter.black.regex = ################################################# This is my profile Canal instance master journal name canal instance master position canal instance master timestamp three parameters are empty, no problem with the first two, no problem, I am running under Linux @whlgh What version do you use? I use 1 0 22 Also 1 0 22 1 You execute as root 2、show variables like '%log_bin%'; Look at the mysql binlog is not open KILL CONNECTION exception has been processed How can I solve this problem? How can I not find the meta dat file? This is my exception log information 2016-11-29 11:17:23.465 [destination = inventory address = /172.172.230.55:3307 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position ::1480262400000 2016-11-29 11:17:57.401 [destination = inventory address = /172.172.230.55:3307 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /172.172.230.55:3307 has an error retrying. caused by java.io.IOException: KILL DUMP 6155706 failure:java.io.IOException: ErrorPacket [errorNumber=1094 fieldCount=-1 message=Unknown thread id: 6155706 sqlState=HY000 sqlStateMarker=#] with command: KILL CONNECTION 6155706 at com.alibaba.otter.canal.parse.driver.mysql.MysqlUpdateExecutor.update(MysqlUpdateExecutor.java:49) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.disconnect(MysqlConnector.java:104) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.reconnect(MysqlConnector.java:82) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.reconnect(MysqlConnection.java:56) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:169) at java.lang.Thread.run(Thread.java:745) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.disconnect(MysqlConnector.java:106) ~[canal.parse.driver-1.0.22.jar:na] at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.reconnect(MysqlConnector.java:82) ~[canal.parse.driver-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.reconnect(MysqlConnection.java:56) ~[canal.parse-1.0.22.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:169) ~[canal.parse-1.0.22.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101] How can you solve your problem? I seem to have spent time with the position.
191,Active and standby synchronization issues There are now two MySQL databases M1 main M2 My current situation is that otter is deployed on M2 and both have the same database and one table. Condition M1 M2 is running normally. Otter does not start synchronization. 1 Insert a data in M1 at this time, this data will not appear because the synchronization M2 is not started. 2 Start otter synchronization 3 Modify the data in the M1 just inserted data does not appear in M2 How should this situation be solved? Thank you all. Update to insert can open the line synchronization mode of otter I can&#39;t use the database-based reverse check when I test it. I can&#39;t sync it. I can successfully synchronize it when I use the current log change. I would like to ask the use of database reverse check and log change in use. Thursday, August 4, 2016 agapple notifications@github.com Write > Closed #191 https://github.com/alibaba/canal/issues/191. > > — > You are receiving this because you authored the thread. > Reply to this email directly view it on GitHub > https://github.com/alibaba/canal/issues/191#event-744692264 or mute > the thread > https://github.com/notifications/unsubscribe-auth/ABGYwgeN6sXmAKefU3pKSRlCkO6zylPdks5qcUUXgaJpZM4JbtQ1 > . I won’t bother you if I see the document. Thank you. Liu Huan, Thursday, August 4, 2016 askingneil@gmail.com Write > I can&#39;t use the database-based reverse check when I test it. I can&#39;t sync it. I can successfully synchronize it when I use the current log change. > I would like to ask the use of database reverse check and log change in use. > > Thursday, August 4, 2016 agapple <notifications@github.com > <javascript:_e(%7B%7D 'cvml' 'notifications@github.com');>> Write > > > Closed #191 https://github.com/alibaba/canal/issues/191. > > > > — > > You are receiving this because you authored the thread. > > Reply to this email directly view it on GitHub > > https://github.com/alibaba/canal/issues/191#event-744692264 or mute > > the thread > > https://github.com/notifications/unsubscribe-auth/ABGYwgeN6sXmAKefU3pKSRlCkO6zylPdks5qcUUXgaJpZM4JbtQ1 > > .
190,How can the qq group refuse to join to join the qq group? Qq group still has free places to try again Qq group can also add people? Is there any other group? Qq group still has free places to try again Can qq group join?
189,Canal database connection information will be automatically resolved to ip in the otter manager Canal database connection information will be automatically resolved to ip in the otter manager However, the service domain name we use will not update the ip regularly, which will result in the canal connection not being on the mysql service. How can I keep the domain name in the configuration instead of parsing it as ip? Let me submit a PR to me. We also encountered this problem we solved by configuring vpn. Can you tell me how to configure vpn to solve the problem of otter manager parsing domain name into IP? https://github.com/alibaba/otter/issues/172 Fixed
188,Mysql has no data for a long time, which may cause the connection between canal and mysql to be blocked forever. The connection established when canal reads the binlog event from mysql sets the read timeout period. channel.socket().setSoTimeout(this.soTimeout); SoTimeout used 30s Read the binlog event using the SocketChannel read buffer method soTimeout invalid for this SocketChannel read method will never time out reference http bugs java com bugdatabase view_bug do bug_id 4614802 If mysql has no data for a while, such as an hour, the connection is invalid. You may refer to http www tldp org HOWTO TCP Keepalive HOWTO overview html 2 4 for firewall problems. Although the channel socket setKeepAlive is set to true, the default keepalive configuration time of the Linux system is too long, but the canal still blocks the read forever. So is it better to use the sock socket getInputStream to read better? If you do not read the data in the soTimeout, it will time out to force the connection to be re-established. At present, the idle link timeout mechanism that relies on the MySQL database is designed to default to 8 hours. The link will be broken. Your link failure is a half-open link. This client can get EOFException. This problem is very easy to reproduce in my own test environment mysql version 5 5 14 Wait_timeout is set to 8 hours canal version 1 0 22 If mysql half an hour without data update canal can not read the new bin log After restarting canalServer is normal, is it related to the configuration of the database or other reasons expected to get guidance PS CanalServer does not have any error logs, it is not easy to analyze the problem Temporary solution can open heartbeat SQL I have never encountered this problem. Mysql slave connection default timeout for master read data slave_net_timeout is 3600s time is too long timeout after rebuilding connection reference http dev mysql com doc refman 5 6 en replication options slave html sysvar_slave_net_timeout After mysql5 5 seems to introduce the heartbeat between the master slave binlog Event master no binlog The event is idle for a period of time and is sent to the slave after MASTER_HEARTBEAT_PERIOD Heartbeat event reference https://www.percona.com/blog/2011/12/29/actively-monitoring-replication-connectivity-with-mysqls-heartbeat/ http dev mysql com doc refman 5 6 en change master to html里的MASTER_HEARTBEAT_PERIOD The default configuration of MASTER_HEARTBEAT_PERIOD is slave_net_timeout 2 The default is 1800s. The time is too long. @tianshazzq Can test these parameters to try The canal program wants to support the default configuration without problems. It is better to control the timeout period to connect to the idle time as shown at the beginning, for example, to re-establish the connection after a maximum of 1 min. Mysql and canl server tcp connection flash canl server is not aware of the subsequent binlog can not be notified to the canal server how to break It is recommended to enable the canal heartbeat setting to update the database periodically to generate the heartbeat binlog. If you do not receive the heartbeat information at the specified time, you can try to rebuild the link. The connection between mysql and canal is abnormal. For example, the case of mysql is not able to passively receive data. Naturally, it will not know whether any link is normal. The key is whether the canal realizes the mechanism of reconnection after idle or mysql is abnormal. Even if you have configured which parameter to configure Canal based passive listening is the most important thing to not know whether the socket is disconnected by the server or open the heartbeat binlog or upgrade mysql5 6 mysql will send a heartbeat binlog event for idle links to maintain tcp links Restarting this situation after Mysql is down. The following opening heartbeat is useless. We need to implement the mysql parameter slave_net_timeout in canal. @agapple Change the database disconnection time to 2 minutes and turn on the heartbeat check for 2 minutes or more. After inserting data, there is no data. Server will not get Canal after a period of indefinite The server can get the data again. 10 30 minutes, and the debug heartbeat data can be sent and received in the log without any abnormal output. ## detecing config canal.instance.detecting.enable = true #canal.instance.detecting.sql = insert into retl.xdual values(1 now()) on duplicate key update x=now() canal.instance.detecting.sql = select 1 canal.instance.detecting.interval.time = 3 canal.instance.detecting.retry.threshold = 3 canal.instance.detecting.heartbeatHaEnable = false Mysql and canl server tcp connection flash canl server is not aware of the subsequent binlog can not be notified to the canal server how to break --------------------------------------------------------------------------------------------------- How do you solve this problem? I have encountered this problem now. @githubcjh Just stepped on a pit canal instance memory buffer size set too small, causing the main library to block large transactions when canal blocked Buffer size is recommended to set 1024 Do you really have this problem? Do you know that the landlord has solved this problem? Changed to inputstream is possible sotimeout is effective but there is no data inputstream for a long time Blocking in the read will not respond to the thread interrupt event Parser needs to make some changes first close Socket then call thread interrupt But I always feel that this method is somewhat contradictory with the cano&#39;s nio design. But the problem is that although the reality exists, it is nio, but the blocking mode is used when reading. buffer Size can solve this problem The solution is to read mysql Binlog change place plus a timeout is good or you can only rely on tcp&#39;s keeplive mechanism. I don&#39;t understand java. I have not used this set to develop a set. SocketChannel read buffer This method does not report timeout exceptions https stackoverflow com questions 2866557 timeout for socketchannel doesnt work answer 9150513 https stackoverflow com questions 2866557 timeout for socketchannel doesnt work answer 9150513 I currently use the 1 0 24 version to temporarily convert the SocketChannel used by DirectLogFetcher to ReadableByteChannel can make a socket timeout Effective throws SocketTimeoutException @vamdt BioSocketChannelPool java https github com alibaba canal blob master driver src main java com alibaba otter canal parse driver mysql socket BioSocketChannelPool java based on native Socket operation soTimeout parameter validation can also take effect @agapple OK Temporary use experience feels 1 0 24 more stable, etc. 1 0 26 officially released and try BIO again It is recommended that the v1 1 1 version based on the bio operation of the native Socket increases the timeout reconnection mechanism.
187,Support mysql5 7 Json type resolution RT. Support mysql5 7 Test data format ``` +-----+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+ | uid | data | +-----+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+ | 1 | {"mail": "jiangchengyao@gmail.com" "name": "David" "address": "Shangahai"} | | 2 | {"mail": "test"} | | 3 | {"int": 1 "bool": true "long1": 4294967296 "long2": 1.8446744073709552e19 "double": 1.11012} | | 4 | {"mail": [1 true 1.11012 4294967296 1.8446744073709552e19]} | | 5 | {"int": 1 "bool": true "mail": [1 true 1.11012 4294967296 1.8446744073709552e19] "long1": 4294967296 "long2": 1.8446744073709552e19 "double": 1.11012} | | 6 | {"time": "23:59:59"} | | 7 | {"time": "2016-06-30 15:03:32.000000"} | | 8 | {"time": "2016-06-30 15:51:25.041000"} | | 9 | {"time": 123.123} | | 10 | {"time": "23:23:23:19"} | | 11 | {"time": "01:00:05.000000"} | | 12 | {"time": "630:19:29.000000"} | | 13 | {"time": "2016-06-30 16:08:58.000000"} | | 14 | {"n1": "v1" "ne1": {"n2": "v2" "ne2": {"n3": "v3" "n32": ["v32" "v33" 1]}}} | +-----+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+ ``` There is a bug under 1 0 23 `com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: java.lang.IllegalArgumentException: illegal json data at com.taobao.tddl.dbsync.binlog.JsonConversion.parse_scalar(JsonConversion.java:150) at com.taobao.tddl.dbsync.binlog.JsonConversion.parse_value(JsonConversion.java:67)` Database version mysql-5.7.11-winx64 Database structure CREATE TABLE `customer_data` ( `id` bigint(19) unsigned NOT NULL AUTO_INCREMENT `supplier_id` bigint(19) unsigned NOT NULL COMMENT Business id `customer_id` bigint(19) unsigned NOT NULL `customer_data` json DEFAULT NULL COMMENT Customer standard attribute `customer_ext_data` json DEFAULT NULL COMMENT Customer extended attribute PRIMARY KEY (`id`) UNIQUE KEY `uk_cust` (`customer_id`) ) ENGINE=InnoDB AUTO_INCREMENT=26036 DEFAULT CHARSET=utf8mb4; After debugging, it is 149 lines str_len 0 @agapple Give the json string you tested
186,ack Wrong 2016-06-06 19:07:07.416 [New I/O server worker #1-25] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400 Caused by : something goes wrong with channel:[id: 0x4d32397e /10.252.158.196:53393 => /10.1.7.109:9930] exception=com.alibaba.otter.canal.server.exception.CanalServerException: ack error clientId:1001 batchId:85 is not exist please check 2016-06-06 19:07:07.418 [New I/O server worker #1-25] ERROR c.a.otter.canal.server.netty.handler.SessionHandler - something goes wrong with channel:[id: 0x4d32397e /10.252.158.196:53393 :> /10.1.7.109:9930] exception=java.nio.channels.ClosedChannelException at org.jboss.netty.channel.socket.nio.NioWorker.cleanUpWriteBuffer(NioWorker.java:643) at org.jboss.netty.channel.socket.nio.NioWorker.writeFromUserCode(NioWorker.java:370) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:137) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) at org.jboss.netty.channel.Channels.write(Channels.java:611) at org.jboss.netty.channel.Channels.write(Channels.java:578) at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:28) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:144) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:275) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndFireMessageReceived(ReplayingDecoder.java:525) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:506) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.cleanup(ReplayingDecoder.java:541) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.channelDisconnected(ReplayingDecoder.java:449) at org.jboss.netty.channel.Channels.fireChannelDisconnected(Channels.java:360) at org.jboss.netty.channel.socket.nio.NioWorker.close(NioWorker.java:593) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:119) at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76) at org.jboss.netty.channel.Channels.close(Channels.java:720) at org.jboss.netty.channel.AbstractChannel.close(AbstractChannel.java:200) at org.jboss.netty.channel.ChannelFutureListener$1.operationComplete(ChannelFutureListener.java:46) at org.jboss.netty.channel.DefaultChannelFuture.notifyListener(DefaultChannelFuture.java:381) at org.jboss.netty.channel.DefaultChannelFuture.addListener(DefaultChannelFuture.java:148) at com.alibaba.otter.canal.server.netty.NettyUtils.write(NettyUtils.java:30) at com.alibaba.otter.canal.server.netty.NettyUtils.error(NettyUtils.java:51) at com.alibaba.otter.canal.server.netty.handler.SessionHandler.messageReceived(SessionHandler.java:200) at org.jboss.netty.handler.timeout.IdleStateAwareChannelHandler.handleUpstream(IdleStateAwareChannelHandler.java:48) at org.jboss.netty.handler.timeout.IdleStateHandler.messageReceived(IdleStateHandler.java:275) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndFireMessageReceived(ReplayingDecoder.java:525) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:506) at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:443) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274) at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261) at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:349) at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:280) at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:200) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:744) I have also encountered this problem is that the consumption is more than the consumption of the canal time. Check if an instance restart occurs on the server side.
185,Canal instance filter regex cannot filter db
184,Problem with deleting the table This happens after the table is deleted. Caused by: java.io.IOException: ErrorPacket [errorNumber=1146 fieldCount=-1 message=Table 'secooOrderWriter1DB.t_increment_sequence_bak' doesn't exist sqlState=42S02 sqlStateMarker=#] with command: desc `orderDB`.`t_increment_sequence_bak` at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.query(MysqlQueryExecutor.java:60) ~[canal.parse.driver-1.0.21.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:69) ~[canal.parse-1.0.21.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta0(TableMetaCache.java:97) ~[canal.parse-1.0.21.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:26) ~[canal.parse-1.0.21.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.apply(TableMetaCache.java:50) ~[canal.parse-1.0.21.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.apply(TableMetaCache.java:41) ~[canal.parse-1.0.21.jar:na] A look at the history of the issue is not recommended to delete the DDL operation including deleting the table and deleting fields, etc. There are clear suggestions in the FAQ FAQ
183,About canal for statement Format log analysis Will the canal Is there a way to parse the statement format log and the class? Similar to the analysis of the row log can resolve the database name table name operation type instead of a simple sql Statement You can rely on the sql parser to parse the output to get the results you want.
182,About MIXED log format data acquisition
181,Client connection problem SimpleCanalConnector.java readNextPacket called by the doConnect method int bodyLen = readHeader.getInt(0); bodyLen Value of 1308622848 How does this number read so big and then the memory overflows? This should be an abnormal data stream It is recommended to first increase the client retry or HA mechanism to ensure that if there is new information and then reopen issue
180,CanalConnectors.newClusterConnector zk Failover problem CanalConnectors.newClusterConnector Report this error when using zk new ClusterNodeAccessStrategy() When clusterPath is not created zkClient getChildren this method gives an error Exception in thread "main" org.I0Itec.zkclient.exception.ZkNoNodeException: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /otter/canal/destinations/192.168.2.2:3306/cluster at org.I0Itec.zkclient.exception.ZkException.create(ZkException.java:47) at org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:685) at org.I0Itec.zkclient.ZkClient.getChildren(ZkClient.java:413) at org.I0Itec.zkclient.ZkClient.getChildren(ZkClient.java:409) at com.alibaba.otter.canal.client.impl.ClusterNodeAccessStrategy.<init>(ClusterNodeAccessStrategy.java:58) at com.alibaba.otter.canal.client.CanalConnectors.newClusterConnector(CanalConnectors.java:69) at com.alibaba.otter.canal.example.ClusterCanalClientTest.main(ClusterCanalClientTest.java:31) The server has not started
179,Does the current version support mysql5 7? Does the current version support mysql5 7? I have never done a test in the test group. I tested it and didn&#39;t encounter any special problems.
178,About the time interval between persistence to file or ZK My test here found that if the canal zookeeper flush period or canal file flush period is set a little longer because the client&#39;s cursor information is not persisted to the file or ZK in memory, then restart Canal or HA to switch to another node. Will receive duplicate data, I want to ask is the direct connection to Canal whether the client wants to do the message to go to the logic of my own. I am currently thinking of the position according to binlog Yes, there will be a repetitive and serious thinking.
177,Exception handling when RDS master-slave library is switched amendment 1 When IOException occurs, retry specifies the number of times more than the number of times to get the current latest point reconnect 2 Error log output and alarm output suggest that mail alert can be implemented later [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=177) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=177) before we can accept your contribution.<br/><hr/>**Yishun.Chen** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user). tks First merge the PR, you need to do some fine-tuning, such as accurately identifying the abnormality of the binlog site.
176,Client high availability active intervention zookeeper&#39;s running will cause the link to not close Manual login to zookeeper delete the current client&#39;s running node will trigger a client&#39;s high availability. The last time the node is running will have priority. But the node that was last run did not close the last open socket link in time. The triggering condition is abnormal. The running node interferes. If the zookeeper has a network jitter caused by the running node disappearing, the client&#39;s network should have problems and canal. Server also disconnected Solve the problem that the running node disappears first to perform the following local shutdown of the socket link.
175,Table filtering set in file instance xml configuration mode does not work 1. In example Instance configuration `# table regex canal.instance.filter.regex = canal\\.ad.*` The debug debug finds that the variable is indeed set in the eventParser eventFilter of the instance instance. 2. Create a client for data request 3. Modify the advice table client under the database canal to receive the corresponding message 4. Modify the advice table client under the database test can also receive the corresponding message Note Both databases are created locally 1. Confirm that row mode is set 2. The filter condition is only valid for the row data. The rest of the query queries are not filtered. I am on the project itself Deployer CanalLuncher found a problem in the process of debugging The server-attached EventParser service is correct for table filtering expressions. However, in the thread that processes the filtering service, the expression pattern is changed. The following is my debugging process. Please take a look. 1. Initial EventParser and start ![qq 20160509143520](https://cloud.githubusercontent.com/assets/5526657/15105668/0aa5d130-15f6-11e6-8007-b427efec5baf.png) 2. Before constructing the Erosa connection ![qq 20160509143526](https://cloud.githubusercontent.com/assets/5526657/15105742/ce1a585c-15f6-11e6-8407-a71284ce097c.png) 3. After constructing the Erosa connection ![qq 20160509145114](https://cloud.githubusercontent.com/assets/5526657/15105758/dbd62980-15f6-11e6-8335-25b6dea0f4f2.png) The client&#39;s subscribe incoming filter has the highest priority and ignores the server&#39;s settings. @agapple But I want to use the server-side settings. If I don&#39;t write the subscribe in the program, I will write the regex on the server side. This is good. I just want to use the contents of the instance. Then your client&#39;s subscribe will pass a null value.
174,Canal Use group instance How to configure destination in mode Name How to configure logical total destination after merging multiple instances Name? Take a look at the group instance xml configuration example The example given in the project is the example of configuring file instance xml. Can you give the address of the group instance xml configuration example? https://raw.githubusercontent.com/alibaba/canal/master/deployer/src/main/resources/spring/group-instance.xml
173,Canal client high availability bug fix Request reason please see issue https://github.com/alibaba/canal/issues/171 [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=173) <br/>Thank you for your submission we really appreciate it. Like many open source projects we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/alibaba/canal?pullRequest=173) before we can accept your contribution.<br/> https://github.com/alibaba/canal/issues/171 tks Later I adjusted the next part of the code mainly to SimpleCanalConnector
172,Rds is configured with an exception when the master-slave library masters the slave library. The error is reported as follows 16-04-22 07:11:24.236 [destination = crm_csc_03 address = rdst58y130m2pdn0th48.mysql.rds.aliyuncs.com/10.248.4.8:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:crm_csc_03[java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) at java.lang.Thread.run(Thread.java:745) ] The master slave library switches from the library without finding the binlog Fetch error I can see that the canal listener can configure the master-slave library address. Can this solve the problem? Is there a good solution like this? The wrong site needs to be subscribed by timestamp Big Brother not find first log file name in binary log index file It is not the wrong place. This is the fetch time when the bingo file is lost. The restart error can only be skipped. This bingo loss should be caused by the master-slave switchover. Have you ever encountered this problem? The problem is that the wrong location is correct but the canal is always running. This is the running exception setting timestamp can skip this section but will cause data loss. I want to know how this error should be avoided to avoid the instance properties configuration standby can avoid Does this problem happen or is there a good solution? Rds vip mode canal can not sense the operation of the active and standby switching using the binlog location of the main library before the new main library to find the library will of course not find Currently only monitor When the active/standby switch is found In time before the binlog is deleted Do it by time My follow-up version will consider providing an idea. If the students are interested, they can follow the implementation. 1. Get the binlog event mysql after subscribing to the binlog server Id is recorded as a part of position in the locus 2. When the node occurs that the binlog location is not available, judge the mysql by creating a new link. server Whether the id is different, if it is different, it will automatically subscribe according to the timestamp. Query the mysql server_id method ``` show variables like 'server_id'; ``` This modification suggestion is very good, but there will be some problems according to this modification. In some cases, the binlog will also be cleared. For example, rds specification upgrade, etc. I have summarized some rules. When an IOException occurs, the error is basically unrecoverable. Continue the following operations and inform the relevant maintenance personnel that the program I am modifying now is 1 When IOException occurs, retry specifies the number of times more than the number of times to get the current latest point reconnect 2 Error log output and alarm output suggest that mail alert can be implemented later 3 ExceptionRetryCount can be processed in the previous way if the configuration file is not configured. The relevant code I have modified and tested has been applied to the production environment. If the moderator feels that I am thinking correctly, I can submit the relevant code. It is recommended to submit the code below to see the code hope that canal can better support this type of vip mode active and standby clusters. You and my two methods can be combined I have also encountered this problem now. We use Amazon&#39;s RDS to flush mysql. Log permissions have been restricted I tried to clear the information in zookeeper and restart the canal. I tried the instance properties and set the current timestamp. Is there any way to manually fix it? Hey Know the problem RDS only keeps the binlog for 10 minutes by default. If the canal is interrupted for more than 10 minutes, it will never get up again. I hope to provide a simple command or reset the configuration file to the latest location. I modified some code. When this error occurs, the number of times is automatically reset. The latest bit code is submitted. See if Ali&#39;s buddy will adopt it. The PR code has been merged, and the abnormal content is accurately identified. It is necessary to perform the active/standby switch as much as possible to ensure that no data is lost. Looks like mysql GTID Have this feature
171,Client high availability bug Problem scene 1 canal has two servers c1 and c2 c1 is active 2 The client uses the ClusterCanalConnector to consume and the two servers d1 and d2 d1 are active. 3 c1 is down. At this point, d1 will execute the restart method of ClusterCanalConnector. There are three steps in restart. The first step of disconnect will release the running node The second step thread sleeps for 5 seconds The third step is to execute the initRunning method when trying to reconnect. 4 After d1 releases the running node, d2 will be triggered immediately. The initRunning method also has two major steps in initRunning. The first step to seize the running node The second step is to execute the processActiveEnter method. This method will definitely report an error. Because the canal of d2 connection is also c1, the zk thread that executes initRunning will exit abnormally. The mutex variable of d2 is still false and the running node is not released. The subsequent reconnection of 5 d1 is meaningless because d2 does not release the running node, so the mutext of d1 becomes false. 6 The end result is that both d1 and d2 are blocked. 7 At this time, the initRunning method of d2 d1 will be triggered, but the processActiveEnter method of d1 will still report error d1 still cannot be restored. 1. Confirm the version of canal 2. Your step 4 d2 start link canal is c1 because of the timeliness problem d2 failed to find out in time c2 is already the main If you can explicitly reproduce the code that has been fixed The canal version is 1 0 20 It is easy to reproduce the above phenomenon because the InitRunning method of ClientRunningMonitor does not handle the exception thrown by processActiveEnter. The running node on Zk occupies the release and the mutex of both clients is false. There is no chance to execute. The fix code is writing about the high-availability code. It’s still not written. You can see if there is a simple modification method. There is also the doConnect method in the SimpleCanalConnector class. private InetSocketAddress doConnect() throws CanalClientException { try { channel = SocketChannel.open(); channel.socket().setSoTimeout(soTimeout); channel.connect(address); Channel connect address The address here should be taken immediately, otherwise D2 will never have the chance to get the latest canal server ip Your step 4 d2 start link canal is c1 because of the timeliness problem d2 failed to find out in time c2 is already the main Yes, the reason is not found in this code. Channel connect address The address here should be taken immediately, otherwise D2 will never have the chance to get the latest canal server ip Look at the code. If the 4th step d2 starts the link, the c1 will be executed when the initRunning performs the doConnect. The original expectation is to start the restart operation through the client and the d2 is because the hot backup may be an error during the asynchronous initRunning callback. Unable to trigger client&#39;s restart and cause long time to hang I thought about the next simple modification. 1. initRunning retryes the asynchronous release callback by actively releasing the running node when the exception is executed. 2. SimpleCanalConnector&#39;s doConnect method dynamically obtains the address address Yes, it’s changed like this. The git client is not installed on the machine. The modified code sent you a mailbox jianghang115 gmail com Take time to look at it.
170,How can canal integrate with RocketMQ Because the effect of implementing multi-client subscriptions seems to be that the canal itself is not supported. So consider using it with RocketMQ Can you not need the canal client to directly push the message directly into the MQ? Is it necessary to customize the canal server? Is the custom workload large? Using canal The client gets the message and writes it to Rocketmq. I am now writing an intermediate application. Redis Subscribe to one Instance then Redis Multiple consumption inside
169,Whether canal server and client can be one-to-many relationship Start a canal server and two clients. The result is only the first one that can receive the message. What is the reason? Client can only work one and the other is hot standby
168,reset The problem that binlog does not find after master pid:2 nid:2 exception:canal:shanghai_canal:java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) at java.lang.Thread.run(Thread.java:745) Through the mysql log found that canal still requests the original binlog. Does this issue have a processing mechanism? Thank you @ch-lgs How is it handled? RESET MASTER Delete all binary logs and recreate a new binary log The original binlog is deleted and the canal cannot be consumed. So the solution to the error is to reset a consumer point. Data loss before this point
167,canal How does the server adapt to the database HA? When the main db hangs and switches to the standby machine this time canal The server can automatically connect to the standby machine to get the binlog? How to set the offset? Look at the wiki
166,Start service successfully monitors database changes 2016-04-15 15:45:39.563 [destination = image address = /127.0.0.1:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just show master status 2016-04-15 15:45:39.564 [destination = image address = /127.0.0.1:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /127.0.0.1:3306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: command : 'show master status' has an error! Caused by: java.io.IOException: ErrorPacket [errorNumber=1227 fieldCount=-1 message=Access denied; you need (at least one of) the SUPER REPLICATION CLIENT privilege(s) for this operation sqlState=42000 sqlStateMarker=#] with command: show master status at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.query(MysqlQueryExecutor.java:60) ~[canal.parse.driver-1.0.21.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:69) ~[canal.parse-1.0.21.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findEndPosition(MysqlEventParser.java:548) ~[canal.parse-1.0.21.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPositionInternal(MysqlEventParser.java:352) ~[canal.parse-1.0.21.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPosition(MysqlEventParser.java:312) ~[canal.parse-1.0.21.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:162) ~[canal.parse-1.0.21.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.7.0_79] Execute show master Insufficient authority when status Low-level questions see the first floor reply Viewing the permissions are given to 777 is not the problem Account permissions are not file permissions Maximum authority Is caused by the zookeeper cache node not being emptied.
165,canal + zookeeper Can&#39;t find the point correctly Zookeeper starts normally > Canal configuration - canal.instance.global.spring.xml = classpath:spring/default-instance.xml > example/instance.properties canal.instance.master.address = 172.16.20.120:3306 canal.instance.master.journal.name = canal.instance.master.position = canal.instance.master.timestamp = > Use the classpath spring default instance xml to catch the database changes normally. Once replaced with the classpath spring default instance xml, you can&#39;t get it normally. 1. 2. \- 2016-04-14 15:41:06.202 [Thread-4] INFO c.a.otter.canal.instance.spring.CanalInstanceWithSpring - stop CannalInstance for 1-example 3. \- 2016-04-14 15:41:06.204 [Thread-4] INFO c.a.otter.canal.instance.spring.CanalInstanceWithSpring - stop successful.... 4. \- [root@Note_140_canal ~]# tailf -n 300 /usr/local/canal.deployer-1.0.21/logs/example/example.log 5. \- 2016-04-14 15:41:34.578 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 6. \- 2016-04-14 15:41:34.588 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties] 7. \- 2016-04-14 15:41:34.645 [main] WARN org.springframework.beans.TypeConverterDelegate - PropertyEditor [com.sun.beans.editors.EnumEditor] found through deprecated global PropertyEditorManager fallback - consider using a more isolated form of registration e.g. on the BeanWrapper/BeanFactory! 8. \- 2016-04-14 15:41:34.759 [main] INFO c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-example 9. \- 2016-04-14 15:41:34.788 [main] INFO c.a.otter.canal.instance.spring.CanalInstanceWithSpring - subscribe filter change to ._.._ 10. \- 2016-04-14 15:41:34.789 [main] INFO c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start successful.... 11. \- 2016-04-14 15:41:34.832 [destination = example address = /172.16.20.120:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position by switch ::1460100169000 12. \- 2016-04-14 15:41:34.970 [destination = example address = /172.16.20.120:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - Didn't find the corresponding binlog files from mysql-bin.000001 to mysql-bin.000004 13. \- 2016-04-14 15:41:34.979 [destination = example address = /172.16.20.120:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /172.16.20.120:3306 has an error retrying. caused by 14. \- com.alibaba.otter.canal.parse.exception.CanalParseException: can't find start position for example 15. \- 2016-04-14 15:41:34.983 [destination = example address = /172.16.20.120:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: can't find start position for example 16. \- ] 17. 18. ``` by Timestamp 1460100169000 can&#39;t find corresponding binlog Log file Now it has been resolved that the compressed package passed to linux is damaged.
164,Listening to the problem of instance regular filtering There is such a special database. The same table is distributed in multiple schemas. I want to listen to all the library names ending with the underscore and the number ending with order. `order_2` `...` Order_11 and so on because of the automatic expansion of this number accumulate At the same time, there are multiple unrelated schemas. I tried to write the following regular filtering configuration. After starting the server, it didn&#39;t take effect and didn&#39;t throw a mistake. **`canal.instance.filter.regex=order_/(\d+)/\\..*`** I don&#39;t know if this kind of demand can be satisfied. This can be done by repeated debugging. `order_\\d+\\..*`
163,CanalServer Is there a performance bottleneck? 51 instances of client are deployed on the canal. All instances are consumed on the same server as the canal and there are no other services on the server. ### Client implementation - Initialize the connection ``` // Create a link without HA based on ip directly CanalConnector connector = CanalConnectors.newSingleConnector( new InetSocketAddress(hostIp port) destination.generateDestination() "" ""); ``` - retrieve data ``` long t1 = System.currentTimeMillis(); Message message = connector.getWithoutAck(10240 1000L TimeUnit.MILLISECONDS); In milliseconds long t2 = System.currentTimeMillis(); long batchId = message.getId(); logger.info("read batchId=[{}] time=[{}]ms" batchId (t2 -t1)); ``` ### problem It is found that the execution time of some instances of the getWithoutAck method is greater than the set value of 1000ms. It is about 2000ms. When the amount of data is large, it takes time to zoom in. The following is the analysis of the client request once - Client sends request t1 - Server received request t2 - embeddedServer getWithoutAck completes and returns data to client t3 10 40 34 - Client received data t4 10 40 37 According to the log, t4 t1 5000ms and t4 t3 3000ms and getWithoutAck takes 1000ms. Therefore, it can be inferred that t2 t1 5000 3000 1000 1000ms So it can be explained that the communication between the client and the CanalServer is time consuming, but this time consuming is generally negligible. If it is CanalServer Can the performance bottleneck be optimized? Server 24 core memory 64g Give priority to canal Server jvm memory and GC state a. It’s not wise to take 10MB of data at once. b. It is not wise for a server to deploy too many binlog parsing queues to balance jvm memory and services. Generally we use 8 queues internally to get 4M 100ms 8 instances deployed on a canal have a cluster version A server itself is 8 consumption queue instance
162,Support IDE to start CanalLauncher RT. Mainly solve the search of the conf directory file Avoiding the scan scan failure triggers the stop mechanism
161,parse row data failed Follow quick start The steps used to start up After startup sh example.log There will be an error as follows 2016-04-01 15:01:37.068 [destination = example address = /10.0.2.40:3550 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: java.lang.IllegalArgumentException: bad format x exceed: 1887436800 999999999 at com.taobao.tddl.dbsync.binlog.LogBuffer.getDecimal0(LogBuffer.java:1355) at com.taobao.tddl.dbsync.binlog.LogBuffer.getDecimal(LogBuffer.java:1265) at com.taobao.tddl.dbsync.binlog.event.RowsLogBuffer.fetchValue(RowsLogBuffer.java:339) at com.taobao.tddl.dbsync.binlog.event.RowsLogBuffer.nextValue(RowsLogBuffer.java:96) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseOneRow(LogEventConvert.java:462) The canal version and the mysql version report mysql version /home/rong/mysql/bin/mysql Ver 14.14 Distrib 5.6.26 for Linux (x86_64) using EditLine wrapper canal Is the latest 1 0 21 Can download mysql Binlog to local and then use FileLogFetcherTest for local parsing test Ok, let me give it a try. Thank you. Estimated to be the problem https github com alibaba canal issues 119 Use the current master branch build and try again How to solve this problem? My MySQL version is 5 6 26 A similar problem has been fixed once. Use a current master branch to package a version locally. If you have any problems, please feedback.
160,Can&#39;t get ROWDATA data From the application log, the EntryType is taken as TRANSACTIONBEGIN和TRANSACTIONEND data Did not get ROWDATA, what is the reason for this? Canal version 1.0.21 mysq version PXC architecture 5.6.20 Answering it yourself is a problem with database port configuration. The database is pxc and has two instances 3306 3307. The strange thing is that the wrong port is connected without any error. Are you sure of binlog format ROW in your mysql configuration? Self-made iPhone > in March 31, 2016 17 22 Song Chang notifications@github.com Write > > From the application log, the EntryType is taken as TRANSACTIONBEGIN和TRANSACTIONEND data > Did not get ROWDATA, what is the reason for this? > Canal version 1.0.21 > mysq version PXC architecture 5.6.20 > > — > You are receiving this because you are subscribed to this thread. > Reply to this email directly or view it on GitHub canal.instance.binlog.format = ROW STATEMENT MIXED The default has not been changed The reason has been detected. See the above answer. The main strange thing is that the canal does not report any errors. The logs are normal. Canal will resolve all binlogs by default. You can open all the log formats here, of course, you will not report an error. I use canal 1 0 25 SNAPSHOT corresponds to MySQL 5 6 5 7 will also have this problem The port is not changed and the configuration is supported by default. canal.instance.binlog.format = ROW STATEMENT MIXED By default, parsing all binlogs will not cause rowdata to resolve. The database is using row After the filter table filtered by the filter filter, the rowdata information is not received, but the corresponding Transaction Begin、Transaction End still has same question Connector subscribe actionlog d 8 I want to subscribe to all actionlogs. The results of the table are not ROWDATA. Is it correct or wrong? You can now run testcase with canal&#39;s PattenUtils locally.
159,ack Can achieve one Message Some of Entry Submit? `com.alibaba.otter.canal.client.CanalConnector.get` There are methods batchSize Parameter when I set it to 100 If I am dealing with certain `entry` Error and then call Connector rollback batchId This will cause all of this time `entry` Rollback? Does that lead to the same repeated processing? `entry` The problem ？ I want to ask canal Available "ack Submit some Message Entry" Program? My current thinking is in the record Library name table name primary key Do the only judgment to prevent rollback Repeated processing Set batchSize smaller when there is no single ack function get ps. Mainly performance considerations How to deal with the repetition of batchSize 1 Need to be designed on the business as an idempotent execution software level can not avoid non-repetition
158,Why at CanalLauncher Running error java.io.IOException: connect /127.0.0.1:3306 failure:java.nio.channels.ClosedChannelException It’s not surprising to run startup sh after packaging. The debug code has too much relevance to your own environment I don’t know shuhuiguo when I encounter the same problem. How did you solve it? Nothing should be a problem with the threading model
157,How to enumerate field values ​​of an enumerated type as a string when mysql Field type is enum 时 column getValue  Returned is int I want to ask if it is easy to get and mysql Same in the middle enum String value? The data packet in the binlog protocol is the int type. You can get the database table structure definition and convert the int to the corresponding enum value. @agapple canal Is there a way to get the definition of the database table structure? I checked it. wiki could not find it There is a mysql in the entry protocol Type attribute
156,Could not find first log file name in binary log index file Error message `2016-03-25 17:56:48.605 [destination = mysql_56688 address = /127.0.0.1:56688 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position /data/mysql_data/data/mysql-bin.000001:4: 2016-03-25 17:56:48.607 [destination = mysql_56688 address = /127.0.0.1:56688 EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) ~[canal.parse-1.0.21.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) [canal.parse-1.0.21.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) [canal.parse-1.0.21.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.7.0_79] 2016-03-25 17:56:48.607 [destination = mysql_56688 address = /127.0.0.1:56688 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /127.0.0.1:56688 has an error retrying. caused by java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) ~[canal.parse-1.0.21.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) ~[canal.parse-1.0.21.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) ~[canal.parse-1.0.21.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.7.0_79] 2016-03-25 17:56:48.607 [destination = mysql_56688 address = /127.0.0.1:56688 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:mysql_56688[java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) at java.lang.Thread.run(Thread.java:745) ` MySQL: 5.6.25 canal: 1.0.21 instance.properties: canal.instance.mysql.slaveId = 3 canal.instance.master.address = 127.0.0.1:56688 canal.instance.master.journal.name = /data/mysql_data/data/mysql-bin.000001 canal.instance.master.position = 4 First of all, the MySQL configuration and file permissions have been checked. No problem. This is because bin Is the log directory not caused by the regular directory? Could not find first log file name in binary log index file at. Estimated manual clean up binlog log did not reset mysql binlog Index information google a lot of solutions to this problem reset Master does not work if you do not specify the binlog location instance Thread will always be stuck in the show master status' Canal will do the retry to clean up the historical location information of the canal record. How to clean up Historical location information of the canal record Delete what file or restart zookeeper Delete the zk node or meta dat and restart the instance The delete link of the node node in the otter manager is that the text cannot be deleted. How can I delete it? Position in the synchronization progress State deletion to recover Each time the store will parse the journalname to the second parse data on zk, it will get the journalname from zk. If it can&#39;t find it, go to the first from the master. index File If the database administrator manually cleans up the log or the disk is too full, the automatic cleanup log will appear. The solution is to delete the node corresponding to the name. It is generally recommended to delete the index file by using the mysql command line. @agapple Hello there I deleted the meta dat of the instance Restart canal Still bursting this wrong Is there any other solution?
155,Time Out Is the version incompatible? hi， I used server V1 0 22 latest version client V1 0 21 The dependency jar of the repository has an inexplicable timeout problem. My zookeeper supports other applications well. zkServer cdh3 2181 cdh1:2181 cdh5:2181` 。 Using zookeeper to schedule timeouts, clinet has been unreachable. With the Simple method, there is no problem with timeout throwing errors as follows `[2016-03-24 19:58:27] ERROR [ZkClient-EventThread-13-cdh3:2181 cdh1:2181 cdh5:2181] ZkEventThread - Error handling event ZkEvent[Data of /otter/canal/destinations/user/1001/running changed sent to com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor$1@45681b65] com.alibaba.otter.canal.protocol.exception.CanalClientException: java.net.ConnectException: Operation timed out at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.doConnect(SimpleCanalConnector.java:167) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.access$000(SimpleCanalConnector.java:48) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector$1.processActiveEnter(SimpleCanalConnector.java:392) at com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor.processActiveEnter(ClientRunningMonitor.java:188) at com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor.initRunning(ClientRunningMonitor.java:110) at com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor$1.handleDataDeleted(ClientRunningMonitor.java:67) at org.I0Itec.zkclient.ZkClient$6.run(ZkClient.java:549) at org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71) Caused by: java.net.ConnectException: Operation timed out at sun.nio.ch.Net.connect0(Native Method) at sun.nio.ch.Net.connect(Net.java:458) at sun.nio.ch.Net.connect(Net.java:450) at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.doConnect(SimpleCanalConnector.java:128) ... 7 more` Debug to BooleanMutex class The specific reason why this get can&#39;t be synchronized causes the timeout is not found. ` public void get() throws InterruptedException { this.sync.innerGet(); } ` Later, changing the server to V1 0 21 is still not possible. Is the version incompatible information retained in zk? Is 21client unable to cooperate with 22 server? Thank you How to interpret this error ERROR ZkEventThread - Error handling event ZkEvent[Data of /otter/canal/destinations/user/1001/running changed sent to com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor$1@5789b4d3] zk Node [/otter/canal/destinations/user/1001/running ] Yes {"active":true "address":"10.1.193.0" "clientId":1001} 1. java.net.ConnectException: Operation timed out at Is not connected to the server or the link timed out 2. Error handling event The exception is due to the problem solved first by timeout Thank you because there are two IP addresses in the server network environment. ， If you do not set the IP default, the client is unable to connect. After the server specifies the IP, the client still throws a similar error. ` ClientRunningMonitor - canal is running in [10.1.193.0] but not in [192.168.56.1:64327] ` The client side is also two IPs. The configuration is not found. I don&#39;t know if I can specify the IP of the client. The IP address of the 10 segment is not the default. The IP of the 192 segment is a LAN. The client does not specify ip. This is the information returned by AddressUtils getHostIp and the ip that creates the socket. Let&#39;s just make a single ip client machine.
154,canal Binlog parsing error Error log Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: java.lang.IllegalArgumentException: limit excceed: 367 at com.taobao.tddl.dbsync.binlog.LogBuffer.getFullString(LogBuffer.java:1122) ~[canal.parse.dbsync-1.0.21.jar:na] at com.taobao.tddl.dbsync.binlog.event.RowsLogBuffer.fetchValue(RowsLogBuffer.java:920) ~[canal.parse.dbsync-1.0.21.jar:na] at com.taobao.tddl.dbsync.binlog.event.RowsLogBuffer.nextValue(RowsLogBuffer.java:96) ~[canal.parse.dbsync-1.0.21.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseOneRow(LogEventConvert.java:462) ~[canal.parse-1.0.21.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:378) ~[canal.parse-1.0.21.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:109) ~[canal.parse-1.0.21.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:62) ~[canal.parse-1.0.21.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:323) ~[canal.parse-1.0.21.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:176) ~[canal.parse-1.0.21.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:121) ~[canal.parse-1.0.21.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) ~[canal.parse-1.0.21.jar:na] mysql 5.6.27 1. Is it possible to reproduce what kind of sql can cause such a problem? 2. Is it possible to provide a binlog file? [root@69_114 mysql]# mysqlbinlog --no-defaults --base64-output=decode-rows -v --start-position=214215706 /data2/logs/mysql/mysql-bin.000105 |more /_!50530 SET @@SESSION.PSEUDO_SLAVE_MODE=1_/; /_!40019 SET @@session.max_insert_delayed_threads=0_/; /_!50003 SET @OLD_COMPLETION_TYPE=@@COMPLETION_TYPE COMPLETION_TYPE=0_/; DELIMITER /_!_/; # at 214215706 # at 214216435 #160324 14:16:33 server id 6915 end_log_pos 214216533 Table_map: `crm`.`t_product` mapped to number 78 # at 214216533 #160324 14:16:33 server id 6915 end_log_pos 214216647 Update_rows: table id 78 flags: STMT_END_F ### UPDATE `crm`.`t_product` ### WHERE ### @1=598342 ### SET ### 3 full car polishing ### @4=30000 ### @5=30000 ### @6=38800 ### 7 car wash decontamination polishing ### @11='1' ### @12='9' ### @15='5' ### @18='2016-03-24 14:16:33' ### @19='18699926655' # at 214216647 # at 214217458 #160324 14:16:33 server id 6915 end_log_pos 214217547 Table_map: `crm`.`t_sys_log` mapped to number 82 # at 214217547 #160324 14:16:33 server id 6915 end_log_pos 214218065 Write_rows: table id 82 flags: STMT_END_F ### INSERT INTO `crm`.`t_sys_log` ### SET ### @1=908095 ### @2='1458800193776' ### 3 DML logs ### @4='1001' ### 5 modification ### @6='modify' ### 7 products full car polishing ### @8='598342' ### @9='' ### @10='{"clearAmt":30000 "dataFlag":"" "firstCategory":"1" "marketAmt":38800 "myself":false "pageNum":1 "pageSize":10 "product4SVo":{"carTypeName":""} productDesc car wash decontamination polishing "productId":"598342" "p RoductName full car polishing "productStatus":"5" "saleAmt":30000 "secondCategory":"9" "storeId":"33813" "sysLogList":[] "updateUser":"18699926655"}' ### 11 Operation succeeded ### @12=18 ### @13='2016-03-24 14:16:33' ### @14='18699926655' ### @15='2016-03-24 14:16:33' ### @16='18699926655' [EntryPosition[included=false journalName=mysql-bin.000105 position=214215706 timestamp=1458800193000]] com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: java.lang.IllegalArgumentException: limit excceed: 3332 at com.taobao.tddl.dbsync.binlog.LogBuffer.getFullString(LogBuffer.java:1122) ~[canal.parse.dbsync-1.0.20.jar:na] at com.taobao.tddl.dbsync.binlog.event.RowsLogBuffer.fetchValue(RowsLogBuffer.java:917) ~[canal.parse.dbsync-1.0.20.jar:na] at com.taobao.tddl.dbsync.binlog.event.RowsLogBuffer.nextValue(RowsLogBuffer.java:96) ~[canal.parse.dbsync-1.0.20.jar:na] canal 1.0.20 my.cnf binlog-row-image=minimal Change to full Normal Impression 1 0 21 only supports the binary mode to provide binborg&#39;s original binary file instead of parsed text canal1.0.21 canal1.0.22-sanpshot Never tried No I can do the code debugging myself or provide a recurring method or provide the original binlog binary file, otherwise I can&#39;t help it. I can do the code debugging myself or provide a recurring method or provide the original binlog binary file, otherwise I can&#39;t help it. 2016-03-30 10:55:31.825 [destination = example address = /127.0.0.1:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - ERROR ## parse this event has an error last position : [EntryPosition[included=false journalName=mysql-bin.000052 position=35751033 timestamp=1459305662000]] com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: java.lang.IllegalArgumentException: limit excceed: 3332 at com.taobao.tddl.dbsync.binlog.LogBuffer.getFullString(LogBuffer.java:1122)
153,Canal can not be closed When the canal auto scan function is enabled and the dynamic increase or decrease instance is actually executed. Execute sh Stop sh can not close the canal server has been stuck waiting state can only end the process through kill Execute the stop after executing jstack to get the thread stack Based on the current latest version, no problem with stop failure has been found.
152,Whether the canal is the same as the MQ message can only guarantee that it can only be guaranteed once at least once. In the database master-slave switch canal crash restart, etc. Canal will repeat the message sent the same insert statement Involving too many network factors No guarantee At present, the so-called high-performance MQ can&#39;t guarantee not repeating at most. Based on some storage, some de-reprocessing is done to understand the uncertainty of the network. In addition to network factors, there will be duplicate messages such as kill -9 canal pid Then restart if there are inevitable duplicates, it is convenient to carry out some tests. kill 9 is a network uncertainty factor, my data is sent to you, you have not returned ack to me, I can only send it to you next time.
151,Jdk version What version of jdk is used? We used to use 1 6 25 How to match all tables in the whole library when otter adds a data table Take a look at both the wiki and the video.
150,Merge pull request #1 from alibaba/master Merge pull request [![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/alibaba/canal?pullRequest=150) <br/>You should sign our Contributor License Agreement in order to get your pull request merged.<br/>
149,ABOUT BINLOG_SEND_ANNOTATE_ROWS_EVENT In the official mysql official document based on the COM_BINLOG_DUMP protocol dump request flags only see BINLOG_DUMP_NON_BLOCK this kind of BINLOG_SEND_ANNOTATE_ROWS_EVENT what does it mean in the official documentation where to find relevant instructions search for mysql binlog protocol
148,Canal currently supports oracle? Canal currently supports oracle? Just looked at the code and it seems that it does not support it. [https://github.com/alibaba/canal/blob/master/instance/manager/src/main/java/com/alibaba/otter/canal/instance/manager/CanalInstanceWithManager.java#L295](https://github.com/alibaba/canal/blob/master/instance/manager/src/main/java/com/alibaba/otter/canal/instance/manager/CanalInstanceWithManager.java#L295) Does it mean that otter only supports mysql read and write? Can look at the wiki has a clear description does not currently support oracle for Oracle needs can refer to my other project https github com alibaba yugong Yugong Support oracle to mysql In turn from mysql Is it supported by oracle? Yugong open source does not support mysql oracle Is the oracle version of the canal yugong or another version based on the ogg implementation? Https github com alibaba yugong yugong is based on oracle materialized view
146,Canal started successfully but viewed instance found error system CentOS6.5 mine /etc/my.cnf Configuration _[mysqld] datadir=/var/lib/mysql socket=/var/lib/mysql/mysql.sock log-bin=mysql-bin binlog-format=ROW server_id=1 # Disabling symbolic-links is recommended to prevent assorted security risks symbolic-links=0 [mysqld_safe] log-error=/var/log/mysqld.log pid-file=/var/run/mysqld/mysqld.pid_ instance.properties Configuration ``` $ cat conf/example/instance.properties ################################################# ## mysql serverId canal.instance.mysql.slaveId = 1234 # position info canal.instance.master.address = 127.0.0.1:3306 canal.instance.master.journal.name = canal.instance.master.position = canal.instance.master.timestamp = #canal.instance.standby.address = #canal.instance.standby.journal.name = #canal.instance.standby.position = #canal.instance.standby.timestamp = # username/password canal.instance.dbUsername = canal canal.instance.dbPassword = canal canal.instance.defaultDatabaseName = canal.instance.connectionCharset = UTF-8 # table regex canal.instance.filter.regex = .*\\..* # table black regex canal.instance.filter.black.regex = ################################################# ``` canal.log information ``` Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=96m; support was removed in 8.0 Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=256m; support was removed in 8.0 Java HotSpot(TM) 64-Bit Server VM warning: UseCMSCompactAtFullCollection is deprecated and will likely be removed in a future release. 2016-03-04 23:01:39.529 [main] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## start the canal server. 2016-03-04 23:01:39.735 [main] WARN com.alibaba.otter.canal.common.utils.AddressUtils - Failed to retriving local host ip address try scan network card ip address. cause: scidb-work601: scidb-work601: unknown error 2016-03-04 23:01:39.743 [main] INFO com.alibaba.otter.canal.deployer.CanalController - ## start the canal server[192.168.100.113:11111] 2016-03-04 23:01:41.046 [main] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## the canal server is running now ...... ``` Example log information ``` 2016-03-04 23:01:40.378 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2016-03-04 23:01:40.414 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties] 2016-03-04 23:01:40.588 [main] WARN org.springframework.beans.TypeConverterDelegate - PropertyEditor [com.sun.beans.editors.EnumEditor] found through deprecated global PropertyEditorManager fallback - consider using a more isolated form of registration e.g. on the BeanWrapper/BeanFactory! 2016-03-04 23:01:40.840 [main] INFO c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-example 2016-03-04 23:01:40.884 [main] INFO c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start successful.... 2016-03-04 23:01:41.147 [destination = example address = /127.0.0.1:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /127.0.0.1:3306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: connect /127.0.0.1:3306 failure:java.io.IOException: Error When doing Client Authentication:ErrorPacket [errorNumber=1045 fieldCount=-1 message=Access denied for user 'canal'@'localhost' (using password: YES) sqlState=28000 sqlStateMarker=#] at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:174) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:68) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:52) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:84) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:158) at java.lang.Thread.run(Thread.java:745) Caused by: java.io.IOException: connect /127.0.0.1:3306 failure:java.io.IOException: Error When doing Client Authentication:ErrorPacket [errorNumber=1045 fieldCount=-1 message=Access denied for user 'canal'@'localhost' (using password: YES) sqlState=28000 sqlStateMarker=#] at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:174) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:68) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:52) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:84) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:158) at java.lang.Thread.run(Thread.java:745) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:71) ~[canal.parse.driver-1.0.21.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:52) ~[canal.parse-1.0.21.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:84) ~[canal.parse-1.0.21.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:158) ~[canal.parse-1.0.21.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_73] 2016-03-04 23:01:41.150 [destination = example address = /127.0.0.1:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: connect /127.0.0.1:3306 failure:java.io.IOException: Error When doing Client Authentication:ErrorPacket [errorNumber=1045 fieldCount=-1 message=Access denied for user 'canal'@'localhost' (using password: YES) sqlState=28000 sqlStateMarker=#] at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:174) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:68) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:52) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:84) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:158) at java.lang.Thread.run(Thread.java:745) Caused by: java.io.IOException: connect /127.0.0.1:3306 failure:java.io.IOException: Error When doing Client Authentication:ErrorPacket [errorNumber=1045 fieldCount=-1 message=Access denied for user 'canal'@'localhost' (using password: YES) sqlState=28000 sqlStateMarker=#] at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:174) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:68) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:52) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:84) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:158) at java.lang.Thread.run(Thread.java:745) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:71) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:52) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:84) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:158) at java.lang.Thread.run(Thread.java:745) ``` MySQL User rights view ``` mysql> SELECT DISTINCT CONCAT('User: ''' user '''@''' host ''';') AS query FROM mysql.user; +-------------------------------+ | query | +-------------------------------+ | User: 'canal'@'%'; | | User: 'root'@'127.0.0.1'; | | User: 'root'@'::1'; | | User: ''@'localhost'; | | User: 'root'@'localhost'; | | User: ''@'scidb-work601'; | | User: 'root'@'scidb-work601'; | +-------------------------------+ 7 rows in set (0.01 sec) ``` Use your own client link to mysql This problem is solved today because different version problems cause the created user&#39;s granted password to appear differently when it is set. Localhost can&#39;t log in. I need to use the command alone to run localhost. I use mysql5 5 version. I also encountered such a problem after searching for information and solved it as follows ```` After adding ordinary users, execute mysql> use mysql mysql> delete from user where user=''; mysql> flush privileges; ```` I don’t know where to come from anonymous users. Change the super permission restart must restart according to @maoyikun The solution is solved Add super permission to mysql user then flush privileges; 2018-10-12 19:27:59.970 [destination = example address = localhost/127.0.0.1:3306 EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address localhost/127.0.0.1:3306 has an error retrying. caused by java.lang.IllegalArgumentException: null at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1314) ~[na:1.8.0_181] at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1237) ~[na:1.8.0_181] at java.util.concurrent.Executors.newFixedThreadPool(Executors.java:151) ~[na:1.8.0_181] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor.start(MysqlMultiStageCoprocessor.java:84) ~[canal.parse-1.1.0.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:238) ~[canal.parse-1.1.0.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_181] 2018-10-12 19:27:59.975 [destination = example address = localhost/127.0.0.1:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.lang.IllegalArgumentException at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1314) at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1237) at java.util.concurrent.Executors.newFixedThreadPool(Executors.java:151) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor.start(MysqlMultiStageCoprocessor.java:84) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:238) at java.lang.Thread.run(Thread.java:748) ] 2018-10-12 19:28:16.965 [destination = example address = localhost/127.0.0.1:3306 EventParser] WARN c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - prepare to find start position just show master status If the server database changes, report this error. Please help me to see it. @shiyuan2he Download the latest 1 1 1 version has been resolved
145,Add rowdata filter Add rowdata filtering for the parseRowsEvent method to filter out the rowdata data for only subscribing to data other than rowdata. Please review it again. Thank you. tks
144,Unregister the attributes already owned by the parent class to avoid super start calling the same name attribute that is not initialized in the child class. fix Issue master branch code reports NullPointerException when using OTTER #143 tks
143,Master branch code reports NullPointerException when using OTTER ``` pid:1 nid:1 exception:setl:java.lang.NullPointerException at com.alibaba.otter.canal.instance.core.AbstractCanalInstance.start(AbstractCanalInstance.java:73) at com.alibaba.otter.canal.instance.manager.CanalInstanceWithManager.start(CanalInstanceWithManager.java:110) at com.alibaba.otter.canal.server.embedded.CanalServerWithEmbedded.start(CanalServerWithEmbedded.java:102) at com.alibaba.otter.node.etl.select.selector.canal.CanalEmbedSelector.start(CanalEmbedSelector.java:206) at com.alibaba.otter.node.etl.select.SelectTask.startup(SelectTask.java:170) at com.alibaba.otter.node.etl.select.SelectTask.run(SelectTask.java:126) ``` The reason is CanalInstanceWithManager versus AbstractCanalInstance declares properties of the same name CanalInstanceWithManager.java Line 110 calls super start to call the property that the parent class has not initialized. ``` public void start() { // Initialize metaManager logger.info("start CannalInstance for {}-{} with parameters:{}" canalId destination parameters); Super start 110 lines } ```
142,Canal1 0 in version zkclient and kafka Zkclient version conflict When canal and kafka are used together Canal1 0 in version zkclient com github sgroschupf Conflict with zkclient version com 101tec zkclient in kafka The artifactId is different Bad exclusion package Is there any solution to this problem? Thank you. Why didn&#39;t you use com 101tec, a slightly popular version of zkclient? Why is this issue turned off? I also encountered this problem on the 1 0 24 version today. This package has a long history. If it is convenient, you can submit a PR to me.
141,I would like to ask a few questions about the use of Canal. 1 The document says that the serverId of multiple canal instances cannot be repeated. It seems that there is no problem with the impact of my test. 2 Instance regularity inside the listener of a table and multiple tables on the canal server performance impact? Because our table is dynamically generated, if the impact is not big, the dynamic subscription on the client is fine. 3 When multiple otter clients are connected to the same canal, I find that a message will only be received once. Is this feature reliable? 1. When the same mysql primary and backup cluster serverId is duplicated, mysql will randomly kill a client link. 2. Regular expressions will have a length limit compared to regular calculations. Network IO will become the bottleneck first. 3. It is recommended that a server with only one client server and client using HA mode can guarantee that only one client works and the client can receive all data.
140,Whether to support python client to read the synchronized binlog Whether to support python client to read the synchronized binlog Canal currently only provides the java version of the server using the protobuf protocol for communication. You can complete a python according to the protocol requirements. Client version development
139,Mysql connection is not released after closing canal When using canal, it is found that some mysql connections are not released when the mysql library does not have any binlog generated. Mysql version 5 6 24 72 2 log Canal version 1 0 21 The stack information before closing is as follows ``` "destination = 90 address = /192.168.12.110:3309 HeartBeatTimeTask" daemon prio=10 tid=0x00007f4b2c00d000 nid=0x395b in Object.wait() [0x00007f4b51d13000] java.lang.Thread.State: TIMED_WAITING (on object monitor) at java.lang.Object.wait(Native Method) at java.util.TimerThread.mainLoop(Timer.java:552) - locked <0x000000076052ffa8> (a java.util.TaskQueue) at java.util.TimerThread.run(Timer.java:505) "destination = 90 address = /192.168.12.110:3309 EventParser" prio=10 tid=0x00007f4bd073e800 nid=0x395a runnable [0x00007f4b51d54000] java.lang.Thread.State: RUNNABLE at sun.nio.ch.FileDispatcherImpl.read0(Native Method) at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39) at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223) at sun.nio.ch.IOUtil.read(IOUtil.java:197) at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379) - locked <0x0000000760a5efb0> (a java.lang.Object) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:154) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) at java.lang.Thread.run(Thread.java:745) ``` The mysql connection information before closing is as follows ``` mysql> show processlist; | 442033 | xxx | 192.168.201.174:47598 | NULL | Sleep | 4 | | NULL | 1 | 1 | | 442035 | xxx | 192.168.201.174:47600 | NULL | Binlog Dump | 4 | Master has sent all binlog to slave; waiting for binlog to be updated | NULL | 0 | 0 | ``` After executing bin stop sh, it is found that the 442035 connection is not released. Log 90 90 log is as follows ``` 2016-01-14 11:55:11.655 [Thread-5] INFO c.a.otter.canal.instance.spring.CanalInstanceWithSpring - stop CannalInstance for 1-90 2016-01-14 11:55:11.670 [Thread-5] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - disConnect MysqlConnection to /192.168.12.110:3309... 2016-01-14 11:55:11.675 [destination = 90 address = /192.168.12.110:3309 EventParser] INFO c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O interrupted while readi ng from client socket java.nio.channels.ClosedByInterruptException: null at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202) ~[na:1.7.0_55] at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:412) ~[na:1.7.0_55] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:154) ~[canal.parse-1.0.21-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70) ~[canal.parse-1.0.21-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) [canal.parse-1.0.21-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) [canal.parse-1.0.21-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.7.0_55] 2016-01-14 11:55:11.676 [destination = 90 address = /192.168.12.110:3309 EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - the channel /192.168.12.110 :3309 is not connected 2016-01-14 11:55:11.676 [destination = 90 address = /192.168.12.110:3309 EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - disConnect MysqlConnection to /192.168.12.110:3309... 2016-01-14 11:55:11.677 [Thread-5] INFO c.a.otter.canal.instance.spring.CanalInstanceWithSpring - stop successful.... 2016-01-14 11:55:11.677 [Thread-5] INFO c.a.otter.canal.server.embedded.CanalServerWithEmbedded - stop CanalInstances[90] successfully ``` In addition, the following inexplicable mysql connection will occasionally appear. ![image](https://cloud.githubusercontent.com/assets/7714843/12315487/4bcd3586-bab6-11e5-8a34-4c15758cab19.png) Whether jvm has completely exited after stop execution @agapple The jvm process has completely exited The JVM exits the TCP management and should completely quit to confirm whether it is the TCP link before the canal corresponding process. Really a canal connection After the debug test, the canal has closed the socket correctly. However, in some cases, the tcp link of the operating system will remain in the close_wait state. You can speed up the recovery of such status by adjusting the operating system parameters. mysql Binlogdump actively pushes the data mode for the server. After the client initiates the socket close, the server consistently sends the ACK packet, resulting in consistent CLOSE_WAIT status. Adopted strategy for binlog Dump link After the socket close is completed Open a new link and initiate a KILL CONNECTION ID request to completely close the binlogdump link MysqlConnector Line 106 Fork a connection to KILL If an exception occurs during CONNECTION, it is not recommended to throw it out and remember the log. After all, it is a non-main process. Today, I see that the log has been reported in an infinite loop. ``` 2016-08-10 15:44:48.865 [destination = ucarorder_to_es address = /10.101.23.101:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position by switch ::1470729600000 2016-08-10 15:44:49.366 [destination = ucarorder_to_es address = /10.101.23.101:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - ERROR ## findAsPerTimestampInSpecificLogFile has an error java.io.IOException: KILL DUMP 7265038 failure:java.io.IOException: ErrorPacket [errorNumber=1094 fieldCount=-1 message=Unknown thread id: 7265038 sqlState=HY000 sqlStateMarker=#] with command: KILL CONNECTION 7265038 at com.alibaba.otter.canal.parse.driver.mysql.MysqlUpdateExecutor.update(MysqlUpdateExecutor.java:49) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.disconnect(MysqlConnector.java:104) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.reconnect(MysqlConnector.java:82) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.reconnect(MysqlConnection.java:56) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findAsPerTimestampInSpecificLogFile(MysqlEventParser.java:674) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findByStartTimeStamp(MysqlEventParser.java:519) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPositionInternal(MysqlEventParser.java:420) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPosition(MysqlEventParser.java:313) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:162) at java.lang.Thread.run(Thread.java:745) ``` But after I restart, I can&#39;t reproduce it, so I will mention here if anyone else has encountered this problem. The latest code has been submitted to record only the exceptions are not thrown out 2016-08-10 18:59 GMT+08:00 hechaoyi notifications@github.com: > MysqlConnector Line 106 > Fork a connection to KILL If an exception occurs during CONNECTION, it is not recommended to throw it out and write down the log. After all, it is a non-master process. > > Today, I see that the log has been reported in an infinite loop. > > 2016-08-10 15:44:48.865 [destination = ucarorder_to_es address = /10.101.23.101:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position by switch ::1470729600000 > 2016-08-10 15:44:49.366 [destination = ucarorder_to_es address = /10.101.23.101:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - ERROR ## findAsPerTimestampInSpecificLogFile has an error > java.io.IOException: KILL DUMP 7265038 failure:java.io.IOException: ErrorPacket [errorNumber=1094 fieldCount=-1 message=Unknown thread id: 7265038 sqlState=HY000 sqlStateMarker=#] > with command: KILL CONNECTION 7265038 > at com.alibaba.otter.canal.parse.driver.mysql.MysqlUpdateExecutor.update(MysqlUpdateExecutor.java:49) > at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.disconnect(MysqlConnector.java:104) > at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.reconnect(MysqlConnector.java:82) > at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.reconnect(MysqlConnection.java:56) > at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findAsPerTimestampInSpecificLogFile(MysqlEventParser.java:674) > at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findByStartTimeStamp(MysqlEventParser.java:519) > at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPositionInternal(MysqlEventParser.java:420) > at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPosition(MysqlEventParser.java:313) > at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:162) > at java.lang.Thread.run(Thread.java:745) > > But after I restart, I can&#39;t reproduce it, so I will mention here if anyone else has encountered this problem. > > — > You are receiving this because you were assigned. > Reply to this email directly view it on GitHub > https://github.com/alibaba/canal/issues/139#issuecomment-238834028 or mute > the thread > https://github.com/notifications/unsubscribe-auth/AAy8t3RasGIBbgz1EEZps-j345g6Cc3uks5qea79gaJpZM4HElfj > .
138,make canal server become a single instance In theory, a JVM process only needs one CanalServer instance to manage multiple CanalInstances by CanalServer. First change this to a single case, then modify the otter code. Added AbstractCanalInstance to remove a lot of duplicate code from the CanalInstance subclass Canal code should not support Otter without premise If this part of the code is no problem, I plan to start modifying Otter&#39;s CanalServer implementation. Praise The act of submitting PR is worthy of recognition and encouragement
137,java.lang.ArrayIndexOutOfBoundsException: 0 com.alibaba.otter.canal.parse.inbound.mysql.dbsync.SimpleDdlParser.parseTableName(SimpleDdlParser.java:163) Version 1 0 19 Error is as follows ![qq 20151218092517](https://cloud.githubusercontent.com/assets/11752250/11886867/b88a512a-a569-11e5-9dba-1feaa8a6637c.png) The binlog location is as follows ![qq 20151218092527](https://cloud.githubusercontent.com/assets/11752250/11886874/c864b91e-a569-11e5-8677-c808ef22c33d.png) This happens after changing the field with pt online schema change Https github com alibaba canal pull 128 has been fixed by users after the release of the new version or can be packaged by the current master
136,When the table has no primary key, the insert data will cause the size of the columnInfo and the tableMeta to not match, and the instance monitors the hang. The problem description is as follows: error log 2015-12-03 16:27:55.718 [destination = address = EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:crm[com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: column size is not match for table:`s1`.`t1` 6 vs 5 ] column size is not match for table:s1.t1 6 vs 5. Should be done the field delete operation and the primary key has nothing to do No field deletions have been made and there is no such problem after adding the primary key. Personal guess is because mysql for the table without the primary key will automatically create a hidden primary key binlog with this hidden primary key data and tableMeta does not have this field can not correspond to both sides, so the program error can be properly handled for this situation You are testing RDS. RDS does by default add hidden fields to the no-key table. Super permissions are required to be visible. Is there any recommended solution for RDS? At present, we need to monitor the RDS library, which often causes problems with canal. Authorize super permission for canal using the autonomous user rights of the new RDS @Librazk How can rds play with canal? The rds active/standby switchover does not currently process well, which will result in a bit error. That needs to clear the meta bat file on the canal to let the canal automatically get the latest point. Currently, the RDS support for vip mode is not good. This will leak data Because the meta file is emptied Go to slava and get the latest position. After discovering that the master was cut, the data for this period of time is gone. Record the timestamp in the last meta file based on the timestamp relocation RDS no primary key table will have a hidden column alibaba_rds_row_id will be in the last column of each table and is int type compatible meta.bat Where is the file location not found in the canal directory? > meta.bat Where is the file location not found in the canal directory? zookeeper Mode you need to execute zkCli inside delete Drop that canal of path
135,The isUpdated property is false when the value is changed from null to an empty string Mysql version is 5 5 Change the value of a varchar type column from NULL to when the isupdated attribute of the entry from the canal is false. Look at the LogEventConverter. It feels that the isUpdated method has a bug newValue is the old value is NULL when the old column getValue is Column isNull method can determine whether it is a null value This is the otter found in otter using the canal&#39;s column isUpdated method to determine if a change has occurred and then some column synchronization errors. Thanks for the feedback problem code has been fixed. The next version can be released. You are looking forward to the perfection of canal and otter.
134,About max memory Size problem Hi jianghang： In the checkFreeSlotAt method of class MemoryEventStoreWithBuffer if (batchMode.isMemSize()) { final long memsize = putMemSize.get() - ackMemSize.get(); if (memsize < bufferSize \* bufferMemUnit) { return true; } else { return false; } } else { return true; } Only judge memsize < bufferSize \* bufferMemUnit is not enough, it should be used The amount of memory currently occupied by the newly added event and the bufferSize \* bufferMemUnit for comparison Hope to teach putMemSize.get() - ackMemSize.get()， The result of this is that the data that is currently not in memory is the memory block in use because the characteristics of Ringbuffer is lazy. Set mode will have some memory data not cleaned up between several sites. However, the total amount of memory in the data high-speed get put process is controllable and not very accurate. Yep understand tks
133,Question about the truncation of the transaction The following is my experimental step found that in addition to the canal instance transaction size parameter may cause the transaction to be truncated, the client&#39;s batchsize setting will also cause the transaction to be truncated. What kind of consideration can be made to the transaction integrity when canla is designed? Can synchronize data to the client to remember ack to ensure the final consistency Here is my experiment 1. Create a test stored procedure procedure code as follows DELIMITER // CREATE PROCEDURE loop_insert(IN count int) begin declare lp int; declare idvalue int; set lp=0; select max(id)+1 into idvalue from test; set autocommit=0; while lp<count do insert into test(id name)values(idvalue 'test-'+idvalue); set idvalue=idvalue+1; set lp=lp+1; end while; commit; end; // DELIMITER; 1. 测试 canal instance transaction size  > Set the canal instance transaction size parameter of the canal server to 1024. The default is also 1024 canal client SimpleCanalClientTest&#39;s batchsize is set to 1024 5 is large enough > Start mysql canal server and canal client in sequence. Before the mysql operation, the canal client must be started in advance, otherwise the transaction truncation phenomenon will not be displayed. > Execute the loop_insert parameter on the mysql side to pass 1022. Observe all events on the canal client side and get it in a batch. > Execute the loop_insert parameter on the mysql side to pass 1023. Observe that the canal client will return two batches. The first one contains 1024 events. The second one contains 1 event. The above verifies the role of the parameter canal instance transaction size. In order to prevent the transaction from being truncated, the parameter value needs to be set larger. 2. Test batchsize 3.1 scene one > Set the parameter of the canal server to ITEMSIZE. Use ITEMSIZE to test. Set the batchsize in the canal client to 2. > Start mysql start canal server execute the loop_insert parameter on the mysql side and then start the canal client > Observe that the canal client will return 6 batches. Each batch contains 2 events. The transaction is truncated. 3.4 Scene two > Set the parameter of the canal server to ITEMSIZE. Use ITEMSIZE to test the batchsize in the canal client to 13 > Start mysql start canal server in the mysql side continuous execution loop_insert two parameters pass 10 and then start the canal client must be completed after the above operation is completed client to ensure the authenticity of 3 4 > Observe that the canal client will return two batches. The first batch13 event, the second batch11 event, the transaction is truncated. This shows that in addition to the need to consider the transaction Size also needs to consider batch Smaller sizesize configuration may also truncate transactions, but if it is too large, it will affect real-time performance. Note that when testing 2 and 3, be sure to isolate each other, that is, when testing the transaction size, ensure that the batchsize is large enough to test the batchsize. The purpose of the canal instance transaction size is to prevent the transaction from being truncated. But if the mysql binlog format is a mixed type, basically every event will cause a flush canal instance transaction size setting. Say the thoughts I considered at the time 1. Canal instance transaction size This parameter mainly controls parser parsing and submits to event When the store wants to ensure the consistency of the transaction visible, the maximum guaranteed consistency of the transaction size is 1024. For example, if you don&#39;t have this consistency protection, you may get a transaction header transaction. The transaction data is the end of the data transaction. The consistency of the transaction is a basis for the transaction block read. 2. At present, the mechanism for the complete read of the transaction is not considered at the time of get data acquisition. The main consideration is that the business requirement is not sensitive to transaction integrity. It is not difficult to ensure complete reading. You can describe what your business needs are based on your business needs. I understand what you mean But the last problem with binlog format doesn&#39;t know if you tried it. If format is mixed, then canal instance transaction size should be invalid because isDml returns false. Every time you receive a rowdata type entry, it flushes once. case ROWDATA: put(entry); // Direct output for non-DML data without buffer control EventType eventType = entry.getHeader().getEventType(); if (eventType != null && !isDml(eventType)) { flush(); } break;
132,Problem with filterEmtryTransactionEntry In the sinkData method of the EntryEventSink class, there is a code segment for judging filterEmtryTransactionEntry as shown below. if (filterEmtryTransactionEntry && !CollectionUtils.isEmpty(events)) { long currentTimestamp = events.get(0).getEntry().getHeader().getExecuteTime(); // Controlling the empty headers and tails based on certain policies to facilitate timely update of database sites indicating normal operation if (Math.abs(currentTimestamp - lastEmptyTransactionTimestamp) > emptyTransactionInterval || lastEmptyTransactionCount.incrementAndGet() > emptyTransctionThresold) { lastEmptyTransactionCount.set(0L); lastEmptyTransactionTimestamp = currentTimestamp; return doSink(events); } } I would like to ask the judgment here based on what purpose does not understand Set the size of the canal instance transaction size to 1024. Run a transaction on mysql for 1023 inserts. Then the canal server will generate 1025 events. When the logic of EventTransactionBuffer reaches 1024, it will flush once and then the remaining event Transactionend type will be performed separately. Flush, but because of the above code, the event is ignored. This should not be reasonable. The head and tail of the transaction are mainly used to distinguish the data of the transaction boundary itself. 1025 transaction The end event may indeed be ignored. In the implementation of the canal otter product, otter is not a strong demand for the filtered empty transaction event but takes up too much canal. The eventBuyer&#39;s ringBuffer space causes the overall service processing speed to be slower. So the design default value is true. To be considered and optimized. thank you for your reply It is true that the case where the transaction is truncated is a small probability event and it is an extreme event when a truncated event happens like the 1025 example. What you said is taking up too much canal The eventStor&#39;s ringBuffer space refers to the case where the mysql binlog format is not equal to the row. When the binlog format is equal to the row, each EventTransactionBuffer is sufficient for a transaction to be flushed unless the transaction is too large to be truncated, otherwise no empty transaction event will occur. The situation then there is no chance to filter this way and does not reduce the memory usage of the ringbuffer. When the binlog format is equal to mixed, the following code segment will enter the if statement. Basically, each statement will be flushed once, which will lead to many empty transaction events. case ROWDATA: put(entry); // Direct output for non-DML data without buffer control EventType eventType = entry.getHeader().getEventType(); if (eventType != null && !isDml(eventType)) { flush(); When the binlog format is mixed, the eventttyp is Query isDml returns false will flush } break; What you said “ Otter is not a strong demand for filtered null transaction events in the canal otter product configuration implementation How to understand this is still the example of 1025. If the TRANSACTIONEND event corresponding to 1025 is filtered, how can Otter define the transaction boundary? Otter non-transactional sensitivity. It mainly uses the transaction header and tail to update the location. It is not the overall submission by the transaction unit. You can see the data import wiki. Yep Non-transactional sensitivity mainly uses the transaction header and tail to update the site This understands Is the above statement about the binlog format correct? If the transaction is not truncated and the binlog foramt is a row, the scene with an empty transaction event should be gone. I found another scenario about the empty transaction event. When configuring the filter, if a table is filtered, the rowdata of the table is filtered. Event will be filtered out but TRANSACTIONBEGIN and TRANSACTIONEDN are not filtered out, so you still need to pass filterEmtryTransactionEntry Controlling what you said is taking up too much canal The eventStor&#39;s ringBuffer space should mainly refer to this situation. Yes, understand correctly RingBuffer is a fixed array size design so empty headers and tails will affect overall throughput
131,parse row data failed Mysql version 5 6 23 log When using otter for data synchronization, the following error will be reported. pid:2 nid:2 exception:canal:canal_mysql:com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: java.lang.IllegalArgumentException: bad format x exceed: 2147483638 999999999 at com.taobao.tddl.dbsync.binlog.LogBuffer.getDecimal0(LogBuffer.java:1355) at com.taobao.tddl.dbsync.binlog.LogBuffer.getDecimal(LogBuffer.java:1265) at com.taobao.tddl.dbsync.binlog.event.RowsLogBuffer.fetchValue(RowsLogBuffer.java:336) at com.taobao.tddl.dbsync.binlog.event.RowsLogBuffer.nextValue(RowsLogBuffer.java:96) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseOneRow(LogEventConvert.java:454) Trouble reporting the canal version and the otter version canal: 1.0.20 otter: 4.2.12 Mysql environment is a master-slave cluster to open a semi-synchronous plugin Even the library will report this error. No problem with the main library Tested the local mysql The 5 6 version has not found a similar error for the decimal type. If it is convenient, can you provide the binlog original file corresponding to the error location? Estimated to be the problem https github com alibaba canal issues 119 Use the master branch to build a package and try Also encountered this problem canal in the parsing Decimal type MYSQL_TYPE_NEWDECIMAL seems to have a problem with the code in the LogBuffer class getDecimal0 method I have changed a version locally and have fixed this issue but may need further refinement to submit the PR later. Welcome to submit a PR report to test the version of mysql and canal mysql Server version: 10.2.14-MariaDB-log Columnstore 1.1.4-1 canal 1.0.24 2018-05-10 17:42 GMT+08:00 agapple <notifications@github.com>: > Welcome to submit a PR report to test the version of mysql and canal > > — > You are receiving this because you commented. > Reply to this email directly view it on GitHub > <https://github.com/alibaba/canal/issues/131#issuecomment-388006054> or mute > the thread > <https://github.com/notifications/unsubscribe-auth/ASsS_qXHnsjFiIsbJ28I1VULEZax_zqjks5txAt6gaJpZM4GX7gy> > . > -- Best Regards Zhiyu Send the corresponding test decimal data. ​ mysql-bin.000014 <https://drive.google.com/file/d/1V9PHa3M4T4UhTPfHBElsqLFMi4ECJU2r/view?usp=drive_web> ​ Google hard drive should be accessible I changed my code yesterday and ran it on mysql5 6. I found that it is not working for mysql5 6. I suspect that the binlog format of mysql and columnstore is a bit different in dealing with decimal. 2018-05-10 19:14 GMT+08:00 agapple <notifications@github.com>: > Send the corresponding test decimal data. > > — > You are receiving this because you commented. > Reply to this email directly view it on GitHub > <https://github.com/alibaba/canal/issues/131#issuecomment-388025033> or mute > the thread > <https://github.com/notifications/unsubscribe-auth/ASsS_iT5TTAzZOB_09yhsHg3h4XE9xEzks5txCEfgaJpZM4GX7gy> > . > -- Best Regards Zhiyu Sql that has business test data They use the load command to load the csv file into the columnstore statement as follows LOAD DATA LOCAL INFILE '"+ ExportData[ Key ].Location +"' INTO TABLE "+ ExportData[ Key ].Table +" CHARACTER SET utf8 COLUMNS TERMINATED BY '|' IGNORE 1 LINES") Write a csv file if you want to reproduce it Then execute load should be able to 2018-05-15 17:58 GMT+08:00 agapple <notifications@github.com>: > Sql that has business test data > > — > You are receiving this because you commented. > Reply to this email directly view it on GitHub > <https://github.com/alibaba/canal/issues/131#issuecomment-389112064> or mute > the thread > <https://github.com/notifications/unsubscribe-auth/ASsS_qVV7534IKkprEIB-GLBjiwb-x6Oks5tyqa9gaJpZM4GX7gy> > . > -- Best Regards Zhiyu It is best to construct the corresponding sql statement to reproduce the problem and check the corresponding binlog parsing code and mysql Parser is consistent
130,mysql 5 6 version When the datetime value is null Sqltype parsing exception Problem Description mysql Version is 5 6 12 When the data of DATETIME type is set to NULL when inserting data, the sqlType of the corresponding field in the data obtained from the canal client is 1111 OTHER And in mysql In the 5 5 version, the same test is performed when the DATETIME is null. The corresponding sqlType obtained by the canal client is 93 TIMESTAMP. Is this a BUG? I will verify it first. Confirmation is a bug mysql5 6 added a new LogEvent type to increase the swtich judgment can be repaired
129,Unable to read Binlog surroundings Fedora 21 docker 1.8.1 docker mysql 5.5.45 Mysql configuration ``` [mysqld] server-id=1042776 log_bin=mysql-bin log_error=mysql-bin.err binlog-format=ROW ``` Canal configuration ``` [snowwolf@snowwolf-dev canal]$ cat conf/example/instance.properties ################################################# ## mysql serverId canal.instance.mysql.slaveId = 1234 # position info canal.instance.master.address = 127.0.0.1:33306 canal.instance.master.journal.name = canal.instance.master.position = canal.instance.master.timestamp = #canal.instance.standby.address = #canal.instance.standby.journal.name = #canal.instance.standby.position = #canal.instance.standby.timestamp = # username/password canal.instance.dbUsername = canal canal.instance.dbPassword = canal canal.instance.defaultDatabaseName = dbroute_01 canal.instance.connectionCharset = UTF-8 # table regex canal.instance.filter.regex = .*\\..* # table black regex canal.instance.filter.black.regex = ################################################# ``` Startup parameter ``` docker run --name dbrouter-mysql-dev -p 33306:3306 -v /etc/mysql/conf.d:/etc/mysql/conf.d -v /var/lib/docker-mysql:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=xxx -d mysql:5.5 ``` Log ``` 2015-09-08 21:11:34.687 [destination = example address = /127.0.0.1:33306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position by switch ::1441612069000 2015-09-08 21:11:34.701 [destination = example address = /127.0.0.1:33306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - Didn't find the corresponding binlog files from mysql-bin.000001 to mysql-bin.000003 2015-09-08 21:11:34.701 [destination = example address = /127.0.0.1:33306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /127.0.0.1:33306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: can't find start position for example 2015-09-08 21:11:34.702 [destination = example address = /127.0.0.1:33306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: can't find start position for example ] ``` Use mysqlbinlog to parse ``` [root@snowwolf-dev ~]# mysqlbinlog -h127.0.0.1 -P33306 -ucanal -pcanal --read-from-remote-server -v --start-position=296 -d dbroute_01 mysql-bin.000003 /*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE=1*/; /*!40019 SET @@session.max_insert_delayed_threads=0*/; /*!50003 SET @OLD_COMPLETION_TYPE=@@COMPLETION_TYPE COMPLETION_TYPE=0*/; DELIMITER /*!*/; # at 296 #150908 20:29:44 server id 1042776 end_log_pos 0 Start: binlog v 4 server v 5.5.45-log created 150908 20:29:44 BINLOG ' uNTuVQ9Y6Q8AZwAAAAAAAAAAAAQANS41LjQ1LWxvZwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA AAAAAAAAAAAAAAAAAAAAAAAAEzgNAAgAEgAEBAQEEgAAVAAEGggAAAAICAgCAA== '/*!*/; # at 296 #150908 20:33:05 server id 1042776 end_log_pos 431 Write_rows: table id 3979 flags: STMT_END_F BINLOG ' gdXuVRdY6Q8AhwAAAK8BAAAAAIsPAAAAAAEAGP///0DSPgMAAACyyO5VssjuVQEAAAAKMTIzNDU2 Nzg5MgwrMTM4MTAwMDEwMDIJAHRlc3R1c2VyMyBlMTBhZGMzOTQ5YmE1OWFiYmU1NmUwNTdmMjBm ODgzZQGyyO5VAQEABGFiY2QAAAAA '/*!*/; ### Row event for unknown table #3979# at 431 #150908 20:33:05 server id 1042776 end_log_pos 458 Xid = 914 COMMIT/*!*/; # at 458 #150908 20:57:33 server id 1042776 end_log_pos 532 Query thread_id=515 exec_time=0 error_code=0 SET TIMESTAMP=1441717053/*!*/; SET @@session.pseudo_thread_id=515/*!*/; SET @@session.foreign_key_checks=1 @@session.sql_auto_is_null=0 @@session.unique_checks=1 @@session.autocommit=1/*!*/; SET @@session.sql_mode=0/*!*/; SET @@session.auto_increment_increment=1 @@session.auto_increment_offset=1/*!*/; /*!\C utf8mb4 *//*!*/; SET @@session.character_set_client=45 @@session.collation_connection=224 @@session.collation_server=8/*!*/; SET @@session.lc_time_names=0/*!*/; SET @@session.collation_database=DEFAULT/*!*/; BEGIN /*!*/; # at 532 # at 639 #150908 20:57:33 server id 1042776 end_log_pos 639 Table_map: `dbroute_01`.`uc_member_000` mapped to number 3979 #150908 20:57:33 server id 1042776 end_log_pos 774 Delete_rows: table id 3979 flags: STMT_END_F BINLOG ' PdvuVRNY6Q8AawAAAH8CAAAAAIsPAAAAAAEACmRicm91dGVfMDEADXVjX21lbWJlcl8wMDAAGAMH BwMPDw8PDwEBBw8BDwECAQ8PDw8PBxg8ADwAPAAsAZYAlgDAADwAeAA8ADwAHgDg9z4= PdvuVRlY6Q8AhwAAAAYDAAAAAIsPAAAAAAEAGP///0DSPgMAAACyyO5VssjuVQEAAAAKMTIzNDU2 Nzg5MgwrMTM4MTAwMDEwMDIJAHRlc3R1c2VyMyBlMTBhZGMzOTQ5YmE1OWFiYmU1NmUwNTdmMjBm ODgzZQGyyO5VAQEABGFiY2QAAAAA '/*!*/; ### DELETE FROM `dbroute_01`.`uc_member_000` ### WHERE ### @1=3 ### @2=1441712306 ### @3=1441712306 ### @4=1 ### @5='1234567892' ### @6='+13810001002' ### @7=NULL ### @8='testuser3' ### @9='e10adc3949ba59abbe56e057f20f883e' ### @10=NULL ### @11=1 ### @12=1441712306 ### @13=NULL ### @14=1 ### @15=NULL ### @16=NULL ### @17=1 ### @18=NULL ### @19=NULL ### @20=NULL ### @21=NULL ### @22=NULL ### @23='abcd' ### @24=0 # at 774 #150908 20:57:33 server id 1042776 end_log_pos 801 Xid = 3451 COMMIT/*!*/; DELIMITER ; # End of log file ROLLBACK /* added by mysqlbinlog */; /*!50003 SET COMPLETION_TYPE=@OLD_COMPLETION_TYPE*/; /*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE=0*/; ``` MariaDB in non-docker 10 docker&#39;s 5 6 are all so what is the problem? prepare to find start position by switch ::1441612069000 Based on the timestamp location, the binlog cannot be found smaller than this time. Suggest Delete historical sites to resubscribe
128,fix rename table ddl parser tablename and schemaname error Previous rename table Regular expressions are matched Tablename or schemaname Inside to When you start the table name or the library name, there will be an abnormal match to the empty specific bad Case can see the test file tks
127,fix not skip filter and read the error data There are two byte filter fields in the official protocol. There is no skip here, which causes the following data to be misread. ``` lenenc_str catalog lenenc_str schema lenenc_str table lenenc_str org_table lenenc_str name lenenc_str org_name lenenc_int length of fixed-length fields [0c] 2 character set 4 column length 1 type 2 flags 1 decimals 2 filler [00] [00] if command was COM_FIELD_LIST { lenenc_int length of default-values string[$len] default values } ``` According to the official website, the filler Field should be 00 So I finally thought string length If it is also 0, no error will be reported. But I use this method to request SHOW BINARY LOGS discovery filter Returned is 31。 So I discovered this problem. Why is it not known to return 31? But the last bit of the reading here is definitely wrong. tks
126,Canal has plans to support data synchronization based on GTID? If MySQL uses VIP to go to HA, when MYSQL master-slave switchover, canal will report an error because it is not compatible with the binlog and position on the standby machine. If the binlog is based on GTID, it should solve this problem. MySQL GTID based on active and standby replication can solve the problem of location search after switching. Technically, specific time can not be guaranteed. Well understand thank you agapple When the class code is submitted, close this issue so that I know to try it. How is this progress? Is there any plan to support it? @agapple So that is by far canal Also no support GTID Is it Will you support GTID now? I don’t expect to have a completed classmate to contribute a PR. A long time ago I finished this function code slightly rough and can&#39;t PR I added GTID support based on 1 0 25 and need help review https github com alibaba canal pull 618 ## Change instructions 1. Increase the instance level configuration gdidon to indicate whether the instance has GTID mode enabled. 2. Driver package adds GTID related class GTIDSet UUIDSet is GTID indicates BinlogDumpGTIDCommandPacket is COM_BINLOG_DUMP_GTID command 3. Completing the resolution of the GtidLogEvent event under the dbsync package 4. The EntryType enumeration item is added under the protocol package. GTIDLOG is the GTIDLOG event in the binlog stream. The header is added. The field gtid is described here. 5. Parse package under ErosaConnection to increase the dump method according to GTID synchronization 6. Parse the package under the AbstractEventParser start. If the instance uses the GTID mode, send the COM_BINLOG_DUMP_GTID command to mysql. ## Add gtid field to CanalEntry Header When the slave synchronizes the binlog through the file location, each event header carries the filename. The position information is subsequently stored in the latest synchronization site as long as the filename of the last event acked by the client is taken. Position can The corresponding GTID mode needs to record the consumed GTIDSet https dev mysql com doc refman 5 7 en replication gtids concepts html After a transaction commits, the slave receives the following sequence of events GTID_LOG TRANSACTIONBEGIN ROWDATA TRANSACTIONEND. Only GTID_LOG has the GTID information of this transaction. Now the com alibaba otter canal parse inbound mysql dbsync LogEventConvertGTID class records the last GTID and assigns it to the Header gtid of subsequent events. I found that I can&#39;t send a pr to a tag. I adjusted the pr to the master. @agapple Com alibaba fastsql package 2 0 0_preview_228 version can provide a copy? @hiwjd There is a fastsql jar in the alpha binary package. Gtid&#39;s PR has been merged, thank you very much @hiwjd Contribution Https dev mysql com doc refman 5 7 en replication mode change online enable gtids html Open gtid
125,canal 1 0 20 with otter canal client 1 0 20 uses different versions of protobuf One is 2 4 1 and one is 2 5 leads to connection with canal client Error when canal ``` java java.lang.UnsupportedOperationException: This is supposed to be overridden by subclasses. at com.google.protobuf.GeneratedMessage.getUnknownFields(GeneratedMessage.java:180) ~[protobuf-java-2.5.0.jar:na] ``` The protobuf versions currently used by canal and otter are 2 4 1 At present it seems that 2 4 1 generated class 2 5 can not support the need to test using 2 5 after the old version of the user uses the 2 4 1 version can be used normally The problem is in maven There is no specified version of protobuf in the dependency of the canal protocol in the repository. http://search.maven.org/#artifactdetails%7Ccom.alibaba.otter%7Ccanal.protocol%7C1.0.20%7Cjar ``` xml <dependency> <groupId>com.google.protobuf</groupId> <artifactId>protobuf-java</artifactId> </dependency> ``` The default will be 2 5 0 Https repo1 maven org maven2 com alibaba otter canal 1 0 20 canal 1 0 20 pom The parent project will display the statement protobuf version is 2 4 1 Change the protobuf version in the pom and then regenerate the protocol file locally and put it in the protocol src main java com alibaba otter canal protocol to solve this problem or directly clone my code https://github.com/chaopengio/canal Https github com alibaba canal commit c86fba6092979c4d42734903829843c3d62342ae Someone has already submitted a PR upgrade to 2 6 1
124,fixed a hard coded mysql host ip address tks
123,Update Event is recognized as INSERT EVENT when the table is stored as ndbcluster and pk is set at the same time Create such a table here with both PK and ndbcluster The designation of the engine only specifies that one will not trigger this situation. create table test3col(id int(3) name varchar(10) last varchar(10) primary key(id))engine=ndbcluster; mysql> insert into test3col values(1 'a' 'b'); mysql> update test3col set last='c' where id=1; The log printed by SimpleCilent can be found that the update command is considered to be an insert type and only shows the primary key and the update column. ================> binlog[log-bin.000001:3946366] name[cudb_user_data test3col] eventType : CREATE empty count : 1 empty count : 2 empty count : 3 empty count : 4 empty count : 5 empty count : 6 ================> binlog[log-bin.000001:3946753] name[mysql ndb_apply_status] eventType : INSERT server_id : 501 update=true epoch : 15301516776701952 update=true log_name : update=true start_pos : 0 update=true end_pos : 0 update=true ================> binlog[log-bin.000001:3946814] name[cudb_user_data test3col] eventType : INSERT id : 1 update=true name : a update=true last : b update=true ================> binlog[log-bin.000001:3947108] name[mysql ndb_apply_status] eventType : INSERT server_id : 501 update=true epoch : 15301538251538433 update=true log_name : update=true start_pos : 0 update=true end_pos : 0 update=true ================> binlog[log-bin.000001:3947169] name[cudb_user_data test3col] eventType : INSERT id : 1 update=true last : c update=true 1. Only show the primary key and change the column estimate and your mysql version related to mysql5 6 support minial Image mode only records change columns and primary keys 2. As for the update is considered insert personal preliminary judgment and ndbcluster related has not been tested before I suggest you look directly at the show binlog Events look at the corresponding update Sql binlog object mysql 5.6 mysql> show binlog events in 'log-bin.000001' from 3960268 limit 100; +----------------+---------+------------+-----------+-------------+-----------------------------------------+ | Log_name | Pos | Event_type | Server_id | End_log_pos | Info | +----------------+---------+------------+-----------+-------------+-----------------------------------------+ | log-bin.000001 | 3960268 | Write_rows | 501 | 3960329 | table_id: 34 | | log-bin.000001 | 3960329 | Write_rows | 501 | 3960368 | table_id: 521 flags: STMT_END_F | | log-bin.000001 | 3960368 | Query | 501 | 3960433 | COMMIT | | log-bin.000001 | 3960433 | Query | 501 | 3960497 | BEGIN | | log-bin.000001 | 3960497 | Table_map | 501 | 3960560 | table_id: 521 (cudb_user_data.test3col) | | log-bin.000001 | 3960560 | Table_map | 501 | 3960622 | table_id: 34 (mysql.ndb_apply_status) | | log-bin.000001 | 3960622 | Write_rows | 501 | 3960683 | table_id: 34 | | log-bin.000001 | 3960683 | Write_rows | 501 | 3960723 | table_id: 521 flags: STMT_END_F | | log-bin.000001 | 3960723 | Query | 501 | 3960788 | COMMIT | +----------------+---------+------------+-----------+-------------+-----------------------------------------+ 9 rows in set (0.00 sec) among them | log-bin.000001 | 3960329 | Write_rows | 501 | 3960368 | table_id: 521 flags: STMT_END_F Corresponding to an insert Statement | log-bin.000001 | 3960683 | Write_rows | 501 | 3960723 | table_id: 521 flags: STMT_END_F Corresponding to an update statement Can&#39;t see the difference between insert and update from here For an InnoDB table, you can see that event_type is divided into Write_rows and Update_rows. mysql> show binlog events in 'log-bin.000001' from 3960268 limit 100; +----------------+---------+-------------+-----------+-------------+-----------------------------------------+ | Log_name | Pos | Event_type | Server_id | End_log_pos | Info | +----------------+---------+-------------+-----------+-------------+-----------------------------------------+ | log-bin.000001 | 3960268 | Write_rows | 501 | 3960329 | table_id: 34 | | log-bin.000001 | 3960329 | Write_rows | 501 | 3960368 | table_id: 521 flags: STMT_END_F | | log-bin.000001 | 3960368 | Query | 501 | 3960433 | COMMIT | | log-bin.000001 | 3960433 | Query | 501 | 3960497 | BEGIN | | log-bin.000001 | 3960497 | Table_map | 501 | 3960560 | table_id: 521 (cudb_user_data.test3col) | | log-bin.000001 | 3960560 | Table_map | 501 | 3960622 | table_id: 34 (mysql.ndb_apply_status) | | log-bin.000001 | 3960622 | Write_rows | 501 | 3960683 | table_id: 34 | | log-bin.000001 | 3960683 | Write_rows | 501 | 3960723 | table_id: 521 flags: STMT_END_F | | log-bin.000001 | 3960723 | Query | 501 | 3960788 | COMMIT | | log-bin.000001 | 3960788 | Query | 501 | 3960852 | BEGIN | | log-bin.000001 | 3960852 | Table_map | 501 | 3960915 | table_id: 521 (cudb_user_data.test3col) | | log-bin.000001 | 3960915 | Table_map | 501 | 3960977 | table_id: 34 (mysql.ndb_apply_status) | | log-bin.000001 | 3960977 | Write_rows | 501 | 3961038 | table_id: 34 | | log-bin.000001 | 3961038 | Write_rows | 501 | 3961078 | table_id: 521 flags: STMT_END_F | | log-bin.000001 | 3961078 | Query | 501 | 3961143 | COMMIT | | log-bin.000001 | 3961143 | Query | 501 | 3961207 | BEGIN | | log-bin.000001 | 3961207 | Table_map | 501 | 3961270 | table_id: 521 (cudb_user_data.test3col) | | log-bin.000001 | 3961270 | Table_map | 501 | 3961332 | table_id: 34 (mysql.ndb_apply_status) | | log-bin.000001 | 3961332 | Write_rows | 501 | 3961393 | table_id: 34 | | log-bin.000001 | 3961393 | Write_rows | 501 | 3961431 | table_id: 521 flags: STMT_END_F | | log-bin.000001 | 3961431 | Query | 501 | 3961496 | COMMIT | | log-bin.000001 | 3961496 | Query | 501 | 3961574 | BEGIN | | log-bin.000001 | 3961574 | Table_map | 501 | 3961630 | table_id: 510 (cudb_user_data.test) | | log-bin.000001 | 3961630 | Write_rows | 501 | 3961668 | table_id: 510 flags: STMT_END_F | | log-bin.000001 | 3961668 | Xid | 501 | 3961695 | COMMIT /\* xid=17139826 _/ | | log-bin.000001 | 3961695 | Query | 501 | 3961773 | BEGIN | | log-bin.000001 | 3961773 | Table_map | 501 | 3961829 | table_id: 510 (cudb_user_data.test) | | log-bin.000001 | 3961829 | Update_rows | 501 | 3961876 | table_id: 510 flags: STMT_END_F | | log-bin.000001 | 3961876 | Xid | 501 | 3961903 | COMMIT /_ xid=17139887 */ | +----------------+---------+-------------+-----------+-------------+-----------------------------------------+ Event through binlog Can the type distinguish between insert and update? Write_rows Update_rows Delete_rows binlog will have a strict distinction for different types. Here is the record for the insert type estimate and the ndb storage you selected.
122,Repair rename The table command contains multiple table parsing failures. Simple repair rename The table command contains multiple tables. Such as https://github.com/alibaba/canal/issues/79 Mentioned in Subsequent perfect solution using mysql similar to druid cobar Sql syntax parsing extraction corresponding table Is an ideal solution Because the canal gets the binlog delay, it may get the binlog. The table structure obtained by the desc command is actually different from the table structure when the binlog is generated. The best solution is that ddl statements directly trigger tablemeta Corresponding change of cache instead of clear tablemetacache。 1 only drop table Clear tablemeta cache 2 only collected table metacache The binlog of the table not in the table will be desc Get the table structure 3 ddl statements are directly applied to tablemeta Cache This is the only way to minimize the occurrence of errors. This is a temporary fix to solve the current problems encountered. rename The table command contains multiple tables.
121,Update README.md tks
120,Added a script to get mysql automatically master Binlog information and automatically update the instance configuration file tks
119,Support for the noblob minimal binlog analysis of mysql5 6 mysql 5 6 starts to support the binlog_row_image property Allows the application to control the contents of the before after field in the binlog. Previously, all fields were recorded by default. 文档 http dev mysql com doc refman 5 6 en replication options binary log html sysvar_binlog_row_image  stand by 1. Analyze recognition changeColumns For the case of noblob minimal Return only the field types in the binlog 2. MysqlEventParser increases the control of supportBinlogFormats Images Allows the user to define the type of support that is expected to be supported if db does not match this type. (for otter Only row mode is supported + FULL image)
118,I am starting the node in windows 4.2.11 Report the following error Listening for transport dt_socket at address: 8099 阌栾 Gallium sentence 垨镞犳硶锷犺 垨镞犳硶锷犺 涓荤 澶勪簬 揿紑 揿紑 揿紑 舵 并 1. Solve the garbled problem first 2. Listening this log is normal output
117,Now many times the binlog format is mixed. Now many times the binlog format is mixed. Will you support this type in the future? Currently canal version resolution supports mixed statement row format
116,ArrayIndexOutOfBoundsException: 0 see : https://github.com/alibaba/otter/issues/76
115,Typo javadoc Correction Update guava To 18 0 Fixed embeded -> embedded section javadoc Correction Update guava To 18 0 Mainly mainly MapMaker of computeMake At 18 0 is the package level There is currently no replacement implementation From Map change to LoadingCache Too troublesome Just expose the package level method Still the original formula therefore guava Is replaceable with other versions No new features used Thank you for your pull request Fixed some of my low-level errors maybe code Style and format have some differences, merge the code and then format it to ensure the uniformity of the internal code style. Please forgive me. 👌
114,Can I update it? Guava version canal Inside guava r09 It is impossible to develop such an old version now. So can&#39;t canal Service embedded in the app for direct use Can you accept a new version? If acceptable I can change it. then pr Currently my branch is ready to use the updated version. guava Up ``` <dependency> <groupId>com.github.wenerme.canal</groupId> <artifactId>canal.server</artifactId> <version>596b060</version> </dependency> <repository> <id>jitpack.io</id> <url>https://jitpack.io</url> </repository> ``` Done at 6a0733bbab7b600c0d41afa8a421e9c443ddecbc
113,add missing api parameter doc Com alibaba otter canal client CanalConnectors method did not change javadoc read code when it caused some confusion tks
112,Mysql5 6 time millisecond precision support Support mysql5 6 millisecond precision analysis ## Test case CREATE TABLE `t1` ( `id` int(11) NOT NULL AUTO_INCREMENT `time0` time DEFAULT NULL `time1` time(1) DEFAULT NULL `time2` time(2) DEFAULT NULL `time3` time(3) DEFAULT NULL `time4` time(4) DEFAULT NULL `time5` time(5) DEFAULT NULL `time6` time(6) DEFAULT NULL `timestamp0` timestamp NOT NULL DEFAULT '0000-00-00 00:00:00' `timestamp1` timestamp(1) NOT NULL DEFAULT '0000-00-00 00:00:00.0' `timestamp2` timestamp(2) NOT NULL DEFAULT '0000-00-00 00:00:00.00' `timestamp3` timestamp(3) NOT NULL DEFAULT '0000-00-00 00:00:00.000' `timestamp4` timestamp(4) NOT NULL DEFAULT '0000-00-00 00:00:00.0000' `timestamp5` timestamp(5) NOT NULL DEFAULT '0000-00-00 00:00:00.00000' `timestamp6` timestamp(6) NOT NULL DEFAULT '0000-00-00 00:00:00.000000' `datetime0` datetime DEFAULT NULL `datetime1` datetime(1) DEFAULT NULL `datetime2` datetime(2) DEFAULT NULL `datetime3` datetime(3) DEFAULT NULL `datetime4` datetime(4) DEFAULT NULL `datetime5` datetime(5) DEFAULT NULL `datetime6` datetime(6) DEFAULT NULL PRIMARY KEY (`id`) ## ) ENGINE=InnoDB AUTO_INCREMENT=7 DEFAULT CHARSET=utf8 Test Data 1. insert into t1 values(null '00:00:00.0' '00:00:00.1' '00:00:00.02' '00:00:00.003' '00:00:00.1004' '00:00:00.1005' '00:00:00.101016' '2015-03-30 16:42:39.0' '2015-03-30 16:42:39.0' '2015-03-30 16:42:39.01' '2015-03-30 16:42:39.032' '2015-03-30 16:42:39.1023' '2015-03-30 16:42:39.10132' '2015-03-30 16:42:39.121232' '2015-03-30 16:42:39.0' '2015-03-30 16:42:39.0' '2015-03-30 16:42:39.01' '2015-03-30 16:42:39.032' '2015-03-30 16:42:39.1023' '2015-03-30 16:42:39.10132' '2015-03-30 16:42:39.121232'); 2. insert into t1 values(null '16:42:39.0' '16:42:39.1' '16:42:39.02' '16:42:39.003' '16:42:39.1004' '16:42:39.1005' '16:42:39.101016' '2015-03-30 16:42:39.0' '2015-03-30 16:42:39.0' '2015-03-30 16:42:39.01' '2015-03-30 16:42:39.032' '2015-03-30 16:42:39.1023' '2015-03-30 16:42:39.10132' '2015-03-30 16:42:39.121232' '2015-03-30 16:42:39.0' '2015-03-30 16:42:39.0' '2015-03-30 16:42:39.01' '2015-03-30 16:42:39.032' '2015-03-30 16:42:39.1023' '2015-03-30 16:42:39.10132' '2015-03-30 16:42:39.121232') 3. insert into t1 values(null '-16:42:39.0' '-16:42:39.1' '-16:42:39.02' '-16:42:39.003' '16:42:39.1004' '-16:42:39.1005' '-16:42:39.101016' '2015-03-30 16:42:39.0' '2015-03-30 16:42:39.0' '2015-03-30 16:42:39.01' '2015-03-30 16:42:39.032' '2015-03-30 16:42:39.1023' '2015-03-30 16:42:39.10132' '2015-03-30 16:42:39.121232' '2015-03-30 16:42:39.0' '2015-03-30 16:42:39.0' '2015-03-30 16:42:39.01' '2015-03-30 16:42:39.032' '2015-03-30 16:42:39.1023' '2015-03-30 16:42:39.10132' '2015-03-30 16:42:39.121232');
111,Whether to consider the merge module There are too many modules at present. Currently 15 modules There are very few content in some modules Inconvenient to develop and read Often jumping between modules For the time being, I don’t think about the merger. I can consider packaging a spring-like package.
110,FileLogFetcher's fetch() method should fix up position and origin even i... ...f we weren't successful at fetching anything from the file
109,Could not find first log file name in binary log index file 2015-01-15 10:40:01.756 [destination = example address = /127.0.0.1:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: Received error packet: errno = 1236 sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:116) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) at java.lang.Thread.run(Thread.java:662) I saw the mysql bin index and mysql bin 000001 in the data directory. But the error said that the index could not be found. File I am using ubuntu system, I don&#39;t know if it is related to the system. I have no problem testing under windows. This should be related to your mysql and specify a canal start site. If there is a problem, you can verify the mysql by command. binlog show binlog events in 'mysql-bin.000001' from xxx limit 10; already solved Is the linux file permissions issue @NewsGitHub I would like to ask which file permissions issue I have encountered the same problem here. @NewsGitHub Pro gave the log R The 777&#39;s permissions seem to be no good. @changsong Delete the meta dat under canal and restart it. The solution to this problem is not described clearly. I specified the location or not. I am also stuck here on the mysql side found that canal this user has been executing Binlog Dump How can this be related to permissions? If canal is not native,
108,New version of compiled code prompts memory overflow parse events has an errorjava.lang.OutOfMemoryError: Java heap space at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57) ~[na:1.7.0_67] at java.nio.ByteBuffer.allocate(ByteBuffer.java:331) ~[na:1.7.0_67] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.readNextPacket(SimpleCanalConnector.java:364) ~[classes/:na] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.doConnect(SimpleCanalConnector.java:129) ~[classes/:na] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.connect(SimpleCanalConnector.java:97) ~[classes/:na] at com.alibaba.otter.canal.example.AbstractCanalClientTest.process(AbstractCanalClientTest.java:110) ~[classes/:na] at com.alibaba.otter.canal.example.AbstractCanalClientTest$2.run(AbstractCanalClientTest.java:80) ~[classes/:na] at java.lang.Thread.run(Thread.java:745) ~[na:1.7.0_67] ## stop the canal client## canal client is down. Running mvn try to adjust the jvm parameter of mvn
107,canal Parsing mysql exception 2015-01-07 17:37:51.436 [destination = example address = /192.168.0.20:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /192.168.0.20:3306 has an error retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. java.lang.IllegalArgumentException: bad format x exceed: 2121395706 1000000000 What is the problem caused by mysql is slave and binlog is not in the default directory Please provide mysql version information and canal version information mysql5.6.21 Canal1 0 20 one test report on the slave library 2015 01 14 10:19:05.121 [main] WARN org.springframework.beans.TypeConverterDelegate - PropertyEditor [com.sun.beans.editors.EnumEditor] found through deprecated global PropertyEditorManager fallback - consider using a more isolated form of registration e.g. on the BeanWrapper/BeanFactory!2015-01-14 10:19:05.231 [main] INFO c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-example2015-01-14 10:19:05.365 [main] INFO c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start successful....2015-01-14 10:19:05.366 [destination = example address = /192.168.0.20:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position by switch ::14207065360002015-01-14 10:19:05.770 [destination = example address = /192.168.0.20:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - ERROR ## parse this event has an error last position : [EntryPosition[included=false journalName=mysql-bin.000092 position=59469 timestamp=1421036672000]]com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed.java.lang.IllegalArgumentException: bad format x exceed: 2016149504 1000000000 at com.taobao.tddl.dbsync.binlog.LogBuffer.getDecimal0(LogBuffer.java:1635) ~[canal.parse.dbsync-1.0.19.jar:na] at com.taobao.tddl.dbsync.binlog.LogBuffer.getDecimal(LogBuffer.java:1561) ~[canal.parse.dbsync-1.0.19.jar:na] at com.taobao.tddl.dbsync.binlog.event.RowsLogBuffer.fetchValue(RowsLogBuffer.java:335) ~[canal.parse.dbsync-1.0.19.jar:na] at com.taobao.tddl.dbsync.binlog.event.RowsLogBuffer.nextValue(RowsLogBuffer.java:95) ~[canal.parse.dbsync-1.0.19.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseOneRow(LogEventConvert.java:449) ~[canal.parse-1.0.19.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:364) ~[canal.parse-1.0.19.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:109) ~[canal.parse-1.0.19.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:62) ~[canal.parse-1.0.19.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:323) ~[canal.parse-1.0.19.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:176) ~[canal.parse-1.0.19.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:124) [canal.parse-1.0.19.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) [canal.parse-1.0.19.jar:na] at java.lang.Thread.run(Thread.java:744) Na 1 7 0_51 two in the master test report 015 01 14 10:10:28.992 [main] WARN org.springframework.beans.TypeConverterDelegate - PropertyEditor [com.sun.beans.editors.EnumEditor] found through deprecated global PropertyEditorManager fallback - consider using a more isolated form of registration e.g. on the BeanWrapper/BeanFactory!2015-01-14 10:10:29.118 [main] INFO c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-example2015-01-14 10:10:29.278 [main] INFO c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start successful....2015-01-14 10:10:29.279 [destination = example address = /192.168.0.21:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position by switch ::14207065360002015-01-14 10:10:29.371 [destination = example address = /192.168.0.21:3306 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - Didn't find the corresponding binlog files from mysql-bin.000001 to mysql-bin.000002 2015-01-14 10:10:29.376 [destination = example address = /192.168.0.21:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /192.168.0.21:3306 has an error retrying. caused bycom.alibaba.otter.canal.parse.exception.CanalParseException: can't find start position for example Li Xun ruson Shanghai Hadoop Engineer ELEME IncEmail：xun.li@ele.me | Mobile:18201873319http://ele.me Are you hungry? The original message Sender Agapplenotifications github com recipient Alibaba canalcanal noreply github com Rusongitxun li ele me send time Tuesday, January 13, 2015 18 41 theme Re: [canal] canal Parsing mysql exception 107 Please provide mysql version information and canal version information —Reply to this email directly or view it on GitHub. mysql-bin.000092 position=59469 Timestamp 1421036672000 Convenient to provide the corresponding binlog file for me not to test the corresponding mysql 5 6 22 version of the decimal type parsing is normal This is our online library is not convenient to provide binlog. Our library is slave library is not caused by this Li Xun ruson Shanghai Hadoop Engineer ELEME IncEmail：xun.li@ele.me | Mobile:18201873319http://ele.me Are you hungry? The original message Sender Agapplenotifications github com recipient Alibaba canalcanal noreply github com Rusongitxun li ele me send time Wednesday, January 14, 2015 15 17 themes Re: [canal] canal Parsing mysql exception (#107)mysql-bin.000092 position=59469 Timestamp 1421036672000 Convenient to provide the corresponding binlog file for me not to test the corresponding mysql 5 6 22 version of the decimal type parsing is normal —Reply to this email directly or view it on GitHub. Not sure if I tried the 5 6 22 version is normal, you can try to reproduce the problem in your test library and send me the binlog. Initial suspicion is that such problems are closed first see : https://github.com/alibaba/canal/issues/119
106,CanalController java exception handling is not standardized Problem Description Reading the configuration file when the project introduces the xerces jar xercesImpl jar will not work properly but because of the CanalController java Line 286 Lack of processing exceptions for exceptions is swallowed and it is difficult to output abnormal information to increase the difficulty of troubleshooting. Exception information ``` org.springframework.beans.factory.BeanDefinitionStoreException: Parser configuration exception parsing XML from class path resource [spring/file-instance.xml]; nested exception is javax.xml.parsers.ParserConfigurationException: Unable to validate using XSD: Your JAXP provider [org.apache.xerces.jaxp.DocumentBuilderFactoryImpl@2e1551b0] does not support XML Schema. Are you running on Java 1.4 with Apache Crimson? Upgrade to Apache Xerces (or Java 1.5) for full XSD support. ``` Suggest Increase try Catch and log the exception log ``` try { // This variable is used when setting the currently loaded channel to load the spring lookup file. System.setProperty(CanalConstants.CANAL_DESTINATION_PROPERTY destination); instanceGenerator.setBeanFactory(getBeanFactory(config.getSpringXml())); instance = instanceGenerator.generate(destination); }catch(Exception e){ if(logger.isErrorEnabled()){ logger.error("generator instance failed." e); } }finally { System.setProperty(CanalConstants.CANAL_DESTINATION_PROPERTY ""); } ``` already fixed
105,Mysql5 6 after the checksum is turned on Find the wrong location based on the time finding site The problem description in the mysql5 6 later version because the introduction of the checksum mechanism will add a 4-byte checksum at the end of the normal event. In the seek method based on time lookup, the decoder parses the event event that does not recognize the FORMAT format, resulting in no determination to open. Checksum therefore causes the normal transaction header BEGIN to have 4 more bytes and judge whether the transaction header uses strict string matching because the extra 4 bytes cause the event type error. The scope of influence is based on mysql5 6 + Opened checksum + Involving time lookups such as active/standby switchover and timestamp based start modify 1. Increase format type resolution ``` LogDecoder decoder = new LogDecoder(); decoder.handle(LogEvent.ROTATE_EVENT); decoder.handle(LogEvent.FORMAT_DESCRIPTION_EVENT); decoder.handle(LogEvent.QUERY_EVENT); decoder.handle(LogEvent.XID_EVENT); LogContext context = new LogContext(); while (fetcher.fetch()) { LogEvent event = null; event = decoder.decode(fetcher context); if (event == null) { throw new CanalParseException("parse failed"); } if (!func.sink(event)) { break; } } ``` 1. Multiple judgments to increase the transaction begin end based on time lookup 13bf7c85a037a9e24d26c8f5c0aa007441fd9ae4 1. Multiple judgments to increase the transaction begin end based on time lookup Do canal based on version 1 0 19 instance ha Be by this issues I didn’t find the correct position. Lost data And there is no abnormality in the log and later contrast switch time with After ha switching binlog timestamp Found only https://github.com/alibaba/canal/commit/13bf7c85a037a9e24d26c8f5c0aa007441fd9ae4 It is recommended to fix the above patch as soon as possible in the lower version.
104,Fixed an issue where duplicate subscriptions didn&#39;t really modify the filter I only use the default group xml inside the configuration group mode has not changed
103,Parse parsing automatic switching failed The reason is that the eventParser inside the haController is not assigned a value. The problem is not clear enough to describe the conditions for recreating the problem.
102,Fix the plugin&#39;s declaration tks
101,Setting filterTransactionEntry causes client to get cursor in MetaManager Not updated 在EvenSink中 https github com alibaba canal blob master sink src main java com alibaba otter canal sink entry EntryEventSink java L77 Here the code will filter to the TRANSACTION_BEGIN and END events when filterTransactionEntry is set to true. And in getWithoutAck -> getEvents -> com/alibaba/otter/canal/store/memory/MemoryEventStoreWithBuffer.java Moderate doGet You can see the setting LogPositionRange in the method. The ack attribute is based on whether or not TRANSACTION_BEGIN exists in this range. And the END event to set If filterTransactionEntry is set Then the ack of the positionRange corresponding to the message returned to the client is null. The server executes the ack method when the client confirms https://github.com/alibaba/canal/blob/master/server/src/main/java/com/alibaba/otter/canal/server/embeded/CanalServerWithEmbeded.java#L348 Here you will find that if ack is null then the meta information is not updated. Looked at the code, although you forced to update the cursor but did not solve the positionRanges getAck null value. If the null value still ignores this update, it should return the last data point. ps. Such an update will still have some problems because your site may record the first update in the transaction. After re-acquiring the binlog after the next reboot, the tableId cannot be found. Oh, filterTransaction, this option was originally designed for what happened. From ChinaXing iPhone > in October 10, 2014 18 32 agapple notifications@github.com Write > > Looked at the code, although you forced to update the cursor but did not solve the positionRanges getAck null value. If the null value still ignores this update, it should return the last data point. > > ps. Such an update will still have some problems because your site may record the first update in the transaction. After re-acquiring the binlog after the next reboot, the tableId cannot be found. > > — > Reply to this email directly or view it on GitHub. This temporary should not be used before the main consideration is the group instance xml mode. When multiple library data is merged, the object begin end is ignored. Later, a version can be made to merge the entire transaction together. This code is equivalent to no use. The begin end of a transaction I have a mechanism that ignores empty transactions You have the need to ignore the transaction begin end
100,Setting filterTransactionEntry causes client to get cursor in MetaManager Not updated 在EvenSink中 https github com alibaba canal blob master sink src main java com alibaba otter canal sink entry EntryEventSink java L77 Here the code will filter to the TRANSACTION_BEGIN and END events when filterTransactionEntry is set to true. And in getWithoutAck -> getEvents -> com/alibaba/otter/canal/store/memory/MemoryEventStoreWithBuffer.java Moderate doGet You can see the setting LogPositionRange in the method. The ack attribute is based on whether or not TRANSACTION_BEGIN exists in this range. And the END event to set If filterTransactionEntry is set Then the ack of the positionRange corresponding to the message returned to the client is null. The server executes the ack method when the client confirms https://github.com/alibaba/canal/blob/master/server/src/main/java/com/alibaba/otter/canal/server/embeded/CanalServerWithEmbeded.java#L348 Here you will find that if ack is null then the meta information is not updated.
99,ParseEvent fails to continue in some cases after restarting canal The exception is as follows 2014-09-23 12:49:09.635 [destination = db2 address = /10.15.2.116:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:db2[com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.TableIdNotFoundException: not found tableId:71 This exception will continue to be repeated, causing the consumption of binlog to stay in this LogPosition without going down. Trigger scene Start after stopping canal The reason for the abnormal situation is inferred because the binlog encountered after the restart The event does not have a TABLE_MAP_EVENT at the beginning and happens to directly follow the event. The table field of the logContext is not set. By querying for the occurrence of an exception at ``` com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert private Entry parseRowsEvent(RowsLogEvent event) { try { TableMapLogEvent table = event.getTable(); if (table == null) { // The record corresponding to tableId does not exist. throw new TableIdNotFoundException("not found tableId:" + event.getTableId()); } ... ``` Event table field setting ``` com.taobao.tddl.dbsync.binlog.event.RowsLogEvent public final void fillTable(LogContext context) { table = context.getTable(tableId); // end of statement check: if ((flags & RowsLogEvent.STMT_END_F) != 0) { // Now is safe to clear ignored map (clear_tables will also // delete original table map events stored in the map). context.clearAllTables(); } } ``` ``` com.taobao.tddl.dbsync.binlog.LogContext public final void putTable(TableMapLogEvent mapEvent) { mapOfTable.put(Long.valueOf(mapEvent.getTableId()) mapEvent); } ``` ``` com.taobao.tddl.dbsync.binlog.LogDecoder.decode method case LogEvent.TABLE_MAP_EVENT: { TableMapLogEvent mapEvent = new TableMapLogEvent(header buffer descriptionEvent); /* updating position in context */ logPosition.position = header.getLogPos(); context.putTable(mapEvent); return mapEvent; } ``` The TableIdNotFoundException occurs mainly because the start position of your binlog is not the beginning of a transaction or the end can only reset the lower site. Oh is through the show bin log events Then find a nearby location 2014-09-30 10:11 GMT+08:00 agapple notifications@github.com: > The TableIdNotFoundException occurs mainly because the start position of your binlog is not the beginning of a transaction or the end can only reset the lower site. > > — > Reply to this email directly or view it on GitHub > https://github.com/alibaba/canal/issues/99#issuecomment-57258051. ## --- Best Regards ! Chen Yunxing Tel : 1866122992 Yes
98,fix bug in test OS with 64bit (follow symbole link files) Fix detection OS Is it a 64bit bug? 1. When JAVA When symbolizing a file Not recognized 2. `$JAVA_HOME/bin/java /= $JAVA`
97,group Configuration implementation Hello, I saw group instance xml Found that two eventParser configured the same destination and the following master And standby Only one database asks how to implement N 1 of Thank you Found the master with standby Two libraries are configured I still have some questions here. 1. group-instance.xml 中logPosition When using Memory, this is the case when the fault is restored. Will become a binlog Last position show master Status will cause a part of the bin log Event lost 2. Based on zk logPosition Persistent implementations cannot be used in group instance xml because of parser1 with parser2 All the same destination value will get the same data in zk 3. Regarding the N 1 mode, multiple db data ordering is not guaranteed. Is there any problem in some cases? The group mode of the site record is based on the timestamp of each destination only one copy and then the different parser based on the timestamp to relocate the duplicate data compared to the single library mode
96,Table blacklist is off by default The current table blacklist definition for the spring mode is configured with a null value for the AviaterRegexFilter null value meaning a full match so all the data is filtered as a blacklist. AviaterRegexFilter has added a special attribute to handle null values.
95,Canal parsing will mysql Set type to unsigned Long type The data representation of set in mysql is described by bit For example, set a 'b 'c ' d insert a a The record of c is represented by bit. It is 1010. It is converted to decimal. It is 5 Note that it is litten endian.
94,canal/example/src/main/bin/startup.sh ALIBABA_JAVA="/usr/alibaba/java/bin/java" TAOBAO_JAVA="/opt/taobao/java/bin/java" ... elif [ -f $ALIBABA_JAVA ] ; then JAVA=$TAOBAO_JAVA ... elif [ -f $TAOBAO_JAVA ] ; then JAVA=$TAOBAO_JAVA ... Seeing the script problem should be like this ... elif [ -f $ALIBABA_JAVA ] ; then JAVA=$ALIBABA_JAVA ... elif [ -f $TAOBAO_JAVA ] ; then JAVA=$TAOBAO_JAVA ...
93,Support table blacklist definition The 1 0 17 version subscription table currently only supports forward matching and the current regular expression is not very convenient for the need to exclude a table separately. Hopefully there can be a whitelist blacklist definition. Strategy 1. If there is only a whitelist, then only forward matching is done. 2. If there is only a blacklist, then only blacklist matching is excluded. 3. If the black and white list is first whitelisted and then blacklisted Whitelist does not work without blacklisting Look at the issue #96 already fixed
92,Skip if a parsing exception occurs Skip the parsing exception and record the corresponding data log for the occurrence of the column inconsistency table. The default is false for the meat processing. Error data can be reported by Alarm, such as otter manager
91,SimpleCanalConnector throws should when competing connect first的Exception _Repro_: 1. Use the default instance mode with Canalkeeper support Server； 2. Setup successfully connected when the first ClusterCanalClient calls connect 3. Setup will throw Exception when the second ClusterCanalClient calls connect. com.alibaba.otter.canal.protocol.exception.CanalClientException: should connect first at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.waitClientRunning(SimpleCanalConnector.java:418) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.subscribe(SimpleCanalConnector.java:186) at com.alibaba.otter.canal.client.impl.ClusterCanalConnector.subscribe(ClusterCanalConnector.java:100) _Expected_: The second ClusterCanalClient should be blocked while connected to other clients. disconnect。 Speculative reasons see the following gist modification https://gist.github.com/caojia/10960073/revisions Your understanding is to estimate the modification of another bug, consider the incomplete code, I made the next modification, thank you. Thank you, if you call waitClientRunning in the connect function, you should also pass a false parameter. In addition, will you push this change to maven? After the verification is passed, it will be submitted to the maven repository.
90,Recovery after Master hangs Hi jianghang， Hello, if in the architecture of Canal, if the Mysql Master hangs, the new Slave is promoted into a new Master Canal to find the location of the new Master corresponding to the location of the original Master in the original Master. Is it possible to guarantee the change in Canal? Event is Exactly Once Thank you Does not guarantee Exactly Once there will be duplicate data
89,The client keeps reconnecting and the client connection thread is stuck after the server is started. When there is only one canal When the server is running, the client keeps reconnecting to the server. When the server is started, the client connection thread will be stuck. The problem occurs in the method of the initClientRunningMonitor of SimpleCanalConnector runningMonitor.setListener(new ClientRunningListener() { ``` public InetSocketAddress processActiveEnter() { InetSocketAddress address = doConnect(); mutex.set(true); if (rollbackOnConnect) { rollback(); } return address; } public void processActiveExit() { mutex.set(false); doDisconnnect(); } }); ``` When doConnect exits abnormally, mutex set true is not executed. The thrown exception will be caught by the outer call function, causing other functions to be stuck on the mutex lock. https://github.com/alibaba/canal/issues/77 Similar to this issue using the 1 0 16 version Temporarily close the problem and then reopen
88,canal deployer The entries accepted by the 1 0 14 version on the client side are TRANSACTION_BEGIN and TRANSACTION_END The specific phenomenon is that the data received in a short period of time started on the server side of the canal is normal, but after about 10 minutes, the entryType of the entry accepted by the client is all TRANSACTION_BEGIN and TRANSACTION_END. I was suspected that the client I wrote had a problem, but changing the client to the sample client is still the same effect. I have tried setting the log level of the server side to debug but I have not got any valuable debugging information. Trouble to provide some ideas to solve this problem or my server side configuration problem may be where the problem Thank you Provide server-side instance properties configuration and client code The reason for this problem was found The database name configured in instance properties is lowercase, but the database on mysql is uppercase and is case sensitive, so most of them are empty. But what is puzzling is why it will be consumed for a while and then it will not work. If it is because of capitalization, it should be that it is not reasonable. It is recommended to add case-sensitive options to canal properties The expression in instance properties turns on the case insensitive parameter Perl5Compiler CASE_INSENSITIVE_MASK when canal is compiled into a regular expression. So your inference may be incorrect. Yes When the client configures the subscription rule, the problem of the subscribed rule is wrong. All the content can&#39;t get in. The client&#39;s subscribe takes precedence over the configuration in the instance properties same question Connector subscribe actionlog d 8 I want to subscribe to all actionlogs. The results of the table are not ROWDATA. Is it correct or wrong? Ask how you want to subscribe to the specified library or table.
87,Group mode turns on data blocking for HeartBeat thread otter issue : https://github.com/alibaba/otter/issues/46
86,Non-HeartBeat HA mode is not supported in the case of group Support otter issue : https://github.com/alibaba/otter/issues/45
85,Unable to set max_packet_size may result in failure to synchronize binlog Max_packet_size default is 16m if mysql If packet is greater than 16m, canal cannot synchronize binlog http://dev.mysql.com/doc/refman/5.5/en/server-system-variables.html#sysvar_max_allowed_packet Need to be solved by setting the session variable Looked at the binlog packet currently default is to read 3 bytes as length That is, you can only mark up to 16MB and the size is more than 16MB. Split into multiple packets. It is not clear that max_allowed_packet is adjusted after mysql. Whether the server will adjust the number of bytes of netlen to solve the transmission of large packets. <pre> // Fetching the first packet(may a multi-packet). int netlen = getUint24(PACKET_LEN_OFFSET); int netnum = getUint8(PACKET_SEQ_OFFSET); </pre> In fact, the single package code sent out with a macro to write the maximum is 16M netlen non-compressed package by default is to parse the first three bytes as the length mysql The server does not adjust this thing according to max_allowed_packet There are several levels of length control max_allowed_packet that do not affect the innermost packet size limit. MYSQL Read the log function inside the handle_slave_io Read_event will call my_net_read and this function will have a package process by default. So MYSQL can spell a log with a length greater than 16M. Event canal this block may not be done Attached ulong my_net_read(NET *net) { size_t len complen; MYSQL_NET_READ_START(); #ifdef HAVE_COMPRESS if (!net->compress) { #endif len= net_read_packet(net &complen); if (len == MAX_PACKET_LENGTH) { /\* First packet of a multi-packet. Concatenate the packets _/ ulong save_pos = net->where_b; size_t total_length= 0; do { net->where_b += len; total_length += len; len= net_read_packet(net &complen); } while (len == MAX_PACKET_LENGTH); if (len != packet_error) len+= total_length; net->where_b = save_pos; } net->read_pos = net->buff + net->where_b; if (len != packet_error) net->read_pos[len]=0; /_ Safeguard for mysql_use_result */ MYSQL_NET_READ_DONE(0 len); return len; #ifdef HAVE_COMPRESS } More feedback mysql implementation mysql through the multi-packet mode to solve the big package problem only need to be re-grouped when the client can handle the packet Current canal processing logic // Fetching the first packet(may a multi-packet). int netlen = getUint24(PACKET_LEN_OFFSET); int netnum = getUint8(PACKET_SEQ_OFFSET); if (!fetch0(NET_HEADER_SIZE netlen)) { return false; } while (netlen == MAX_PACKET_LENGTH) // Continue reading the next packet for the 16MB packet { netlen = getUint24(PACKET_LEN_OFFSET); netnum = getUint8(PACKET_SEQ_OFFSET); if (!fetch0(limit netlen)) { return false; } }
84,Support marialb5 5 34 10 0 7 version of binlog analysis Support for maraidb 5 10 version of binlog analysis Change content https://github.com/alibaba/canal/wiki/BinlogChange%28MariaDB5%2610%29
82,Canal will cause ZooKeeper&#39;s disk IO load to be too high In the test, we found that Canal will cause ZooKeeper&#39;s IO load to be extremely high. Under iotop, ZooKeeper can continue to write transaction data to disk at 400KB s. Therefore, the disk IO load is high. We do not currently provide a separate server to run ZooKeeper. So we hope to reduce its disk IO usage by reducing ZooKeeper&#39;s write update pressure. According to my understanding, Canal should update the latest binlog offset into ZooKeeper every time the client ACKs. This causes the ZooKeeper transaction to have a lot of disk IO usage. I modified the canal zookeeper flush period in canal properties to 5000. I hope that Canal can update ZooKeeper every 5 seconds, but after the configuration, it is found that it does not work at all. ZooKeeper&#39;s writing pressure is still huge. After that, I monitored ZooKeeper&#39;s logs and found that most of the writes were not from the Canal server but from the Canal client. Why does the Canal client write the binlog offset directly to the ZooKeeper server at high frequency? Should the update or write of the binlog offset be scheduled by the Canal server? Disk IO load of the server where ZooKeeper is located > # iostat -xdm 1 > > Device: rrqm/s wrqm/s r/s w/s rMB/s wMB/s avgrq-sz avgqu-sz await svctm %util > sda 228.00 2.00 442.00 15.00 39.32 0.06 176.47 11.21 24.79 2.19 100.00 > sda1 228.00 2.00 442.00 15.00 39.32 0.06 176.47 11.21 24.79 2.19 100.00 > sda2 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ZooKeeper&#39;s disk IO occupancy > # iotop -p 10411 zookeeper的pid  > > 10411 be/4 datacent 0.00 B/s 475.99 K/s 52.38 % 48.88 % java -Dzook...oo.cfg Find the problem is not a problem with Canal The Canal client also writes data to hbase, causing zookeeper IO is too high is hbase instead of canal
81,SlaveID is forced to update to 1 after Canal server restarts Operating environment Zookeeper cluster Single Canal server test Single Canal client test 2 MySQL instances constantly have new binlog problem Stop operation is unresponsive when a Canal is closed. -9 Pid forcibly close the canal process ``` The next time you start Canal, the Canal node is thrown and the Canal is restarted. It is abnormally disappeared. It should be because Zookeeper detected that Canal was automatically deleted offline. After a successful turnaround, Canal will no longer read the binlog but print the warn log prepare. to find start position just last Position at this time mysql binlog is persistent I manually found the canal data stored in zookeeper and found [zookeeper] get  otter canal destinations 名称 1001 cursor {"@type":"com.alibaba.otter.canal.protocol.position.LogPosition" "identity":{"slaveId":-1 "sourceAddress":{"address":"..." "port":3310}} "postion":{"included":false "journalName":"localhost-bin.000008" "position":689389260 "timestamp":1388062800000}} Please note that slaveId 1 I am using the Canal default configuration slaveId 1234 It’s really inexplicable now. I suspect that it is automatically changed to 1 after the exception occurs in Canal. So I manually changed this salveID from 1 to 1234. But after starting Canal again, I found that this slaveID was forced to change to 1 again. Since slaveid is changed to 1 After the canal can not read the binlog data ``` The following is the startup log of one of the mysql instances of canal. The debug level is enabled. I hope everyone can help me solve this problem. 2013-12-27 09:51:57.745 [canal-instance-scan-0] DEBUG o.s.beans.factory.xml.PluggableSchemaResolver - Found XML schema [http://www.springframework.org/schema/beans/spring-beans-2.0.xsd] in classpath: org/springframework/beans/factory/xml/spring-beans-2.0.xsd 2013-12-27 09:51:57.860 [canal-instance-scan-0] DEBUG o.s.b.factory.xml.DefaultBeanDefinitionDocumentReader - Loading bean definitions 2013-12-27 09:51:57.898 [canal-instance-scan-0] DEBUG o.s.beans.factory.xml.BeanDefinitionParserDelegate - Neither XML 'id' nor 'name' specified - using generated bean name [com.alibaba.otter.canal.instance.spring.support.PropertyPlaceholderConfigurer#0] 2013-12-27 09:51:57.903 [canal-instance-scan-0] DEBUG o.s.beans.factory.xml.BeanDefinitionParserDelegate - Neither XML 'id' nor 'name' specified - using generated bean name [org.springframework.beans.factory.config.CustomEditorConfigurer#0] 2013-12-27 09:51:57.907 [canal-instance-scan-0] DEBUG o.s.beans.factory.xml.BeanDefinitionParserDelegate - Neither XML 'id' nor 'name' specified - using generated bean name [com.alibaba.otter.canal.meta.ZooKeeperMetaManager#291792b7] 2013-12-27 09:51:57.911 [canal-instance-scan-0] DEBUG o.s.beans.factory.xml.BeanDefinitionParserDelegate - Neither XML 'id' nor 'name' specified - using generated bean name [com.alibaba.otter.canal.parse.ha.HeartBeatHAController#4aa14174] 2013-12-27 09:51:57.913 [canal-instance-scan-0] DEBUG o.s.beans.factory.xml.BeanDefinitionParserDelegate - Neither XML 'id' nor 'name' specified - using generated bean name [com.alibaba.otter.canal.filter.aviater.AviaterRegexFilter#55d580a8] 2013-12-27 09:51:57.915 [canal-instance-scan-0] DEBUG o.s.beans.factory.xml.BeanDefinitionParserDelegate - Neither XML 'id' nor 'name' specified - using generated bean name [com.alibaba.otter.canal.parse.index.MemoryLogPositionManager#33f1c19e] 2013-12-27 09:51:57.915 [canal-instance-scan-0] DEBUG o.s.beans.factory.xml.BeanDefinitionParserDelegate - Neither XML 'id' nor 'name' specified - using generated bean name [com.alibaba.otter.canal.parse.index.MetaLogPositionManager#44de86b6] 2013-12-27 09:51:57.915 [canal-instance-scan-0] DEBUG o.s.beans.factory.xml.BeanDefinitionParserDelegate - Neither XML 'id' nor 'name' specified - using generated bean name [com.alibaba.otter.canal.parse.index.FailbackLogPositionManager#6d5e3a0c] 2013-12-27 09:51:57.916 [canal-instance-scan-0] DEBUG o.s.beans.factory.xml.BeanDefinitionParserDelegate - Neither XML 'id' nor 'name' specified - using generated bean name [com.alibaba.otter.canal.parse.support.AuthenticationInfo#453521ec] 2013-12-27 09:51:57.917 [canal-instance-scan-0] DEBUG o.s.beans.factory.xml.BeanDefinitionParserDelegate - Neither XML 'id' nor 'name' specified - using generated bean name [com.alibaba.otter.canal.parse.support.AuthenticationInfo#23463073] 2013-12-27 09:51:57.918 [canal-instance-scan-0] DEBUG o.s.beans.factory.xml.BeanDefinitionParserDelegate - Neither XML 'id' nor 'name' specified - using generated bean name [com.alibaba.otter.canal.protocol.position.EntryPosition#5516e01c] 2013-12-27 09:51:57.919 [canal-instance-scan-0] DEBUG o.s.beans.factory.xml.BeanDefinitionParserDelegate - Neither XML 'id' nor 'name' specified - using generated bean name [com.alibaba.otter.canal.protocol.position.EntryPosition#50d379e8] 2013-12-27 09:51:57.920 [canal-instance-scan-0] DEBUG o.s.beans.factory.xml.XmlBeanDefinitionReader - Loaded 10 bean definitions from location pattern [classpath:spring/default-instance.xml] 2013-12-27 09:51:57.920 [canal-instance-scan-0] INFO o.s.context.support.ClassPathXmlApplicationContext - Bean factory for application context [org.springframework.context.support.ClassPathXmlApplicationContext@344d638a]: org.springframework.beans.factory.support.DefaultListableBeanFactory@6be897cc 2013-12-27 09:51:57.920 [canal-instance-scan-0] DEBUG o.s.context.support.ClassPathXmlApplicationContext - 10 beans defined in org.springframework.context.support.ClassPathXmlApplicationContext@344d638a: display name [org.springframework.context.support.ClassPathXmlApplicationContext@344d638a]; startup date [Fri Dec 27 09:51:57 CST 2013]; root of context hierarchy 2013-12-27 09:51:57.980 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating shared instance of singleton bean 'com.alibaba.otter.canal.instance.spring.support.PropertyPlaceholderConfigurer#0' 2013-12-27 09:51:57.981 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating instance of bean 'com.alibaba.otter.canal.instance.spring.support.PropertyPlaceholderConfigurer#0' 2013-12-27 09:51:58.021 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Eagerly caching bean 'com.alibaba.otter.canal.instance.spring.support.PropertyPlaceholderConfigurer#0' to allow for resolving potential circular references 2013-12-27 09:51:58.066 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Invoking afterPropertiesSet() on bean with name 'com.alibaba.otter.canal.instance.spring.support.PropertyPlaceholderConfigurer#0' 2013-12-27 09:51:58.067 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Finished creating instance of bean 'com.alibaba.otter.canal.instance.spring.support.PropertyPlaceholderConfigurer#0' 2013-12-27 09:51:58.069 [canal-instance-scan-0] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2013-12-27 09:51:58.070 [canal-instance-scan-0] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [war_wist_ios_cn_h17_ww2/instance.properties] 2013-12-27 09:51:58.138 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating shared instance of singleton bean 'org.springframework.beans.factory.config.CustomEditorConfigurer#0' 2013-12-27 09:51:58.138 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating instance of bean 'org.springframework.beans.factory.config.CustomEditorConfigurer#0' 2013-12-27 09:51:58.139 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Eagerly caching bean 'org.springframework.beans.factory.config.CustomEditorConfigurer#0' to allow for resolving potential circular references 2013-12-27 09:51:58.143 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating shared instance of singleton bean 'socketAddressEditor' 2013-12-27 09:51:58.144 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating instance of bean 'socketAddressEditor' 2013-12-27 09:51:58.144 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Eagerly caching bean 'socketAddressEditor' to allow for resolving potential circular references 2013-12-27 09:51:58.157 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Finished creating instance of bean 'socketAddressEditor' 2013-12-27 09:51:58.169 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Finished creating instance of bean 'org.springframework.beans.factory.config.CustomEditorConfigurer#0' 2013-12-27 09:51:58.174 [canal-instance-scan-0] DEBUG o.s.context.support.ClassPathXmlApplicationContext - Unable to locate MessageSource with name 'messageSource': using default [org.springframework.context.support.DelegatingMessageSource@d170baf] 2013-12-27 09:51:58.177 [canal-instance-scan-0] DEBUG o.s.context.support.ClassPathXmlApplicationContext - Unable to locate ApplicationEventMulticaster with name 'applicationEventMulticaster': using default [org.springframework.context.event.SimpleApplicationEventMulticaster@5ee5c122] 2013-12-27 09:51:58.180 [canal-instance-scan-0] INFO o.s.beans.factory.support.DefaultListableBeanFactory - Pre-instantiating singletons in org.springframework.beans.factory.support.DefaultListableBeanFactory@6be897cc: defining beans [com.alibaba.otter.canal.instance.spring.support.PropertyPlaceholderConfigurer#0 socketAddressEditor org.springframework.beans.factory.config.CustomEditorConfigurer#0 instance alarmHandler zkClientx metaManager eventStore eventSink eventParser]; root of factory hierarchy 2013-12-27 09:51:58.180 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Returning cached instance of singleton bean 'com.alibaba.otter.canal.instance.spring.support.PropertyPlaceholderConfigurer#0' 2013-12-27 09:51:58.180 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Returning cached instance of singleton bean 'socketAddressEditor' 2013-12-27 09:51:58.180 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Returning cached instance of singleton bean 'org.springframework.beans.factory.config.CustomEditorConfigurer#0' 2013-12-27 09:51:58.180 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating shared instance of singleton bean 'instance' 2013-12-27 09:51:58.181 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating instance of bean 'instance' 2013-12-27 09:51:58.183 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Eagerly caching bean 'instance' to allow for resolving potential circular references 2013-12-27 09:51:58.215 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating shared instance of singleton bean 'eventParser' 2013-12-27 09:51:58.215 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating instance of bean 'eventParser' 2013-12-27 09:51:58.225 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Eagerly caching bean 'eventParser' to allow for resolving potential circular references 2013-12-27 09:51:58.246 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating shared instance of singleton bean 'eventSink' 2013-12-27 09:51:58.246 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating instance of bean 'eventSink' 2013-12-27 09:51:58.249 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Eagerly caching bean 'eventSink' to allow for resolving potential circular references 2013-12-27 09:51:58.260 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating shared instance of singleton bean 'eventStore' 2013-12-27 09:51:58.260 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating instance of bean 'eventStore' 2013-12-27 09:51:58.264 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Eagerly caching bean 'eventStore' to allow for resolving potential circular references 2013-12-27 09:51:58.283 [canal-instance-scan-0] DEBUG org.springframework.beans.BeanUtils - No property editor [com.alibaba.otter.canal.store.model.BatchModeEditor] found for type com.alibaba.otter.canal.store.model.BatchMode according to 'Editor' suffix convention 2013-12-27 09:51:58.285 [canal-instance-scan-0] WARN org.springframework.beans.TypeConverterDelegate - PropertyEditor [com.sun.beans.editors.EnumEditor] found through deprecated global PropertyEditorManager fallback - consider using a more isolated form of registration e.g. on the BeanWrapper/BeanFactory! 2013-12-27 09:51:58.286 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Finished creating instance of bean 'eventStore' 2013-12-27 09:51:58.286 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Finished creating instance of bean 'eventSink' 2013-12-27 09:51:58.287 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Added autowiring by name from bean name 'eventParser' via property 'eventSink' to bean named 'eventSink' 2013-12-27 09:51:58.288 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating instance of bean 'com.alibaba.otter.canal.parse.ha.HeartBeatHAController#4aa14174' 2013-12-27 09:51:58.296 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Finished creating instance of bean 'com.alibaba.otter.canal.parse.ha.HeartBeatHAController#4aa14174' 2013-12-27 09:51:58.296 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating shared instance of singleton bean 'alarmHandler' 2013-12-27 09:51:58.297 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating instance of bean 'alarmHandler' 2013-12-27 09:51:58.297 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Eagerly caching bean 'alarmHandler' to allow for resolving potential circular references 2013-12-27 09:51:58.302 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Finished creating instance of bean 'alarmHandler' 2013-12-27 09:51:58.303 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating instance of bean 'com.alibaba.otter.canal.filter.aviater.AviaterRegexFilter#55d580a8' 2013-12-27 09:51:58.466 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Finished creating instance of bean 'com.alibaba.otter.canal.filter.aviater.AviaterRegexFilter#55d580a8' 2013-12-27 09:51:58.467 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating instance of bean 'com.alibaba.otter.canal.parse.index.FailbackLogPositionManager#6d5e3a0c' 2013-12-27 09:51:58.476 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating instance of bean 'com.alibaba.otter.canal.parse.index.MemoryLogPositionManager#33f1c19e' 2013-12-27 09:51:58.483 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Finished creating instance of bean 'com.alibaba.otter.canal.parse.index.MemoryLogPositionManager#33f1c19e' 2013-12-27 09:51:58.484 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating instance of bean 'com.alibaba.otter.canal.parse.index.MetaLogPositionManager#44de86b6' 2013-12-27 09:51:58.491 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating shared instance of singleton bean 'metaManager' 2013-12-27 09:51:58.491 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating instance of bean 'metaManager' 2013-12-27 09:51:58.493 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Eagerly caching bean 'metaManager' to allow for resolving potential circular references 2013-12-27 09:51:58.504 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating instance of bean 'com.alibaba.otter.canal.meta.ZooKeeperMetaManager#291792b7' 2013-12-27 09:51:58.511 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating shared instance of singleton bean 'zkClientx' 2013-12-27 09:51:58.511 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating instance of bean 'zkClientx' 2013-12-27 09:51:58.512 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Eagerly caching bean 'zkClientx' to allow for resolving potential circular references 2013-12-27 09:51:58.523 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Invoking afterPropertiesSet() on bean with name 'zkClientx' 2013-12-27 09:51:58.523 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Finished creating instance of bean 'zkClientx' 2013-12-27 09:51:58.525 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Finished creating instance of bean 'com.alibaba.otter.canal.meta.ZooKeeperMetaManager#291792b7' 2013-12-27 09:51:58.525 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Finished creating instance of bean 'metaManager' 2013-12-27 09:51:58.526 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Finished creating instance of bean 'com.alibaba.otter.canal.parse.index.MetaLogPositionManager#44de86b6' 2013-12-27 09:51:58.526 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Finished creating instance of bean 'com.alibaba.otter.canal.parse.index.FailbackLogPositionManager#6d5e3a0c' 2013-12-27 09:51:58.526 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating instance of bean 'com.alibaba.otter.canal.parse.support.AuthenticationInfo#453521ec' 2013-12-27 09:51:58.531 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Finished creating instance of bean 'com.alibaba.otter.canal.parse.support.AuthenticationInfo#453521ec' 2013-12-27 09:51:58.531 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating instance of bean 'com.alibaba.otter.canal.parse.support.AuthenticationInfo#23463073' 2013-12-27 09:51:58.532 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Finished creating instance of bean 'com.alibaba.otter.canal.parse.support.AuthenticationInfo#23463073' 2013-12-27 09:51:58.532 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating instance of bean 'com.alibaba.otter.canal.protocol.position.EntryPosition#5516e01c' 2013-12-27 09:51:58.541 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Finished creating instance of bean 'com.alibaba.otter.canal.protocol.position.EntryPosition#5516e01c' 2013-12-27 09:51:58.541 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Creating instance of bean 'com.alibaba.otter.canal.protocol.position.EntryPosition#50d379e8' 2013-12-27 09:51:58.542 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Finished creating instance of bean 'com.alibaba.otter.canal.protocol.position.EntryPosition#50d379e8' 2013-12-27 09:51:58.543 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Finished creating instance of bean 'eventParser' 2013-12-27 09:51:58.543 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Returning cached instance of singleton bean 'eventSink' 2013-12-27 09:51:58.543 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Returning cached instance of singleton bean 'eventStore' 2013-12-27 09:51:58.543 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Returning cached instance of singleton bean 'metaManager' 2013-12-27 09:51:58.544 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Returning cached instance of singleton bean 'alarmHandler' 2013-12-27 09:51:58.544 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Finished creating instance of bean 'instance' 2013-12-27 09:51:58.544 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Returning cached instance of singleton bean 'alarmHandler' 2013-12-27 09:51:58.544 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Returning cached instance of singleton bean 'zkClientx' 2013-12-27 09:51:58.545 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Returning cached instance of singleton bean 'metaManager' 2013-12-27 09:51:58.545 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Returning cached instance of singleton bean 'eventStore' 2013-12-27 09:51:58.545 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Returning cached instance of singleton bean 'eventSink' 2013-12-27 09:51:58.545 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Returning cached instance of singleton bean 'eventParser' 2013-12-27 09:51:58.546 [canal-instance-scan-0] DEBUG o.s.beans.factory.support.DefaultListableBeanFactory - Returning cached instance of singleton bean 'instance' 2013-12-27 09:51:58.546 [canal-instance-scan-0] INFO c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-war_wist_ios_cn_h17_ww2 2013-12-27 09:51:58.577 [destination = war_wist_ios_cn_h17_ww2 address = /118.26.235.88:3310 EventParser] INFO c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - start heart beat.... 2013-12-27 09:51:58.578 [destination = war_wist_ios_cn_h17_ww2 address = /118.26.235.88:3310 EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - connect MysqlConnection to /118.26.235.88:3310... 2013-12-27 09:51:58.596 [canal-instance-scan-0] INFO c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start successful.... 2013-12-27 09:51:58.596 [canal-instance-scan-0] INFO c.a.otter.canal.server.embeded.CanalServerWithEmbeded - start CanalInstances[war_wist_ios_cn_h17_ww2] successfully 2013-12-27 09:52:09.329 [destination = war_wist_ios_cn_h17_ww2 address = /118.26.235.88:3310 EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - handshake initialization packet received prepare the client authentication packet to send 2013-12-27 09:52:09.338 [destination = war_wist_ios_cn_h17_ww2 address = /118.26.235.88:3310 EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - client authentication packet is sent out. 2013-12-27 09:52:09.355 [destination = war_wist_ios_cn_h17_ww2 address = /118.26.235.88:3310 EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - connect MysqlConnection to /118.26.235.88:3310... 2013-12-27 09:52:19.509 [destination = war_wist_ios_cn_h17_ww2 address = /118.26.235.88:3310 EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - handshake initialization packet received prepare the client authentication packet to send 2013-12-27 09:52:19.510 [destination = war_wist_ios_cn_h17_ww2 address = /118.26.235.88:3310 EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - client authentication packet is sent out. 2013-12-27 09:52:19.541 [destination = war_wist_ios_cn_h17_ww2 address = /118.26.235.88:3310 EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position 2013-12-27 09:52:19.580 [destination = war_wist_ios_cn_h17_ww2 address = /118.26.235.88:3310 EventParser] INFO c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - find start position : EntryPosition[included=false journalName=localhost-bin.000008 position=679706051 timestamp=1388060804000] 2013-12-27 09:52:19.580 [destination = war_wist_ios_cn_h17_ww2 address = /118.26.235.88:3310 EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - disConnect MysqlConnection to /118.26.235.88:3310... 2013-12-27 09:52:19.580 [destination = war_wist_ios_cn_h17_ww2 address = /118.26.235.88:3310 EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - connect MysqlConnection to /118.26.235.88:3310... 2013-12-27 09:52:29.239 [destination = war_wist_ios_cn_h17_ww2 address = /118.26.235.88:3310 EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - handshake initialization packet received prepare the client authentication packet to send 2013-12-27 09:52:29.240 [destination = war_wist_ios_cn_h17_ww2 address = /118.26.235.88:3310 EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - client authentication packet is sent out. 2013-12-27 09:52:29.244 [destination = war_wist_ios_cn_h17_ww2 address = /118.26.235.88:3310 EventParser] DEBUG c.a.otter.canal.parse.driver.mysql.MysqlUpdateExecutor - read update result... 2013-12-27 09:52:29.748 [destination = war_wist_ios_cn_h17_ww2 address = /118.26.235.88:3310 EventParser] DEBUG c.a.otter.canal.parse.driver.mysql.MysqlUpdateExecutor - read update result... 2013-12-27 09:52:29.830 [destination = war_wist_ios_cn_h17_ww2 address = /118.26.235.88:3310 EventParser] DEBUG c.a.otter.canal.parse.driver.mysql.MysqlUpdateExecutor - read update result... 2013-12-27 09:52:29.831 [destination = war_wist_ios_cn_h17_ww2 address = /118.26.235.88:3310 EventParser] DEBUG c.a.otter.canal.parse.driver.mysql.MysqlUpdateExecutor - read update result... 2013-12-27 09:52:29.957 [destination = war_wist_ios_cn_h17_ww2 address = /118.26.235.88:3310 EventParser] DEBUG c.a.otter.canal.parse.driver.mysql.MysqlUpdateExecutor - read update result... 2013-12-27 09:52:30.053 [destination = war_wist_ios_cn_h17_ww2 address = /118.26.235.88:3310 EventParser] INFO c.a.otter.canal.parse.inbound.mysql.MysqlConnection - COM_BINLOG_DUMP with position:BinlogDumpCommandPacket[binlogPosition=679706051 slaveServerId=1234 binlogFileName=localhost-bin.000008 command=18] 2013-12-27 09:52:30.346 [destination = war_wist_ios_cn_h17_ww2 address = /118.26.235.88:3310 EventParser] INFO com.taobao.tddl.dbsync.binlog.LogEvent - common_header_len= 19 number_of_event_types= 27 2013-12-27 09:52:50.072 [New I/O server worker #1-2] INFO c.a.otter.canal.server.embeded.CanalServerWithEmbeded - rollback successfully clientId:1001 2013-12-27 09:52:50.073 [New I/O server worker #1-2] INFO c.a.otter.canal.server.embeded.CanalServerWithEmbeded - subscribe successfully use last cursor position:ClientIdentity[destination=war_wist_ios_cn_h17_ww2 clientId=1001 filter=._.._] 2013-12-27 09:52:50.115 [New I/O server worker #1-2] INFO c.a.otter.canal.server.embeded.CanalServerWithEmbeded - rollback successfully clientId:1001 2013-12-27 09:52:50.118 [New I/O server worker #1-2] INFO c.a.otter.canal.server.embeded.CanalServerWithEmbeded - getWithoutAck successfully clientId:1001 batchSize:1000 real size is 619 and result is [batchId:1 position:PositionRange[start=LogPosition[identity=LogIdentity[sourceAddress=/118.26.235.88:3310 slaveId=-1] postion=EntryPosition[included=false journalName=localhost-bin.000008 position=679706051 timestamp=1388060804000]] ack=LogPosition[identity=LogIdentity[sourceAddress=/118.26.235.88:3310 slaveId=-1] postion=EntryPosition[included=false journalName=localhost-bin.000008 position=689389260 timestamp=1388062800000]] end=LogPosition[identity=LogIdentity[sourceAddress=/118.26.235.88:3310 slaveId=-1] postion=EntryPosition[included=false journalName=localhost-bin.000008 position=689389260 timestamp=1388062800000]]]] 2013-12-27 09:52:50.240 [New I/O server worker #1-2] INFO c.a.otter.canal.server.embeded.CanalServerWithEmbeded - ack successfully clientId:1001 batchId:1 position:PositionRange[start=LogPosition[identity=LogIdentity[sourceAddress=/118.26.235.88:3310 slaveId=-1] postion=EntryPosition[included=false journalName=localhost-bin.000008 position=679706051 timestamp=1388060804000]] ack=LogPosition[identity=LogIdentity[sourceAddress=/118.26.235.88:3310 slaveId=-1] postion=EntryPosition[included=false journalName=localhost-bin.000008 position=689389260 timestamp=1388062800000]] end=LogPosition[identity=LogIdentity[sourceAddress=/118.26.235.88:3310 slaveId=-1] postion=EntryPosition[included=false journalName=localhost-bin.000008 position=689389260 timestamp=1388062800000]]] 2013-12-27 09:52:50.248 [New I/O server worker #1-2] DEBUG c.a.otter.canal.server.embeded.CanalServerWithEmbeded - getWithoutAck successfully clientId:1001 batchSize:1000 but result is null 2013-12-27 09:52:51.291 [New I/O server worker #1-2] DEBUG c.a.otter.canal.server.embeded.CanalServerWithEmbeded - getWithoutAck successfully clientId:1001 batchSize:1000 but result is null 2013-12-27 09:52:52.335 [New I/O server worker #1-2] DEBUG c.a.otter.canal.server.embeded.CanalServerWithEmbeded - getWithoutAck successfully clientId:1001 batchSize:1000 but result is null 2013-12-27 09:52:53.380 [New I/O server worker #1-2] INFO c.a.otter.canal.server.embeded.CanalServerWithEmbeded - getWithoutAck successfully clientId:1001 batchSize:1000 real size is 327 and result is [batchId:2 position:PositionRange[start=LogPosition[identity=LogIdentity[sourceAddress=/118.26.235.88:3310 slaveId=-1] postion=EntryPosition[included=false journalName=localhost-bin.000008 position=696177601 timestamp=1388062803000]] ack=LogPosition[identity=LogIdentity[sourceAddress=/118.26.235.88:3310 slaveId=-1] postion=EntryPosition[included=false journalName=localhost-bin.000008 position=696612594 timestamp=1388062803000]] end=LogPosition[identity=LogIdentity[sourceAddress=/118.26.235.88:3310 slaveId=-1] postion=EntryPosition[included=false journalName=localhost-bin.000008 position=696612594 timestamp=1388062803000]]]] PS I use the Canal version as 1 0 15 Explain a few questions 1. prepare to find start position just last position. Here is the normal current start is to continue here from the last bit here Position is understood as the last consumption point 2. kill 9 forced to close after throwing the Canal node has been abnormally deleted automatically after the understanding is correct is zookeeper The server has a sessionTimeout time longer than this time will automatically delete the node. When the normal exit, the canal will delete itself, so there will be no problem. 3. slaveId = 1 This is the currently known code that does not write the correct slaveId to the zookeeper itself. Every time it reads the slaveId from the canal properties, this does not affect Many thanks to agapple for an explanation. I would like to take this opportunity to consult a Canal that causes the zookeeper&#39;s IO load to be too high. In the test, we found that Canal will cause ZooKeeper&#39;s IO load to be quite high. Under iotop, ZooKeeper can continue to write transaction data to disk at 400KB s, resulting in high disk IO load. We do not currently provide a separate server to run. ZooKeeper So we hope to reduce its disk IO usage by reducing ZooKeeper&#39;s write update pressure. According to my understanding, Canal should update the latest binlog offset into ZooKeeper every time the client ACKs. This causes the ZooKeeper transaction to have a lot of disk IO usage. Is there any way for Canal to update the binlog offset every once in a while? We hope to reduce the canal zookeeper flush period in ZooKeeper&#39;s write pressure configuration by this method. It seems that it does not work. Configure it to 5000 after ZooKeeper write pressure. Still huge @agapple Hello, you can help me see the problem upstairs. Ignore the upstairs, causing ZooKeeper&#39;s IO load to be too high, not a problem with Canal.
80,Optimized under should start First exception For example, the following exception occurs <pre> failed to subscribe with reason: something goes wrong with channel:[id: 0x01039fc0 /192.168.22.153:15686 => /192.168.22.19:11111] exception=com.alibaba.otter.canal.server.exception.CanalServerException: destination:example should start first </pre> The general reason canal The server has a switch, for example, if the previous node just executes getWithoutAck and cuts to the next node, if it continues to execute getWithoutAck, because there is no subscribe call, the server will not initialize the call to getWithoutAck. start first. The optimization method caches the last filter condition and calls disconnect connect subscribe filter in the next automatic switch to ensure the new canal After the server completes initialization, it calls getWithoutAck to get the data.
79,Multi-table ddl sql tableName parsing failed <pre> DROP /*!40005 TEMPORARY */ TABLE IF EXISTS `temp_bond_keys` `temp_bond_key_id` RENAME TABLE A TO B C TO D . </pre> Currently known two kinds of ddl will exist multi-table operation, currently do not support multi-table analysis Subsequent perfect solution using mysql similar to druid cobar Sql syntax parsing extraction corresponding table Ddl can not properly resolve the impact of the existence of the table name 1. Ddl can&#39;t sync properly 2. Ddl statement will not be sent to canal Client does not block the entire parsing because it does not satisfy the corresponding filter First temporary solution to solve the subsequent unified modification for SQL parser optimization
78,The filter condition of the client subscription in the default instance xml mode is in canal The server will lose the filter after switching ZooKeeperMetaManager&#39;s listAllSubscribeInfo method only reads the clientId information and does not read the corresponding filter, resulting in a canal Client submitted information in canal The next time the server is started, it does not load. The filter submitted by the client after the switch does not take effect. ps . MemoryMetaManager FileMetaManager mode is normal <pre> String path = ZookeeperPathUtils.getDestinationPath(destination); List<String> childs = null; try { childs = zkClientx.getChildren(path); } catch (ZkNoNodeException e) { // ignore } if (CollectionUtils.isEmpty(childs)) { return new ArrayList<ClientIdentity>(); } List<Short> clientIds = new ArrayList<Short>(); for (String child : childs) { if (StringUtils.isNumeric(child)) { clientIds.add(ZookeeperPathUtils.getClientId(child)); } } Collections.sort(clientIds); // Make a sort List<ClientIdentity> clientIdentities = Lists.newArrayList(); for (Short clientId : clientIds) { clientIdentities.add(new ClientIdentity(destination clientId)); } return clientIdentities; } </pre>
77,canal Server lost after canal Client may not be able to cut the standby server canal server Do clustering through ZK When a server crashes, if the temporary node on ZK does not disappear, then canal Client may block all the time Caused by: java.net.ConnectException: Connection refused: connect at sun.nio.ch.Net.connect(Native Method) ~[na:1.6.0_45] at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:532) ~[na:1.6.0_45] at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.doConnect(SimpleCanalConnector.java:117) [canal.client-1.0.9.jar:na] ... 14 common frames omitted [2013-12-13 21:40:47 INFO com.alibaba.otter.canal.client.impl.ClusterCanalConnector:167] [shared-pool-thread-1] restart the connector for next round retry. No longer intercept the exception connect failure will directly throw an exception. If the cluster mode will automatically call restart, you can get retry. If it is simple mode, you need to manually call disconnect and connect. server Running also has this risk only because there is no possibility of external dependencies. The only error is the network link with zk. So once the temporary node is created, the actions of the local class initialization are basically not going wrong.
76,Canal heartbeat check appears ArrayIndexOutOfBoundsException <pre> Exception in thread "destination = transfer1 address = /192.168.237.26:3306 MysqlHeartBeatTimeTask" java.lang.ArrayIndexOutOfBoundsException: 1 at com.alibaba.otter.canal.parse.driver.mysql.utils.ByteHelper.readBinaryCodedLengthBytes(ByteHelper.java:83) at com.alibaba.otter.canal.parse.driver.mysql.packets.server.OKPacket.fromBytes(OKPacket.java:44) at com.alibaba.otter.canal.parse.driver.mysql.MysqlUpdateExecutor.update(MysqlUpdateExecutor.java:53) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.update(MysqlConnection.java:73) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser$MysqlHeartBeatTimeTask.run(MysqlEventParser.java:208) at java.util.TimerThread.mainLoop(Timer.java:512) at java.util.TimerThread.run(Timer.java:462) </pre> Corresponding heartbeat check sql is show master status; The main reason is that the mysql protocol is divided into two types of query update. The corresponding packet parsing is not the same. This problem is wrong. master Status uses the update protocol for parsing and causes the error to use the query protocol.
75,Heartbeat heartbeat checker appears Timer already cancelled. <pre> 2013-11-26 15:33:28.895 [destination = inter_ttsdb1 address = /192.168.24.130:3306 EventParser] WARN c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - Received EOF packet from server apparent master disconnected. 2013-11-26 15:33:28.895 [destination = inter_ttsdb1 address = /192.168.24.130:3306 EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - disConnect MysqlConnection to /192.168.24.130:3306... 2013-11-26 15:33:28.896 [destination = inter_ttsdb1 address = /192.168.24.130:3306 EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - disConnect MysqlConnection to /192.168.24.130:3306... 2013-11-26 15:33:42.819 [destination = inter_ttsdb1 address = /192.168.24.130:3306 EventParser] INFO c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - start heart beat.... 2013-11-26 15:33:42.825 [destination = inter_ttsdb1 address = /192.168.24.130:3306 EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /192.168.24.130:3306 has an error retrying. caused by java.lang.IllegalStateException: Timer already cancelled. at java.util.Timer.sched(Timer.java:354) ~[na:1.6.0_20] at java.util.Timer.schedule(Timer.java:222) ~[na:1.6.0_20] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.startHeartbeat(MysqlEventParser.java:153) ~[canal.parse-1.0.7.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:83) ~[canal.parse-1.0.7.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:143) ~[canal.parse-1.0.7.jar:na] at java.lang.Thread.run(Thread.java:619) [na:1.6.0_20] 2013-11-26 15:33:42.826 [destination = inter_ttsdb1 address = /192.168.24.130:3306 EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:inter_ttsdb1[java.lang.IllegalStateException: Timer already cancelled. at java.util.Timer.sched(Timer.java:354) at java.util.Timer.schedule(Timer.java:222) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.startHeartbeat(MysqlEventParser.java:153) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:83) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:143) at java.lang.Thread.run(Thread.java:619) ] 2013-11-26 15:33:42.826 [destination = inter_ttsdb1 address = /192.168.24.130:3306 EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - the channel /192.168.24.130:3306 is not connected 2013-11-26 15:33:42.826 [destination = inter_ttsdb1 address = /192.168.24.130:3306 EventParser] INFO c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - the channel /192.168.24.130:3306 is not connected </pre> Abnormal problem analysis 1. The first is the mysql exception closes the resulting binlog Dump and meta mysql link are disconnected 2. MysqlHeartBeatTimeTask will be abnormal in the next timer schedule. The current try catch exception only contains SocketTimeoutException IOException. If other occurrences such as RuntimeException will cause Timer to exit and set to cancel state, this issue will be triggered.
74,mysql create Table statement parsing error <pre> public static final String TABLE_PATTERN = "^(IF\\s*NOT\\s*EXIST\\s*)?(IF\\s*EXIST\\s*)?(`?.+?`?[;\\(\\s]+?)?.*$"; // Non-greedy mode </pre> Correct in the mysql syntax for EXISTS
73,EntryProtocol adds mysql Type type for custom processing <pre> Data structure for each field message Column { Field subscript optional int32 index = 1; Field java type optional int32 sqlType = 2; Field name ignore case is not available in mysql optional string name = 3; Is it the primary key? optional bool isKey = 4; If EventType UPDATE Used to identify whether this field value has been modified optional bool updated = 5; /** Is the ID empty? **/ optional bool isNull = 6 [default = false]; Reserved extension repeated Pair props = 7; /** Field value timestamp Datetime is a time formatted text **/ optional string value = 8; /** Corresponding data object original length **/ optional int32 length = 9; Field mysql type optional string mysqlType = 10; } </pre> Field information contains sqlType and mysqlType sqlType is processed by canal conversion, such as unsigned Int will be converted to Long unsigned Long will be converted to BigDecimal mysqlType is the field text description in the original mysql can be passed through desc Xxx to get Output result <pre> ID : 38 type=int(10) unsigned ADDRESS : hello type=varchar(32) EVENT_DATA : 2013-11-12 11:43:06 type=datetime update=true CHAR_VALUES : type=char(1) NUMBER_VALUES : type=decimal(19 8) FIOAT_VALUES : type=float(19 8) DOUBLE_VALUES : type=double(19 8) TINYINT_VALUES : -128 type=tinyint(3) TINYINT_UN_VALUES : 255 type=tinyint(3) unsigned SMALLINT_VALUES : 30000 type=smallint(5) SMALLINT_UN_VALUES : 65535 type=smallint(5) unsigned MEDIUMINT_VALUES : 8077215 type=mediumint(7) MEDIUMINT_UN_VALUES : 16777215 type=mediumint(7) unsigned INT_VALUES : 2094967296 type=int(10) INT_UN_VALUES : 4294967295 type=int(10) unsigned BIGINT_VALUES : 9223372036854775807 type=bigint(19) BIGINT_UN_VALUES : 9223372036854775808 type=bigint(19) unsigned DATETIME_VALUES : 0000-00-00 00:00:00 type=datetime TIMESTAMP_VALUES : 0000-00-00 00:00:00 type=timestamp DATE_VALUES : 0000-00-00 type=date TIME_VALUES : 00:00:00 type=time YEAR4_VALUES : 0000 type=year(4) YEAR2_VALUES : 0000 type=year(4) BLOB_VALUE : hello type=blob TINY_BLOB_VALUE : type=tinyblob MEDIA_BLOB_VALUE : type=mediumblob LONG_BLOB_VALUE : type=longblob TEXT_VALUE : ä¸­æ–‡1 type=text TINY_TEXT_VALUE : type=tinytext MEDIA_TEXT_VALUE : type=mediumtext LONG_TEXT_VALUE : type=longtext </pre>
72,Mysql5 6 date field cannot be synchronized Version otter 4 2 3 os：CentOS release 6.3 mysql：5.6.10-log MySQL Community Server Field Type create_time | datetime | YES | | NULL | | update_time | datetime | YES | | NULL | | Configured a one-way synchronous channel source table with create_time update_time field to start channel synchronization will appear hanged to view the log found error message prompt time format is not the corresponding error log is as follows pid:3 nid:1 exception:setl:com.alibaba.otter.node.etl.load.exception.LoadException: java.util.concurrent.ExecutionException: com.alibaba.otter.node.etl.load.exception.LoadException: com.alibaba.otter.node.etl.load.exception.LoadException: com.alibaba.otter.node.etl.load.exception.LoadException: org.springframework.dao.DataIntegrityViolationException: PreparedStatementCallback; SQL [update tb set `create_time` = ? `update_time` = ? where ]; Data truncation: Incorrect datetime value: '1383277380000' for column 'create_time' at row 1; nested exception is com.mysql.jdbc.MysqlDataTruncation: Data truncation: Incorrect datetime value: '1383277380000' for column 'create_time' at row 1 at org.springframework.jdbc.support.SQLStateSQLExceptionTranslator.doTranslate(SQLStateSQLExceptionTranslator.java:101) at org.springframework.jdbc.support.AbstractFallbackSQLExceptionTranslator.translate(AbstractFallbackSQLExceptionTranslator.java:72) at org.springframework.jdbc.support.AbstractFallbackSQLExceptionTranslator.translate(AbstractFallbackSQLExceptionTranslator.java:80) at org.springframework.jdbc.support.AbstractFallbackSQLExceptionTranslator.translate(AbstractFallbackSQLExceptionTranslator.java:80) at org.springframework.jdbc.core.JdbcTemplate.execute(JdbcTemplate.java:603) at org.springframework.jdbc.core.JdbcTemplate.update(JdbcTemplate.java:812) at org.springframework.jdbc.core.JdbcTemplate.update(JdbcTemplate.java:868) at com.alibaba.otter.node.etl.load.loader.db.DbLoadAction$DbLoadWorker$2.doInTransaction(DbLoadAction.java:607) at org.springframework.transaction.support.TransactionTemplate.execute(TransactionTemplate.java:130) at com.alibaba.otter.node.etl.load.loader.db.DbLoadAction$DbLoadWorker.doCall(DbLoadAction.java:599) at com.alibaba.otter.node.etl.load.loader.db.DbLoadAction$DbLoadWorker.call(DbLoadAction.java:527) at com.alibaba.otter.node.etl.load.loader.db.DbLoadAction.doTwoPhase(DbLoadAction.java:449) at com.alibaba.otter.node.etl.load.loader.db.DbLoadAction.doLoad(DbLoadAction.java:274) at com.alibaba.otter.node.etl.load.loader.db.DbLoadAction.load(DbLoadAction.java:160) at com.alibaba.otter.node.etl.load.loader.db.DbLoadAction$$FastClassByCGLIB$$d932a4cb.invoke() at net.sf.cglib.proxy.MethodProxy.invoke(MethodProxy.java:191) at org.springframework.aop.framework.Cglib2AopProxy$DynamicAdvisedInterceptor.intercept(Cglib2AopProxy.java:618) at com.alibaba.otter.node.etl.load.loader.db.DbLoadAction$$EnhancerByCGLIB$$80fd23c2.load() at com.alibaba.otter.node.etl.load.loader.db.DataBatchLoader$2.call(DataBatchLoader.java:198) at com.alibaba.otter.node.etl.load.loader.db.DataBatchLoader$2.call(DataBatchLoader.java:189) at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334) at java.util.concurrent.FutureTask.run(FutureTask.java:166) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334) at java.util.concurrent.FutureTask.run(FutureTask.java:166) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:724) Caused by: com.mysql.jdbc.MysqlDataTruncation: Data truncation: Incorrect datetime value: '1383277380000' for column 'create_time' at row 1 at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3560) at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3494) at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:1960) at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2114) at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2696) at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2105) at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:2398) at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:2316) at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:2301) at org.apache.commons.dbcp.DelegatingPreparedStatement.executeUpdate(DelegatingPreparedStatement.java:105) at org.apache.commons.dbcp.DelegatingPreparedStatement.executeUpdate(DelegatingPreparedStatement.java:105) at org.springframework.jdbc.core.JdbcTemplate$2.doInPreparedStatement(JdbcTemplate.java:818) at org.springframework.jdbc.core.JdbcTemplate$2.doInPreparedStatement(JdbcTemplate.java:1) at org.springframework.jdbc.core.JdbcTemplate.execute(JdbcTemplate.java:587) ... 23 more :----------------- - PairId: 5 TableId: 1 EventType : U Time : 1383277861000 ## \- Consistency : M Mode : ---Pks EventColumn[index=0 columnType=12 columnName=sub_serial columnValue=426931043 isNull=false isKey=true isUpdate=true] ---oldPks ---Columns EventColumn[index=7 columnType=93 columnName=create_time columnValue=1383277380000 isNull=false isKey=false isUpdate=true] Intermediate omitted EventColumn[index=8 columnType=93 columnName=update_time columnValue=1383277864000 isNull=false isKey=false isUpdate=true] EventColumn[index=9 columnType=4 columnName=status columnValue=1 isNull=false isKey=false isUpdate=true] I removed the time-related fields in the synchronization table and then turned on the synchronization. The problem does not occur. See otter issue : https://github.com/alibaba/otter/issues/30
71,Increase the filterQueryDdl function Previously supported filterQueryDcl filterQueryDml part of the user expects only relevant row data. Also add filterQueryDdl control to allow ddl event to be ignored.
70,mysql Bit type support Tested table <pre> CREATE TABLE `test_bit_all` ( id int(10) unsigned NOT NULL AUTO_INCREMENT bit1 bit(1) bit2 bit(9) bit3 bit(17) bit4 bit(25) bit5 bit(33) bit6 bit(41) bit7 bit(49) bit8 bit(57) bit9 bit(64) PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=gbk; </pre> Construction data insert into test_bit_all(id bit1 bit2 bit3 bit4 bit5 bit6 bit7 bit8 bit9) values(null 18446744073709551615 18446744073709551615 18446744073709551615 18446744073709551615 18446744073709551615 18446744073709551615 18446744073709551615 18446744073709551615 18446744073709551615) ; Output result EventColumn[index=1 columnType=-7 columnName=bit1 columnValue=1 isNull=false isKey=false isUpdate=true] EventColumn[index=2 columnType=-7 columnName=bit2 columnValue=511 isNull=false isKey=false isUpdate=true] EventColumn[index=3 columnType=-7 columnName=bit3 columnValue=131071 isNull=false isKey=false isUpdate=true] EventColumn[index=4 columnType=-7 columnName=bit4 columnValue=33554431 isNull=false isKey=false isUpdate=true] EventColumn[index=5 columnType=-7 columnName=bit5 columnValue=8589934591 isNull=false isKey=false isUpdate=true] EventColumn[index=6 columnType=-7 columnName=bit6 columnValue=2199023255551 isNull=false isKey=false isUpdate=true] EventColumn[index=7 columnType=-7 columnName=bit7 columnValue=562949953421311 isNull=false isKey=false isUpdate=true] EventColumn[index=8 columnType=-7 columnName=bit8 columnValue=144115188075855871 isNull=false isKey=false isUpdate=true] EventColumn[index=9 columnType=-7 columnName=bit9 columnValue=18446744073709551615 isNull=false isKey=false isUpdate=true]
69,Mysql login failed <pre> Caused by: java.io.IOException: connect /192.168.1.206:3306 failure:java.lang.ArrayIndexOutOfBoundsException: 1 at com.alibaba.otter.canal.parse.driver.mysql.utils.ByteHelper.readUnsignedShortLittleEndian(ByteHelper.java:56) at com.alibaba.otter.canal.parse.driver.mysql.packets.server.ErrorPacket.fromBytes(ErrorPacket.java:35) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:170) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:66) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:51) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:92) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:143) at java.lang.Thread.run(Thread.java:662) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:69) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:51) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:92) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:143) at java.lang.Thread.run(Thread.java:662) </pre> mysql version : 5.5.30-log Mysql environment old_passwords=1 Mysql user password UPDATE mysql.user SET Password = OLD_PASSWORD('ottermysql') WHERE user = 'ottermysql'. It will return 2 when using normal auth411 login authentication. You need to use auth311 for login verification.
68,Canal monitors configuration file changes when considering the output with meta dat files During the test, an instance was removed and the discovery channel was not stopped. Instead, it continued to run. Checking the conf directory and regenerating the directory. A meta dat file exists in the directory. Cause automatic stop failure solve Ignore empty files. Every instance must contain instance properties to be normal. Otherwise, it needs to be closed.
67,Canal increases heartbeat event Support for Otter requirements https://github.com/alibaba/otter/issues/25 At present, the heartbeat information is only transmitted between the parser and the sink module. Currently, the store has no requirements, so it is not output to the store and is directly consumed and filtered in the sink.
66,mysql Binary type processing otter issue : https://github.com/alibaba/otter/issues/22 For the synchronous binary type, the binlog is recorded as a String, causing subsequent errors in processing according to the string during processing.
65,When the schema is pure number, desc 123 table packet syntax error causes the table structure to be abnormal <pre> mysql> desc 123.pre_common_addon ; ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '123.pre_common_addon' at line 1 </pre> Need to schema / table Name do the escape character
64,canal client Example exception handling 1. canal All nodes in the server cluster are linked to the corresponding canal. client Example needs to be processed no alive canal Server exception 2. ClusterCanalConnector connect When retrying retryTimes times, if you can&#39;t create a link, add exception feedback instead of just logging.
63,NoSuchMethodError appears after zkClient uses version 0 2 ![zkclient](https://f.cloud.github.com/assets/834743/1148993/adb7ffce-1ed2-11e3-85d7-135e9c1d3490.jpg) NoSuchMethodError appears after zkClient uses version 0 2 Zkclient modified the method in version 0 2 https://github.com/sgroschupf/zkclient/commit/e72d8fd682907004d0b1488e47d8fe55d405b751 Then at zkclinet 0 3 version fixes incompatible methods https://github.com/sgroschupf/zkclient/commit/26f4d50fe8b3f5f6a4aa3747f254162c6a6c410b
62,Ddl statement parsing exception For the following ddl statement <pre> CREATE TABLE `cm_settle_incash` ( `batch_id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT batch `counter_acct_code` bigint(20) DEFAULT NULL COMMENT Counter account number `channel_sum_amount` decimal(18 2) NOT NULL COMMENT total amount `channel_sum_cost` decimal(18 2) NOT NULL DEFAULT '0.00' COMMENT total cost `bank_sum_amount` decimal(18 2) NOT NULL DEFAULT '0.00' COMMENT Total payment `begin_date` date DEFAULT NULL COMMENT start date `end_date` date DEFAULT NULL COMMENT End date `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT Creation time `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT Update time `cdk_amount` decimal(18 2) DEFAULT '0.00' COMMENT Length and length `cdk_type` tinyint(4) DEFAULT NULL COMMENT Length type 1 long section 2 short paragraph `deal_id` bigint(20) DEFAULT NULL COMMENT Transaction id `cdk_deal_id` bigint(20) DEFAULT NULL COMMENT Length and length id PRIMARY KEY (`batch_id`) ) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8; </pre> Unable to parse the correct schema table via SimpleDdlParser Name caused parsing error main reason 1. SimpleDdlParser is a case where the table name is extracted by regular rules, for example, if the schema table is processed, if there is a number in the entire ddl, the schema matching error will occur.
61,Canal support under otter Ddl synchronization requirements otter issue : https://github.com/alibaba/otter/issues/12 Ddl synchronization can not support idempotent processing needs to ensure that ddl dml is independent of each other as much as possible single ddl serial execution does not consider batch solve 1. canal Server add parameter canal instance get ddl isolation to represent whether to return a single ddl statement in isolation
60,SimpleDdlParser process query failed. Similar anomaly <pre> pid:1 nid:1 exception:canal:6.21->6.20:com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: SimpleDdlParser process query failed. pls submit issue with this queryString: CREATE table `bak591`.`j_order_log_back_201309` like j_order_log Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: SimpleDdlParser process query failed. pls submit issue with this queryString: CREATE table `bak591`.`j_order_log_back_201309` like j_order_log </pre> Logging in to the database to execute the ddl statement under the bak library to build a table across the library causes parsing exceptions
59,canal server High availability connection problem client The server is connected to two databases. The default is database A standby. Database B When the database A hangs, the number of connections between the can and the database B increases, causing the connection to refuse. Is there a heartbeat SQL? ？ What is the corresponding canal version? Temporarily turn off the problem suspect and early canal version heartbeat sql The bug caused the link to execute heartbeat sql not released has been fixed
58,Cannot specify DDL subscriptions by specific rules Like subscribing to DML pop.\* For DDL, the data of __ will be subscribed.
57,Special data type resolution Special data 1. The year type default value is 0000 2. The date type default value is 0000 00 00 00:00:00 canal <= 1 0 8 version of the current test Insert data +----+---------------------+---------------------+-------------+-------------+--------------+--------------+ | ID | DATETIME_VALUES | TIMESTAMP_VALUES | DATE_VALUES | TIME_VALUES | YEAR4_VALUES | YEAR2_VALUES | +----+---------------------+---------------------+-------------+-------------+--------------+--------------+ | 3 | 0000-00-00 00:00:00 | 0000-00-00 00:00:00 | 0000-00-00 | 00:00:00 | 0000 | 2000 | +----+---------------------+---------------------+-------------+-------------+--------------+--------------+ Actual output 3 0002-11-30 00:00:00.0 1970-01-01 08:00:00.0 0002-11-30 00:00:00 1900 2000 Example during the test +----+---------------------+---------------------+-------------+-------------+--------------+--------------+ | ID | DATETIME_VALUES | TIMESTAMP_VALUES | DATE_VALUES | TIME_VALUES | YEAR4_VALUES | YEAR2_VALUES | +----+---------------------+---------------------+-------------+-------------+--------------+--------------+ | 26 | 9999-12-31 23:59:59 | 2038-01-18 23:59:59 | 9999-12-31 | 838:59:59 | 2155 | 2155 | | 27 | 0000-00-00 00:00:00 | 0000-00-00 00:00:00 | 0000-00-00 | -838:59:59 | 0000 | 0000 | +----+---------------------+---------------------+-------------+-------------+--------------+--------------+ Actual test output --- 26 9999-12-31 23:59:59 2038-01-18 23:59:59 9999-12-31 838:59:59 2155 2155 --- 27 0000-00-00 00:00:00 0000-00-00 00:00:00 0000-00-00 -838:59:59 0000 0000 --- it's fixed. You can look at the mysql time type http dev mysql com doc refman 5 0 en date and time types html
56,canal Server will appear mysql link skyrocket after opening heartbeat encounter parsing exception canal Server will appear mysql link skyrocket after opening heartbeat encounter parsing exception User is using canal to parse mysql 5 6 13 version when parsing the following exception <pre> Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: java.lang.IllegalArgumentException: !! Unknown BLOB packlen = 0 at com.taobao.tddl.dbsync.binlog.event.RowsLogBuffer.fetchValue(RowsLogBuffer.java:752) at com.taobao.tddl.dbsync.binlog.event.RowsLogBuffer.nextValue(RowsLogBuffer.java:97) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseOneRow(LogEventConvert.java:294) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:249) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:102) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:60) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:297) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:161) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:120) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:189) </pre> After running for a while, the mysql link gradually grows and eventually exceeds the maximum number of connections. Cause analysis MysqlEventParser has a total of 3 socket links to mysql 1. binlog Dump link The start of the start parsing exception in start stop will trigger the preDump afterDump call. 2. table Meta query link Both preDump afterDump stop will be closed 3. heartbeat Execution link Closed in preDump stop So just happen to resolve the exception when repeatedly triggering the preDump afterDump call and heartbeat does not close the link in afterDump causing it to be created repeatedly The original heartbeat design idea only starts once and then re-links when the switch appears. Need to avoid repeated calls to prefDump to trigger the creation of multiple heartbeat links
55,Client appears cpu100 when the server is unavailable User feedback on Linux LinuxCanalConnector this way there is only one address If this is down Reconnecting is very fast cpu100%. Rough client code while(true) { client.connect() client.getWithoutAck()/client.ack(); } Cpu under windows will not be obvious under 100 linux environment
54,Missing thred Id message When the program transaction log is used, the lack of front-end context needs to be added to the binlog itself. The existing thredid hopes to be added in the next version.
53,Can you increase mysql And oracle Handbook Can you add a manual? Or detailed documentation thank you Look at the wiki and you should have what you want. wiki地址 https github com alibaba canal wiki
52,canal client 支持 getWithout Interface call for timeout timeout control RT. solve New method for CanalConnector 1. Message getWithoutAck(int batchSize Long timeout TimeUnit unit) 2. Message get(int batchSize Long timeout TimeUnit unit)
51,Mysql address configuration error increases exception description information Caused by: org.springframework.beans.factory.BeanCreationException: Error creatng bean with name 'com.alibaba.otter.canal.parse.support.AuthenticationInfo#1f5ea4a' defined in class path resource [spring/file-instance.xml]: Initialization of bean failed; nested exception is java.lang.ArrayIndexOutOfBoundsException: 1 at org.springframework.beans.factory.support.AbstractAutowireCapableBeaFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:480) ~[spring-2.5..jar:2.5.6] at org.springframework.beans.factory.support.AbstractAutowireCapableBeaFactory$1.run(AbstractAutowireCapableBeanFactory.java:409) ~[spring-2.5.6.jar:25.6] at java.security.AccessController.doPrivileged(Native Method) ~[na:1.6._17] solve Record the original configuration information when the exception configuration throws an exception throw new RuntimeException("address[" + text + "] is illegal eg.127.0.0.1:3306");
50,The output table name in canal is all lowercase canal Schema parsed before version 1 0 6 tablename Convert all to lowercase Solve the need to keep the contents of the record in the binlog without case conversion Need to set lower_case_table_names 0 in my cnf to ensure mysql capitalization Otherwise, the table name for mysql is all lowercase.
49,canal The server fails to start under Windows and fails to modify the configuration in the conf directory. canal The deployer will also insert the conf into the jar when it is packaged. In some specific environments, the classloader will load the conf configuration file in the jar package first, resulting in the configuration in the package directory has not been in effect. That is to say, there are canal properties and instance properties in conf and canal deployer xxxx jar. Solution 1. Remove the conf directory when the canal deployer xxx jar is packaged fixed issue 49 9515b83
48,Mysql master-slave switchover uses virtual ip switch canal&#39;s failover mechanism When using virtual ip for mysql active/standby management, only one ip is exposed. The current canal identification is based on the ip address and the ip address in the corresponding meta information is inconsistent. . Therefore, when the virtual ip backend occurs, the active/standby switch front end cannot perceive the error that the corresponding binlog does not exist, and the error is always retried, resulting in the canal being completely unavailable. Resolve attempt to relocate binlog by timestamp when an error that does not exist in binlog occurs . Under what circumstances binlog will not exist 1. Virtual ip Mysql active and standby switch 2. The canal parsing delay is too long and the corresponding binlog has been deleted. For situation 2, you need to avoid the solution. If you relocate the binlog according to the timestamp, if the current timestamp of all binlogs is later than the timestamp of the lookup, it should be suspended. You cannot use the first binlog for parsing. Binlog does not exist more types of errors Binlog that does not exist : mysql> show binlog events in 'mysql-bin.000273' from 75489806 limit 1; ERROR 1220 (HY000): Error when executing command SHOW BINLOG EVENTS: Could not find target log The existence of the binlog error is off mysql> show binlog events in 'mysql-bin.000374' from 75489806 limit 1; ERROR 1220 (HY000): Error when executing command SHOW BINLOG EVENTS: Wrong offset or I/O error --- binlog Dump instruction Binlog that does not exist : : Received error packet: errno = 1236 sqlstate = HY000 errmsg = Client requested master to start replication from position > file size; the first event 'mysql-bin.000011' at 12313 the last event read from './mysql-bin.000011' at 4 the last byte read from './mysql-bin.000011' at 4. The existence of the binlog error is off Received error packet: errno = 1236 sqlstate = HY000 errmsg = binlog truncated in the middle of event; consider out of disk space on master; the first event 'mysql-bin.000011' at 123 the last event read from './mysql-bin.000011' at 123 the last byte read from './mysql-bin.000011' at 142. The situation has not yet been fully enumerated temporarily hold the issue
47,canal When the server is started, the default listener address is AddressUtils getHostIp when the canal ip is configured to be empty, causing multiple IP addresses to be inaccessible. canal When the server is started, the default listener address is AddressUtils getHostIp when the canal ip is configured to be empty, causing multiple IP addresses to be inaccessible. solve 1. If the ip address is empty, the default socket listener is Canal port does not specify ip binding to receive any request from any ip of this machine 2. At the same time, an ip is selected by AddressUtils getHostIp to be exposed to the zk and accessed by the client.
46,lll
45,Merge original author update
44,The condition change of the attention table corresponds to the effective time problem The current rules of interest can be set in two ways. 1. In the instance properties of conf, you can set the table regex to load directly when you first start it. 2. A client can submit a filter by submitting a filter If the current strategy of submitting the filter in mode 2 is to directly override the condition in mode 1 if canal The server selects file zookeeper to persist the meta. Even if the next server restarts the corresponding attention table condition, it will be the data submitted by mode 2. Effective time issue Mode 1 Modify instance properties Need to restart instance If you turn on scan, it will restart instance. Restart will roll back some binlog. For example, the client does not ack data or consumes half of the transaction, and the memory location data is not flushed out to the persistent medium. Therefore, the corresponding effective time will affect the previous part of the data. Mode 2: Close the client After modifying the subscription condition, it will be synchronized immediately. After the subscribe call succeeds, the canal is directly replaced. The old version of the server filter
43,After the canal restarts, the filter condition submitted by the client will be lost and the filter configured by the instance properties will be overwritten. canal After the server restarts, it does not re-read the filter record submitted by the client subscriber, causing the instance properties client to be reloaded at startup. Filter condition is discarded Solve canal When the instance restarts, the client recorded in the MetaManager is read first. Filter information
42,Destination idbbond 1 will appear when canal is automatically reconnected should start First information <pre title="code"> 2013-06-05 18:08:40.960 [main] INFO c.alibaba.otter.canal.client.impl.ClusterCanalConnector - restart the connector for next round retry. 2013-06-05 18:08:40.975 [main] WARN c.alibaba.otter.canal.client.impl.ClusterCanalConnector - something goes wrong when getWithoutAck data from server:/192.168.1.134:11111 com.alibaba.otter.canal.protocol.exception.CanalClientException: something goes wrong with reason: something goes wrong with channel:[id: 0x3622e177 /192.168.1.134:50993 => /192.168.1.134:11111] exception=com.alibaba.otter.canal.server.exception.CanalServerException: destination:idbbond-1 should start first at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.getWithoutAck(SimpleCanalConnector.java:241) at com.alibaba.otter.canal.client.impl.ClusterCanalConnector.getWithoutAck(ClusterCanalConnector.java:135) at SimpleCanalClientExample.main(SimpleCanalClientExample.java:35) </pre> Problem description 1. canal When the server starts, it first exposes the port and then starts the instance that needs to be started. Because it takes a little time to start the instance, there is a certain probability that it will crash into the instance.
41,mysql 5 6 new two type events MYSQL_TYPE_TIMESTAMP2 MYSQL_TYPE_DATETIME2 Need to pay attention to the new time type based on digital compression storage and the use of big endian mode and other binlog object parsing is different
39,Mysql in HA mode Master standby cannot switch automatically canal the Lord canal config ## detecing config canal.instance.detecting.enable = true canal.instance.detecting.sql = insert into retl.xdual values(1 now()) on duplicate key update x=now() canal.instance.detecting.interval.time = 3 canal.instance.detecting.retry.threshold = 3 canal.instance.detecting.heartbeatHaEnable = true # example log 2013-05-14 10:23:29.509 [destination = example address = /192.168.196.82:3306 EventParser] WARN c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - connect failed!java.net.ConnectException: Connection refused at sun.nio.ch.Net.connect(Native Method) at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:525) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:65) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:51) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:141) at java.lang.Thread.run(Thread.java:679) canal From ## detecing config canal.instance.detecting.enable = true canal.instance.detecting.sql = insert into retl.xdual values(1 now()) on duplicate key update x=now() canal.instance.detecting.interval.time = 3 canal.instance.detecting.retry.threshold = 3 canal.instance.detecting.heartbeatHaEnable = true
38,Scan timing scan needs to ignore canal Meta paser dat file In the default configuration, the canal selects the file intance xml mechanism to periodically refresh the relevant data to the file. The file storage is by default with the instance configuration file. such as conf/example/meta.dat. Therefore, when scanning scans, you will find that the file under instance changes, causing a reload. solve Ignore dat file changes during scan scan
37,DDL statements cannot be output in time Ddl statement will be in Transaction Buffer is cached only in the next Transaction The message of Begin End will be output to the store and the client will be visible. Solve the ddl statement in the binlog protocol before and after the begin commit event requires special processing into the Transaction Buffer is also immediately flushed to the store Impact version <= 1.0.3
36,Launch canal directly in eclipse server Eclipse directly running CanalLanuncher will start failure corresponding to the dependent canal properties and spring xxx instance xml can not be found the reason 1. canal In order to simplify the path dependencies, the relative path or classpath is used to describe the resource dependencies when the server is started. Therefore, the search for resources fails after the eclipse is started. solve 1. Add the configuration in the conf directory to the classpath of eclipse Eclipse directly runs CanalLauncher to find the resource files in src main resources under the deployer project. such as 1. logback.xml 2. Canal properties relies on spring xxxx instance xml as the classpath path 3. Spring xxxx instance xml which depends on xxxx instance properties as the classpath path After the direct run, the corresponding log output is the logs directory of the canal logs project root directory.
35,Support binlog_format for statement Analytical output of mixed mode Binlog_format mode has statement row The main difference between the three modes is whether the insert update delete record takes sql mode or detailed record changes. Expectation 1. Canal can also support non-row mode data parsing and parsing corresponding sql Before after in non-row mode Columns information
34,Canal new file based based on file record information does not depend on zookeeper persistent canal Server status Plan to support non-zookeeper mode can also have persistent features to close canal After the server starts next time, it can continue to the last position to consume data without losing data. note A problem with file persistence-based mode is that the function of cluster failover cannot be implemented. Because file belongs to the local failover and cannot find the file after switching to another machine, the last resolution location cannot be found.
33,Create a new example module <name>canal driver module for otter ${project.version}</name> modulename Too long Current example Module is not included in the canal Trunk management can move it into easy learning
32,After the client is disconnected based on the zookeeper address, the NPE will be abnormal after restarting. Problem Description 1. Client obtains canal based on zookeeper address Server working node 2. The client is abnormally exited after killing the first time, such as kill Then immediately restart 3. Client will have a NullPointException Cause Analysis 1. After the client starts for the first time, it will record a client in zookeeper. After the running node exits abnormally, the running node will not stand for hours. 2. When the client starts up immediately, it finds that the running node exists and does not create a socket immediately. When the subsequent get data operation is executed, it is found that the socket is null, and the NPE problem occurs. Affected version <= 1.0.3. solution 1. It is found that the running node does not immediately create a socket but blocks the subsequent get data operation until the socket is created.
31,mysql YEAR type binlog output replaces Date with short problem mysql The YEAR type uses the java sql Date object in the canal1 0 3 version to indicate that the year information cannot be accurately represented. The original year 2013 output will be 2013 01 01 if (cal == null) cal = Calendar.getInstance(); cal.clear(); cal.set(Calendar.YEAR i32 + 1900); value = new java.sql.Date(cal.getTimeInMillis()); solve 1. Use digital storage directly to indicate protobuf output as 2013
30,Mac startup shsh startup script error problem bin_abs_path=$(readlink -f $(dirname $0)) Mac under readlink f does not work properly solve case "`uname`" in Linux) bin_abs_path=$(readlink -f $(dirname $0)) ;; *) bin_abs_path=`cd $(dirname $0); pwd` ;;
29,Canal parsing DDL operation exception causes the entire parsing to hang Problem Description 1. Start canal server/client 2. Perform ddl operations such as create alter Delete create table operation Specific case triggers DDL table name parsing error 3. Canal finds binlog after parsing binlog Column information and current tablemeta Inconsistent data in cache throws an exception and retry Problem Is that after the exception occurs in the third step, the corresponding cache data is not updated, causing the last error to use the tablemeta for the next parsing.
28,canal HA mode cluster cluster list data update problem Below each instance there will be a cluster directory representing the canal of this instance of service. Server list of machines <pre> [zk: localhost:2181(CONNECTED) 15] ls /otter/canal/destinations/example/cluster [10.20.144.22:11111] </pre> The 1 0 2 version of the cluster list will only generate a running node after the running node has been successfully preempted, and the positioning of the original cluseter is contradictory. solve canal When the server starts the corresponding instance, whether or not it is preempted to the running node, the cluster node should be created to indicate that it is workable. for this Instance If the running node deletes the client at this time, you can find a node from the cluster list to link lazy to make it start. (ps. Of course the running node deletes the canal The other machine of the server can sense and start the instance itself.
27,canal server Running node determines whether to optimize the operation of the machine canal Server in zookeeper&#39;s node <pre class="java" name="code"> [zk: localhost:2181(CONNECTED) 15] get /otter/canal/destinations/example/running {"active":true "address":"10.20.144.51:11111" "cid":1} </pre> At present, a machine judges whether the current working node is judged by whether the cid is the same as the id of the local machine, but the current cid information is started at the canal. The server does not strictly check whether there is a duplication, so both machines think that it is their own machine. There is a risk for subsequent command control operations. The solution will determine whether the operation of the local machine is modified to determine whether the ip port can uniquely define a jvm based on whether the address is the same or not.
26,In cluster mode, the client disconnects from the instance zookeeper. Running node Automatically deleted problem In cluster mode, the client disconnects from the instance zookeeper. Running node Automatically deleted and view canal log No abnormal information output Is this a current bug or a zookeeper configuration problem? ？ What I expected After the client disconnects the link, the running node still exists on the next client connection to continue to use the information before including the cursor information. Ps seems to see a similar problem is 1 0 3 solve now is the 啥 version Can look at this issue : https://github.com/alibaba/canal/issues/22 Mainly canal In the 1 0 2 version, when an instance has only one client link and the client link is disconnected, it will automatically stop. The next time the client starts, it will automatically select a re-established link from the list of clusters of all the instances. Instance will restart Is it currently closed at canal? Version 1 0 3 will be made configurable mode allowing user definition (PS. 1 0 3 is not officially released after the Qingming period will be released New parameters in canal properties #as far as possible to stop canal instance where client disconnect canal.stopInstanceAsPossible = true / false # False means that the client does not actively close the instance when it is disconnected, that is, you want to lock the reserved running node. Another point is that you have chosen the zookeeper&#39;s cluster mode configuration to start the canal. Server it will periodically record the cursor information to the default refresh rate visible in zookeeper Canal zookeeper flush period This parameter defaults to 1 second So restarting the instance at the next relink will continue to be a bit of duplicate data from the last commit to the cursor location in zookeeper. Currently canal 1 0 2 There is also a known bug : #23 There will be an NPE issue that causes a startup failure when relinking So currently canal 1 0 2 Use cluster deployment instance to switch or the client actively disconnects the link will cause the instance environment to be unavailable. If you want to use cluster deployment or wait for canal If there is a problem with the 1 0 3 version, you can go to the group to discuss the wiki home page with group number information or continue to leave a message on the issue. Thank you
25,windows Bat script failed to start Error prompt 'conf_dir' Not an internal or external command or a runnable program Or batch file 'canal_conf' Not an internal or external command or a runnable program Or batch file 'logback_configurationFile' Not an internal or external command or a runnable program Or batch file Listening for transport dt_socket at address: 9099 Exception in thread "main" java.lang.NoClassDefFoundError: com/alibaba/otter/can al/deployer/CanalLauncher Caused by: java.lang.ClassNotFoundException: com.alibaba.otter.canal.deployer.Ca nalLauncher at java.net.URLClassLoader$1.run(URLClassLoader.java:202) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:190) at java.lang.ClassLoader.loadClass(ClassLoader.java:307) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301) at java.lang.ClassLoader.loadClass(ClassLoader.java:248) Could not find the main class: com.alibaba.otter.canal.deployer.CanalLauncher. Program will exit. Windows environment test passed surroundings windows Xp system Java version java version "1.6.0_18" Java(TM) SE Runtime Environment (build 1.6.0_18-b07) Java HotSpot(TM) Client VM (build 16.0-b13 mixed mode sharing)
24,Windows appears unexpected when building a link with mysql blocking io Behavior exception Exception stack <code> 0:42:02.725 [main] WARN c.a.o.c.p.d.mysql.MysqlConnector - connect failed!java.io.IOException: unexpected blocking io behavior at com.alibaba.otter.canal.parse.driver.mysql.utils.PacketManager.write(PacketManager.java:54) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:159) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:66) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnectorTest.testQuery(MysqlConnectorTest.java:20) </code> Corresponding code <code> public static void write(SocketChannel ch ByteBuffer[] srcs) throws IOException { long total = 0; for (ByteBuffer buffer : srcs) { total += buffer.remaining(); System.out.println(total); } long size = ch.write(srcs); if (size != total) { throw new IOException("unexpected blocking io behavior"); } } </code> Looking for the windows environment test did not reproduce the problem Temporary close
23,canal Client if canal when establishing connection for the first time The server is configured with the lazy mode client failed to start. The client needs to do a check when it starts. : 1. If the current instance already has a running canal Server directly select this node to link 2. If there is no running node, then randomly select a node from the instance cluster node to link. I just met 26 This question I want to know who deleted the running node Delete the code of the running node 1. https://github.com/alibaba/canal/blob/master/server/src/main/java/com/alibaba/otter/canal/server/netty/handler/SessionHandler.java The stopCanalInstanceIfNecessary method finds that the client is trying to close the instance when it is disconnected. 2. https://github.com/alibaba/canal/blob/master/common/src/main/java/com/alibaba/otter/canal/common/zookeeper/running/ServerRunningMonitor.java The corresponding stop method will delete the running node.
22,canal Whether the server needs to close the instance when processing the client to actively close the connection. In canal The 1 0 2 version and the previous version were designed at the time of canal Server for instance management has always been lazy mode processing When canal Canal when the client disconnects the link The server will actively close the instance and so on the next canal Re-create after the client link Previous considerations 1. canal Server actively closes the instance mainly considering that it wants to be canal Server made stateless next canal Client can randomly select a machine to get data new canal The server will restart the instance. The entire HA model will be simpler. Improve 1. canal The server does not actively close the instance resource when the client exits. 2. Next canal After the client is re-linked, you need to obtain the canal running in HA mode through zookeeper through Cluster mode. The machine information of the server can then be re-established with the link. A premise client disconnect connect interval will not be long, as much as possible to reduce resource consumption
21,canal MemoryStore supports definition by memory size canal The memoryEventStoreWithBuffer supported by the v1 0 2 version controls the memory usage by defining the bufferSize. However, when the large text field is encountered, it is easy to explode the memory according to the number of records. Therefore, it is necessary to support EventStore based on memory size management and support data acquisition by memory size for convenient client memory control. MemoryEventStoreWithBuffer.setBufferMemSize()
20,Support mysql 5 6? Support mysql 5 6? Currently does not support mysql5 6 is mainly mysql 5 6 Incompatibility on the protocol has been modified. The LogEvent event type has to be upgraded. Plan to support mysql in v1 0 3 5 6 agreement current release version release tentatively scheduled for 2 weeks a cycle Thanks agapple for waiting for your new version. I haven&#39;t found it for a long time. 5 6 Binlog format document do you know where? http://dev.mysql.com/doc/internals/en/binary-log.html This link is the content you listed should be below 5 5 http://dev.mysql.com/doc/refman/5.6/en/binary-log.html Support for commit records https://github.com/alibaba/canal/commit/286a8f82916d99d3464ac544e2267397290e7880 Mysql5 6 protocol change document https github com alibaba canal wiki BinlogChange 28mysql5 6 29
19,mysql metaConnection link leak After running for a period of time, the database link between mysql and mysql has reached several thousand and all are in ESTABLISHED state. Jmap Dump memory object can not find the relevant instance of SocketChannelImpl Description is full Gc recycled
18,mysql Text Chinese characters appear garbled mysql When the field type in the table is text, the database encoding and table encoding are both utf 8 canal configuration parsing code is utf 8 parsed data records are garbled Cause Analysis 1. mysql The text blob type is recorded as a LogEvent MYSQL_TYPE_BLOB in binlog 2. Canal recognizes that BLOB information cannot distinguish whether text or blob is encoded according to iso 8859 1 and causes problems. solve 1. After getting the binlog, you need to check the table for the BLOB type. The meta information gets the real field type to distinguish the text and then parses it according to the encoding.
17,mysql Varchar type handles 000 character problem Online test encountered a problem a. The business execution sql inserts a record in which one field is 210012 000 000 000 b. Otter and China and the United States have updated this record to update the field to '210012' Removed 000 c. Once again, the canal finds that before and after values ​​are the same. No field changes cause the otter synchronization sql to fail. Cause Analysis 1. Dbsync parses 210012 000 000 000 equivalent to 210012 automatically ignores 000 requests Code for (; (found < end) && buf[found] != '\0'; found++) 说明  0 is the c style-style string terminator. As for the business execution, how to insert 000 is temporarily unknown.
16,Column field change information is lost Column information is incorrect when processing in LogEventConvert 1. isUpdate all are true. Correct should be based on before and after words. 2. sqlType all are 0. Correct should be int type corresponding to java sql Types 1. The isNull field is not added to the before after change field list
15,Modify the canal address in canal properties to canal ip Modify the canal address in canal properties to canal ip Corresponding to the com alibaba otter canal deployer CanalConstants CANAL_IP constant
14,Is the formulation option in cannal properties slightly wrong? In the canal properties of the deployer project canal.address= Should it be changed to canal.ip= Because CanalConstants This class is defined as follows public static final String CANAL_IP = ROOT + "." + "ip"; My eyes are sharp. It’s true that there is such logic in such code. ``` java ip = getProperty(properties CanalConstants.CANAL_IP); if (StringUtils.isEmpty(ip)) { ip = AddressUtils.getHostIp(); } ``` So I have never found out @wenzhihong2003 Would you like to change it and do it once? pull request？ #15 Thank you
13,mysql Instance support group mode Support multiple mysql Parse data merged into a store output for consumption A typical business data is split into 16 banks and then merged into a logical canal. Destination for the consumer client does not care about the link of the subsequent 16 libraries Need to consider 1. Strong consistency All parse in the group works normally to run the client consumption data. 2. Weakly consistent Allows the client to consume data as long as there is a parse working properly in the group The current solution only supports strong consistency, requiring all parse to work properly to allow client-side consumption data to be resolved for weak consistency recommendations using multi-channel deployment.
12,1 0 0 version code submission 1 0 0 version code submission
11,mysql Driver is getting the table Meta when there is no blocking for the table mysql Driver does not consider exceptions when processing returned packets fixed at v1.0.1
10,Mysql specific ddl operation delete field rename Table etc. causes parsing to hang Currently known to cause a suspended ddl operation 1. Field deletion 2. rename table Lead to desc Cannot find the corresponding meta information when table cannot complete the Entry information Ddl operation that can lead to information confusion 1. First delete a field and then add a new field, not added to the end the reason - mysql Tablemap in binlog LogEvent does not contain the corresponding column The name information simply lists the field types in the order of the columns in the current table. - Fall back to the binlog before the ddl operation and start parsing the desc in the database at this time. The information obtained by table will be with table Inconsistent in map cannot be matched, which will lead to information confusion. Avoided operation 1. Business operations avoid ddl with delete properties, such as field deletion rename table drop table 2. Business operations avoid adding fields when the adjustment order can only be added to the end If it is inevitable that the above operation will occur, as long as the canal resolution position is not rolled back, the data can be parsed normally without data confusion. However, it is necessary to consider that when the canal parsing delay is relatively large, the ddl operation with the delete property is still in the binlog to be parsed. The table data corresponding to the ddl still exists. The problem may be as long as possible to select the appropriate time for the ddl operation. Need to use to avoid is not a bug It is best to add the full steps to allow the canal to re-analyze again in this mode of operation. If you encounter the operation of ddl with delete property, then you only need to ensure that the client and server do not restart when it is not a problem. If it happens, it can only reset the site. Reset site method 1. Delete zookeeper&#39;s cursor node information 2. Modify the instance properties configuration to a new starting point 3. Restart the boot server and start the client Resetting the location may cause data loss during this period of time. This data can only be supplemented by human flesh. Parsing out the binlog and removing some ddl operation information and constructing sql Now I am also considering whether a problem can be considered to introduce a strict mode. If the non-strict mode directly ignores the logging of the data for these ddl problems, Manually switch to non-strict when there is a problem. Skip data as little as possible to avoid large data loss.
9,Canal added DevGuide Add DevGuide to guide users how to develop canal components mainly introduce canal class structure and model Where is devguide? Completed DevGuide https github com alibaba canal wiki DevGuide
8,Canal add AdminGuide Need to add AdminGuide to guide users how to correctly configure canal parameters and basic operations of operation and maintenance https://github.com/alibaba/canal/wiki/AdminGuide
7,table Meta is the primary key information error The data returned by isKey in the Entry data returns all true.
6,dbsync Binlog parsing increases testcase use case Increase testcase use cases to guide others to use
5,Extract mysql The packet protocol package is easy to reuse for independent projects. mysql Packet introduction simulation mysql login select update statement and mysql communication fix issue 5 1. Added driver project Provided MysqlConnector MysqlQueryExector MysqlUpdateExector 2. Modify the original parse package dependency to refactor MysqlConnection using the new MysqlConnector
3,mysql The encoding parameter removes charsetNumber according to the encoding set by connectionCharset. In the v1 0 0 version, the encoding needs to set the charsetNumber and connectionCharset charsetNumber at the same time. The main purpose is to specify the return code of the character_set_results connectioncharset when the handshak is used with mysql.
2,Instance adds automatic scanning mechanism Scan the subdirectory under the conf directory to exclude the directory name as an instance destination 自动加载该instance 默认读取global lazy
1,Fix zmserver without modifying the memory instance xml to start and shut down normally fixed at version 1.0.1
