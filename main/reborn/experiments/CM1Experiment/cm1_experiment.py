import os
import argparse

from VSM import VSM
from common import DATA_DIR
from reborn.DataReader import CM1Reader
from reborn.Datasets import Dataset, MAP_cal


class CM1_Experiment:
    def __init__(self, replace_word_interval, link_threshold_interval=5, model_type="vsm"):
        self.exp_name = self.__class__.__name__
        self.selected_replace_words = dict()
        reader = CM1Reader()
        self.dataSet = reader.readData()
        self.replace_word_population_percentage = 10
        self.replace_word_inverval = replace_word_interval
        self.link_threshold_interval = link_threshold_interval
        self.model_type = model_type

    def run(self):
        full_replace_list = self.read_replace_list()
        replace_percentage = self.replace_word_population_percentage

        while replace_percentage < 100:
            # Create a sub set of replace list
            replace_dict = dict()
            rep_word_size = int(len(full_replace_list) * (replace_percentage / 100))
            tmp = full_replace_list[:rep_word_size]
            for (en_word, fo_word) in tmp:
                replace_dict[en_word] = fo_word

            impacted_dataSet = self.dataSet.get_impacted_dataSet(replace_dict)
            replaced_dataSet = self.dataSet.get_replaced_dataSet(replace_dict)

            vsm_origin = VSM(fo_lang_code="en")
            vsm_origin.build_model(self.dataSet.get_docs())
            origin_results = self.run_model(vsm_origin, self.dataSet)

            vsm_impacted = VSM(
                fo_lang_code="en")  # vsm_impacted is trained with a dataset that contains foreign language
            vsm_impacted.build_model(replaced_dataSet.get_docs())

            # replaced_result contains scores for all links, impacted_results contains scores for impacted links only
            impacted_results = self.run_model(vsm_impacted, impacted_dataSet)
            replaced_results = self.run_model(vsm_impacted, replaced_dataSet)

            for linkset_id in impacted_results:
                origin_result = sorted(origin_results[linkset_id], key=lambda k: k[2], reverse=True)
                replaced_result = sorted(replaced_results[linkset_id], key=lambda k: k[2], reverse=True)
                impacted_result = sorted(impacted_results[linkset_id], key=lambda k: k[2], reverse=True)

                # calculate the MAP score for the impacted gold links on origin rank order
                origin_map = MAP_cal(origin_result, impacted_dataSet.gold_link_sets[
                    linkset_id].links, do_sort=False).run()

                # calculate the MAP score for the impacted gold links on the new ranking generated by the vsm_impacted
                impacted_map = MAP_cal(replaced_result, impacted_dataSet.gold_link_sets[
                    linkset_id].links, do_sort=False).run()

                # filter the origin result to keep only the impacted links. This set is the origin model score on impacted links
                impacted_links = set([(x[0], x[1]) for x in impacted_result])
                filter_origin_result = [x for x in origin_result if (x[0], x[1]) in impacted_links]

                threshold = 0
                origin_scores = []
                impacted_scores = []
                while threshold <= 100:
                    filter_origin_above_threshold = [x for x in filter_origin_result if x[2] >= threshold]
                    impacted_result_above_threshold = [x for x in impacted_result if x[2] >= threshold]
                    origin_eval_score = impacted_dataSet.evaluate_link_set(linkset_id, filter_origin_above_threshold)
                    impacted_eval_score = impacted_dataSet.evaluate_link_set(linkset_id,
                                                                             impacted_result_above_threshold)
                    origin_scores.append(origin_eval_score)
                    impacted_scores.append(impacted_eval_score)
                    threshold += self.link_threshold_interval / 100
                file_name = "cm1_{}_{}_{}.txt".format(self.model_type, linkset_id,
                                                      replace_percentage)
                write_dir = os.path.join("results", self.model_type)
                if not os.path.isdir(write_dir):
                    os.mkdir(write_dir)
                output_file_path = os.path.join(write_dir, file_name)
                with open(output_file_path, 'w') as fout:
                    self.write_result(fout, origin_scores, origin_map, impacted_scores, impacted_map)
                print("origin MAP=", origin_map)
                print("Origin P,C,F")
                print(origin_scores)

                print("impacted_MAP=", impacted_map)
                print("impacted P,C,F")
                print(impacted_scores)
            replace_percentage += self.replace_word_inverval

    def create_replace_list(self):
        pass

    def read_replace_list(self):
        file_path = os.path.join(DATA_DIR, "cm1", 'word_replace_list', "high_frequency.txt")
        with open(file_path, encoding='utf8') as fin:
            tmp = []
            for line in fin:
                parts = line.split(",")
                if len(parts) == 3:
                    en_word = parts[0].strip()
                    cnt = parts[1]
                    fo_word = parts[2].strip()
                    tmp.append((en_word, fo_word))
            return tmp

    def get_links(self, trace_model, source_artifact, target_artifact):
        links = []
        total = len(source_artifact) * len(target_artifact)
        cnt = 0
        for s_id in source_artifact:
            for t_id in target_artifact:
                # if cnt % 1000 == 0:
                #     print(str(cnt) + "/" + str(total))
                cnt += 1
                s_content = source_artifact[s_id]
                t_content = target_artifact[t_id]
                score = trace_model.get_doc_similarity(s_content, t_content)
                links.append((s_id, t_id, score))
        return links

    def run_model(self, model, dataset: Dataset):
        results = dict()
        for link_set_id in dataset.gold_link_sets:
            link_set = dataset.gold_link_sets[link_set_id]
            source_aritf = link_set.artiPair.source_artif
            target_artif = link_set.artiPair.target_artif
            gen_links = self.get_links(model, source_aritf, target_artif)
            results[link_set_id] = gen_links
        return results

    def write_result(self, writer, origin_score, origin_map, impacted_score, impacted_map):
        writer.write("origin MAP=", origin_map)
        writer.write("Origin P,C,F")
        writer.write(origin_score)

        writer.write("impacted_MAP=", impacted_score)
        writer.write("impacted P,C,F")
        writer.write(impacted_map)


if __name__ == "__main__":
    parser = argparse.ArgumentParser("CM1 Experiment")
    parser.add_argument("-c", "--create_replace_list",
                        help="Create a list of word, this list will be used to replace words in target artifacts after manually adding Chinese translation")
    parser.add_argument("--replace_interval", default=10, type=int,
                        help="Increase the usage percentage of replace list that will be used for replacement by this interval ")
    args = parser.parse_args()

    exp = CM1_Experiment(args.replace_interval)
    if args.create_replace_list:
        exp.create_list(args.create_replace_list)
    else:
        exp.run()
